<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.2.0">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="https://fonts.loli.net/css?family=Monda:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">

<link rel="stylesheet" href="//cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.1/css/all.min.css">
  <link rel="stylesheet" href="//cdn.jsdelivr.net/npm/animate.css@3.1.1/animate.min.css">
  <link rel="stylesheet" href="//cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.css">

<script class="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"soundmemories.github.io","root":"/","images":"/images","scheme":"Muse","version":"8.0.2","exturl":true,"sidebar":{"position":"left","display":"always","padding":18,"offset":12},"copycode":true,"bookmark":{"enable":true,"color":"#222","save":"auto"},"fancybox":true,"mediumzoom":true,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"},"path":"/search.xml","localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false}};
  </script>

  <meta name="description" content="机器学习过拟合和欠拟合？过拟合：模型在训练集上表现好，在测试集和新数据上表现差。欠拟合：模型在训练集和测试集上表现都差。 降低过拟合风险：增加数据。降低模型复杂度，如减少神经元数量、网络层数，降低树深度，剪枝。增加正则化。使用集成学习方法，降低单一模型过拟合风险。 降低欠拟合风险：增加新特征。增加模型复杂度。减小正则化系数。 多分类用什么评估指标，AUC是什么？多个二分类混淆矩阵，两两类别的组合都">
<meta property="og:type" content="article">
<meta property="og:title" content="面试题集">
<meta property="og:url" content="https://soundmemories.github.io/2019/01/21/Other/%E9%9D%A2%E8%AF%95%E9%A2%98%E9%9B%86/index.html">
<meta property="og:site_name" content="SoundMemories">
<meta property="og:description" content="机器学习过拟合和欠拟合？过拟合：模型在训练集上表现好，在测试集和新数据上表现差。欠拟合：模型在训练集和测试集上表现都差。 降低过拟合风险：增加数据。降低模型复杂度，如减少神经元数量、网络层数，降低树深度，剪枝。增加正则化。使用集成学习方法，降低单一模型过拟合风险。 降低欠拟合风险：增加新特征。增加模型复杂度。减小正则化系数。 多分类用什么评估指标，AUC是什么？多个二分类混淆矩阵，两两类别的组合都">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://soundmemories.github.io/images/面试/L1图.jpg">
<meta property="og:image" content="https://soundmemories.github.io/images/面试/梯度下降.png">
<meta property="og:image" content="https://soundmemories.github.io/images/面试/Bagging和Boosting.png">
<meta property="og:image" content="https://soundmemories.github.io/images/面试/GBDT和Xgboost.jpg">
<meta property="og:image" content="https://soundmemories.github.io/images/面试/GBDT和Xgboost2.jpg">
<meta property="og:image" content="https://soundmemories.github.io/images/面试/超参数调优.jpg">
<meta property="article:published_time" content="2019-01-20T16:00:00.000Z">
<meta property="article:modified_time" content="2022-06-05T09:10:31.350Z">
<meta property="article:author" content="SoundMemories">
<meta property="article:tag" content="Other">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://soundmemories.github.io/images/面试/L1图.jpg">


<link rel="canonical" href="https://soundmemories.github.io/2019/01/21/Other/%E9%9D%A2%E8%AF%95%E9%A2%98%E9%9B%86/">


<script data-pjax class="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>面试题集 | SoundMemories</title>
  






  <noscript>
  <style>
  body { margin-top: 2rem; }

  .use-motion .menu-item,
  .use-motion .sidebar,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header {
    visibility: visible;
  }

  .use-motion .header,
  .use-motion .site-brand-container .toggle,
  .use-motion .footer { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle,
  .use-motion .custom-logo-image {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line {
    transform: scaleX(1);
  }

  .search-pop-overlay, .sidebar-nav { display: none; }
  .sidebar-panel { display: block; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <h1 class="site-title">SoundMemories</h1>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu">
        <li class="menu-item menu-item-主页">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>主页</a>

  </li>
        <li class="menu-item menu-item-分类">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a>

  </li>
        <li class="menu-item menu-item-标签">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a>

  </li>
        <li class="menu-item menu-item-归档">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" maxlength="80"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="search-result-container no-result">
  <div class="search-result-icon">
    <i class="fa fa-spinner fa-pulse fa-5x"></i>
  </div>
</div>

    </div>
  </div>

</div>
        
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>

  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>
	   
		  
      <div class="sidebar-panel-container">
        <!--noindex-->
        <section class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0"><span class="nav-number">1.</span> <span class="nav-text">机器学习</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%BF%87%E6%8B%9F%E5%90%88%E5%92%8C%E6%AC%A0%E6%8B%9F%E5%90%88%EF%BC%9F"><span class="nav-number">1.1.</span> <span class="nav-text">过拟合和欠拟合？</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%A4%9A%E5%88%86%E7%B1%BB%E7%94%A8%E4%BB%80%E4%B9%88%E8%AF%84%E4%BC%B0%E6%8C%87%E6%A0%87%EF%BC%8CAUC%E6%98%AF%E4%BB%80%E4%B9%88%EF%BC%9F"><span class="nav-number">1.2.</span> <span class="nav-text">多分类用什么评估指标，AUC是什么？</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%A0%B7%E6%9C%AC%E4%B8%8D%E5%B9%B3%E8%A1%A1%E5%A4%84%E7%90%86%EF%BC%9F"><span class="nav-number">1.3.</span> <span class="nav-text">样本不平衡处理？</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%BC%BA%E5%A4%B1%E5%80%BC%E5%A4%84%E7%90%86%EF%BC%9F"><span class="nav-number">1.4.</span> <span class="nav-text">缺失值处理？</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#L1%E6%AD%A3%E5%88%99%E5%92%8CL2%E6%AD%A3%E5%88%99%E7%9A%84%E5%8C%BA%E5%88%AB%E6%98%AF%E4%BB%80%E4%B9%88%EF%BC%9F"><span class="nav-number">1.5.</span> <span class="nav-text">L1正则和L2正则的区别是什么？</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#L1%E6%AD%A3%E5%88%99%E4%B8%BA%E4%BB%80%E4%B9%88%E5%8F%AF%E4%BB%A5%E5%BE%97%E5%88%B0%E7%A8%80%E7%96%8F%E8%A7%A3%EF%BC%9F"><span class="nav-number">1.6.</span> <span class="nav-text">L1正则为什么可以得到稀疏解？</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#SGD%E5%92%8CGD%E7%9A%84%E5%8C%BA%E5%88%AB%EF%BC%8C%E4%BB%80%E4%B9%88%E5%9C%BA%E6%99%AF%E4%B8%8B%E7%94%A8%EF%BC%9F"><span class="nav-number">1.7.</span> <span class="nav-text">SGD和GD的区别，什么场景下用？</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%86%B3%E7%AD%96%E6%A0%91%E5%90%AF%E5%8F%91%E5%87%BD%E6%95%B0%EF%BC%9F"><span class="nav-number">1.8.</span> <span class="nav-text">决策树启发函数？</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#bagging%E5%92%8Cboosting%EF%BC%9F"><span class="nav-number">1.9.</span> <span class="nav-text">bagging和boosting？</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%AF%94%E8%BE%83LR%E5%92%8CGBDT-%E8%AF%B4%E8%AF%B4%E4%BB%80%E4%B9%88%E6%83%85%E5%A2%83%E4%B8%8BGBDT%E4%B8%8D%E5%A6%82LR%EF%BC%9F"><span class="nav-number">1.10.</span> <span class="nav-text">比较LR和GBDT, 说说什么情境下GBDT不如LR？</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#RF%E5%92%8CGBDT%E7%9A%84%E5%8C%BA%E5%88%AB%EF%BC%9F"><span class="nav-number">1.11.</span> <span class="nav-text">RF和GBDT的区别？</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#GBDT%E5%92%8Cxgboost%E5%8C%BA%E5%88%AB%EF%BC%9F"><span class="nav-number">1.12.</span> <span class="nav-text">GBDT和xgboost区别？</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%AE%80%E5%8D%95%E4%BB%8B%E7%BB%8D%E4%B8%8Bxgboost%EF%BC%9F"><span class="nav-number">1.13.</span> <span class="nav-text">简单介绍下xgboost？</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#xgboost%E4%B8%BA%E4%BB%80%E4%B9%88%E4%BD%BF%E7%94%A8%E6%B3%B0%E5%8B%92%E4%BA%8C%E9%98%B6%E5%B1%95%E5%BC%80%EF%BC%9F"><span class="nav-number">1.14.</span> <span class="nav-text">xgboost为什么使用泰勒二阶展开？</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#XGBoost%E4%B8%BA%E4%BB%80%E4%B9%88%E5%8F%AF%E4%BB%A5%E5%B9%B6%E8%A1%8C%E8%AE%AD%E7%BB%83%EF%BC%9F-%E5%A6%82%E4%BD%95%E5%AF%BB%E6%89%BE%E6%9C%80%E4%BD%B3%E5%88%86%E8%A3%82%E7%82%B9%EF%BC%9F"><span class="nav-number">1.15.</span> <span class="nav-text">XGBoost为什么可以并行训练？&#x2F;如何寻找最佳分裂点？</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#XGBOOST%E4%B8%BA%E4%BB%80%E4%B9%88%E5%BF%AB%EF%BC%9F"><span class="nav-number">1.16.</span> <span class="nav-text">XGBOOST为什么快？</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#XGBOOST%E9%98%B2%E6%AD%A2%E8%BF%87%E6%8B%9F%E5%90%88%E7%9A%84%E6%96%B9%E6%B3%95%EF%BC%9F"><span class="nav-number">1.17.</span> <span class="nav-text">XGBOOST防止过拟合的方法？</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#XGBOOST%E5%A6%82%E4%BD%95%E5%A4%84%E7%90%86%E7%BC%BA%E5%A4%B1%E5%80%BC%EF%BC%9F"><span class="nav-number">1.18.</span> <span class="nav-text">XGBOOST如何处理缺失值？</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#XBGOOST%E4%B8%AD%E5%8F%B6%E5%AD%90%E8%8A%82%E7%82%B9%E7%9A%84%E6%9D%83%E9%87%8D%E5%A6%82%E4%BD%95%E8%AE%A1%E7%AE%97%E5%87%BA%E6%9D%A5%E7%9A%84%EF%BC%9F"><span class="nav-number">1.19.</span> <span class="nav-text">XBGOOST中叶子节点的权重如何计算出来的？</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#XGBOOST%E4%B8%AD%E7%9A%84%E4%B8%80%E6%A3%B5%E6%A0%91%E7%9A%84%E5%81%9C%E6%AD%A2%E7%94%9F%E9%95%BF%E6%9D%A1%E4%BB%B6%EF%BC%9F"><span class="nav-number">1.20.</span> <span class="nav-text">XGBOOST中的一棵树的停止生长条件？</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#xgboost%E5%A6%82%E4%BD%95%E5%A4%84%E7%90%86%E4%B8%8D%E5%B9%B3%E8%A1%A1%E6%95%B0%E6%8D%AE%EF%BC%9F"><span class="nav-number">1.21.</span> <span class="nav-text">xgboost如何处理不平衡数据？</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#XGBOOS%E4%B8%AD%E5%A6%82%E4%BD%95%E5%AF%B9%E6%A0%91%E8%BF%9B%E8%A1%8C%E5%89%AA%E6%9E%9D%EF%BC%9F"><span class="nav-number">1.22.</span> <span class="nav-text">XGBOOS中如何对树进行剪枝？</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#XGBoost%E7%9A%84Scalable-%E5%8F%AF%E6%89%A9%E5%B1%95%E6%80%A7-%E6%80%A7%E5%A6%82%E4%BD%95%E4%BD%93%E7%8E%B0%EF%BC%9F"><span class="nav-number">1.23.</span> <span class="nav-text">XGBoost的Scalable(可扩展性)性如何体现？</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#XGBOOST%E5%A6%82%E4%BD%95%E8%AF%84%E4%BB%B7%E7%89%B9%E5%BE%81%E7%9A%84%E9%87%8D%E8%A6%81%E6%80%A7%EF%BC%9F"><span class="nav-number">1.24.</span> <span class="nav-text">XGBOOST如何评价特征的重要性？</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#XGBOOST%E5%8F%82%E6%95%B0%E8%B0%83%E4%BC%98%E7%9A%84%E4%B8%80%E8%88%AC%E6%AD%A5%E9%AA%A4%EF%BC%9F"><span class="nav-number">1.25.</span> <span class="nav-text">XGBOOST参数调优的一般步骤？</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#XGBOOST%E6%A8%A1%E5%9E%8B%E5%A6%82%E6%9E%9C%E8%BF%87%E6%8B%9F%E5%90%88%E4%BA%86%E6%80%8E%E4%B9%88%E8%A7%A3%E5%86%B3%EF%BC%9F"><span class="nav-number">1.26.</span> <span class="nav-text">XGBOOST模型如果过拟合了怎么解决？</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#XGBOOST%E5%92%8ClightGBM%E7%9A%84%E5%8C%BA%E5%88%AB%EF%BC%9F"><span class="nav-number">1.27.</span> <span class="nav-text">XGBOOST和lightGBM的区别？</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%B6%85%E5%8F%82%E6%95%B0%E8%B0%83%E4%BC%98%EF%BC%9F"><span class="nav-number">1.28.</span> <span class="nav-text">超参数调优？</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%A8%A1%E5%9E%8B%E5%B8%B8%E7%94%A8%E8%B6%85%E5%8F%82%E6%95%B0%EF%BC%9F"><span class="nav-number">1.29.</span> <span class="nav-text">模型常用超参数？</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%86%B5%E7%9A%84%E6%A6%82%E5%BF%B5%EF%BC%8C%E4%BA%A4%E5%8F%89%E7%86%B5%EF%BC%9F"><span class="nav-number">1.30.</span> <span class="nav-text">熵的概念，交叉熵？</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BA%A4%E5%8F%89%E7%86%B5%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%EF%BC%9F%E5%9C%A8%E4%BB%80%E4%B9%88%E5%9C%B0%E6%96%B9%E4%BD%BF%E7%94%A8%E8%BF%87%EF%BC%9F"><span class="nav-number">1.31.</span> <span class="nav-text">交叉熵损失函数？在什么地方使用过？</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BA%A4%E5%8F%89%E7%86%B5%E5%87%BD%E6%95%B0%E4%B8%8E%E6%9C%80%E5%A4%A7-%E5%AF%B9%E6%95%B0-%E4%BC%BC%E7%84%B6%E5%87%BD%E6%95%B0%E7%9A%84%E5%85%B3%E7%B3%BB%E5%92%8C%E5%8C%BA%E5%88%AB%EF%BC%9F"><span class="nav-number">1.32.</span> <span class="nav-text">交叉熵函数与最大(对数)似然函数的关系和区别？</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0"><span class="nav-number">2.</span> <span class="nav-text">深度学习</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%AD%EF%BC%8C%E6%9C%89%E5%93%AA%E4%BA%9B%E5%8A%9E%E6%B3%95%E9%98%B2%E6%AD%A2%E8%BF%87%E6%8B%9F%E5%90%88%EF%BC%9F"><span class="nav-number">2.1.</span> <span class="nav-text">神经网络中，有哪些办法防止过拟合？</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Dropout%E5%8E%9F%E7%90%86%EF%BC%9F%E4%B8%BA%E4%BB%80%E4%B9%88%E8%83%BD%E5%87%8F%E5%B0%91%E8%BF%87%E6%8B%9F%E5%90%88%EF%BC%9F"><span class="nav-number">2.2.</span> <span class="nav-text">Dropout原理？为什么能减少过拟合？</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#BN%E5%92%8CLN%E7%9A%84%E5%8C%BA%E5%88%AB%EF%BC%9F"><span class="nav-number">2.3.</span> <span class="nav-text">BN和LN的区别？</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1%E3%80%81%E7%88%86%E7%82%B8%EF%BC%9F%E5%A6%82%E4%BD%95%E8%A7%A3%E5%86%B3%EF%BC%9F"><span class="nav-number">2.4.</span> <span class="nav-text">梯度消失、爆炸？如何解决？</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#batchsize%E5%A4%A7%E6%88%96%E5%B0%8F%E6%9C%89%E4%BB%80%E4%B9%88%E9%97%AE%E9%A2%98%EF%BC%9F"><span class="nav-number">2.5.</span> <span class="nav-text">batchsize大或小有什么问题？</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%BA%E4%BB%80%E4%B9%88%E7%94%A8%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%EF%BC%9F%E5%B8%B8%E7%94%A8%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%EF%BC%9F%E4%BC%98%E7%BC%BA%E7%82%B9%EF%BC%9F"><span class="nav-number">2.6.</span> <span class="nav-text">为什么用激活函数？常用激活函数？优缺点？</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%B8%B8%E7%94%A8%E7%9A%84%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95%E6%9C%89%E5%93%AA%E4%BA%9B%EF%BC%9F%E5%90%84%E8%87%AA%E7%9A%84%E4%BC%98%E7%BC%BA%E7%82%B9%EF%BC%9F"><span class="nav-number">2.7.</span> <span class="nav-text">常用的优化算法有哪些？各自的优缺点？</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#CNN%E5%92%8CRNN%E5%8C%BA%E5%88%AB%EF%BC%9F"><span class="nav-number">2.8.</span> <span class="nav-text">CNN和RNN区别？</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#LSTM%E5%92%8CRNN%E5%8C%BA%E5%88%AB%EF%BC%8C%E6%9C%89%E4%BB%80%E4%B9%88%E4%BC%98%E5%8C%96%EF%BC%9FRNN%E4%B8%BA%E4%BB%80%E4%B9%88%E4%BC%9A%E6%9C%89%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1%EF%BC%8CLSTM%E5%A6%82%E4%BD%95%E8%A7%A3%E5%86%B3%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1%EF%BC%9F"><span class="nav-number">2.9.</span> <span class="nav-text">LSTM和RNN区别，有什么优化？RNN为什么会有梯度消失，LSTM如何解决梯度消失？</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#LSTM%E4%B8%8EGRU%E5%8C%BA%E5%88%AB%EF%BC%9F"><span class="nav-number">2.10.</span> <span class="nav-text">LSTM与GRU区别？</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#LSTM%E4%B8%8ETransformer%E7%9A%84%E5%8C%BA%E5%88%AB%EF%BC%9F"><span class="nav-number">2.11.</span> <span class="nav-text">LSTM与Transformer的区别？</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#seq2seq%E6%98%AF%E4%BB%80%E4%B9%88%E7%BB%93%E6%9E%84%EF%BC%8Ctransformer%E5%91%A2%EF%BC%9F"><span class="nav-number">2.12.</span> <span class="nav-text">seq2seq是什么结构，transformer呢？</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Attention%E4%BB%8B%E7%BB%8D%EF%BC%8C%E6%9C%89%E5%93%AA%E4%BA%9BAttention%E6%96%B9%E6%B3%95%EF%BC%9F"><span class="nav-number">2.13.</span> <span class="nav-text">Attention介绍，有哪些Attention方法？</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#self-attention%E6%AF%94seq2seq%E7%9A%84attention%E4%BC%98%E8%B6%8A%E5%9C%A8%E5%93%AA%E9%87%8C%EF%BC%88%E6%9C%80%E5%A5%BD%E7%BB%93%E5%90%88%E4%B8%9A%E5%8A%A1%E5%9C%BA%E6%99%AF%E8%AF%B4%E4%B8%80%E8%AF%B4%EF%BC%89%EF%BC%9F"><span class="nav-number">2.14.</span> <span class="nav-text">self-attention比seq2seq的attention优越在哪里（最好结合业务场景说一说）？</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Transformer%E6%98%AF%E8%87%AA%E5%9B%9E%E5%BD%92%E6%A8%A1%E5%9E%8B%E8%BF%98%E6%98%AF%E8%87%AA%E7%BC%96%E7%A0%81%E6%A8%A1%E5%9E%8B%EF%BC%9F"><span class="nav-number">2.15.</span> <span class="nav-text">Transformer是自回归模型还是自编码模型？</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#transformer%E7%9A%84position-embedding%E4%BD%9C%E7%94%A8%EF%BC%9F%E6%9C%89%E4%BB%80%E4%B9%88%E6%84%8F%E4%B9%89%E5%92%8C%E4%BC%98%E7%BC%BA%E7%82%B9%EF%BC%9F"><span class="nav-number">2.16.</span> <span class="nav-text">transformer的position embedding作用？有什么意义和优缺点？</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BD%A0%E8%BF%98%E4%BA%86%E8%A7%A3%E5%93%AA%E4%BA%9B%E5%85%B3%E4%BA%8E%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81%E7%9A%84%E6%8A%80%E6%9C%AF%EF%BC%8C%E5%90%84%E8%87%AA%E7%9A%84%E4%BC%98%E7%BC%BA%E7%82%B9%E6%98%AF%E4%BB%80%E4%B9%88%EF%BC%9F"><span class="nav-number">2.17.</span> <span class="nav-text">你还了解哪些关于位置编码的技术，各自的优缺点是什么？</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Transformer%E4%B8%BA%E4%BD%95%E4%BD%BF%E7%94%A8%E5%A4%9A%E5%A4%B4%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%EF%BC%9F%EF%BC%88%E4%B8%BA%E4%BB%80%E4%B9%88%E4%B8%8D%E4%BD%BF%E7%94%A8%E4%B8%80%E4%B8%AA%E5%A4%B4%EF%BC%89"><span class="nav-number">2.18.</span> <span class="nav-text">Transformer为何使用多头注意力机制？（为什么不使用一个头）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Transformer%E4%B8%BA%E4%BB%80%E4%B9%88Q%E5%92%8CK%E4%BD%BF%E7%94%A8%E4%B8%8D%E5%90%8C%E7%9A%84%E6%9D%83%E9%87%8D%E7%9F%A9%E9%98%B5%E7%94%9F%E6%88%90%EF%BC%8C%E4%B8%BA%E4%BD%95%E4%B8%8D%E8%83%BD%E4%BD%BF%E7%94%A8%E5%90%8C%E4%B8%80%E4%B8%AA%E5%80%BC%E8%BF%9B%E8%A1%8C%E8%87%AA%E8%BA%AB%E7%9A%84%E7%82%B9%E4%B9%98%EF%BC%9F"><span class="nav-number">2.19.</span> <span class="nav-text">Transformer为什么Q和K使用不同的权重矩阵生成，为何不能使用同一个值进行自身的点乘？</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Transformer%E4%B8%AD%EF%BC%8C%E4%B8%BA%E4%BB%80%E4%B9%88%E5%9C%A8%E8%BF%9B%E8%A1%8Csoftmax%E4%B9%8B%E5%89%8D%E9%9C%80%E8%A6%81%E5%AF%B9attention%E8%BF%9B%E8%A1%8Cscaled%EF%BC%88%E4%B8%BA%E4%BB%80%E4%B9%88%E9%99%A4%E4%BB%A5dk%E7%9A%84%E5%B9%B3%E6%96%B9%E6%A0%B9%EF%BC%89%EF%BC%9F%E5%B9%B6%E4%BD%BF%E7%94%A8%E5%85%AC%E5%BC%8F%E6%8E%A8%E5%AF%BC%E8%BF%9B%E8%A1%8C%E8%AE%B2%E8%A7%A3"><span class="nav-number">2.20.</span> <span class="nav-text">Transformer中，为什么在进行softmax之前需要对attention进行scaled（为什么除以dk的平方根）？并使用公式推导进行讲解</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Transformer%E4%B8%AD%E7%9A%84mask%E6%9C%BA%E5%88%B6%E6%9C%89%E4%BB%80%E4%B9%88%E4%BD%9C%E7%94%A8-%E8%AE%A1%E7%AE%97attention-score%E7%9A%84%E6%97%B6%E5%80%99%E5%A6%82%E4%BD%95%E5%AF%B9padding%E5%81%9Amask%E6%93%8D%E4%BD%9C%EF%BC%9F"><span class="nav-number">2.21.</span> <span class="nav-text">Transformer中的mask机制有什么作用?计算attention score的时候如何对padding做mask操作？</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%BA%E4%BB%80%E4%B9%88%E5%9C%A8%E8%BF%9B%E8%A1%8C%E5%A4%9A%E5%A4%B4%E6%B3%A8%E6%84%8F%E5%8A%9B%E7%9A%84%E6%97%B6%E5%80%99%E9%9C%80%E8%A6%81%E5%AF%B9%E6%AF%8F%E4%B8%AAhead%E8%BF%9B%E8%A1%8C%E9%99%8D%E7%BB%B4%EF%BC%9F"><span class="nav-number">2.22.</span> <span class="nav-text">为什么在进行多头注意力的时候需要对每个head进行降维？</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%A4%A7%E6%A6%82%E8%AE%B2%E4%B8%80%E4%B8%8BTransformer%E7%9A%84Encoder%E6%A8%A1%E5%9D%97%EF%BC%9F"><span class="nav-number">2.23.</span> <span class="nav-text">大概讲一下Transformer的Encoder模块？</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%BA%E4%BD%95%E5%9C%A8%E8%8E%B7%E5%8F%96%E8%BE%93%E5%85%A5%E8%AF%8D%E5%90%91%E9%87%8F%E4%B9%8B%E5%90%8E%E9%9C%80%E8%A6%81%E5%AF%B9%E7%9F%A9%E9%98%B5%E4%B9%98%E4%BB%A5embedding-size%E7%9A%84%E5%BC%80%E6%96%B9%EF%BC%9F"><span class="nav-number">2.24.</span> <span class="nav-text">为何在获取输入词向量之后需要对矩阵乘以embedding size的开方？</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%9D%83%E9%87%8D%E5%88%9D%E5%A7%8B%E5%8C%96%EF%BC%9F"><span class="nav-number">2.25.</span> <span class="nav-text">权重初始化？</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%AE%80%E5%8D%95%E8%AE%B2%E4%B8%80%E4%B8%8BTransformer%E4%B8%AD%E7%9A%84%E6%AE%8B%E5%B7%AE%E7%BB%93%E6%9E%84%E4%BB%A5%E5%8F%8A%E6%84%8F%E4%B9%89%EF%BC%9F"><span class="nav-number">2.26.</span> <span class="nav-text">简单讲一下Transformer中的残差结构以及意义？</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Transformer%E5%9C%A8%E8%AE%AD%E7%BB%83%E4%B8%8E%E9%AA%8C%E8%AF%81%E7%9A%84%E6%97%B6%E5%80%99%E6%9C%89%E4%BB%80%E4%B9%88%E4%B8%8D%E5%90%8C%EF%BC%9F"><span class="nav-number">2.27.</span> <span class="nav-text">Transformer在训练与验证的时候有什么不同？</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Transformer%E6%A8%A1%E5%9E%8B%E7%9A%84%E8%AE%A1%E7%AE%97%E5%A4%8D%E6%9D%82%E5%BA%A6%E6%98%AF%E5%A4%9A%E5%B0%91%EF%BC%9F"><span class="nav-number">2.28.</span> <span class="nav-text">Transformer模型的计算复杂度是多少？</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Transformer%E4%B8%AD%E4%B8%89%E4%B8%AA%E5%A4%9A%E5%A4%B4%E8%87%AA%E6%B3%A8%E6%84%8F%E5%8A%9B%E5%B1%82%E5%88%86%E5%88%AB%E6%9C%89%E4%BB%80%E4%B9%88%E6%84%8F%E4%B9%89%E4%B8%8E%E4%BD%9C%E7%94%A8%EF%BC%9F"><span class="nav-number">2.29.</span> <span class="nav-text">Transformer中三个多头自注意力层分别有什么意义与作用？</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%BA%E4%BB%80%E4%B9%88transformer%E5%9D%97%E4%BD%BF%E7%94%A8LayerNorm%E8%80%8C%E4%B8%8D%E6%98%AFBatchNorm%EF%BC%9FLayerNorm-%E5%9C%A8Transformer%E7%9A%84%E4%BD%8D%E7%BD%AE%E6%98%AF%E5%93%AA%E9%87%8C%EF%BC%9F"><span class="nav-number">2.30.</span> <span class="nav-text">为什么transformer块使用LayerNorm而不是BatchNorm？LayerNorm 在Transformer的位置是哪里？</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%AE%80%E5%8D%95%E6%8F%8F%E8%BF%B0%E4%B8%80%E4%B8%8BTransformer%E4%B8%AD%E7%9A%84%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%9F%E4%BD%BF%E7%94%A8%E4%BA%86%E4%BB%80%E4%B9%88%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%EF%BC%9F%E7%9B%B8%E5%85%B3%E4%BC%98%E7%BC%BA%E7%82%B9%EF%BC%9F"><span class="nav-number">2.31.</span> <span class="nav-text">简单描述一下Transformer中的前馈神经网络？使用了什么激活函数？相关优缺点？</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Encoder%E7%AB%AF%E5%92%8CDecoder%E7%AB%AF%E6%98%AF%E5%A6%82%E4%BD%95%E8%BF%9B%E8%A1%8C%E4%BA%A4%E4%BA%92%E7%9A%84%EF%BC%9F"><span class="nav-number">2.32.</span> <span class="nav-text">Encoder端和Decoder端是如何进行交互的？</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Decoder%E9%98%B6%E6%AE%B5%E7%9A%84%E5%A4%9A%E5%A4%B4%E8%87%AA%E6%B3%A8%E6%84%8F%E5%8A%9B%E5%92%8Cencoder%E7%9A%84%E5%A4%9A%E5%A4%B4%E8%87%AA%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%89%E4%BB%80%E4%B9%88%E5%8C%BA%E5%88%AB%EF%BC%9F"><span class="nav-number">2.33.</span> <span class="nav-text">Decoder阶段的多头自注意力和encoder的多头自注意力有什么区别？</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Transformer%E7%9A%84%E5%B9%B6%E8%A1%8C%E5%8C%96%E6%8F%90%E7%8E%B0%E5%9C%A8%E5%93%AA%E4%B8%AA%E5%9C%B0%E6%96%B9%EF%BC%9FDecoder%E7%AB%AF%E5%8F%AF%E4%BB%A5%E5%81%9A%E5%B9%B6%E8%A1%8C%E5%8C%96%E5%90%97%EF%BC%9F"><span class="nav-number">2.34.</span> <span class="nav-text">Transformer的并行化提现在哪个地方？Decoder端可以做并行化吗？</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Transformer%E8%AE%AD%E7%BB%83%E7%9A%84%E6%97%B6%E5%80%99%E5%AD%A6%E4%B9%A0%E7%8E%87%E6%98%AF%E5%A6%82%E4%BD%95%E8%AE%BE%E5%AE%9A%E7%9A%84%EF%BC%9FDropout%E4%BD%8D%E7%BD%AE%E5%9C%A8%E5%93%AA%E9%87%8C%EF%BC%9FDropout-%E5%9C%A8%E6%B5%8B%E8%AF%95%E7%9A%84%E9%9C%80%E8%A6%81%E6%9C%89%E4%BB%80%E4%B9%88%E9%9C%80%E8%A6%81%E6%B3%A8%E6%84%8F%E7%9A%84%E5%90%97%EF%BC%9F"><span class="nav-number">2.35.</span> <span class="nav-text">Transformer训练的时候学习率是如何设定的？Dropout位置在哪里？Dropout 在测试的需要有什么需要注意的吗？</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#bert%E7%9A%84mask%E4%B8%BA%E4%BD%95%E4%B8%8D%E5%AD%A6%E4%B9%A0transformer%E5%9C%A8attention%E5%A4%84%E8%BF%9B%E8%A1%8C%E5%B1%8F%E8%94%BDscore%E7%9A%84%E6%8A%80%E5%B7%A7%EF%BC%9F"><span class="nav-number">2.36.</span> <span class="nav-text">bert的mask为何不学习transformer在attention处进行屏蔽score的技巧？</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Transformer%E5%9C%A8%E5%93%AA%E9%87%8C%E5%81%9A%E4%BA%86%E6%9D%83%E9%87%8D%E5%85%B1%E4%BA%AB%EF%BC%8C%E4%B8%BA%E4%BB%80%E4%B9%88%E5%8F%AF%E4%BB%A5%E5%81%9A%E6%9D%83%E9%87%8D%E5%85%B1%E4%BA%AB%EF%BC%9F"><span class="nav-number">2.37.</span> <span class="nav-text">Transformer在哪里做了权重共享，为什么可以做权重共享？</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#pre-norm%E5%92%8Cpost-norm%E7%9A%84%E5%8C%BA%E5%88%AB%EF%BC%9F"><span class="nav-number">2.38.</span> <span class="nav-text">pre-norm和post-norm的区别？</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%BB%9D%E5%AF%B9%E4%BD%8D%E7%BD%AE%E5%92%8C%E7%9B%B8%E5%AF%B9%E4%BD%8D%E7%BD%AE%E7%9A%84%E5%8C%BA%E5%88%AB%EF%BC%9F"><span class="nav-number">2.39.</span> <span class="nav-text">绝对位置和相对位置的区别？</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#position-embedding%E7%9A%84%E5%AE%9E%E7%8E%B0%E6%96%B9%E5%BC%8F%E6%9C%89%E5%93%AA%E4%B8%A4%E7%A7%8D%EF%BC%9F"><span class="nav-number">2.40.</span> <span class="nav-text">position embedding的实现方式有哪两种？</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%AE%B2%E4%B8%80%E4%B8%8BBert%E5%8E%9F%E7%90%86%EF%BC%9F"><span class="nav-number">2.41.</span> <span class="nav-text">讲一下Bert原理？</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Bert%E7%9A%84MLM%E9%A2%84%E8%AE%AD%E7%BB%83%E4%BB%BB%E5%8A%A1mask%E7%9A%84%E7%9B%AE%E7%9A%84%E6%98%AF%E4%BB%80%E4%B9%88%EF%BC%9F"><span class="nav-number">2.42.</span> <span class="nav-text">Bert的MLM预训练任务mask的目的是什么？</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#bert%E5%8F%8A%E5%85%B6%E5%8F%98%E4%BD%93%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84mask%E6%96%B9%E5%BC%8F%E5%8F%8A%E7%89%B9%E7%82%B9%EF%BC%9F"><span class="nav-number">2.43.</span> <span class="nav-text">bert及其变体中常用的mask方式及特点？</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Bert-%E9%87%87%E7%94%A8%E5%93%AA%E7%A7%8DNormalization%E7%BB%93%E6%9E%84%EF%BC%8CLayerNorm%E7%BB%93%E6%9E%84%E6%9C%89%E5%8F%82%E6%95%B0%E5%90%97%EF%BC%8C%E5%8F%82%E6%95%B0%E7%9A%84%E4%BD%9C%E7%94%A8%EF%BC%9F"><span class="nav-number">2.44.</span> <span class="nav-text">Bert 采用哪种Normalization结构，LayerNorm结构有参数吗，参数的作用？</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%A6%82%E4%BD%95%E4%BC%98%E5%8C%96BERT%E6%95%88%E6%9E%9C%EF%BC%9F"><span class="nav-number">2.45.</span> <span class="nav-text">如何优化BERT效果？</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Bert%E5%92%8CAlbert%E5%8C%BA%E5%88%AB%EF%BC%9F"><span class="nav-number">2.46.</span> <span class="nav-text">Bert和Albert区别？</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Bert%E5%92%8CFinBert%E5%8C%BA%E5%88%AB%EF%BC%9F"><span class="nav-number">2.47.</span> <span class="nav-text">Bert和FinBert区别？</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#elmo%E3%80%81GPT%E3%80%81bert%E4%B8%89%E8%80%85%E4%B9%8B%E9%97%B4%E6%9C%89%E4%BB%80%E4%B9%88%E5%8C%BA%E5%88%AB%EF%BC%9F"><span class="nav-number">2.48.</span> <span class="nav-text">elmo、GPT、bert三者之间有什么区别？</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Siamese%E7%BB%93%E6%9E%84%E4%BB%80%E4%B9%88%E6%A0%B7%EF%BC%9F"><span class="nav-number">2.49.</span> <span class="nav-text">Siamese结构什么样？</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%92%B8%E9%A6%8F%E7%9A%84%E6%80%9D%E6%83%B3%EF%BC%8C%E4%B8%BA%E4%BB%80%E4%B9%88%E8%A6%81%E8%92%B8%E9%A6%8F%EF%BC%9F"><span class="nav-number">2.50.</span> <span class="nav-text">蒸馏的思想，为什么要蒸馏？</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%9C%89%E5%93%AA%E4%BA%9B%E8%92%B8%E9%A6%8F%E6%96%B9%E5%BC%8F"><span class="nav-number">2.51.</span> <span class="nav-text">有哪些蒸馏方式?</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#softmax-temperature%E8%92%B8%E9%A6%8F%E4%B8%AD%E4%BD%9C%E7%94%A8%EF%BC%9F"><span class="nav-number">2.52.</span> <span class="nav-text">softmax-temperature蒸馏中作用？</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Beam-Search%EF%BC%9F%E7%BC%BA%E7%82%B9%EF%BC%9F%E4%BC%98%E5%8C%96%EF%BC%9F"><span class="nav-number">2.53.</span> <span class="nav-text">Beam Search？缺点？优化？</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%A6%82%E4%BD%95%E5%81%9A%E8%AF%AD%E4%B9%89%E7%9B%B8%E4%BC%BC%E5%BA%A6%E7%9A%84%EF%BC%9F"><span class="nav-number">2.54.</span> <span class="nav-text">如何做语义相似度的？</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%88%86%E7%B1%BB%E4%BB%BB%E5%8A%A1%E4%B8%AD%E4%BC%9A%E7%94%A8%E5%88%B0%E5%93%AA%E4%BA%9Bloss-function%EF%BC%9F"><span class="nav-number">2.55.</span> <span class="nav-text">分类任务中会用到哪些loss function？</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E8%AE%AD%E7%BB%83%E5%87%BA%E7%8E%B0nan%EF%BC%8C%E5%BA%94%E8%AF%A5%E4%BB%8E%E5%93%AA%E4%BA%9B%E6%96%B9%E9%9D%A2%E5%8E%BB%E6%9F%A5%EF%BC%9F"><span class="nav-number">2.56.</span> <span class="nav-text">神经网络训练出现nan，应该从哪些方面去查？</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#loss%E4%B8%8D%E4%B8%8B%E9%99%8D%E5%8F%AF%E8%83%BD%E6%98%AF%E4%BB%80%E4%B9%88%E6%83%85%E5%86%B5%EF%BC%9F%E5%A6%82%E4%BD%95%E8%A7%A3%E5%86%B3%E6%A8%A1%E5%9E%8B%E4%B8%8D%E6%94%B6%E6%95%9B%E9%97%AE%E9%A2%98%EF%BC%9F"><span class="nav-number">2.57.</span> <span class="nav-text">loss不下降可能是什么情况？如何解决模型不收敛问题？</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86"><span class="nav-number">3.</span> <span class="nav-text">自然语言处理</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%AF%A6%E7%BB%86%E8%AE%B2%E4%B8%80%E4%B8%8B%E4%B8%8D%E5%90%8C%E7%9A%84%E6%96%87%E6%9C%AC%E8%A1%A8%E5%BE%81%E5%90%91%E9%87%8F%E5%8C%96%E6%96%B9%E6%B3%95%EF%BC%8C%E5%AF%B9%E6%AF%94%E5%8C%BA%E5%88%AB%EF%BC%9F"><span class="nav-number">3.1.</span> <span class="nav-text">详细讲一下不同的文本表征向量化方法，对比区别？</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#NLP%E7%9A%84%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86%E9%80%9A%E5%B8%B8%E5%81%9A%E4%BB%80%E4%B9%88%EF%BC%9F"><span class="nav-number">3.2.</span> <span class="nav-text">NLP的数据预处理通常做什么？</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%AE%B2%E4%B8%80%E4%B8%8B-word2vec%EF%BC%9F-cbow-%E4%B8%8E-skip-gram-%E7%9A%84%E5%8C%BA%E5%88%AB%E5%92%8C%E4%BC%98%E7%BC%BA%E7%82%B9%EF%BC%9F"><span class="nav-number">3.3.</span> <span class="nav-text">讲一下 word2vec？ cbow 与 skip-gram 的区别和优缺点？</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#FastText%E5%92%8CGlovec%E5%8E%9F%E7%90%86%E4%BB%8B%E7%BB%8D%EF%BC%9F"><span class="nav-number">3.4.</span> <span class="nav-text">FastText和Glovec原理介绍？</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#fastText%E5%92%8Cword2vec%E7%9A%84%E5%8C%BA%E5%88%AB%EF%BC%9F"><span class="nav-number">3.5.</span> <span class="nav-text">fastText和word2vec的区别？</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#CNN-textcnn-%E4%B8%BA%E4%BB%80%E4%B9%88%E8%83%BD%E5%81%9A%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%EF%BC%8C%E8%B6%85%E5%8F%82%E6%95%B0%E6%80%8E%E4%B9%88%E7%A1%AE%E5%AE%9A%EF%BC%9F"><span class="nav-number">3.6.</span> <span class="nav-text">CNN(textcnn)为什么能做文本分类，超参数怎么确定？</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#CRF%E5%8E%9F%E7%90%86%EF%BC%9F"><span class="nav-number">3.7.</span> <span class="nav-text">CRF原理？</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#HMM%E3%80%81MEMM-vs-CRF-%E5%AF%B9%E6%AF%94%EF%BC%9F"><span class="nav-number">3.8.</span> <span class="nav-text">HMM、MEMM vs CRF 对比？</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#QA%E5%BB%BA%E6%A8%A1%E6%96%B9%E6%B3%95%E6%9C%89%E5%93%AA%E4%BA%9B%EF%BC%9F"><span class="nav-number">3.9.</span> <span class="nav-text">QA建模方法有哪些？</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%B2%BE%E6%8E%92learning-to-rank-%E6%96%B9%E6%B3%95%E6%9C%89%E5%93%AA%E4%BA%9B%EF%BC%9F"><span class="nav-number">3.10.</span> <span class="nav-text">精排learning to rank 方法有哪些？</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%AD%89"><span class="nav-number">4.</span> <span class="nav-text">大数据等</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#mysql%E3%80%81hadoop%E3%80%81spark%E5%8C%BA%E5%88%AB%EF%BC%9F"><span class="nav-number">4.1.</span> <span class="nav-text">mysql、hadoop、spark区别？</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#reducebykey%E5%A6%82%E4%BD%95%E8%BF%90%E8%A1%8C%EF%BC%8C%E5%92%8Cgroupbykey%E7%9B%B8%E6%AF%94%E9%AB%98%E6%95%88%E5%9C%A8%E5%93%AA%EF%BC%9F"><span class="nav-number">4.2.</span> <span class="nav-text">reducebykey如何运行，和groupbykey相比高效在哪？</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%AE%BD%E4%BE%9D%E8%B5%96-%E7%AA%84%E4%BE%9D%E8%B5%96%EF%BC%9F"><span class="nav-number">4.3.</span> <span class="nav-text">宽依赖&#x2F;窄依赖？</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%95%B0%E5%AD%A6%E9%A2%98"><span class="nav-number">5.</span> <span class="nav-text">数学题</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%8F%82%E8%80%83%E6%96%87%E7%8C%AE"><span class="nav-number">6.</span> <span class="nav-text">参考文献</span></a></li></ol></div>
        </section>
        <!--/noindex-->

        <section class="site-overview-wrap sidebar-panel">
          <div class="site-author site-overview-item animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="SoundMemories"
      src="https://i.loli.net/2020/11/04/6JhNuwtBe4adylS.png">
  <p class="site-author-name" itemprop="name">SoundMemories</p>
  <div class="site-description" itemprop="description">今日事，今日毕</div>
</div>
<div class="site-state-wrap site-overview-item animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives">
          <span class="site-state-item-count">118</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
        <span class="site-state-item-count">8</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
        <span class="site-state-item-count">8</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author site-overview-item animated">
      <span class="links-of-author-item">
        <span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL3NvdW5kbWVtb3JpZXM=" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;soundmemories"><i class="fab fa-github fa-fw"></i>GitHub</span>
      </span>
      <span class="links-of-author-item">
        <span class="exturl" data-url="bWFpbHRvOnNvdW5kbWVtb3JpZXNAMTYzLmNvbQ==" title="E-Mail → mailto:soundmemories@163.com"><i class="fa fa-envelope fa-fw"></i>E-Mail</span>
      </span>
  </div>
  <div class="cc-license site-overview-item animated" itemprop="license">
    <span class="exturl cc-opacity" data-url="aHR0cHM6Ly9jcmVhdGl2ZWNvbW1vbnMub3JnL2xpY2Vuc2VzL2J5LW5jLXNhLzQuMC8="><img src="/images/cc-by-nc-sa.svg" alt="Creative Commons"></span>
  </div>



        </section>
      </div>
    </div>
  </aside>
  <div class="sidebar-dimmer"></div>


    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>
  <a role="button" class="book-mark-link book-mark-link-fixed"></a>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


    <div class="main-inner post posts-expand">
      

      

  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://soundmemories.github.io/2019/01/21/Other/%E9%9D%A2%E8%AF%95%E9%A2%98%E9%9B%86/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="https://i.loli.net/2020/11/04/6JhNuwtBe4adylS.png">
      <meta itemprop="name" content="SoundMemories">
      <meta itemprop="description" content="今日事，今日毕">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="SoundMemories">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          面试题集
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2019-01-21 00:00:00" itemprop="dateCreated datePublished" datetime="2019-01-21T00:00:00+08:00">2019-01-21</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Other/" itemprop="url" rel="index"><span itemprop="name">Other</span></a>
        </span>
    </span>

  
      </div>
      <div class="post-meta">
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>29k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>26 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
        <h1 id="机器学习"><a href="#机器学习" class="headerlink" title="机器学习"></a>机器学习</h1><h2 id="过拟合和欠拟合？"><a href="#过拟合和欠拟合？" class="headerlink" title="过拟合和欠拟合？"></a>过拟合和欠拟合？</h2><p><strong>过拟合</strong>：模型在训练集上表现好，在测试集和新数据上表现差。<br><strong>欠拟合</strong>：模型在训练集和测试集上表现都差。</p>
<p><strong>降低过拟合风险</strong>：<br>增加数据。<br>降低模型复杂度，如减少神经元数量、网络层数，降低树深度，剪枝。<br>增加正则化。<br>使用集成学习方法，降低单一模型过拟合风险。</p>
<p><strong>降低欠拟合风险</strong>：<br>增加新特征。<br>增加模型复杂度。<br>减小正则化系数。</p>
<h2 id="多分类用什么评估指标，AUC是什么？"><a href="#多分类用什么评估指标，AUC是什么？" class="headerlink" title="多分类用什么评估指标，AUC是什么？"></a>多分类用什么评估指标，AUC是什么？</h2><p>多个二分类混淆矩阵，两两类别的组合都能对应一个混淆矩阵。<br><strong>宏混淆矩阵</strong>：先计算每个混淆矩阵的指标(P、R、F1)，再求和取均值。<br><strong>微混淆矩阵</strong>：先对混淆矩阵求和(TP,FP,FN,TN)，再计算指标(P、R、F1)。</p>
<p><strong>AUC是ROC曲线下部分的面积。ROC曲线横坐标FPR，纵坐标TPR。AUC越大说明模型把真正例排在前面，分类效果越好。</strong></p>
<h2 id="样本不平衡处理？"><a href="#样本不平衡处理？" class="headerlink" title="样本不平衡处理？"></a>样本不平衡处理？</h2><p><strong>欠采样</strong>：对样本多的类别，进行欠采样。比如原型生成(利用K-means聚类选择多数样本，保证样本分布不变)，原型选择(多数样本中选取代表性样本，每个少数类样本选择K个最近的多数类样本)。<br><strong>过采样</strong>：对样本少的类别，进行过采样。比如SMOTE(一个少数样本与k近邻少数样本连线，取中点作为新少数样本)，NLP中数据增强，同义词替换。<br><strong>损失函数</strong>：对不同类别样本赋予不同权重，比如Focal loss。<br><strong>模型算法</strong>：通过引入有权重的模型算法，针对少量样本着重拟合，以提升对少量样本特征的学习。比如xgb的scale_pos_weight。<br><strong>评价指标</strong>：选择对样本不平衡不敏感的指标，比如roc，auc，f1。</p>
<h2 id="缺失值处理？"><a href="#缺失值处理？" class="headerlink" title="缺失值处理？"></a>缺失值处理？</h2><p>离散型变量：用出现次数最多的特征值填充。<br>连续型变量：用中位数或者均值填充。</p>
<h2 id="L1正则和L2正则的区别是什么？"><a href="#L1正则和L2正则的区别是什么？" class="headerlink" title="L1正则和L2正则的区别是什么？"></a>L1正则和L2正则的区别是什么？</h2><p>L1正则化是指在损失函数中加入权值向量w的绝对值之和，即各个元素的绝对值之和，L2正则化指在损失函数中加入权值向量w的平方和。<br>L1的功能是使权重稀疏，而L2的功能是使权重平滑。<br>从贝叶斯角度来看，L1正则化相当于对模型参数w引入了拉普拉斯先验，L2正则化相当于引入了高斯先验。</p>
<h2 id="L1正则为什么可以得到稀疏解？"><a href="#L1正则为什么可以得到稀疏解？" class="headerlink" title="L1正则为什么可以得到稀疏解？"></a>L1正则为什么可以得到稀疏解？</h2><p>解空间形状，这是我们最常使用的一种答案，就是给面试官画下面的图：</p>
<p><img src="/images/面试/L1图.jpg" width="80%"></p>
<p>L2正则化相当于为参数定义了一个圆形的解空间，L1正则化相当于为参数定义了一个菱形的解空间。L1”棱角分明”的解空间显然更容易与凸优化问题中目标函数等高线在轴上碰撞，从而产生稀疏解。<br>事实上，“带正则项”和“带约束条件”是等价的。加了正则化的损失函数 $L = Loss(y, \hat{y}) +\lambda ||w||_2^2$。带约束条件的 损失函数 $L = Loss(y, \hat{y}), st.||w||_2^2&lt;=m$ 等价于求 $L = Loss(y, \hat{y}) +\lambda (||w||_2^2 - m)$ 其中 L 对 w 求导为0，就跟上面一致了。</p>
<h2 id="SGD和GD的区别，什么场景下用？"><a href="#SGD和GD的区别，什么场景下用？" class="headerlink" title="SGD和GD的区别，什么场景下用？"></a>SGD和GD的区别，什么场景下用？</h2><p><img src="/images/面试/梯度下降.png" width="60%"><br>梯度下降：每次迭代，计算所有样本的梯度均值，然后更新参数。<br>随机梯度下降：选一个样本计算一个梯度，然后马上更新参数。</p>
<h2 id="决策树启发函数？"><a href="#决策树启发函数？" class="headerlink" title="决策树启发函数？"></a>决策树启发函数？</h2><p><strong>ID3</strong>：最大信息增益。<br><strong>c4.5</strong>：最大信息增益比。校正信息增益趋向于取值多的特征问题。<br><strong>cart</strong>：基尼系数。与信息熵含义类似，每次选择基尼系数最小的特征。</p>
<p>ID3对缺失值敏感。<br>ID3只能离散变量，c4.5和cart还适用于连续变量(划分区间变为离散)。<br>ID3和c4.5是多叉树(特征不复用)，cart是二叉树(特征可复用)。<br>ID3和c4.5只能分类任务，cart还适用于回归任务。</p>
<h2 id="bagging和boosting？"><a href="#bagging和boosting？" class="headerlink" title="bagging和boosting？"></a>bagging和boosting？</h2><p><strong>bagging</strong>：对训练集多次采样，产生若干不同的子集，每个子集训练一个基学习器，预测分类任务投票法，回归任务均值法。<br><strong>boosting</strong>：初始训练集训练一个基分类器，再根据基分类器表现对训练样本分布调整(通过学习残差改变样本权重)，基于调整后的分布来训练下一个基学习器，重复直到达到设定阈值，最终结果为所有分类器的加权求和。<br><strong>bagging降低方差，boosting降低偏差。</strong></p>
<p><img src="/images/面试/Bagging和Boosting.png" width="100%"></p>
<h2 id="比较LR和GBDT-说说什么情境下GBDT不如LR？"><a href="#比较LR和GBDT-说说什么情境下GBDT不如LR？" class="headerlink" title="比较LR和GBDT, 说说什么情境下GBDT不如LR？"></a>比较LR和GBDT, 说说什么情境下GBDT不如LR？</h2><p>先说说LR和GBDT的区别：<br>（1）LR是线性模型，可解释性强，很容易并行化，但是学习能力有限，需要大量的人工特征工程。<br>（2）GBDT是非线性模型，有天然的特征组合优势，特征表达能力强，但是树与树之前无法并行训练，而且树模型很容易过拟合。</p>
<p>当在高维稀疏特征的场景下，LR的效果一般会比GBDT好。原因如下：<br>（1）假设一个二分类问题，label为0和1， 特征为100维， 如果有1w个样本，但其中只有10个正样本1，而这些样本的特征f1的值全为1， 而其余9990条样本的f1特征都为0（在高维稀疏的情况下这种情况很常见）。 我们都知道在这种情况下，树模型很容易优化出一个使用f1特征作为重要分裂节点的树，因为这个节点直接能够将训练数据划分的很好，但是当测的时候，却会发现效果很差，因为这个特征f1只是刚好偶然间跟y拟合到了这个规律，这也就是我们常说的过拟合。<br>（2）那么这种情况下， 如果采用lr的话，应该也会出现类似过拟合的情况呀。<code>y=w1*f1 + w2*f2+ w3*f3 +.....</code>  其中，w1特别大足够拟合这10个样本，为什么此时树模型就过拟合的更严重呢？因为现在的模型普遍都会带着正则项，而LR等线性模型的正则项是对<strong>权重</strong>的惩罚，也就是W1一旦变大，惩罚就会很大，进一步压缩w1的值，使他不至于过大，但是，树模型则不一样，树模型的惩罚通常为<strong>叶子节点数</strong>和<strong>深度</strong>等。而我们知道，对于上面的case,树只需要一个节点就可以完美分割9990和10个样本，一个节点，最终产生的惩罚项及其之小。<br><strong>这也就是为什么在高维稀疏特征的时候，线性模型会比非线性模型好的原因了：带正则化项的线性模型比较不容易对稀疏特征过拟合。</strong></p>
<h2 id="RF和GBDT的区别？"><a href="#RF和GBDT的区别？" class="headerlink" title="RF和GBDT的区别？"></a>RF和GBDT的区别？</h2><p><strong>随机森林</strong>：基于bagging思想，cart决策树作为基学习器。可并行化。随机选择样本（放回抽样）；随机选择特征(计算增益)；构建决策树；随机森林投票/平均。<br>优点：易于并行化，在大数据集上有很大的优势；能够处理高维度数据，不用做特征选择。</p>
<p><strong>GBDT</strong>：基于boosting思想，cart决策树作为基学习器，使用。串行。GBDT中的树都是回归树，每次学习上一个基分类器的残差，每一步残差计算其实变相地增大了被分错样本的权重，而对于分对样本的权重趋于0，这样后面的树就能专注于那些被分错的样本。<br>优点：可以自动进行特征组合，拟合非线性数据；可以灵活处理各种类型的数据。<br>缺点：对异常点敏感。回归类的损失函数会用<strong>绝对损失</strong>或者<strong>Huber损失</strong>函数来代替平方损失函数。</p>
<p>相同点：都是由多棵树组成，最终的结果都是由多棵树一起决定。<br>不同点：<br>集成学习： RF属于bagging 思想，而GBDT属于boosting思想。<br>偏差-方差衡量：RF不断的降低模型的方差，而GBDT不断的降低模型的偏差。<br>训练样本：RF每次迭代的样本是从全部训练集中有放回抽样形成的，而GBDT每次使用全部样本。<br>并行性：RF的树可以并行生成，而GBDT只能顺序生成（需要等上一棵树完全生成）。<br>最终结果： RF最终是多棵树进行多数表决（回归问题是取平均）， 而GBDT是加权融合。<br>数据敏感度：RF对异常值不敏感，而GBDT对异常值比较敏感。<br>泛化能力： RF不易过拟合，而GBDT容易过拟合。</p>
<h2 id="GBDT和xgboost区别？"><a href="#GBDT和xgboost区别？" class="headerlink" title="GBDT和xgboost区别？"></a>GBDT和xgboost区别？</h2><p><img src="/images/面试/GBDT和Xgboost.jpg" width="80%"><br><img src="/images/面试/GBDT和Xgboost2.jpg" width="60%"></p>
<h2 id="简单介绍下xgboost？"><a href="#简单介绍下xgboost？" class="headerlink" title="简单介绍下xgboost？"></a>简单介绍下xgboost？</h2><p>（1）首先需要说一下GBDT，它是一种基于boosting的增强策略的加法模型，训练的时候才用前向分布算法进行贪婪的学习，每次迭代都学习一颗cart树来拟合前t-1颗树的预测结果与训练样本真实值的差值。<br>（2）xbgoost对gbdt进行了一系列的优化，比如损失函数进行了二阶泰勒展开，目标函数加入正则项，支持并行和默认缺失值处理等等，在可扩展性和训练速度上有了巨大的提升，但其核心思想多大的变化。</p>
<h2 id="xgboost为什么使用泰勒二阶展开？"><a href="#xgboost为什么使用泰勒二阶展开？" class="headerlink" title="xgboost为什么使用泰勒二阶展开？"></a>xgboost为什么使用泰勒二阶展开？</h2><p>精准性：相对于GBDT的一阶展开，XGBOOST采用二阶泰勒展开，可以更为精准的逼近真实的损失函数。<br>扩展性：损失函数支持自定义，只需要新的损失函数二阶可导即可。</p>
<h2 id="XGBoost为什么可以并行训练？-如何寻找最佳分裂点？"><a href="#XGBoost为什么可以并行训练？-如何寻找最佳分裂点？" class="headerlink" title="XGBoost为什么可以并行训练？/如何寻找最佳分裂点？"></a>XGBoost为什么可以并行训练？/如何寻找最佳分裂点？</h2><p>（1）xgboost的并行，并不是说每棵树可以并行训练，XGB本质上仍然采用boosting的思想，每棵树训练前需要等前面的树训练完成以后才能开始训练。<br>（2）xgboost的并行，指的是特征维度的并行。在训练之前，每个特征按特征值对样本进行预排序，并存储为block结构，在后面查找特征分割点时可以重复使用，在寻找每个特征的最佳分割点的时候，可以利用多线程对每个block并行计算。</p>
<h2 id="XGBOOST为什么快？"><a href="#XGBOOST为什么快？" class="headerlink" title="XGBOOST为什么快？"></a>XGBOOST为什么快？</h2><p>（1）分块并行：训练前每个特征按值进行排序并存储为block结构，后面查找特征分割点时重复使用，并且支持并行查找每个特征的分割点。<br>（2）候选分位点：每个特征采用常数个分位点作为候选分割点。<br>（3）CPU cache命中优化：使用缓存预取的方法，对每个线程分配一个连续的buffer,读取每个block中样本的梯度信息并存入连续的buffer中。<span class="exturl" data-url="aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2RwZW5nd2FuZy9hcnRpY2xlL2RldGFpbHMvOTU5MTM0MTc=">https://blog.csdn.net/dpengwang/article/details/95913417<i class="fa fa-external-link-alt"></i></span><br>（4）block 处理优化： block预先存入内存，block按列进行解压缩，将block划分到不同硬盘来提高吞吐。</p>
<h2 id="XGBOOST防止过拟合的方法？"><a href="#XGBOOST防止过拟合的方法？" class="headerlink" title="XGBOOST防止过拟合的方法？"></a>XGBOOST防止过拟合的方法？</h2><p>（1）正则项：叶子节点个数+叶子节点权重的L2正则化。<br>（2）列抽样：训练的时候只用一部分特征（不考虑剩余的block块即可）。<br>（3）子采样：每轮计算可以不使用全部样本，使算法更加保守。<br>（4）shrinkage: 可以叫做学习率或者步长，为了给后面的训练留出更多的学习空间。</p>
<h2 id="XGBOOST如何处理缺失值？"><a href="#XGBOOST如何处理缺失值？" class="headerlink" title="XGBOOST如何处理缺失值？"></a>XGBOOST如何处理缺失值？</h2><p>（1）在特征k上寻找最佳split point的时候，不会对该列特征missing的样本进行遍历，而只对该列特征值为non-missing的样本上对应的特征值进行遍历，通过这个技巧来减少为系数离散特征寻找split point的时间开销。<br>（2）在逻辑实现上，为了保证完备性，会将该特征值missing的样本分别分配到左叶子节点和右叶子节点，两种情况都计算一遍后，选择分裂后增益最大的那个方向（左分支或者右分支），<strong>作为预测时特征值却是样本的默认分支方向</strong>。<br>（3）如果在训练中没有缺失值而在预测中出现缺失，那么会自动将缺失值的划分方向放到右子节点。</p>
<h2 id="XBGOOST中叶子节点的权重如何计算出来的？"><a href="#XBGOOST中叶子节点的权重如何计算出来的？" class="headerlink" title="XBGOOST中叶子节点的权重如何计算出来的？"></a>XBGOOST中叶子节点的权重如何计算出来的？</h2><p>目标函数：<a href="https://soundmemories.github.io/2021/06/25/Paper/02.XGBoost%20A%20Scalable%20Tree%20Boosting%20System/">XGBoost</a></p>
<h2 id="XGBOOST中的一棵树的停止生长条件？"><a href="#XGBOOST中的一棵树的停止生长条件？" class="headerlink" title="XGBOOST中的一棵树的停止生长条件？"></a>XGBOOST中的一棵树的停止生长条件？</h2><p>（1）当新引入的一次分类所带来的增益Gain &lt; 0 时，放弃当前的分裂。这是训练损失和模型结构复杂度的博弈过程。<br>（2）当树达到最大深度时，停止建树，因为树深度太深容易出现过拟合，这里需要设置一个超参数max_depth。<br>（3）当引入一次分裂后，重新计算新生产的左右两个叶子节点的样本权重和。如果任一个叶子节点的样本权重低于某一个阈值，也会放弃此次分裂。这涉及到一个超参数： 最小样本权重和，是指如果一个叶子节点包含的样本数量太少也会放弃分裂，防止树分的太细。</p>
<h2 id="xgboost如何处理不平衡数据？"><a href="#xgboost如何处理不平衡数据？" class="headerlink" title="xgboost如何处理不平衡数据？"></a>xgboost如何处理不平衡数据？</h2><p>（1）如果你在意AUC，采用AUC来评估模型的性能，那你可以通过设置 scale_pos_weight来平衡正样本和负样本的权重。例如，当正负样本比例为1：10的，scale_pos_weigth可以设置为10；<br>（2）如果你在意概率（预测得分的合理性），你不能重新平衡数据集（会破坏数据的真实分布），应该设置max_delta_step为一个有限数字来帮助收敛（基模型为LR时有效）。<br>那么源码到底是怎么利用scale_pos_weight来平衡样本的呢， 是调节权重还是过采样呢？ 源码 <code>if (info.labels[i] == 1.0f)  w *= param_.scale_pos_weight</code> 可以看出，应该是增大了少数样本的权重。<br>除此之外，还可以通过上采样，下采样，SMOTE算法或者自定义代价函数的方式解决正负样本不平衡的问题。</p>
<h2 id="XGBOOS中如何对树进行剪枝？"><a href="#XGBOOS中如何对树进行剪枝？" class="headerlink" title="XGBOOS中如何对树进行剪枝？"></a>XGBOOS中如何对树进行剪枝？</h2><p><span class="exturl" data-url="aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2Fuc2h1YWlfYXcxL2FydGljbGUvZGV0YWlscy84NTA5MzEwNg==">https://blog.csdn.net/anshuai_aw1/article/details/85093106<i class="fa fa-external-link-alt"></i></span><br>（1）在目标函数中增加了正则项。适用叶子节点的数目和叶子节点权重的L2模的平方，控制树的复杂度。<br>（2）在结点分裂时，定义了一个阈值，如何分裂后目标函数的增益小于该阈值，则不分裂。<br>（3）当引入一次分裂后，重新计算新生成的左右两个叶子节点的样本权重和。如果任一个叶子节点的样本权重低于 某一个阈值（最小样本权重值min_child_weight），也会放弃该次分裂。<br>（4）XGBOOST先从顶到底建立树直到最大深度，再从低到顶反向检查是否有不满足分裂条件的结点，进行剪枝。比起GBM，这样不容易陷入局部最优解。</p>
<h2 id="XGBoost的Scalable-可扩展性-性如何体现？"><a href="#XGBoost的Scalable-可扩展性-性如何体现？" class="headerlink" title="XGBoost的Scalable(可扩展性)性如何体现？"></a>XGBoost的Scalable(可扩展性)性如何体现？</h2><p>基分类器: 弱分类器可以支持cart决策树，也可以支持LR和Linear.<br>目标函数: 支持自定义loss function. 只需要其一阶，二阶可导。有这个特性是因为泰勒二阶展开，得到通用的目标函数形式。<br>学习方法: block结构支持并行化，支持out-of-core计算。</p>
<h2 id="XGBOOST如何评价特征的重要性？"><a href="#XGBOOST如何评价特征的重要性？" class="headerlink" title="XGBOOST如何评价特征的重要性？"></a>XGBOOST如何评价特征的重要性？</h2><p>（1）权重：该特征在所有树中被用作分割样本的特征的总次数。<br>（2）增益：该特征在其出现过的所有树中产生的平均增益。<br>（3）覆盖程度：该特征在其出现过的所有树中的平均覆盖范围。 注意：覆盖范围这里指的是 一个特征用作分割点后，其影响的样本数量， 即有多少样本经过该特征分割为两个子节点。这个是通过被分到该节点的样本的二阶导数之和。举个例子来说，某个特征作为结点的对应分割样本的数目为10， 那么此特征在这棵树上的覆盖度就是这10个样本的二阶导数之和。</p>
<p>很多时候，特征重要度只是给我们一些关于业务场景的信息，同时告诉我们下一步特征工程的方向。只关心哪些更重要，哪些更弱，不是绝对序。不同方式top部分的feature应该基本一致，哪有那么大差异。</p>
<h2 id="XGBOOST参数调优的一般步骤？"><a href="#XGBOOST参数调优的一般步骤？" class="headerlink" title="XGBOOST参数调优的一般步骤？"></a>XGBOOST参数调优的一般步骤？</h2><p>(1) learning rate，estimator<br>    learning rate可以先用0.1， 用cv来寻找最优的estimator（树）的数量</p>
<p>(2) max_depth 和 min_child_weght<br>    这两个参数对输出结果的影响很大，我们首先将这两个参数设置为较大的数，然后通过迭代的方式不断修正，缩小范围。<br>    max_depth  每棵子树的最大深度， check from range(3,10, 2)<br>    min_child_weight, 子节点的权重阈值， check from range(1, 6, 2)<br>    如果一个结点分裂后，他的所有子节点的权重之和都大于该阈值，该叶子节点才可以划分。</p>
<p>(3) gamma<br>    也称做最小划分损失 min_split_loss， check from 0.1 to 0.5,指的是，对于一个叶子节点，当对它采取划分之后，损失函数的降低值的阈值。<br>    如果大于该阈值，则该叶子节点值得继续划分<br>    如果小于该阈值，则该叶子节点不值得继续划分</p>
<p>(4)subsample, colsamle_bytree<br>    subsample是对训练的样本的采样比例<br>    colsample_bytree是对特征的采样比例<br>    both check from 0.6 to 0.9</p>
<p>(5)正则化系数<br>    alpha 是 L1正则化系数， try 1e~5  1e~2  0.1  1 100<br>    lambda是L2正则化系数</p>
<p>(6)降低学习率<br>    降低学习率的同时增加树的数量， 通常最后设置学习率为0.01-0.1</p>
<h2 id="XGBOOST模型如果过拟合了怎么解决？"><a href="#XGBOOST模型如果过拟合了怎么解决？" class="headerlink" title="XGBOOST模型如果过拟合了怎么解决？"></a>XGBOOST模型如果过拟合了怎么解决？</h2><p>当出现过拟合时，有两类参数可以缓解：<br>（1）第一类参数：用于直接控制模型的复杂度。包括 max_depth, min_child_weight, gamma 等参数。<br>（2）第二类参数： 用于增加随机性，从而使得模型在训练时对于噪音不敏感，包括 subsample, colsample_bytree。<br>（3）还有就是直接减小 learning_rate, 但需要同时增加estimator参数。</p>
<h2 id="XGBOOST和lightGBM的区别？"><a href="#XGBOOST和lightGBM的区别？" class="headerlink" title="XGBOOST和lightGBM的区别？"></a>XGBOOST和lightGBM的区别？</h2><p>（1）树生长策略： XGB采用<strong>level-wise</strong>的分裂策略，LGB采用<strong>leaf-wise</strong>的分类策略。<br>XGB的level-wise，对每一层的所有节点做无差别分类，但是可能有些节点的增益非常小，对结果影响不大，带来不必要的开销。leaf-wise是在所有叶子节点中选取分裂收益最大的节点进行的，但是很容易出现过拟合问题，所以需要对最大深度做限制。</p>
<blockquote>
<p>level-wise过一次数据可以同时分裂同一层的叶子，容易进行多线程优化，也好控制模型复杂度，不容易过拟合。但实际上level-wise是一种低效的算法，因为他不加区分的对待同一层的叶子，带来了很多不必要的开销，因为实际上很多叶子的分裂增益较低，没必要进行搜索和分裂。<br>leaf-wise 则是一种更为高效的策略，每次从当前所有叶子中，找到分割增益最大的一个叶子分裂。因此同level-wise相比，leaf-wise可以降低更多的误差，得到更多的精度。leaf-wise的缺点是可能会长出比较深的决策树，产生过拟合。因此LightGBM在leaf-wise之上增加了一个最大深度的限制，以保证高效率的同时防止过拟合。</p>
</blockquote>
<p>（2）分割点查找算法：XGB使用<strong>特征预排序</strong>算法，LGB使用<strong>直方图</strong>算法。</p>
<blockquote>
<p>直方图算法：先把连续的浮点特征值离散化成为k个整数，同时构造一个宽度为k的直方图。遍历数据时，根据离散后的值作为索引在直方图中累计统计量，当遍历一次数据后，直方图累积了需要的统计量，然后根据直方图的离散值，遍历寻找最后的分割点。</p>
<p>直方图算法优势：<br>（1）<strong>减少内存占用</strong>：比如离散为256个bin时，只需要用8位整形就可以保存一个样本被映射为哪个bin(这个bin可以说就是转换后的特征)，对比预排序的exect greedy 算法来说（用int_32来存储索引+用float_32保存特征值）， 可以节省7/8的空间。<br>（2）<strong>缓存命中率提高</strong>：直方图中梯度存放是连续的。<br>（3）<strong>计算效率提高</strong>：预排序的Exact greedy 对<strong>每个特征都需要遍历一遍数据</strong>，并计算增益，复杂度为𝑂(#𝑓𝑒𝑎𝑡𝑢𝑟𝑒×#𝑑𝑎𝑡𝑎)。而直方图算法在建立完直方图后，只需要对<strong>每个特征遍历直方图即可</strong>，复杂度为𝑂(#𝑓𝑒𝑎𝑡𝑢𝑟𝑒×#𝑏𝑖𝑛𝑠)。<br>（4）<strong>直方图做差加速</strong>：一个叶子的直方图可以由它的父亲结点的直方图与它兄弟的直方图做差得到。</p>
<p>当然，Histogram算法不是完美的，由于特征离散化后，找到的并不是很精确的分割点，所以会对结果产生影响。但是在不同的数据集上的结果表明，离散化的分割点对最终的精度影响并不是很大，甚至有时候会更好一点。原因就是决策树本来就是弱模型，分割点是不是精确并不是太重要，较粗的分割点也有正则化的效果，可以有效地防止过拟合；即使单颗树的训练误差比精确分割的算法稍大，但在梯度提升的框架下没有太大的影响。</p>
<p>XGBOOST的近似直方图算法也类似于lightGBM这里的直方图算法，为什么xgboost近似算法比lightGBM还是慢很多呢？</p>
<blockquote>
<p>XGBOOST在每一层都动态构建直方图，因为xgboost的直方图算法不是针对某个特定的feature，而是所有feature共享一个直方图（每个样本的权重是二阶导），所以每一层都要重新构建直方图，而lightgbm中对每个特征都有一个直方图，所以构建一次直方图就够了。</p>
</blockquote>
</blockquote>
<p>（3）对直方图算法改进：降低样本数量(GOSS算法)/降低特征维度(EFB算法)。<br>GOSS算法：保留梯度大的样本，随机采样小梯度样本。<br>EFB算法：高维数据通常稀疏的，只需找到互斥特征并合并，就能降维。</p>
<p>（4）支持离散变量：xgb无法直接输入类别型变量，需要对类别型变量进行编码(例如独热编码)，而lightGBM可以直接处理类别型变量。</p>
<p>（5）缓存命中率：xgb使用block结构的一个缺点就是取梯度的时候，是通过索引来获取的，而这些梯度的获取顺序是按照特征的大小排序的，这将导致非连续的内存访问，会使缓存命中率低，从而影响算法效率。而LGB是基于直方图分裂特征的，梯度信息都存储在一个个bin中，所以访问梯度是连续的，缓存命中率高。</p>
<p>（6）lightGBM和XGBOOST的并行策略不同：<br><strong>特征并行</strong>：指并行化决策树寻找最佳切分点的过程中，每个worker先寻找局部最佳切分点，之后点对点通信找到全局最佳切分点。<br><strong>XBG特征并行</strong>：XGB每个worker节点中存储<strong>部分特征集</strong>，每个worker先寻找局部最佳切分点，之后worker之间相互通信寻找全局最佳切分点，然后在具有最佳切分点的worker上进行节点分裂，然后广播切分后的左右子树数据结果，其他worker收到结果后也进行划分。<br><strong>LGB特征并行</strong>：每个worker保存了<strong>所有特征集</strong>，找到全局最佳划分点后每个worker可自行划分，<strong>不需要广播划分结果，减小了网络通信量</strong>，但存储代价变高。</p>
<p><strong>数据并行</strong>：指并行化整个决策学习的过程，每个worker中拥有<strong>部分数据</strong>，独立的构建局部直方图，合并后得到全局直方图，在全局直方图中寻找最优切分点进行分裂。<br>XGB：每个worker上先建立起局部的直方图，然后合并成全局的直方图。不同在于根据全局直方图，<strong>各个worker进行节点分裂时会单独计算子节点的样本索引</strong>，因此效率贼低，每个worker间的通信量也就变的很大。<br>LGB：每个worker上先建立起局部的直方图，然后合并成全局的直方图。采用<strong>直方图做差加速</strong>的方式，这个直方图算法使得worker间的通信成本降低一倍，因为只通信一个节点的直方图就能得到兄弟节点的直方图，通信开销降为O(0.5∗#feature∗#bin)。</p>
<blockquote>
<p>投票并行（LGB）：当数据量和维度都很大时，选用投票并行，该方法是数据并行的一个改进。数据并行中的合并直方图的代价相对较大，尤其是当特征维度很大时，大致思想是 每个worker拥有<strong>部分数据集</strong>，独自构建直方图找到局部最优特征，然后进行全局投票，得到全局最优特征进行直方图的合并，再寻求全局的最优分割点。</p>
</blockquote>
<h2 id="超参数调优？"><a href="#超参数调优？" class="headerlink" title="超参数调优？"></a>超参数调优？</h2><p>网格搜索，随机搜索，贝叶斯优化。</p>
<h2 id="模型常用超参数？"><a href="#模型常用超参数？" class="headerlink" title="模型常用超参数？"></a>模型常用超参数？</h2><p><img src="/images/面试/超参数调优.jpg" width="80%"></p>
<h2 id="熵的概念，交叉熵？"><a href="#熵的概念，交叉熵？" class="headerlink" title="熵的概念，交叉熵？"></a>熵的概念，交叉熵？</h2><p>熵：衡量随机变量的不确定性。<br>KL散度：衡量两个概率分布相对差距。等价于交叉熵。<br>交叉熵：估计模型与真实概率分布之间差异。交叉熵越小，假设分布离真实分布越近，模型越好。</p>
<h2 id="交叉熵损失函数？在什么地方使用过？"><a href="#交叉熵损失函数？在什么地方使用过？" class="headerlink" title="交叉熵损失函数？在什么地方使用过？"></a>交叉熵损失函数？在什么地方使用过？</h2><p>逻辑回归：$p(y=1|x)=\hat{y}$，$p(y=0|x)=1-\hat{y}$<br>极大似然：$p(y|x)=\hat{y}^y \cdot (1-\hat{y})^{1-y}$<br>对数极大似然加负号：$p(y|x)= -[ylog(\hat{y}) \cdot (1-y)log(1-\hat{y})]$<br>最小化估计模型与真实概率分布。</p>
<h2 id="交叉熵函数与最大-对数-似然函数的关系和区别？"><a href="#交叉熵函数与最大-对数-似然函数的关系和区别？" class="headerlink" title="交叉熵函数与最大(对数)似然函数的关系和区别？"></a>交叉熵函数与最大(对数)似然函数的关系和区别？</h2><p>交叉熵损失函数：估计模型与真实概率分布之间差异。<br>对数似然函数：衡量在某个参数下，整体的估计和真实情况一样的概率，越大代表越相近。<br><strong>最小化交叉熵的本质就是对数似然函数的最大化</strong>。</p>
<h1 id="深度学习"><a href="#深度学习" class="headerlink" title="深度学习"></a>深度学习</h1><h2 id="神经网络中，有哪些办法防止过拟合？"><a href="#神经网络中，有哪些办法防止过拟合？" class="headerlink" title="神经网络中，有哪些办法防止过拟合？"></a>神经网络中，有哪些办法防止过拟合？</h2><p>Dropout。<br>加L1/L2正则化。<br>Batch Normalization。</p>
<h2 id="Dropout原理？为什么能减少过拟合？"><a href="#Dropout原理？为什么能减少过拟合？" class="headerlink" title="Dropout原理？为什么能减少过拟合？"></a>Dropout原理？为什么能减少过拟合？</h2><p>在训练时，以一定概率随机丢弃一部分神经元，也就是用部分神经元参与训练，类似Bagging思想。</p>
<h2 id="BN和LN的区别？"><a href="#BN和LN的区别？" class="headerlink" title="BN和LN的区别？"></a>BN和LN的区别？</h2><p>BN是对Batch中所有样本的每个特征的标准化，<br>LN是对单个样本所有特征维度进行标准化。</p>
<p>形象点来说，假设有一个二维矩阵。行为batch-size，列为样本特征。那么BN是每列归一化，LN就是每行归一化。<br>一般来说，如果你的特征依赖于不同样本间的统计参数，那BN更有效，因为它抹杀了不同特征之间的大小关系，但是保留了不同样本间的大小关系。（CV领域）<br>而在NLP领域，LN就更加合适。因为它抹杀了不同样本间的大小关系，但是保留了一个样本内不同特征之间的大小关系。对于NLP或者序列任务来说，一条样本的不同特征就是时序上字符取值的变化，样本内的特征关系是非常紧密的。</p>
<p>作用：允许使用更大的学习率，提高收敛速度，加速训练。有一定的抗过拟合作用，使训练过程更加平稳。防止梯度消失，增加网络对数据的敏感度。<br>BN作者还解释了其原理：通过减少内部协变量偏移（internal covariate shift）。即<strong>变量值的分布在训练过程中会发生变</strong>化。<br>论文《How Does Batch Normalization Help Optimization》 2018 Shibani Santurkar etc. 说明 BN 对训练带来的增益与 ICS 的减少没有任何联系，或者说这种联系非常脆弱。研究发现 BN 甚至不会减少 ICS。论文说明 BN 的成功的真正原因是：<strong>它使得优化问题的解空间更加平滑了</strong>。<strong>这确保梯度更具有预测性，从而允许使用更大范围的学习率，实现更快的网络收敛</strong>。</p>
<h2 id="梯度消失、爆炸？如何解决？"><a href="#梯度消失、爆炸？如何解决？" class="headerlink" title="梯度消失、爆炸？如何解决？"></a>梯度消失、爆炸？如何解决？</h2><p>在深层神经网络中：<br><strong>梯度消失</strong>：损失函数，比如sigmoid(梯度不超过0.25，可考虑tanh但梯度也小于1)。<br><strong>梯度爆炸</strong>：损失函数、权重初始化太大。解决方法：<strong>梯度截断</strong>、权重正则化(参数是权重衰退)。</p>
<p>对于激活函数求导大于1，那么随着层数增多，最终求出的梯度更新将以指数形式增加，发生梯度爆炸。如果激活函数求导小于1，那么随着层数增多，最终求出的梯度更新将以指数形式衰减，发生梯度消失。</p>
<p>通用解决方法：<br>（1）换激活函数，比如tanh, relu，elu等。<br>（2）使用BN。消除了输入对梯度的影响。<br>（3）使用残差结构。可以无损的传播梯度。</p>
<h2 id="batchsize大或小有什么问题？"><a href="#batchsize大或小有什么问题？" class="headerlink" title="batchsize大或小有什么问题？"></a>batchsize大或小有什么问题？</h2><p>batchsize大：会使得训练更快(迭代步数少了，有效利用大规模并行)，但是可能过拟合，导致泛化能力下降。解决方案：提高学习率，从而放大梯度噪声的贡献。<br>batchsize小：对于多核架构来讲，太小的batch并不会得训练更快。训练数据就会非常难收敛，从而导致欠拟合。</p>
<p>一般在Batchsize增加的同时，对所有样本的训练次数(epoch)增加。</p>
<h2 id="为什么用激活函数？常用激活函数？优缺点？"><a href="#为什么用激活函数？常用激活函数？优缺点？" class="headerlink" title="为什么用激活函数？常用激活函数？优缺点？"></a>为什么用激活函数？常用激活函数？优缺点？</h2><p>不用激活函数，输出都是输入的线性组合，激活函数通常都是非线性函数，目的是引入非线性性，深度网络可以逼近任意函数。</p>
<p>常用激活函数：<br><strong>sigmiod</strong>：输出范围<code>(0,1)</code>，导数范围<code>(0,0.25)</code>。<br>优点：平滑、易于求导。<br>缺点：反向传播链反应，会出现梯度消失情况；输出不是0均值，这会导致后一层的神经元将得到上一层输出的非0均值的信号作为输入，随着网络的加深，会改变数据的原始分布；幂运算计算量大。<br><strong>tanh</strong>：输出范围<code>(-1,1)</code>，导数范围<code>(0,1)</code>。<br>优点：一定程度解决了Sigmoid函数梯度消失、输出不是0均值问题。<br>缺点：幂运算计算量大。<br><strong>ReLU</strong>：输出范围<code>[0,+inf]</code>，导数为<code>1</code>(正数)/<code>0</code>(负数)。<br>优点：解决了梯度消失问题，提高了运算速度(非幂运算)。<br>缺点：梯度下降强度完全取决于权值的乘积，这样就可能会出现梯度爆炸问题(控制权值范围在<code>[0,1]</code>，或梯度裁剪)。强制将<code>x&lt;0</code>部分的输出置为0(置为0就是屏蔽该特征)，如果学习率设置的太大，就可能会导致网络的大部分神经元处于死亡状态，所以使用ReLU的网络，<strong>学习率不能设置太大</strong>。</p>
<p>为了防止模型的死亡情况，后人将<code>x&lt;0</code>部分并没有直接置为0，而是给了一个很小的负数梯度值$\alpha$。<br><strong>Leaky ReLU</strong>：$\alpha$为常数，一般设置为0.01/0.02。这个函数通常比 Relu 激活函数效果要好，但是效果不是很稳定，所以在实际中 Leaky ReLu 使用的并不多。<br><strong>PReLU(Parametric Relu)</strong>：$\alpha$为可学习的参数，会在训练的过程中进行更新。<br><strong>RReLU(Random ReLU)</strong>：<code>x&lt;0</code>部分的斜率$\alpha$在训练中是随机的，在之后的测试中就变成了固定的了。RReLU的亮点在于，在训练环节中，$\alpha$是从一个均匀的分布<code>U(I,u)</code>中随机抽取的数值。</p>
<p>Sigmoid和tanh的特点是将输出限制在<code>(0,1)</code>和<code>(-1,1)</code>之间，说明Sigmoid和tanh适合做概率值的处理，例如LSTM中的各种门。</p>
<h2 id="常用的优化算法有哪些？各自的优缺点？"><a href="#常用的优化算法有哪些？各自的优缺点？" class="headerlink" title="常用的优化算法有哪些？各自的优缺点？"></a>常用的优化算法有哪些？各自的优缺点？</h2><p><strong>SGD</strong>：每次利用mini-batch样本计算梯度。<br>优点：计算速度快。<br>缺点：更新方向依赖当前mini-batch，不稳定。<br><strong>动量(Momentum)方法</strong>：模拟物体运动惯性，梯度更新时保留一部分之前的更新方向，一定程度上增加稳定性，相当于对梯度做平滑。<br>$v_t=\beta v_{t-1}+g_t$，$w_t=w_{t-1}-\eta v_t$，梯度平滑：$v_t=g_t+\beta g_{t-1}+\beta^2 g_{t-2}+\beta^3 g_{t-3}+…$<br>$\beta$越大则早期的梯度对当前的更新方向的影响越大。$\beta$取值一般为[0.5、0.9、0.99]。<br><strong>Adagrad</strong>：自适应为各个参数分配不同学习率。如果代价函数在某个方向上具有较大的偏导数，学习率会相应降低；反之提高。<br>Adagrad思想：参数空间每个方向的学习率反比于某个值的平方根。这个值就是该方向上梯度分量的所有历史平方值之和。<br>$r_t = r_{t-1}+g_t^2$，$w_t = w_{t-1}-\dfrac{\eta}{\sqrt{r_t}+\epsilon}g_t$，梯度越大，对学习率的惩罚越大。<br>缺点：训练中后期，分母上梯度平方的累加将会越来越大，会使使梯度趋于0，使得训练提前结束。<br><strong>Adadelta</strong>：Adadelta是AdaGrad的一个修改：加权梯度平方和主要由窗口内的梯度决定。动态确定学习率，不需要提前设置全局学习率这一超参数。<br>$r_t = \beta r_{t-1}+ (1-\beta)g_t^2$，$s_t=\alpha s_{t-1}+(1-\alpha)\hat{g_t}^2$，$\hat{g_t}=\dfrac{\sqrt{s_{t-1}+\epsilon}}{\sqrt{r_t}+\epsilon}g_t$，$w_t=w_{t-1}-\hat{g_t}$，初始$s_t$为0。<br><strong>RMSProp</strong>：RMSProp是AdaGrad的一个修改：将梯度累计策略修改为指数加权的移动平均。<br>$r_t = \beta r_{t-1}+ (1-\beta)g_t^2$，$w_t = w_{t-1}-\dfrac{\eta}{\sqrt{r_t}+\epsilon}g_t$<br>实践证明RMSProp是一种有效、实用的深度神经网络优化算法，目前它是深度学习业界经常采用的优化方法之一。<br><strong>动量RMSProp</strong>：结合动量算法对梯度平滑，RMSProp对学习率的平滑。<br>$r_t = \beta_1 r_{t-1}+ (1-\beta_1)g_t^2$，$v_t=\beta_2 v_{t-1}+\dfrac{\eta}{\sqrt{r_t}+\epsilon}g_t$，$w_t = w_{t-1}-v_t$<br><strong>Adam</strong>：另一种动量RMSProp。<br>$r_t = \beta_1 r_{t-1}+ (1-\beta_1)g_t^2$，$v_t=\beta_2 v_{t-1}+(1-\beta_2)g_t$，<br>$\hat{r_t}=\dfrac{r_t}{1-\beta_1^t}$，$\hat{v_t}=\dfrac{v_t}{1-\beta_2^t}$，t为时间步，此过程修正使权重和为1。<br>$w_t = w_{t-1}-\eta \dfrac{\hat{v_t}}{\sqrt{\hat{r_t}}+\epsilon}$</p>
<h2 id="CNN和RNN区别？"><a href="#CNN和RNN区别？" class="headerlink" title="CNN和RNN区别？"></a>CNN和RNN区别？</h2><p>CNN主要用于CV；RNN主要用于NLP。<br>区别就在循环层上。卷积神经网络没有时序性的概念，输入直接和输出挂钩；循环神经网络具有时序性，当前决策还跟上一次决策有关。</p>
<h2 id="LSTM和RNN区别，有什么优化？RNN为什么会有梯度消失，LSTM如何解决梯度消失？"><a href="#LSTM和RNN区别，有什么优化？RNN为什么会有梯度消失，LSTM如何解决梯度消失？" class="headerlink" title="LSTM和RNN区别，有什么优化？RNN为什么会有梯度消失，LSTM如何解决梯度消失？"></a>LSTM和RNN区别，有什么优化？RNN为什么会有梯度消失，LSTM如何解决梯度消失？</h2><p>RNN层数过深会出现梯度消失，无法处理长期依赖问题。<br>RNN本时刻的隐藏层信息只来源于当前输入和上一时刻的隐藏层信息，没有记忆功能。</p>
<p>LSTM通过引入包含了候选记忆单元、输入门(控制采用多少当前候选cell数据)、遗忘门(控制保留多少之前cell数据)、记忆单元(输入门+遗忘门)、输出门(控制隐状态采用多少cell数据)。结构改善了RNN不能长期依赖问题。LSTM就是在RNN的基础上，增加了对过去状态的过滤，从而可以选择哪些状态对当前更有影响，而不是简单的选择最近的状态。<br>RNN激活函数只有tanh；LSTM通过输入门、遗忘门、输出门引入sigmoid函数并结合tanh函数，添加求和操作，减少梯度消失和梯度爆炸的可能性。</p>
<h2 id="LSTM与GRU区别？"><a href="#LSTM与GRU区别？" class="headerlink" title="LSTM与GRU区别？"></a>LSTM与GRU区别？</h2><p>GRU算是简化版的LSTM，GRU只有两个门（重置、更新），LSTM有三个门(遗忘、输入、输出)，GRU直接将隐状态传给下一个单元，而LSTM则用记忆单元把隐状态包装起来。</p>
<h2 id="LSTM与Transformer的区别？"><a href="#LSTM与Transformer的区别？" class="headerlink" title="LSTM与Transformer的区别？"></a>LSTM与Transformer的区别？</h2><p>Transformer整个网络结构完全是由Attention机制组成，前后没有“时序”(利用positional encoding加入词序信息)，可以实现并行计算，更高效；而LSTM是传统的RNN改进结构，有时序的概念，不能并行计算。<br>LSTM引入三个控制门，拥有了长期记忆，更好的解决了RNN的梯度消失和梯度爆炸的问题；而Transformers依然存在顶层梯度消失的问题。<br>LSTM针对长序列依然计算有效；而Transformers针对长序列低效，计算量太大，self.attention的复杂度是n的2次方。</p>
<h2 id="seq2seq是什么结构，transformer呢？"><a href="#seq2seq是什么结构，transformer呢？" class="headerlink" title="seq2seq是什么结构，transformer呢？"></a>seq2seq是什么结构，transformer呢？</h2><p>enncoder-decoder结构，transformer也是这个结构。</p>
<h2 id="Attention介绍，有哪些Attention方法？"><a href="#Attention介绍，有哪些Attention方法？" class="headerlink" title="Attention介绍，有哪些Attention方法？"></a>Attention介绍，有哪些Attention方法？</h2><p>注意力机制模拟了人的行为，比如在欣赏一幅画时看的全貌，关注细节时看的是局部，这说明大脑在处理信号时是有一定权重划分的，注意力机制参考了这点。<br><strong>Bahdanau attention</strong>：也就是additive attention(加注意力)，seq2seq使用这种方式。计算方式是decoder上一时刻的隐状态作为query，encoder所有隐状态作为key，计算socre $s(q,k)=w_v^T tanh(w_q q+ w_k k)$后归一化得到权重，应用在encoder所有隐状态得到上下文向量，最后和decoder上一时刻的隐状态concat输入计算。<br><strong>Luong attention</strong>：计算方式是decoder当前时刻的隐状态作为query，计算方式是decoder上一时刻的隐状态作为query，应用在encoder所有隐状态得到上下文向量，把上下文向量和decoder当前时刻的隐状态作为输入，放入新的循环神经网络得到当前时刻的隐状态和输出。此外还做了全局、局部与concat、dot等方式对齐对比。</p>
<p><span class="exturl" data-url="aHR0cHM6Ly96aHVhbmxhbi56aGlodS5jb20vcC8xMjkzMTY0MTUv">一文看懂 Bahdanau 和 Luong 两种 Attention 机制的区别<i class="fa fa-external-link-alt"></i></span></p>
<h2 id="self-attention比seq2seq的attention优越在哪里（最好结合业务场景说一说）？"><a href="#self-attention比seq2seq的attention优越在哪里（最好结合业务场景说一说）？" class="headerlink" title="self-attention比seq2seq的attention优越在哪里（最好结合业务场景说一说）？"></a>self-attention比seq2seq的attention优越在哪里（最好结合业务场景说一说）？</h2><p>self-attention可以并行计算，更高效。seq2seq的attention只能串行计算。<br>score计算可以是内积($qk$)、带单个参数($qWk$)、两个参数拼接($w_q q+ w_k k$)，感知机($w_v^T tanh(w_q q+ w_k k)$)</p>
<h2 id="Transformer是自回归模型还是自编码模型？"><a href="#Transformer是自回归模型还是自编码模型？" class="headerlink" title="Transformer是自回归模型还是自编码模型？"></a>Transformer是自回归模型还是自编码模型？</h2><p>自回归模型。所谓自回归，即使用当前自己预测的字符再去预测接下来的信息。<br>Transformer在预测阶段（机器翻译任务）会先预测第一个字，然后在第一个预测的字的基础上接下来再去预测后面的字，是典型的自回归模型。<br>Bert中的Mask任务是典型的自编码模型，即根据上下文字符来预测当前信息。</p>
<h2 id="transformer的position-embedding作用？有什么意义和优缺点？"><a href="#transformer的position-embedding作用？有什么意义和优缺点？" class="headerlink" title="transformer的position embedding作用？有什么意义和优缺点？"></a>transformer的position embedding作用？有什么意义和优缺点？</h2><p>因为self-attention是位置无关的，无论句子的顺序是什么样的，通过self-attention计算的token的embedding都是一样的。模型需要表达出一个token的位置信息，transformer使用了固定的位置编码来表示token在句子中的位置信息。</p>
<p>行为token，列为token维度，从一行(pos)看，开始每列使用了周期短的sin和cos作为编码，随着行（pos）的增加，其列的周期变长，即每一个token都有独一无二的位置编码。<br>而用sin和cos是因为具有周期性，这样可以让模型关注token的相对位置信息。位置i+detle 可以用i的线性变换（参数w）得到，在测试集遇到过长句子也能通过训练集pos计算，增强了泛化能力。</p>
<p>位置编码特点：<br>它能为每个时间步输出一个独一无二的编码。<br>不同长度的句子之间，任何两个时间步之间的距离应该保持一致。<br>模型应该能毫不费力地泛化到更长的句子。它的值应该是有界的。<br>它必须是确定性的。</p>
<h2 id="你还了解哪些关于位置编码的技术，各自的优缺点是什么？"><a href="#你还了解哪些关于位置编码的技术，各自的优缺点是什么？" class="headerlink" title="你还了解哪些关于位置编码的技术，各自的优缺点是什么？"></a>你还了解哪些关于位置编码的技术，各自的优缺点是什么？</h2><p>相对位置信息是在self-attention计算时候丢失的（计算attention score的wq和wk增加了非线性导致相对位置信息丢失），那么最直接的想法就是在计算self-attention的时候再加回来。</p>
<p>1.在计算attention score和weighted value时各加入一个可训练的表示相对位置的参数。相对位置编码(RPE)。<br>2.在生成多头注意力时，把对key来说将绝对位置转换为相对query的位置。<br>3.复数域函数，已知一个词在某个位置的词向量表示，可以计算出它在任何位置的词向量表示。前两个方法是词向量+位置编码，属于亡羊补牢，复数域是生成词向量的时候即生成对应的位置信息。</p>
<h2 id="Transformer为何使用多头注意力机制？（为什么不使用一个头）"><a href="#Transformer为何使用多头注意力机制？（为什么不使用一个头）" class="headerlink" title="Transformer为何使用多头注意力机制？（为什么不使用一个头）"></a>Transformer为何使用多头注意力机制？（为什么不使用一个头）</h2><p>多头目的是对同一 qkv 希望抽取不同的信息，比如我希望一个头抽取长信息，一个头抽取短信息。类似卷积的多输出通道。<br>多头可以使参数矩阵形成多个子空间，矩阵整体的size不变，只是改变了每个head对应的维度大小，但是计算量和单个head差不多。</p>
<h2 id="Transformer为什么Q和K使用不同的权重矩阵生成，为何不能使用同一个值进行自身的点乘？"><a href="#Transformer为什么Q和K使用不同的权重矩阵生成，为何不能使用同一个值进行自身的点乘？" class="headerlink" title="Transformer为什么Q和K使用不同的权重矩阵生成，为何不能使用同一个值进行自身的点乘？"></a>Transformer为什么Q和K使用不同的权重矩阵生成，为何不能使用同一个值进行自身的点乘？</h2><p>（1）不同的权重是为了解决可能输入句长与输出句长不一致的问题。<br>（2）K和Q使用了不同的W_k, W_Q来计算，可以理解为是在不同空间上的投影。正因为有了这种不同空间的投影，增加了表达能力，这样计算得到的attention score矩阵的泛化能力更高。<br>（3）并且假如QK维度一致，如果不用Q，直接拿K和K点乘的话，你会发现attention score 矩阵是一个对称矩阵。<br>因为是同样一个矩阵，都投影到了同样一个空间，所以泛化能力很差。</p>
<h2 id="Transformer中，为什么在进行softmax之前需要对attention进行scaled（为什么除以dk的平方根）？并使用公式推导进行讲解"><a href="#Transformer中，为什么在进行softmax之前需要对attention进行scaled（为什么除以dk的平方根）？并使用公式推导进行讲解" class="headerlink" title="Transformer中，为什么在进行softmax之前需要对attention进行scaled（为什么除以dk的平方根）？并使用公式推导进行讲解"></a>Transformer中，为什么在进行softmax之前需要对attention进行scaled（为什么除以dk的平方根）？并使用公式推导进行讲解</h2><p>当dk很大时，Q和K的点积结果会变得很大，容易落入到 softmax 函数的饱和区间导致反向传播时梯度很小，容易发生梯度消失。</p>
<h2 id="Transformer中的mask机制有什么作用-计算attention-score的时候如何对padding做mask操作？"><a href="#Transformer中的mask机制有什么作用-计算attention-score的时候如何对padding做mask操作？" class="headerlink" title="Transformer中的mask机制有什么作用?计算attention score的时候如何对padding做mask操作？"></a>Transformer中的mask机制有什么作用?计算attention score的时候如何对padding做mask操作？</h2><p>作用：对不等长的序列做padding补齐。掩码防止信息泄露。<br>使用位置：mask机制的作用1在三个多头自注意力层中都用了，作用2只用在了解码器的第一个多头自注意力层。<br>方法：对需要mask的位置设为负无穷，再对attention score进行相加</p>
<h2 id="为什么在进行多头注意力的时候需要对每个head进行降维？"><a href="#为什么在进行多头注意力的时候需要对每个head进行降维？" class="headerlink" title="为什么在进行多头注意力的时候需要对每个head进行降维？"></a>为什么在进行多头注意力的时候需要对每个head进行降维？</h2><p>将原有的高维空间转化为多个低维空间并再最后进行拼接，形成同样维度的输出，借此丰富特性信息，降低了计算量。</p>
<h2 id="大概讲一下Transformer的Encoder模块？"><a href="#大概讲一下Transformer的Encoder模块？" class="headerlink" title="大概讲一下Transformer的Encoder模块？"></a>大概讲一下Transformer的Encoder模块？</h2><p>输入嵌入-加上位置编码-多个编码器层（每个编码器层包含全连接层，多头注意力层和点式前馈网络层（包含激活函数层））</p>
<h2 id="为何在获取输入词向量之后需要对矩阵乘以embedding-size的开方？"><a href="#为何在获取输入词向量之后需要对矩阵乘以embedding-size的开方？" class="headerlink" title="为何在获取输入词向量之后需要对矩阵乘以embedding size的开方？"></a>为何在获取输入词向量之后需要对矩阵乘以embedding size的开方？</h2><p>embedding matrix的初始化方式是xavier init，这种方式的方差是$\dfrac{1}{embedding\_size}$，因此embedding matrix乘以$\sqrt{embedding\_size}$使得embedding matrix的方差是1，在这个scale下可能更有利于embedding matrix的收敛。</p>
<h2 id="权重初始化？"><a href="#权重初始化？" class="headerlink" title="权重初始化？"></a>权重初始化？</h2><p>正态分布（Normal）、均匀分布（Uniform）和截断正态分布（Truncated Normal）。<br>BERT默认的初始化方法是标准差为0.02的截断正态分布。相比Xavier初始化这个标准差偏小，这会使输出整体偏小，不易梯度消失。但太小的标准差会使模型丧失多样性。</p>
<p><strong>Xavier</strong>：尽可能的让输入和输出服从相同的分布$N(0, \dfrac{1}{embedding\_size})$，这样就能够避免后面层的激活函数的输出值趋向于0。在sigmoid和tanh激活函数上有很好的表现，但是在Relu激活函数上表现很差(relu层会将负数映射到0，影响整体方差)。<br><strong>Kaiming</strong>：因为relu会抛弃掉小于0的值，对于一个均值为0的data来说，这就相当于砍掉了一半的值，均值就会变大。将每个随机选择的数字乘$\sqrt{2/embedding\_size}$。对于其他层权重初始化，$embedding\_size$就是每层输入维度$d_{model}$。<br><span class="exturl" data-url="aHR0cHM6Ly96aHVhbmxhbi56aGlodS5jb20vcC82Mjg1MDI1OA==">神经网络中的权重初始化一览：从基础到Kaiming<i class="fa fa-external-link-alt"></i></span><br><span class="exturl" data-url="aHR0cHM6Ly96aHVhbmxhbi56aGlodS5jb20vcC80NDI1MDk2MDI=">word embedding 输入为什么要乘以 embedding size的开方<i class="fa fa-external-link-alt"></i></span></p>
<h2 id="简单讲一下Transformer中的残差结构以及意义？"><a href="#简单讲一下Transformer中的残差结构以及意义？" class="headerlink" title="简单讲一下Transformer中的残差结构以及意义？"></a>简单讲一下Transformer中的残差结构以及意义？</h2><p>encoder和decoder的self-attention层和ffn层都有残差连接。反向传播的时候不会造成梯度消失。</p>
<h2 id="Transformer在训练与验证的时候有什么不同？"><a href="#Transformer在训练与验证的时候有什么不同？" class="headerlink" title="Transformer在训练与验证的时候有什么不同？"></a>Transformer在训练与验证的时候有什么不同？</h2><p>Transformer在训练的时候是并行的，在验证的时候是串行的。这个问题与Transformer是否是自回归模型考察的是同一个知识点。</p>
<h2 id="Transformer模型的计算复杂度是多少？"><a href="#Transformer模型的计算复杂度是多少？" class="headerlink" title="Transformer模型的计算复杂度是多少？"></a>Transformer模型的计算复杂度是多少？</h2><p>n是序列长度，d是embedding的长度。Transformer中最大的计算量就是多头自注意力层，这里的计算量主要就是QK相乘再乘上V，即两次矩阵相乘。QK相乘是矩阵(n,d)乘以(d,n)，这个复杂度就是O(n^2 d)。</p>
<h2 id="Transformer中三个多头自注意力层分别有什么意义与作用？"><a href="#Transformer中三个多头自注意力层分别有什么意义与作用？" class="headerlink" title="Transformer中三个多头自注意力层分别有什么意义与作用？"></a>Transformer中三个多头自注意力层分别有什么意义与作用？</h2><p>Transformer中有三个多头自注意力层，编码器中有一个，解码器中有两个。<br>编码器中的多头自注意力层的作用是将原始文本序列信息做整合，转换后的文本序列中每个字符都与整个文本序列的信息相关.<br>解码器的第一个多头自注意力层比较特殊，原论文给其起名叫Masked Multi-Head-Attention。即对输入文本做整合（对与翻译任务来说，编码器的输入是翻译前的文本，解码器的输入是翻译后的文本）。另一个任务是做掩码，防止信息泄露。拓展解释一下就是在做信息整合的时候，第一个字符其实不应该看到后面的字符，第二个字符也只能看到第一个、第二个字符的信息，以此类推。<br>解码器的第二个多头自注意力层与编码器的第一个多头自注意力层功能是完全一样的。不过输入需要额外强调下，我们都知道多头自注意力层是通过计算QKV三个矩阵最后完成信息整合的。在这里，Q是解码器整合后的信息，KV两个矩阵是编码器整合后的信息，是两个完全相同的矩阵。QKV矩阵相乘后，翻译前与翻译后的文本也做了充分的交互整合。至此最终得到的向量矩阵用来做后续下游工作。</p>
<h2 id="为什么transformer块使用LayerNorm而不是BatchNorm？LayerNorm-在Transformer的位置是哪里？"><a href="#为什么transformer块使用LayerNorm而不是BatchNorm？LayerNorm-在Transformer的位置是哪里？" class="headerlink" title="为什么transformer块使用LayerNorm而不是BatchNorm？LayerNorm 在Transformer的位置是哪里？"></a>为什么transformer块使用LayerNorm而不是BatchNorm？LayerNorm 在Transformer的位置是哪里？</h2><p>多头注意力层和激活函数层之间。CV使用BN是认为channel维度的信息对cv方面有重要意义，如果对channel维度也归一化会造成不同通道信息一定的损失。而同理nlp领域认为句子长度不一致，并且各个batch的信息没什么关系，因此只考虑句子内信息的归一化，也就是LN。</p>
<h2 id="简单描述一下Transformer中的前馈神经网络？使用了什么激活函数？相关优缺点？"><a href="#简单描述一下Transformer中的前馈神经网络？使用了什么激活函数？相关优缺点？" class="headerlink" title="简单描述一下Transformer中的前馈神经网络？使用了什么激活函数？相关优缺点？"></a>简单描述一下Transformer中的前馈神经网络？使用了什么激活函数？相关优缺点？</h2><p>输入嵌入-加上位置编码-多个编码器层（每个编码器层包含全连接层，多头注意力层和点式前馈网络层（包含激活函数层））-多个解码器层（每个编码器层包含全连接层，多头注意力层和点式前馈网络层）-全连接层，使用了relu激活函数。</p>
<h2 id="Encoder端和Decoder端是如何进行交互的？"><a href="#Encoder端和Decoder端是如何进行交互的？" class="headerlink" title="Encoder端和Decoder端是如何进行交互的？"></a>Encoder端和Decoder端是如何进行交互的？</h2><p>通过encoder-decoder attention交互，encoder输出作为key和value，decoder的masked muti-head attention输出作为query。</p>
<h2 id="Decoder阶段的多头自注意力和encoder的多头自注意力有什么区别？"><a href="#Decoder阶段的多头自注意力和encoder的多头自注意力有什么区别？" class="headerlink" title="Decoder阶段的多头自注意力和encoder的多头自注意力有什么区别？"></a>Decoder阶段的多头自注意力和encoder的多头自注意力有什么区别？</h2><p>Decoder是masked，Encoder是非masked。Encoder考虑双向信息，decoder考虑单向信息。</p>
<h2 id="Transformer的并行化提现在哪个地方？Decoder端可以做并行化吗？"><a href="#Transformer的并行化提现在哪个地方？Decoder端可以做并行化吗？" class="headerlink" title="Transformer的并行化提现在哪个地方？Decoder端可以做并行化吗？"></a>Transformer的并行化提现在哪个地方？Decoder端可以做并行化吗？</h2><p>Transformer的并行化主要体现在self-attention模块，在Encoder端Transformer可以并行处理整个序列，并得到整个输入序列经过Encoder端的输出，但是rnn只能从前到后的执行。<br>Decoder端训练时可以并行，因为使用teacher forcing方式，预测时不行，需要上一时刻输出。</p>
<h2 id="Transformer训练的时候学习率是如何设定的？Dropout位置在哪里？Dropout-在测试的需要有什么需要注意的吗？"><a href="#Transformer训练的时候学习率是如何设定的？Dropout位置在哪里？Dropout-在测试的需要有什么需要注意的吗？" class="headerlink" title="Transformer训练的时候学习率是如何设定的？Dropout位置在哪里？Dropout 在测试的需要有什么需要注意的吗？"></a>Transformer训练的时候学习率是如何设定的？Dropout位置在哪里？Dropout 在测试的需要有什么需要注意的吗？</h2><p>Transformer学习率是warmup的(先从小到原本学习率，再减少学习率)，原因是模型的权重(weights)是随机初始化的，此时若选择一个较大的学习率,可能带来模型的不稳定(振荡)，选择Warmup预热学习率的方式，可以使得开始训练的几个epoches内学习率较小,在预热的小学习率下，模型可以慢慢趋于稳定,等模型相对稳定后再选择预先设置的学习率进行训练,使得模型收敛速度变得更快，模型效果更佳。</p>
<p>dropout是为了解决过拟合的问题，加在LN之后，测试时dropout去掉。</p>
<h2 id="bert的mask为何不学习transformer在attention处进行屏蔽score的技巧？"><a href="#bert的mask为何不学习transformer在attention处进行屏蔽score的技巧？" class="headerlink" title="bert的mask为何不学习transformer在attention处进行屏蔽score的技巧？"></a>bert的mask为何不学习transformer在attention处进行屏蔽score的技巧？</h2><p>BERT和transformer的目标不一致，bert是语言的预训练模型，需要充分考虑上下文的关系，而transformer主要考虑句子中第i个元素与前i-1个元素的关系。</p>
<h2 id="Transformer在哪里做了权重共享，为什么可以做权重共享？"><a href="#Transformer在哪里做了权重共享，为什么可以做权重共享？" class="headerlink" title="Transformer在哪里做了权重共享，为什么可以做权重共享？"></a>Transformer在哪里做了权重共享，为什么可以做权重共享？</h2><p>Transformer在两个地方进行了权重共享：<br>（1）Encoder和Decoder间的Embedding层权重共享；<br>（2）Decoder中Embedding层和FC层权重共享。</p>
<p>对于（1），《Attention is all you need》中Transformer被应用在机器翻译任务中，源语言和目标语言是不一样的，但它们可以共用一张大词表，对于两种语言中共同出现的词（比如：数字，标点等等）可以得到更好的表示，而且对于Encoder和Decoder，嵌入时都只有对应语言的embedding会被激活，因此是可以共用一张词表做权重共享的。论文中，Transformer词表用了bpe来处理，所以最小的单元是subword。英语和德语同属日耳曼语族，有很多相同的subword，可以共享类似的语义。而像中英这样相差较大的语系，语义共享作用可能不会很大。但是，共用词表会使得词表数量增大，增加softmax的计算时间，因此实际使用中是否共享可能要根据情况权衡。</p>
<p>对于（2），Embedding层可以说是通过onehot去取到对应的embedding向量，FC层可以说是相反的，通过向量（定义为 x）去得到它可能是某个词的softmax概率，取概率最大（贪婪情况下）的作为预测值。那哪一个会是概率最大的呢？在FC层的每一行量级相同的前提下，理论上和 x 相同的那一行对应的点积和softmax概率会是最大的（可类比本文问题1）。因此，Embedding层和FC层权重共享，Embedding层中和向量 x 最接近的那一行对应的词，会获得更大的预测概率。实际上，Decoder中的Embedding层和FC层有点像互为逆过程。通过这样的权重共享可以减少参数的数量，加快收敛。</p>
<h2 id="pre-norm和post-norm的区别？"><a href="#pre-norm和post-norm的区别？" class="headerlink" title="pre-norm和post-norm的区别？"></a>pre-norm和post-norm的区别？</h2><p>Add之后做layer normalization的方式叫做post-norm(transformer这么做的)。反之叫pre-norm。<br>目前比较明确的结论是：同一设置之下，Pre Norm结构往往更容易训练，但最终效果通常不如Post Norm。<br>Pre Norm结构无形地增加了模型的宽度而降低了模型的深度，而我们知道深度通常比宽度更重要，所以是无形之中的降低深度导致最终效果变差了。<br>Post Norm每Norm一次就削弱一次恒等分支的权重，所以Post Norm反而是更突出残差分支的，因此Post Norm中的层数更加“足秤”，一旦训练好之后效果更优。<br><span class="exturl" data-url="aHR0cHM6Ly93d3cua2V4dWUuZm0vYXJjaGl2ZXMvOTAwOQ==">为什么Pre Norm的效果不如Post Norm？<i class="fa fa-external-link-alt"></i></span></p>
<h2 id="绝对位置和相对位置的区别？"><a href="#绝对位置和相对位置的区别？" class="headerlink" title="绝对位置和相对位置的区别？"></a>绝对位置和相对位置的区别？</h2><p>绝对位置：不一样位置positional embedding不一样。<br>相对位置：位置1和位置2的距离，位置3和位置4的距离应该相等。<br>一般self-attention模型要引这两个位置信息，transformer用的的是sin-cos，函数式编码。</p>
<h2 id="position-embedding的实现方式有哪两种？"><a href="#position-embedding的实现方式有哪两种？" class="headerlink" title="position embedding的实现方式有哪两种？"></a>position embedding的实现方式有哪两种？</h2><p>functional position embedding，如transformer。<br>parametric position embedding，如bert。<br><span class="exturl" data-url="aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3hpeGlhb3lhb3d3L2FydGljbGUvZGV0YWlscy8xMDU0NTkzNzY/b3BzX3JlcXVlc3RfbWlzYz0lMjU3QiUyNTIycmVxdWVzdCUyNTVGaWQlMjUyMiUyNTNBJTI1MjIxNjU0MjI1OTA1MTY3ODI0MjUxNzU4ODMlMjUyMiUyNTJDJTI1MjJzY20lMjUyMiUyNTNBJTI1MjIyMDE0MDcxMy4xMzAxMDIzMzQucGMlMjU1RmJsb2cuJTI1MjIlMjU3RCZhbXA7cmVxdWVzdF9pZD0xNjU0MjI1OTA1MTY3ODI0MjUxNzU4ODMmYW1wO2Jpel9pZD0wJmFtcDt1dG1fbWVkaXVtPWRpc3RyaWJ1dGUucGNfc2VhcmNoX3Jlc3VsdC5ub25lLXRhc2stYmxvZy0yfmJsb2d+Zmlyc3RfcmFua19lY3BtX3YxfnJhbmtfdjMxX2VjcG0tMS0xMDU0NTkzNzYtbnVsbC1udWxsLm5vbmVjYXNlJmFtcDt1dG1fdGVybT1wb3NpdGlvbmFsK2VuY29kaW5nJUU0JUJEJThEJUU3JUJEJUFFJUU3JUJDJTk2JUU3JUEwJTgxJUU4JUFGJUE2JUU4JUE3JUEzJmFtcDtzcG09MTAxOC4yMjI2LjMwMDEuNDQ1MA==">positional encoding位置编码详解：绝对位置与相对位置编码对比<i class="fa fa-external-link-alt"></i></span></p>
<h2 id="讲一下Bert原理？"><a href="#讲一下Bert原理？" class="headerlink" title="讲一下Bert原理？"></a>讲一下Bert原理？</h2><p>BERT的全称是Bidirectional Encoder Representation from Transformers，即双向Transformer的Encoder，因为decoder是不能获得要预测的信息的。模型的主要创新点都在pre-train方法上，BERT的预训练阶段包括两个任务，一个是Masked Language Model，还有一个是Next Sentence Prediction，两种方法分别捕捉词语和句子级别的特征表示。</p>
<p>Masked Language Model：MLM可以理解为完形填空，作者会随机mask每一个句子中15%的词，用其上下文来做预测。<br>Next Sentence Prediction：选择一些句子对A与B，其中50%的数据B是A的下一条句子，剩余50%的数据B是语料库中随机选择的，学习其中的相关性，添加这样的预训练的目的是目前很多NLP的任务比如QA和NLI都需要理解两个句子之间的关系，从而能让预训练的模型更好的适应这样的任务。</p>
<h2 id="Bert的MLM预训练任务mask的目的是什么？"><a href="#Bert的MLM预训练任务mask的目的是什么？" class="headerlink" title="Bert的MLM预训练任务mask的目的是什么？"></a>Bert的MLM预训练任务mask的目的是什么？</h2><p>让模型学习一个句子中词与词之间的关系。</p>
<h2 id="bert及其变体中常用的mask方式及特点？"><a href="#bert及其变体中常用的mask方式及特点？" class="headerlink" title="bert及其变体中常用的mask方式及特点？"></a>bert及其变体中常用的mask方式及特点？</h2><p>dynamic mask(RoBerta，每个epoch中同一个样本mask结果不一样)，whole word mask(bert-wwm)，phrase mask(ERNIE)， entity mask(ERNIE)。</p>
<h2 id="Bert-采用哪种Normalization结构，LayerNorm结构有参数吗，参数的作用？"><a href="#Bert-采用哪种Normalization结构，LayerNorm结构有参数吗，参数的作用？" class="headerlink" title="Bert 采用哪种Normalization结构，LayerNorm结构有参数吗，参数的作用？"></a>Bert 采用哪种Normalization结构，LayerNorm结构有参数吗，参数的作用？</h2><p>采用LayerNorm结构，和BatchNorm的区别主要是做规范化的维度不同。<br>LayerNorm有参数，引入了b再平移参数和w再放缩参数。目的是为了恢复原始数据分布。</p>
<h2 id="如何优化BERT效果？"><a href="#如何优化BERT效果？" class="headerlink" title="如何优化BERT效果？"></a>如何优化BERT效果？</h2><p>个人认为最有效的方式是数据扩增。<br>其次加一些网络结构，比如attention layer等。</p>
<h2 id="Bert和Albert区别？"><a href="#Bert和Albert区别？" class="headerlink" title="Bert和Albert区别？"></a>Bert和Albert区别？</h2><p>1.在Albert中，综合了embedding table和transformer的用途不同，以及为了实现更深的模型结构，通过矩阵分解的方式对embedding table和transformer进行了解耦。embedding table的向量大小不再与hidden size相同，即one hot向量（字符或者token）不再会通过embedding table 直接映射到H（hidden size）的隐向量空间，而是首先映射到一个低维空间E（E远小于H），然后再由E映射到H的向量空间。计算复杂度由O(VxH)变为O(VxE+ExH)。<br>2.transformer是利用self-attention (Multi-Head attention)和FFW的堆叠结构，随着模型结构的深化即堆叠层数更多，模型参数会更多。Albert中，通过采用transformer中层间参数共享的方法，在增加模型深度的同时，降低了模型参数量，有助于提高模型训练速度和减少内存空间占用。<br>3.在bert中，通过预测next sentence prediction 学习句子之间的相关性。构造sentence pair的负样本时，从不同document随机选取。负样本的构造过于简单，使得NSP的模型任务的学习能力有限，有研究证明去除NSP的预训练语言模型反而使得下游任务效果更好。鉴于NSP的任务过于简单，对模型的能力提高帮助有限，Albert中提出了SOP任务（sentence order prediction）用于学习句子之间的相关性。</p>
<h2 id="Bert和FinBert区别？"><a href="#Bert和FinBert区别？" class="headerlink" title="Bert和FinBert区别？"></a>Bert和FinBert区别？</h2><p>项目中使用FinBERT 1.0，和Bert结构相同，FinBERT在预训练上使用的是金融类新闻、公告、财报、百科词条等数据。<br>任务和Bert相同，MLM和NSP，但MLM使用了wwm。<br>采用Whole Word Masking (wwm)，一般翻译为全词 Mask 或整词 Mask。在谷歌原生的中文 BERT 中，输入是以字为粒度进行切分，没有考虑到领域内共现单词或词组之间的关系，从而无法学习到领域内隐含的先验知识，降低了模型的学习效果。我们将全词Mask的方法应用在金融领域语料预训练中，即对组成的同一个词的汉字全部进行Mask。<br>引入词组和语义级别任务，并提取领域内的专有名词或词组，采用全词 Mask的掩盖方式以及两类有监督任务进行预训练<br>为了更充分的利用预训练语料，采用类似Roberta模型的动态掩盖mask机制，将dupe-factor参数设置为10。<br>Tensorflow XLA 和 Automatic Mixed Precision 这两类技术进行预训练加速。</p>
<h2 id="elmo、GPT、bert三者之间有什么区别？"><a href="#elmo、GPT、bert三者之间有什么区别？" class="headerlink" title="elmo、GPT、bert三者之间有什么区别？"></a>elmo、GPT、bert三者之间有什么区别？</h2><p>word2vec、Glove词向量均是静态的词向量，无法解决一次多义等问题。elmo、GPT、bert词向量，它们都是基于语言模型的<strong>动态词向量</strong>(在ELMo 中每个单词的词向量不再是固定的，而是单词所在的句子的函数，由单词所在的上下文决定。因此ELMo 词向量可以解决多义词问题)。<br>特征提取器：<br>elmo采用LSTM进行提取，GPT和bert则采用Transformer进行提取。Transformer特征提取能力强于LSTM。</p>
<p>单/双向语言模型：<br>GPT采用单向语言模型(Transformer-decoder)，elmo和bert(Transformer-encoder)采用双向语言模型。但是elmo实际上是两个单向语言模型（方向相反）的拼接，这种融合特征的能力比bert一体化融合特征方式弱。</p>
<p>BERT 比 ELMo 效果好的原因：<br>LSTM 抽取特征的能力远弱于 Transformer。<br>拼接方式双向融合的特征融合能力偏弱。<br>BERT 的训练数据以及模型参数均多于 ELMo。</p>
<h2 id="Siamese结构什么样？"><a href="#Siamese结构什么样？" class="headerlink" title="Siamese结构什么样？"></a>Siamese结构什么样？</h2><p>Siamese LSTM孪生神经网络，第一层由两个平行的双向LSTM构成的特征提取部分，第二部分由单层或单层全连接层构成的分类/拟合层，最后使用距离公式来对比Gw(X1)与Gw(X2)两个向量的距离（可以使用欧式距离，余弦距离）。<br>总之，用来对比两个input的相似程度，(0,1]代表完全不相似到非常相似。<br>改进：<br>1.单选可以选择GRU，训练速度会更快（其实也快不了多少）。<br>2.语料大的时候选bilstm，小的时候选GRU。<br>3.可以在全连接层输出做降维。<br>note: 训练时两个单元权重不共享（权重共享需要统一两个句子的长度），则全连接层需要使用加和取平均的方式，若只是采用全连接，则Q1Q2交换位置后，与输入Q1Q2结果不一致。</p>
<h2 id="蒸馏的思想，为什么要蒸馏？"><a href="#蒸馏的思想，为什么要蒸馏？" class="headerlink" title="蒸馏的思想，为什么要蒸馏？"></a>蒸馏的思想，为什么要蒸馏？</h2><p>蒸馏目的是让模型的体积更小、速度更快，能耗更低。<br>模型压缩方法：剪裁(不重要的网络层、节点删掉)，量化(float32精度变成int8)，蒸馏(老师教学生，大模型是老师，小模型是学生)，神经网络架构搜索(网络结构优化)。</p>
<h2 id="有哪些蒸馏方式"><a href="#有哪些蒸馏方式" class="headerlink" title="有哪些蒸馏方式?"></a>有哪些蒸馏方式?</h2><p>DistillBERT：<br>（1）结构和BERT类似，只是layer的数量减半。<br>（2）采用了RoBERTa的优化策略，动态mask，增大batch size<br>（3）训练时损失：学生模型与真实标签之间的交叉熵；学生模型与教师模型的输出之间的交叉熵；学生模型和教师模型最后一层隐状态余弦相似度。</p>
<p>TinyBERT：<br>（1）结构和transformer类似，但layer的数量减少。<br>（2）分预训练、fine-tuning两个阶段蒸馏。<br>（3）训练时损失：transformer(包括attention和hidden states损失)、embedding、prediction。</p>
<h2 id="softmax-temperature蒸馏中作用？"><a href="#softmax-temperature蒸馏中作用？" class="headerlink" title="softmax-temperature蒸馏中作用？"></a>softmax-temperature蒸馏中作用？</h2><script type="math/tex; mode=display">p_i=\dfrac{exp(z_i/T)}{\sum_j exp(z_i/T)}</script><p>其中，$T$控制输出分布的平滑度，$T=1$时代表传统的softmax，$T<1$时分布逐渐极端化，最终等价于argmax，$T>1$分布逐渐趋于均匀分布。当$T$变大时，类别之间的差距变小(平滑)，从而导致loss变小；当$T$变小时类别间的差距变大(陡峭)，从而导致loss变小。</p>
<p>由于训练好的模型本身会出现<strong>过度自信</strong>的问题，所以除以一个大于1的T，让分布变得平滑，降低过度自信。</p>
<h2 id="Beam-Search？缺点？优化？"><a href="#Beam-Search？缺点？优化？" class="headerlink" title="Beam Search？缺点？优化？"></a>Beam Search？缺点？优化？</h2><p>相比greedy search更合理，每次维护k个最佳候选。<br>缺点：<br>（1）<strong>数据下溢</strong>：求序列概率的时候，序列概率是多个条件概率的乘积，每个概率都小于1甚至远远小于1，很多概率相乘起来，会得到很小很小的数字，会造成数据下溢，即数值太小，计算机的浮点表示不能精确储存。<br>（2）<strong>倾向于生成短的序列</strong>：生成的句子序列越长，概率值相乘（或者对数概率相加）的结果就越小，所以倾向于生成短序列。<br>（3）<strong>单一性问题</strong>：k个最佳候选差异性很小，无法体现语言的多样性。<br>优化：<br>（1）<strong>概率取log值</strong>。取log不会影响排序结果，但是在数值上会更稳定，不容易出现数据下溢。<br>（2）<strong>对序列长度进行惩罚</strong>，降低生成短序列的倾向，对于概率值相乘（或者对数概率相加）的结果，除以序列长度$L^{\alpha}$，$\alpha \in (0,1)$就是在完全归一化和没有归一化之间。<br>（3）可分组 加入<strong>相似性惩罚</strong>(diverse beam search)。<br><span class="exturl" data-url="aHR0cHM6Ly9iYWlqaWFoYW8uYmFpZHUuY29tL3M/aWQ9MTY3NjE5MzQ0NjMxMzU5ODk3Ng==">详细介绍 Beam Search 及其优化方法<i class="fa fa-external-link-alt"></i></span><br><span class="exturl" data-url="aHR0cHM6Ly96aHVhbmxhbi56aGlodS5jb20vcC80MzcwMzEzNg==">文本生成解码之 Beam Search<i class="fa fa-external-link-alt"></i></span></p>
<h2 id="如何做语义相似度的？"><a href="#如何做语义相似度的？" class="headerlink" title="如何做语义相似度的？"></a>如何做语义相似度的？</h2><p>simcse，simbert(追一科技)，sentence bert<br><span class="exturl" data-url="aHR0cHM6Ly96aHVhbmxhbi56aGlodS5jb20vcC80NzY0Mjk1NDU=">大道至简：SimCSE介绍及注意事项<i class="fa fa-external-link-alt"></i></span><br><span class="exturl" data-url="aHR0cHM6Ly9zcGFjZXMuYWMuY24vYXJjaGl2ZXMvNzQyNw==">鱼与熊掌兼得：融合检索和生成的SimBERT模型<i class="fa fa-external-link-alt"></i></span>，<span class="exturl" data-url="aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTMyNTA4NjEvYXJ0aWNsZS9kZXRhaWxzLzEyMzY0OTA0Nw==">SimBERT<i class="fa fa-external-link-alt"></i></span></p>
<p>Triplet Loss是深度学习中的一种损失函数，用于训练差异性较小的样本，如人脸等， Feed数据包括锚（Anchor）示例、正（Positive）示例、负（Negative）示例，通过优化锚示例与正示例的距离小于锚示例与负示例的距离，实现样本的相似性计算。<span class="exturl" data-url="aHR0cHM6Ly93d3cuamlhbnNodS5jb20vcC80NmM2ZjY4MjY0YTE=">Triplet Loss 损失函数<i class="fa fa-external-link-alt"></i></span></p>
<h2 id="分类任务中会用到哪些loss-function？"><a href="#分类任务中会用到哪些loss-function？" class="headerlink" title="分类任务中会用到哪些loss function？"></a>分类任务中会用到哪些loss function？</h2><p>交叉熵损失、0-1损失(分类正确数量)、Focal loss等。</p>
<h2 id="神经网络训练出现nan，应该从哪些方面去查？"><a href="#神经网络训练出现nan，应该从哪些方面去查？" class="headerlink" title="神经网络训练出现nan，应该从哪些方面去查？"></a>神经网络训练出现nan，应该从哪些方面去查？</h2><p>nan的错误多源于<strong>学习率太大</strong>或者<strong>batch size太大</strong>，可以不断的10倍减小学习率直到nan错误不出现。</p>
<p><strong>梯度爆炸</strong>：这个错误是因为logits输出太大变成INF，对这个取log就会在求梯度就会变成nan。<strong>梯度截断</strong>来解决。<br><strong>损失函数</strong>：有时候损失层中loss的计算可能导致NaN的出现。比如，给信息熵损失的输入没有归一化的值，使用带有bug的自定义损失层，<strong>0作为除数</strong>。观测训练产生的log时一开始并不能看到异常，loss也在逐步的降低，但突然之间NaN就出现了。</p>
<p><strong>输入中有nan</strong>：输入中就含有NaN，每当学习的过程中碰到这个错误的输入，就会变成NaN。观察log的时候也许不能察觉任何异常，loss逐步的降低，但突然间就变成NaN了。</p>
<h2 id="loss不下降可能是什么情况？如何解决模型不收敛问题？"><a href="#loss不下降可能是什么情况？如何解决模型不收敛问题？" class="headerlink" title="loss不下降可能是什么情况？如何解决模型不收敛问题？"></a>loss不下降可能是什么情况？如何解决模型不收敛问题？</h2><p>（1）可能数据里有错误，数据本身就含有了nan的数据，错误的数据导致网络无法收敛。<br>（2）学习率设置不好，可从0.1-&gt;0.01..尝试，学习率设置太低会走不出低谷，可提高冲量、提高mini-batch值。<br>（3）网络层数太低，无法收敛，可适当加深网络。<br>（4）标签设置错误，比如把<code>[0,1,2]</code>设置成<code>[1,2,3]</code>。<br>（5）数据未进行归一化，需要归一化处理。</p>
<h1 id="自然语言处理"><a href="#自然语言处理" class="headerlink" title="自然语言处理"></a>自然语言处理</h1><h2 id="详细讲一下不同的文本表征向量化方法，对比区别？"><a href="#详细讲一下不同的文本表征向量化方法，对比区别？" class="headerlink" title="详细讲一下不同的文本表征向量化方法，对比区别？"></a>详细讲一下不同的文本表征向量化方法，对比区别？</h2><p>稀疏的：one-hot、count、tf-idf。<br>稠密的：word2vec、fasttext、Glove。</p>
<h2 id="NLP的数据预处理通常做什么？"><a href="#NLP的数据预处理通常做什么？" class="headerlink" title="NLP的数据预处理通常做什么？"></a>NLP的数据预处理通常做什么？</h2><p>中文：分词、去停用词、特殊符号清洗、地点组织人名数字替换、统计词频、词性词频。<br>数据增强：同义词替换、同义词随机插入、回译。</p>
<h2 id="讲一下-word2vec？-cbow-与-skip-gram-的区别和优缺点？"><a href="#讲一下-word2vec？-cbow-与-skip-gram-的区别和优缺点？" class="headerlink" title="讲一下 word2vec？ cbow 与 skip-gram 的区别和优缺点？"></a>讲一下 word2vec？ cbow 与 skip-gram 的区别和优缺点？</h2><p>cbow和skip-gram都是在word2vec中用于将文本进行向量表示的实现方法。</p>
<p>主要有两种方法：<br><strong>CBOW</strong>：上下文词来预测中心词。预测次数跟整个文本的词数是相等的，复杂度大概是O(V)。CBOW 训练速度比Skip-Gram更快。但在计算时，CBOW 会将上下文词语加起来， 在遇到生僻词时预测效果将会大大降低。<br><strong>Skip-Gram</strong>：由中心词来预测上下文词。每次预测 K(窗口大小) 次，因此时间的复杂度为O(KV)。Skip-Gram 则会预测生僻字的使用环境来学习生僻词语义。</p>
<p>优缺点：<br>skip-gram 准确率比 cbow 高，但训练时间要比 cbow 要长。<br>在计算时，cbow会将上下文向量加起来取均值，在遇到生僻词时预测效果将会大大降低。skip-gram则会预测生僻字的使用环境，预测效果更好。</p>
<p>优化方法：<br><strong>分层Softmax</strong>：词典按词频建立哈夫曼树，然后将词语与节点计算作为二分类任务，选择下一步分支。<br><strong>负采样</strong>：上下文词语为正例，从词典中抽取 n 个不在上下文中的词作为负例，负例抽取的方法与词频有关。词频使用了 3/4 次方（工程 trick）。</p>
<h2 id="FastText和Glovec原理介绍？"><a href="#FastText和Glovec原理介绍？" class="headerlink" title="FastText和Glovec原理介绍？"></a>FastText和Glovec原理介绍？</h2><p>FastText是将句子中的每个词通过一个lookup层映射成词向量，对词向量叠加取平均作为句子的向量，然后直接用线性分类器进行分类，FastText中没有非线性的隐藏层，结构相对简单而且模型训练的更快。<br>Glovec融合了矩阵分解和全局统计信息的优势，统计语料库的词-词之间的共现矩阵，加快模型的训练速度而且又可以控制词的相对权重。<br>word2vec损失函数实质上是带权重的交叉熵，权重固定；glove的损失函数是最小平方损失函数，权重可以做映射变换。</p>
<h2 id="fastText和word2vec的区别？"><a href="#fastText和word2vec的区别？" class="headerlink" title="fastText和word2vec的区别？"></a>fastText和word2vec的区别？</h2><p>除了训练词向量，fastText还能能胜任分类任务。fastText速度优于word2vec。<br>相似点：都采用很多相似的优化方法，比如使用Hierarchical softmax优化训练和预测中的打分速度。<br>不同点：<br>训练词向量：word2vec的输入层是上下文单词(有序的)；而fasttext对应的整个句子/文本的单词及其n-gram特征(无序的词袋思想)。<br>分类任务：fasttext结构和CBOW类似，但学习目标是人工标注的分类结果。<br>fastText 词向量得到的相似度是基于分类类别的相似。<br>word2vec 词向量得到的相似度是基于语义的相似。</p>
<h2 id="CNN-textcnn-为什么能做文本分类，超参数怎么确定？"><a href="#CNN-textcnn-为什么能做文本分类，超参数怎么确定？" class="headerlink" title="CNN(textcnn)为什么能做文本分类，超参数怎么确定？"></a>CNN(textcnn)为什么能做文本分类，超参数怎么确定？</h2><p>CNN核心思想是获取局部特征，对文本来说局部特征就是上下文信息，CNN应用于文本类似N-gram。<br>其优势在于能自动地对N-gram特征进行组合和筛选。且在每次卷积中使用权重共享，训练速度较快。</p>
<h2 id="CRF原理？"><a href="#CRF原理？" class="headerlink" title="CRF原理？"></a>CRF原理？</h2><p>设X与Y是随机变量，P(Y|X)是给定X的条件下Y的条件概率分布，若随机变量Y构成一个由无向图G=(V,E)表示的马尔科夫随机场。则称条件概率分布P(Y|X)为条件随机场。因为是在X条件下的马尔科夫随机场，所以叫条件随机场。</p>
<h2 id="HMM、MEMM-vs-CRF-对比？"><a href="#HMM、MEMM-vs-CRF-对比？" class="headerlink" title="HMM、MEMM vs CRF 对比？"></a>HMM、MEMM vs CRF 对比？</h2><p>HMM(隐马模型)是有向图模型，是生成模型；HMM有两个假设：一阶马尔科夫假设+观测独立性假设；但对于序列标注问题不仅和单个词相关，而且和观察序列的长度，单词的上下文，等等相关。<br>MEMM（最大熵马尔科夫模型）是有向图模型，是判别模型；MEMM打破了HMM的观测独立性假设，MEMM考虑到相邻状态之间依赖关系，且考虑整个观察序列，因此MEMM的表达能力更强；但MEMM会带来标注偏置问题：由于局部归一化问题，MEMM倾向于选择拥有更少转移的状态。这就是标记偏置问题。<br>CRF模型解决了标注偏置问题，去除了HMM中两个不合理的假设，当然，模型相应得也变复杂了。</p>
<p>CRF，HMM(隐马模型)，MEMM(最大熵隐马模型)都常用来做序列标注的建模，像分词、词性标注，以及命名实体标注。<br>HMM一个最大的缺点就是由于其输出独立性假设，导致其不能考虑上下文的特征，限制了特征的选择。<br>MEMM则解决了隐马的问题，可以任意选择特征，但由于其在每一节点都要进行归一化，所以只能找到局部的最优值，同时也带来了标记偏见的问题，即凡是训练语料中未出现的情况全都忽略掉。<br>CRF则很好的解决了这一问题，他并不在每一个节点进行归一化，而是所有特征进行全局归一化，因此可以求得全局的最优值。</p>
<h2 id="QA建模方法有哪些？"><a href="#QA建模方法有哪些？" class="headerlink" title="QA建模方法有哪些？"></a>QA建模方法有哪些？</h2><h2 id="精排learning-to-rank-方法有哪些？"><a href="#精排learning-to-rank-方法有哪些？" class="headerlink" title="精排learning to rank 方法有哪些？"></a>精排learning to rank 方法有哪些？</h2><p>Point wise：将训练集中的每个item看作一个样本获取rank函数，主要解决方法是把分类问题转换为单个item的分类或回归问题。<br>pairwise：将同一个查询中两个不同的item作为一个样本，主要思想是把rank问题转换为二值分类问题。<br>list wise：将整个item序列看作一个样本，通过直接优化信息检索的评价方法和定义损失函数两种方法实现。</p>
<h1 id="大数据等"><a href="#大数据等" class="headerlink" title="大数据等"></a>大数据等</h1><h2 id="mysql、hadoop、spark区别？"><a href="#mysql、hadoop、spark区别？" class="headerlink" title="mysql、hadoop、spark区别？"></a>mysql、hadoop、spark区别？</h2><p>mysql：数据库。<br>Spark：分布式计算平台，是一个用scala语言编写的计算框架，基于<strong>内存</strong>的快速、通用、可扩展的大数据分析引擎。<br>Hadoop，是分布式管理、存储、计算的生态系统；包括HDFS（存储）、MapReduce（计算）、Yarn（资源调度）。</p>
<p>Spark基于内存的，速度比hadoop快。<br>Spark没有提供文件管理系统，所以和其他的分布式文件系统(如HDFS的HBase数据库/mysql)进行集成才能运作。<br>Hadoop适合处理静态数据，对于迭代式流式数据的处理能力差；Spark通过在内存中缓存处理的数据，提高了处理流式数据和迭代式数据的性能<br>Hadoop中对于数据计算只提供了Map和Reduce两个操作，Spark提供了丰富的算子，可以通过RDD转换算子和RDD行动算子，实现很多复杂算法操作，这些在复杂的算法在Hadoop中需要自己编写，而在Spark中直接通过scala语言封装好了，直接用就ok。</p>
<h2 id="reducebykey如何运行，和groupbykey相比高效在哪？"><a href="#reducebykey如何运行，和groupbykey相比高效在哪？" class="headerlink" title="reducebykey如何运行，和groupbykey相比高效在哪？"></a>reducebykey如何运行，和groupbykey相比高效在哪？</h2><p>reduceByKey和groupByKey都存在shuffle的操作，但是reduceByKey可以在shuffle前对分区内相同key的数据进行<strong>预聚合</strong>(combine)功能，这样会减少落磁盘的数据量；而groupByKey只是进行分组，不存在数据量减少的问题，reduceByKey的性能高。</p>
<p>从功能的角度：<br>reduceByKey其实包含分组和聚合的功能。groupByKey只能分组，不能聚合。所以在分组聚合的场合下，推荐使用reduceByKey，而仅仅只是分组而不需要聚合的，那么只能使用groupByKey。</p>
<h2 id="宽依赖-窄依赖？"><a href="#宽依赖-窄依赖？" class="headerlink" title="宽依赖/窄依赖？"></a>宽依赖/窄依赖？</h2><p>RDD和它依赖的父RDD(s)的关系有两种不同的类型，即窄依赖(narrow dependency)和宽依赖(wide dependency)。<br>宽依赖：父RDD的每个分区都可能被多个子RDD分区使用。<br>窄依赖：父RDD的每个分区只被某一个子RDD分区使用。<br>宽依赖往往需要 shuffle 操作，stage 会增加。</p>
<h1 id="数学题"><a href="#数学题" class="headerlink" title="数学题"></a>数学题</h1><p>ABC三个人抛硬币，其抛硬币的正反概率都为1/2。A先抛，若为正，则A赢，否则B抛，若B抛正，B赢，否则C抛。如果都没有抛正，进行下一轮，继续从A开始抛，问ABC赢得比赛的概率是？<br>A赢得概率是1/2，<br>B赢得概率是(1/2)^2=1/4<br>C赢的概率是(1/2)^3=1/8<br>如果都没赢，第二轮：<br>A赢得概率是(1/2)^4=1/16，<br>B赢得概率是(1/2)^5=1/32<br>C赢的概率是(1/2)^6=1/64<br>所以总的：<br>A赢得概率是1/2+1/16+1/128+。。。， 无限多个不好算，但有办法<br>B赢得概率是1/4+1/32+。。。。 =1/2×A的概率<br>C赢的概率是1/8+1/64+。。。。 =1/4×A的概率<br>那么就有<br>A+B+C=7/4*A=1<br>A=4/7<br>B=2/7<br>C=1/7</p>
<h1 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h1><p><span class="exturl" data-url="aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2xpYW5nZG9uZzIwMTQvYXJ0aWNsZS9kZXRhaWxzLzc5NTE3NjM4">为什么L1正则项产生稀疏的权重，L2正则项产生相对平滑的权重<i class="fa fa-external-link-alt"></i></span><br><span class="exturl" data-url="aHR0cHM6Ly96aHVhbmxhbi56aGlodS5jb20vcC84NTczMTIwNg==">【机器学习】决策树（上）——ID3、C4.5、CART<i class="fa fa-external-link-alt"></i></span><br><span class="exturl" data-url="aHR0cHM6Ly96aHVhbmxhbi56aGlodS5jb20vcC84NjI2Mzc4Ng==">【机器学习】决策树（中）——Random Forest、Adaboost、GBDT<i class="fa fa-external-link-alt"></i></span><br><span class="exturl" data-url="aHR0cHM6Ly96aHVhbmxhbi56aGlodS5jb20vcC84Nzg4NTY3OA==">【机器学习】决策树（下）——XGBoost、LightGBM<i class="fa fa-external-link-alt"></i></span><br><span class="exturl" data-url="aHR0cHM6Ly9tdWJ1LmNvbS9kb2MvY1VOOHBiRTdNMA==">总结<i class="fa fa-external-link-alt"></i></span><br><a href="随机森林(Random Forest">随机森林(Random Forest)</a>)<br><span class="exturl" data-url="aHR0cHM6Ly96aHVhbmxhbi56aGlodS5jb20vcC83MzIxNDgxMA==">激活函数总结<i class="fa fa-external-link-alt"></i></span><br><span class="exturl" data-url="aHR0cHM6Ly96aHVhbmxhbi56aGlodS5jb20vcC83NDUxNjkzMA==">NLP中 batch normalization与 layer normalization<i class="fa fa-external-link-alt"></i></span><br><span class="exturl" data-url="aHR0cHM6Ly93d3cubm93Y29kZXIuY29tL2Rpc2N1c3MvOTYzMDI0P3R5cGU9YWxsJmFtcDtvcmRlcj1yZWNhbGwmYW1wO3Bvcz0mYW1wO3BhZ2U9MSZhbXA7bmNUcmFjZUlkPSZhbXA7Y2hhbm5lbD0tMSZhbXA7c291cmNlX2lkPXNlYXJjaF9hbGxfbmN0cmFjayZhbXA7Z2lvX2lkPTgyODQ0NDkzQUY0RUEzQTlDQUQ2OUFCNjY3OUE3QzJDLTE2NTQxNTI3NjEzNzA=">硕一 nlp算法社招面经<i class="fa fa-external-link-alt"></i></span><br><span class="exturl" data-url="aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTMwNjk1NTIvYXJ0aWNsZS9kZXRhaWxzLzEwODA3NDM0OQ==">Transform详解<i class="fa fa-external-link-alt"></i></span><br><span class="exturl" data-url="aHR0cHM6Ly96aHVhbmxhbi56aGlodS5jb20vcC8zNTkzNjY3MTc=">Transformer 中的 positional embedding<i class="fa fa-external-link-alt"></i></span><br><span class="exturl" data-url="aHR0cHM6Ly96aHVhbmxhbi56aGlodS5jb20vcC80NjgzNTY2OTI=">Transformers之自定义学习率动态调整<i class="fa fa-external-link-alt"></i></span><br><span class="exturl" data-url="aHR0cHM6Ly96aHVhbmxhbi56aGlodS5jb20vcC84NDYxNDQ5MA==">Transformer中warm-up和LayerNorm的重要性探究<i class="fa fa-external-link-alt"></i></span></p>
<p><span class="exturl" data-url="aHR0cHM6Ly96aHVhbmxhbi56aGlodS5jb20vcC8xNDg4MTMwNzk=">CRF条件随机场的原理、例子、公式推导和应用<i class="fa fa-external-link-alt"></i></span><br><span class="exturl" data-url="aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ppYW93b3Nob3V6aS9hcnRpY2xlL2RldGFpbHMvODkwNzM5NDQ=">一文读懂BERT(原理篇)<i class="fa fa-external-link-alt"></i></span><br><span class="exturl" data-url="aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ppYW93b3Nob3V6aS9hcnRpY2xlL2RldGFpbHMvOTMzODExMDQ/c3BtPTEwMDEuMjAxNC4zMDAxLjU1MDI=">后BERT时代<i class="fa fa-external-link-alt"></i></span></p>

    </div>

    
    
    
      
  <div class="popular-posts-header">相关文章</div>
  <ul class="popular-posts">
    <li class="popular-posts-item">
      <div class="popular-posts-title"><a href="\2019\01\31\Other\算法题合集\" rel="bookmark">算法题</a></div>
    </li>
  </ul>


    <footer class="post-footer">
          

<div class="post-copyright">
<ul>
  <li class="post-copyright-author">
      <strong>本文作者： </strong>SoundMemories
  </li>
  <li class="post-copyright-link">
      <strong>本文链接：</strong>
      <a href="https://soundmemories.github.io/2019/01/21/Other/%E9%9D%A2%E8%AF%95%E9%A2%98%E9%9B%86/" title="面试题集">https://soundmemories.github.io/2019/01/21/Other/面试题集/</a>
  </li>
  <li class="post-copyright-license">
    <strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <span class="exturl" data-url="aHR0cHM6Ly9jcmVhdGl2ZWNvbW1vbnMub3JnL2xpY2Vuc2VzL2J5LW5jLXNhLzQuMC8="><i class="fab fa-fw fa-creative-commons"></i>BY-NC-SA</span> 许可协议。转载请注明出处！
  </li>
</ul>
</div>

          <div class="post-tags">
              <a href="/tags/Other/" rel="tag"><i class="fa fa-tag"></i> Other</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2019/01/20/Python/09.Python-%E5%87%BD%E6%95%B0%E3%80%81map_reduce%E3%80%81lambda%E5%8C%BF%E5%90%8D%E5%87%BD%E6%95%B0/" rel="prev" title="Python-函数、map/reduce、lambda匿名函数">
                  <i class="fa fa-chevron-left"></i> Python-函数、map/reduce、lambda匿名函数
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2019/01/21/Python/10.Python-%E9%AB%98%E9%98%B6%E5%87%BD%E6%95%B0%E3%80%81%E8%A3%85%E9%A5%B0%E5%99%A8%E3%80%81functools%E6%BA%90%E7%A0%81/" rel="next" title="Python-高阶函数、装饰器、functools源码">
                  Python-高阶函数、装饰器、functools源码 <i class="fa fa-chevron-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






      

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      const activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      const commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

    </div>
  </main>

  <footer class="footer">
    <div class="footer-inner">
      

      

<div class="copyright">
  
  &copy; 2019 – 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">SoundMemories</span>
</div>
  <div class="powered-by">由 <span class="exturl theme-link" data-url="aHR0cHM6Ly9oZXhvLmlv">Hexo</span> & <span class="exturl theme-link" data-url="aHR0cHM6Ly90aGVtZS1uZXh0LmpzLm9yZy9tdXNlLw==">NexT.Muse</span> 强力驱动
  </div>

    </div>
  </footer>

  
  <script src="//cdn.jsdelivr.net/npm/animejs@3.2.1/lib/anime.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/@next-theme/pjax@0.4.0/pjax.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/jquery@3.5.1/dist/jquery.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js"></script>
<script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/muse.js"></script><script src="/js/next-boot.js"></script><script src="/js/bookmark.js"></script>
  <script>
var pjax = new Pjax({
  selectors: [
    'head title',
    '.page-configurations',
    '.main-inner',
    '.post-toc-wrap',
    '.languages',
    '.pjax'
  ],
  analytics: false,
  cacheBust: false,
  scrollRestoration: false,
  scrollTo: !CONFIG.bookmark.enable
});

document.addEventListener('pjax:success', () => {
  pjax.executeScripts(document.querySelectorAll('script[data-pjax], .pjax script'));
  NexT.boot.refresh();
  // Define Motion Sequence & Bootstrap Motion.
  if (CONFIG.motion.enable) {
    NexT.motion.integrator
      .init()
      .add(NexT.motion.middleWares.subMenu)
      .add(NexT.motion.middleWares.postList)
      .bootstrap();
  }
  const hasTOC = document.querySelector('.post-toc');
  document.querySelector('.sidebar-inner').classList.toggle('sidebar-nav-active', hasTOC);
  document.querySelector(hasTOC ? '.sidebar-nav-toc' : '.sidebar-nav-overview').click();
  NexT.utils.updateSidebarPosition();
});
</script>


  




  <script src="/js/local-search.js"></script>








<script data-pjax>
document.querySelectorAll('.pdfobject-container').forEach(element => {
  const url = element.dataset.target;
  const pdfOpenParams = {
    navpanes : 0,
    toolbar  : 0,
    statusbar: 0,
    pagemode : 'thumbs',
    view     : 'FitH'
  };
  const pdfOpenFragment = '#' + Object.entries(pdfOpenParams).map(([key, value]) => `${key}=${encodeURIComponent(value)}`).join('&');
  const fullURL = `/lib/pdf/web/viewer.html?file=${encodeURIComponent(url)}${pdfOpenFragment}`;

  if (NexT.utils.supportsPDFs()) {
    element.innerHTML = `<embed class="pdfobject" src="${url + pdfOpenFragment}" type="application/pdf" style="height: ${element.dataset.height};">`;
  } else {
    element.innerHTML = `<iframe src="${fullURL}" style="height: ${element.dataset.height};" frameborder="0"></iframe>`;
  }
});
</script>


<script data-pjax>
if (document.querySelectorAll('.mermaid').length) {
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/mermaid@8.8.2/dist/mermaid.min.js', () => {
    mermaid.init({
      theme    : 'neutral',
      logLevel : 3,
      flowchart: { curve     : 'linear' },
      gantt    : { axisFormat: '%m/%d/%Y' },
      sequence : { actorMargin: 50 }
    }, '.mermaid');
  }, window.mermaid);
}
</script>





  








    <div class="pjax">
  

  
      <script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              const target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    const script = document.createElement('script');
    script.src = '//cdn.jsdelivr.net/npm/mathjax@3.1.2/es5/tex-mml-chtml.js';
    script.defer = true;
    document.head.appendChild(script);
  } else {
    MathJax.startup.document.state(0);
    MathJax.typesetClear();
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  
  <script src="//cdn.jsdelivr.net/npm/quicklink@2.0.0/dist/quicklink.umd.js"></script>
  <script>
      window.addEventListener('load', () => {
      quicklink.listen({
        timeout : 3000,
        priority: true,
        ignores : [uri => uri.includes('#'),uri => uri === 'https://soundmemories.github.io/2019/01/21/Other/%E9%9D%A2%E8%AF%95%E9%A2%98%E9%9B%86/',]
      });
      });
  </script>

    </div>
</body>
</html>
