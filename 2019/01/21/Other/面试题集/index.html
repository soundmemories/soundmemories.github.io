<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.2.0">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="https://fonts.loli.net/css?family=Monda:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">

<link rel="stylesheet" href="//cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.1/css/all.min.css">
  <link rel="stylesheet" href="//cdn.jsdelivr.net/npm/animate.css@3.1.1/animate.min.css">
  <link rel="stylesheet" href="//cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.css">

<script class="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"soundmemories.github.io","root":"/","images":"/images","scheme":"Muse","version":"8.0.2","exturl":true,"sidebar":{"position":"left","display":"always","padding":18,"offset":12},"copycode":true,"bookmark":{"enable":true,"color":"#222","save":"auto"},"fancybox":true,"mediumzoom":true,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"},"path":"/search.xml","localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false}};
  </script>

  <meta name="description" content="机器学习过拟合和欠拟合？过拟合：模型在训练集上表现好，在测试集和新数据上表现差。欠拟合：模型在训练集和测试集上表现都差。 降低过拟合风险：增加数据。降低模型复杂度，如减少神经元数量、网络层数，降低树深度，剪枝。增加正则化。使用集成学习方法，降低单一模型过拟合风险。 降低欠拟合风险：增加新特征。增加模型复杂度。减小正则化系数。 多分类用什么评估指标，AUC是什么？多个二分类混淆矩阵，两两类别的组合都">
<meta property="og:type" content="article">
<meta property="og:title" content="面试题集">
<meta property="og:url" content="https://soundmemories.github.io/2019/01/21/Other/%E9%9D%A2%E8%AF%95%E9%A2%98%E9%9B%86/index.html">
<meta property="og:site_name" content="SoundMemories">
<meta property="og:description" content="机器学习过拟合和欠拟合？过拟合：模型在训练集上表现好，在测试集和新数据上表现差。欠拟合：模型在训练集和测试集上表现都差。 降低过拟合风险：增加数据。降低模型复杂度，如减少神经元数量、网络层数，降低树深度，剪枝。增加正则化。使用集成学习方法，降低单一模型过拟合风险。 降低欠拟合风险：增加新特征。增加模型复杂度。减小正则化系数。 多分类用什么评估指标，AUC是什么？多个二分类混淆矩阵，两两类别的组合都">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://soundmemories.github.io/images/面试/L1图.jpg">
<meta property="og:image" content="https://soundmemories.github.io/images/面试/梯度下降.png">
<meta property="og:image" content="https://soundmemories.github.io/images/面试/Bagging和Boosting.png">
<meta property="og:image" content="https://soundmemories.github.io/images/面试/GBDT和Xgboost.jpg">
<meta property="og:image" content="https://soundmemories.github.io/images/面试/GBDT和Xgboost2.jpg">
<meta property="og:image" content="https://soundmemories.github.io/images/面试/超参数调优.jpg">
<meta property="article:published_time" content="2019-01-20T16:00:00.000Z">
<meta property="article:modified_time" content="2022-05-30T08:00:20.360Z">
<meta property="article:author" content="SoundMemories">
<meta property="article:tag" content="Other">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://soundmemories.github.io/images/面试/L1图.jpg">


<link rel="canonical" href="https://soundmemories.github.io/2019/01/21/Other/%E9%9D%A2%E8%AF%95%E9%A2%98%E9%9B%86/">


<script data-pjax class="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>面试题集 | SoundMemories</title>
  






  <noscript>
  <style>
  body { margin-top: 2rem; }

  .use-motion .menu-item,
  .use-motion .sidebar,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header {
    visibility: visible;
  }

  .use-motion .header,
  .use-motion .site-brand-container .toggle,
  .use-motion .footer { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle,
  .use-motion .custom-logo-image {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line {
    transform: scaleX(1);
  }

  .search-pop-overlay, .sidebar-nav { display: none; }
  .sidebar-panel { display: block; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <h1 class="site-title">SoundMemories</h1>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu">
        <li class="menu-item menu-item-主页">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>主页</a>

  </li>
        <li class="menu-item menu-item-分类">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a>

  </li>
        <li class="menu-item menu-item-标签">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a>

  </li>
        <li class="menu-item menu-item-归档">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" maxlength="80"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="search-result-container no-result">
  <div class="search-result-icon">
    <i class="fa fa-spinner fa-pulse fa-5x"></i>
  </div>
</div>

    </div>
  </div>

</div>
        
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>

  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>
	   
		  
      <div class="sidebar-panel-container">
        <!--noindex-->
        <section class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0"><span class="nav-number">1.</span> <span class="nav-text">机器学习</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%BF%87%E6%8B%9F%E5%90%88%E5%92%8C%E6%AC%A0%E6%8B%9F%E5%90%88%EF%BC%9F"><span class="nav-number">1.1.</span> <span class="nav-text">过拟合和欠拟合？</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%A4%9A%E5%88%86%E7%B1%BB%E7%94%A8%E4%BB%80%E4%B9%88%E8%AF%84%E4%BC%B0%E6%8C%87%E6%A0%87%EF%BC%8CAUC%E6%98%AF%E4%BB%80%E4%B9%88%EF%BC%9F"><span class="nav-number">1.2.</span> <span class="nav-text">多分类用什么评估指标，AUC是什么？</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%A0%B7%E6%9C%AC%E4%B8%8D%E5%B9%B3%E8%A1%A1%E5%A4%84%E7%90%86%EF%BC%9F"><span class="nav-number">1.3.</span> <span class="nav-text">样本不平衡处理？</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%BC%BA%E5%A4%B1%E5%80%BC%E5%A4%84%E7%90%86%EF%BC%9F"><span class="nav-number">1.4.</span> <span class="nav-text">缺失值处理？</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#L1%E6%AD%A3%E5%88%99%E5%92%8CL2%E6%AD%A3%E5%88%99%E7%9A%84%E5%8C%BA%E5%88%AB%E6%98%AF%E4%BB%80%E4%B9%88%EF%BC%9F"><span class="nav-number">1.5.</span> <span class="nav-text">L1正则和L2正则的区别是什么？</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#L1%E6%AD%A3%E5%88%99%E4%B8%BA%E4%BB%80%E4%B9%88%E5%8F%AF%E4%BB%A5%E5%BE%97%E5%88%B0%E7%A8%80%E7%96%8F%E8%A7%A3%EF%BC%9F"><span class="nav-number">1.6.</span> <span class="nav-text">L1正则为什么可以得到稀疏解？</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#SGD%E5%92%8CGD%E7%9A%84%E5%8C%BA%E5%88%AB%EF%BC%8C%E4%BB%80%E4%B9%88%E5%9C%BA%E6%99%AF%E4%B8%8B%E7%94%A8%EF%BC%9F"><span class="nav-number">1.7.</span> <span class="nav-text">SGD和GD的区别，什么场景下用？</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%86%B3%E7%AD%96%E6%A0%91%E5%90%AF%E5%8F%91%E5%87%BD%E6%95%B0%EF%BC%9F"><span class="nav-number">1.8.</span> <span class="nav-text">决策树启发函数？</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#bagging%E5%92%8Cboosting%EF%BC%9F"><span class="nav-number">1.9.</span> <span class="nav-text">bagging和boosting？</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%AF%94%E8%BE%83LR%E5%92%8CGBDT-%E8%AF%B4%E8%AF%B4%E4%BB%80%E4%B9%88%E6%83%85%E5%A2%83%E4%B8%8BGBDT%E4%B8%8D%E5%A6%82LR%EF%BC%9F"><span class="nav-number">1.10.</span> <span class="nav-text">比较LR和GBDT, 说说什么情境下GBDT不如LR？</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#RF%E5%92%8CGBDT%E7%9A%84%E5%8C%BA%E5%88%AB%EF%BC%9F"><span class="nav-number">1.11.</span> <span class="nav-text">RF和GBDT的区别？</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#GBDT%E5%92%8Cxgboost%E5%8C%BA%E5%88%AB%EF%BC%9F"><span class="nav-number">1.12.</span> <span class="nav-text">GBDT和xgboost区别？</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%AE%80%E5%8D%95%E4%BB%8B%E7%BB%8D%E4%B8%8Bxgboost%EF%BC%9F"><span class="nav-number">1.13.</span> <span class="nav-text">简单介绍下xgboost？</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#xgboost%E4%B8%BA%E4%BB%80%E4%B9%88%E4%BD%BF%E7%94%A8%E6%B3%B0%E5%8B%92%E4%BA%8C%E9%98%B6%E5%B1%95%E5%BC%80%EF%BC%9F"><span class="nav-number">1.14.</span> <span class="nav-text">xgboost为什么使用泰勒二阶展开？</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#XGBoost%E4%B8%BA%E4%BB%80%E4%B9%88%E5%8F%AF%E4%BB%A5%E5%B9%B6%E8%A1%8C%E8%AE%AD%E7%BB%83%EF%BC%9F-%E5%A6%82%E4%BD%95%E5%AF%BB%E6%89%BE%E6%9C%80%E4%BD%B3%E5%88%86%E8%A3%82%E7%82%B9%EF%BC%9F"><span class="nav-number">1.15.</span> <span class="nav-text">XGBoost为什么可以并行训练？&#x2F;如何寻找最佳分裂点？</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#XGBOOST%E4%B8%BA%E4%BB%80%E4%B9%88%E5%BF%AB%EF%BC%9F"><span class="nav-number">1.16.</span> <span class="nav-text">XGBOOST为什么快？</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#XGBOOST%E9%98%B2%E6%AD%A2%E8%BF%87%E6%8B%9F%E5%90%88%E7%9A%84%E6%96%B9%E6%B3%95%EF%BC%9F"><span class="nav-number">1.17.</span> <span class="nav-text">XGBOOST防止过拟合的方法？</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#XGBOOST%E5%A6%82%E4%BD%95%E5%A4%84%E7%90%86%E7%BC%BA%E5%A4%B1%E5%80%BC%EF%BC%9F"><span class="nav-number">1.18.</span> <span class="nav-text">XGBOOST如何处理缺失值？</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#XBGOOST%E4%B8%AD%E5%8F%B6%E5%AD%90%E8%8A%82%E7%82%B9%E7%9A%84%E6%9D%83%E9%87%8D%E5%A6%82%E4%BD%95%E8%AE%A1%E7%AE%97%E5%87%BA%E6%9D%A5%E7%9A%84%EF%BC%9F"><span class="nav-number">1.19.</span> <span class="nav-text">XBGOOST中叶子节点的权重如何计算出来的？</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#XGBOOST%E4%B8%AD%E7%9A%84%E4%B8%80%E6%A3%B5%E6%A0%91%E7%9A%84%E5%81%9C%E6%AD%A2%E7%94%9F%E9%95%BF%E6%9D%A1%E4%BB%B6%EF%BC%9F"><span class="nav-number">1.20.</span> <span class="nav-text">XGBOOST中的一棵树的停止生长条件？</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#xgboost%E5%A6%82%E4%BD%95%E5%A4%84%E7%90%86%E4%B8%8D%E5%B9%B3%E8%A1%A1%E6%95%B0%E6%8D%AE%EF%BC%9F"><span class="nav-number">1.21.</span> <span class="nav-text">xgboost如何处理不平衡数据？</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#XGBOOS%E4%B8%AD%E5%A6%82%E4%BD%95%E5%AF%B9%E6%A0%91%E8%BF%9B%E8%A1%8C%E5%89%AA%E6%9E%9D%EF%BC%9F"><span class="nav-number">1.22.</span> <span class="nav-text">XGBOOS中如何对树进行剪枝？</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#XGBoost%E7%9A%84Scalable-%E5%8F%AF%E6%89%A9%E5%B1%95%E6%80%A7-%E6%80%A7%E5%A6%82%E4%BD%95%E4%BD%93%E7%8E%B0%EF%BC%9F"><span class="nav-number">1.23.</span> <span class="nav-text">XGBoost的Scalable(可扩展性)性如何体现？</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#XGBOOST%E5%A6%82%E4%BD%95%E8%AF%84%E4%BB%B7%E7%89%B9%E5%BE%81%E7%9A%84%E9%87%8D%E8%A6%81%E6%80%A7%EF%BC%9F"><span class="nav-number">1.24.</span> <span class="nav-text">XGBOOST如何评价特征的重要性？</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#XGBOOST%E5%8F%82%E6%95%B0%E8%B0%83%E4%BC%98%E7%9A%84%E4%B8%80%E8%88%AC%E6%AD%A5%E9%AA%A4%EF%BC%9F"><span class="nav-number">1.25.</span> <span class="nav-text">XGBOOST参数调优的一般步骤？</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#XGBOOST%E6%A8%A1%E5%9E%8B%E5%A6%82%E6%9E%9C%E8%BF%87%E6%8B%9F%E5%90%88%E4%BA%86%E6%80%8E%E4%B9%88%E8%A7%A3%E5%86%B3%EF%BC%9F"><span class="nav-number">1.26.</span> <span class="nav-text">XGBOOST模型如果过拟合了怎么解决？</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#XGBOOST%E5%92%8ClightGBM%E7%9A%84%E5%8C%BA%E5%88%AB%EF%BC%9F"><span class="nav-number">1.27.</span> <span class="nav-text">XGBOOST和lightGBM的区别？</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%B6%85%E5%8F%82%E6%95%B0%E8%B0%83%E4%BC%98%EF%BC%9F"><span class="nav-number">1.28.</span> <span class="nav-text">超参数调优？</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%A8%A1%E5%9E%8B%E5%B8%B8%E7%94%A8%E8%B6%85%E5%8F%82%E6%95%B0%EF%BC%9F"><span class="nav-number">1.29.</span> <span class="nav-text">模型常用超参数？</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%86%B5%E7%9A%84%E6%A6%82%E5%BF%B5%EF%BC%8C%E4%BA%A4%E5%8F%89%E7%86%B5%EF%BC%9F"><span class="nav-number">1.30.</span> <span class="nav-text">熵的概念，交叉熵？</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BA%A4%E5%8F%89%E7%86%B5%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E6%98%AF%E4%BB%80%E4%B9%88%EF%BC%8C%E5%9C%A8%E4%BB%80%E4%B9%88%E5%9C%B0%E6%96%B9%E4%BD%BF%E7%94%A8%E8%BF%87%EF%BC%9F"><span class="nav-number">1.31.</span> <span class="nav-text">交叉熵损失函数是什么，在什么地方使用过？</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0"><span class="nav-number">2.</span> <span class="nav-text">深度学习</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#LSTM%E5%92%8CRNN%E5%8C%BA%E5%88%AB%EF%BC%8C%E6%9C%89%E4%BB%80%E4%B9%88%E4%BC%98%E5%8C%96%EF%BC%9FRNN%E4%B8%BA%E4%BB%80%E4%B9%88%E4%BC%9A%E6%9C%89%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1%EF%BC%8CLSTM%E5%A6%82%E4%BD%95%E8%A7%A3%E5%86%B3%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1%EF%BC%9F"><span class="nav-number">2.1.</span> <span class="nav-text">LSTM和RNN区别，有什么优化？RNN为什么会有梯度消失，LSTM如何解决梯度消失？</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#CNN%E4%B8%BA%E4%BB%80%E4%B9%88%E8%83%BD%E5%81%9A%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%EF%BC%8C%E8%B6%85%E5%8F%82%E6%95%B0%E6%80%8E%E4%B9%88%E7%A1%AE%E5%AE%9A%EF%BC%9F"><span class="nav-number">2.2.</span> <span class="nav-text">CNN为什么能做文本分类，超参数怎么确定？</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%AF%A6%E7%BB%86%E8%AE%B2%E4%B8%80%E4%B8%8B%E4%B8%8D%E5%90%8C%E7%9A%84%E6%96%87%E6%9C%AC%E8%A1%A8%E5%BE%81%E5%90%91%E9%87%8F%E5%8C%96%E6%96%B9%E6%B3%95%EF%BC%8C%E5%AF%B9%E6%AF%94%E5%8C%BA%E5%88%AB%EF%BC%9F"><span class="nav-number">2.3.</span> <span class="nav-text">详细讲一下不同的文本表征向量化方法，对比区别？</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%85%B6%E4%BB%96"><span class="nav-number">3.</span> <span class="nav-text">其他</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#reducebykey%E5%A6%82%E4%BD%95%E8%BF%90%E8%A1%8C%EF%BC%8C%E5%92%8Cgroupbykey%E7%9B%B8%E6%AF%94%E9%AB%98%E6%95%88%E5%9C%A8%E5%93%AA%EF%BC%9F"><span class="nav-number">3.1.</span> <span class="nav-text">reducebykey如何运行，和groupbykey相比高效在哪？</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#word2vec%E7%9A%84%E5%8E%9F%E7%90%86%EF%BC%8C%E8%B4%9F%E4%BE%8B%E9%87%87%E6%A0%B7%E6%80%8E%E4%B9%88%E5%81%9A%EF%BC%8Cword2vec%E6%9C%89%E4%BB%80%E4%B9%88%E7%89%B9%E7%82%B9%EF%BC%8CCBOW%E5%92%8CSkip-gram%E5%8C%BA%E5%88%AB%EF%BC%9F"><span class="nav-number">4.</span> <span class="nav-text">word2vec的原理，负例采样怎么做，word2vec有什么特点，CBOW和Skip-gram区别？</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#attention%E6%9C%BA%E5%88%B6%E6%9C%89%E4%BB%80%E4%B9%88%E7%89%B9%E5%88%AB%E7%9A%84%E5%9C%B0%E6%96%B9%EF%BC%9F%E4%BA%86%E8%A7%A3%E5%93%AA%E4%BA%9B%E4%B8%8D%E5%90%8C%E7%9A%84attention%EF%BC%8Cself-attention%E4%B8%BA%E4%BB%80%E4%B9%88%E6%9C%89%E6%95%88%EF%BC%9F"><span class="nav-number">5.</span> <span class="nav-text">attention机制有什么特别的地方？了解哪些不同的attention，self-attention为什么有效？</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#seq2seq%E6%98%AF%E4%BB%80%E4%B9%88%E7%BB%93%E6%9E%84%EF%BC%8Ctransformer%E5%91%A2%EF%BC%9F"><span class="nav-number">6.</span> <span class="nav-text">seq2seq是什么结构，transformer呢？</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#NLP%E7%9A%84%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86%E9%80%9A%E5%B8%B8%E5%81%9A%E4%BB%80%E4%B9%88%EF%BC%9F"><span class="nav-number">7.</span> <span class="nav-text">NLP的数据预处理通常做什么？</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Fasttext%E5%92%8Cword2vec%E5%8C%BA%E5%88%AB%EF%BC%9F"><span class="nav-number">8.</span> <span class="nav-text">Fasttext和word2vec区别？</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#QA%E5%BB%BA%E6%A8%A1%E6%96%B9%E6%B3%95%E6%9C%89%E5%93%AA%E4%BA%9B%EF%BC%9F"><span class="nav-number">9.</span> <span class="nav-text">QA建模方法有哪些？</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Bert%E5%81%9AQA%E7%B3%BB%E7%BB%9F%EF%BC%8C%E6%95%B0%E6%8D%AE%E8%BE%93%E5%85%A5%E5%92%8C%E4%B8%8B%E6%B8%B8%E4%BB%BB%E5%8A%A1%E5%A6%82%E4%BD%95%E6%94%B9%E9%80%A0%EF%BC%9F"><span class="nav-number">10.</span> <span class="nav-text">Bert做QA系统，数据输入和下游任务如何改造？</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%9F%90%E4%B8%AA%E7%B1%BB%E5%88%AB%E6%95%88%E6%9E%9C%E5%BE%88%E5%B7%AE%EF%BC%8C%E6%80%8E%E4%B9%88%E5%8A%9E%EF%BC%9F"><span class="nav-number">11.</span> <span class="nav-text">某个类别效果很差，怎么办？</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%A6%82%E4%BD%95%E6%89%BE%E5%88%B0%E5%AF%B9%E5%88%86%E7%B1%BB%E5%BD%B1%E5%93%8D%E6%9C%80%E5%A4%A7%E7%9A%84%E5%85%B3%E9%94%AE%E8%AF%8D%EF%BC%9F"><span class="nav-number">12.</span> <span class="nav-text">如何找到对分类影响最大的关键词？</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%A6%82%E4%BD%95%E6%8F%90%E5%8F%96query%E7%89%B9%E5%BE%81%EF%BC%8C%E5%A6%82%E4%BD%95%E5%88%A9%E7%94%A8%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AE%A1%E7%AE%97%E8%AF%AD%E4%B9%89%E7%9B%B8%E4%BC%BC%E5%BA%A6%EF%BC%9F"><span class="nav-number">13.</span> <span class="nav-text">深度学习如何提取query特征，如何利用深度学习计算语义相似度？</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Dropout%E5%8E%9F%E7%90%86%EF%BC%9F"><span class="nav-number">14.</span> <span class="nav-text">Dropout原理？</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E8%AE%AD%E7%BB%83%E5%87%BA%E7%8E%B0nan%EF%BC%8C%E5%BA%94%E8%AF%A5%E4%BB%8E%E5%93%AA%E4%BA%9B%E6%96%B9%E9%9D%A2%E5%8E%BB%E6%9F%A5%EF%BC%9F"><span class="nav-number">15.</span> <span class="nav-text">神经网络训练出现nan，应该从哪些方面去查？</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#loss%E4%B8%8D%E4%B8%8B%E9%99%8D%E5%8F%AF%E8%83%BD%E6%98%AF%E4%BB%80%E4%B9%88%E6%83%85%E5%86%B5%EF%BC%9F"><span class="nav-number">16.</span> <span class="nav-text">loss不下降可能是什么情况？</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%8F%82%E8%80%83%E6%96%87%E7%8C%AE"><span class="nav-number">17.</span> <span class="nav-text">参考文献</span></a></li></ol></div>
        </section>
        <!--/noindex-->

        <section class="site-overview-wrap sidebar-panel">
          <div class="site-author site-overview-item animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="SoundMemories"
      src="https://i.loli.net/2020/11/04/6JhNuwtBe4adylS.png">
  <p class="site-author-name" itemprop="name">SoundMemories</p>
  <div class="site-description" itemprop="description">今日事，今日毕</div>
</div>
<div class="site-state-wrap site-overview-item animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives">
          <span class="site-state-item-count">118</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
        <span class="site-state-item-count">8</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
        <span class="site-state-item-count">8</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author site-overview-item animated">
      <span class="links-of-author-item">
        <span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL3NvdW5kbWVtb3JpZXM=" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;soundmemories"><i class="fab fa-github fa-fw"></i>GitHub</span>
      </span>
      <span class="links-of-author-item">
        <span class="exturl" data-url="bWFpbHRvOnNvdW5kbWVtb3JpZXNAMTYzLmNvbQ==" title="E-Mail → mailto:soundmemories@163.com"><i class="fa fa-envelope fa-fw"></i>E-Mail</span>
      </span>
  </div>
  <div class="cc-license site-overview-item animated" itemprop="license">
    <span class="exturl cc-opacity" data-url="aHR0cHM6Ly9jcmVhdGl2ZWNvbW1vbnMub3JnL2xpY2Vuc2VzL2J5LW5jLXNhLzQuMC8="><img src="/images/cc-by-nc-sa.svg" alt="Creative Commons"></span>
  </div>



        </section>
      </div>
    </div>
  </aside>
  <div class="sidebar-dimmer"></div>


    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>
  <a role="button" class="book-mark-link book-mark-link-fixed"></a>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


    <div class="main-inner post posts-expand">
      

      

  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://soundmemories.github.io/2019/01/21/Other/%E9%9D%A2%E8%AF%95%E9%A2%98%E9%9B%86/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="https://i.loli.net/2020/11/04/6JhNuwtBe4adylS.png">
      <meta itemprop="name" content="SoundMemories">
      <meta itemprop="description" content="今日事，今日毕">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="SoundMemories">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          面试题集
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2019-01-21 00:00:00" itemprop="dateCreated datePublished" datetime="2019-01-21T00:00:00+08:00">2019-01-21</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Other/" itemprop="url" rel="index"><span itemprop="name">Other</span></a>
        </span>
    </span>

  
      </div>
      <div class="post-meta">
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>11k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>10 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
        <h1 id="机器学习"><a href="#机器学习" class="headerlink" title="机器学习"></a>机器学习</h1><h2 id="过拟合和欠拟合？"><a href="#过拟合和欠拟合？" class="headerlink" title="过拟合和欠拟合？"></a>过拟合和欠拟合？</h2><p><strong>过拟合</strong>：模型在训练集上表现好，在测试集和新数据上表现差。<br><strong>欠拟合</strong>：模型在训练集和测试集上表现都差。</p>
<p><strong>降低过拟合风险</strong>：<br>增加数据。<br>降低模型复杂度，如减少神经元数量、网络层数，降低树深度，剪枝。<br>增加正则化。<br>使用集成学习方法，降低单一模型过拟合风险。</p>
<p><strong>降低欠拟合风险</strong>：<br>增加新特征。<br>增加模型复杂度。<br>减小正则化系数。</p>
<h2 id="多分类用什么评估指标，AUC是什么？"><a href="#多分类用什么评估指标，AUC是什么？" class="headerlink" title="多分类用什么评估指标，AUC是什么？"></a>多分类用什么评估指标，AUC是什么？</h2><p>多个二分类混淆矩阵，两两类别的组合都能对应一个混淆矩阵。<br><strong>宏混淆矩阵</strong>：先计算每个混淆矩阵的指标(P、R、F1)，再求和取均值。<br><strong>微混淆矩阵</strong>：先对混淆矩阵求和(TP,FP,FN,TN)，再计算指标(P、R、F1)。</p>
<p><strong>AUC是ROC曲线下部分的面积。ROC曲线横坐标FPR，纵坐标TPR。AUC越大说明模型把真正例排在前面，分类效果越好。</strong></p>
<h2 id="样本不平衡处理？"><a href="#样本不平衡处理？" class="headerlink" title="样本不平衡处理？"></a>样本不平衡处理？</h2><p><strong>欠采样</strong>：对样本多的类别，进行欠采样。比如原型生成(利用K-means聚类选择多数样本，保证样本分布不变)，原型选择(多数样本中选取代表性样本，每个少数类样本选择K个最近的多数类样本)。<br><strong>过采样</strong>：对样本少的类别，进行过采样。比如SMOTE(一个少数样本与k近邻少数样本连线，取中点作为新少数样本)，NLP中数据增强，同义词替换。<br><strong>损失函数</strong>：对不同类别样本赋予不同权重，比如Focal loss。<br><strong>模型算法</strong>：通过引入有权重的模型算法，针对少量样本着重拟合，以提升对少量样本特征的学习。比如xgb的scale_pos_weight。<br><strong>评价指标</strong>：选择对样本不平衡不敏感的指标，比如roc，auc，f1。</p>
<h2 id="缺失值处理？"><a href="#缺失值处理？" class="headerlink" title="缺失值处理？"></a>缺失值处理？</h2><p>离散型变量：用出现次数最多的特征值填充。<br>连续型变量：用中位数或者均值填充。</p>
<h2 id="L1正则和L2正则的区别是什么？"><a href="#L1正则和L2正则的区别是什么？" class="headerlink" title="L1正则和L2正则的区别是什么？"></a>L1正则和L2正则的区别是什么？</h2><p>L1正则化是指在损失函数中加入权值向量w的绝对值之和，即各个元素的绝对值之和，L2正则化指在损失函数中加入权值向量w的平方和。<br>L1的功能是使权重稀疏，而L2的功能是使权重平滑。<br>从贝叶斯角度来看，L1正则化相当于对模型参数w引入了拉普拉斯先验，L2正则化相当于引入了高斯先验。</p>
<h2 id="L1正则为什么可以得到稀疏解？"><a href="#L1正则为什么可以得到稀疏解？" class="headerlink" title="L1正则为什么可以得到稀疏解？"></a>L1正则为什么可以得到稀疏解？</h2><p>解空间形状，这是我们最常使用的一种答案，就是给面试官画下面的图：</p>
<p><img src="/images/面试/L1图.jpg" width="80%"></p>
<p>L2正则化相当于为参数定义了一个圆形的解空间，L1正则化相当于为参数定义了一个菱形的解空间。L1”棱角分明”的解空间显然更容易与凸优化问题中目标函数等高线在轴上碰撞，从而产生稀疏解。<br>事实上，“带正则项”和“带约束条件”是等价的。加了正则化的损失函数 $L = Loss(y, \hat{y}) +\lambda ||w||_2^2$。带约束条件的 损失函数 $L = Loss(y, \hat{y}), st.||w||_2^2&lt;=m$ 等价于求 $L = Loss(y, \hat{y}) +\lambda (||w||_2^2 - m)$ 其中 L 对 w 求导为0，就跟上面一致了。</p>
<h2 id="SGD和GD的区别，什么场景下用？"><a href="#SGD和GD的区别，什么场景下用？" class="headerlink" title="SGD和GD的区别，什么场景下用？"></a>SGD和GD的区别，什么场景下用？</h2><p><img src="/images/面试/梯度下降.png" width="60%"><br>梯度下降：每次迭代，计算所有样本的梯度均值，然后更新参数。<br>随机梯度下降：选一个样本计算一个梯度，然后马上更新参数。</p>
<h2 id="决策树启发函数？"><a href="#决策树启发函数？" class="headerlink" title="决策树启发函数？"></a>决策树启发函数？</h2><p><strong>ID3</strong>：最大信息增益。<br><strong>c4.5</strong>：最大信息增益比。校正信息增益趋向于取值多的特征问题。<br><strong>cart</strong>：基尼系数。与信息熵含义类似，每次选择基尼系数最小的特征。</p>
<p>ID3对缺失值敏感。<br>ID3只能离散变量，c4.5和cart还适用于连续变量(划分区间变为离散)。<br>ID3和c4.5是多叉树(特征不复用)，cart是二叉树(特征可复用)。<br>ID3和c4.5只能分类任务，cart还适用于回归任务。</p>
<h2 id="bagging和boosting？"><a href="#bagging和boosting？" class="headerlink" title="bagging和boosting？"></a>bagging和boosting？</h2><p><strong>bagging</strong>：对训练集多次采样，产生若干不同的子集，每个子集训练一个基学习器，预测分类任务投票法，回归任务均值法。<br><strong>boosting</strong>：初始训练集训练一个基分类器，再根据基分类器表现对训练样本分布调整(通过学习残差改变样本权重)，基于调整后的分布来训练下一个基学习器，重复直到达到设定阈值，最终结果为所有分类器的加权求和。<br><strong>bagging降低方差，boosting降低偏差。</strong></p>
<p><img src="/images/面试/Bagging和Boosting.png" width="100%"></p>
<h2 id="比较LR和GBDT-说说什么情境下GBDT不如LR？"><a href="#比较LR和GBDT-说说什么情境下GBDT不如LR？" class="headerlink" title="比较LR和GBDT, 说说什么情境下GBDT不如LR？"></a>比较LR和GBDT, 说说什么情境下GBDT不如LR？</h2><p>先说说LR和GBDT的区别：<br>（1）LR是线性模型，可解释性强，很容易并行化，但是学习能力有限，需要大量的人工特征工程。<br>（2）GBDT是非线性模型，有天然的特征组合优势，特征表达能力强，但是树与树之前无法并行训练，而且树模型很容易过拟合。</p>
<p>当在高维稀疏特征的场景下，LR的效果一般会比GBDT好。原因如下：<br>（1）假设一个二分类问题，label为0和1， 特征为100维， 如果有1w个样本，但其中只有10个正样本1，而这些样本的特征f1的值全为1， 而其余9990条样本的f1特征都为0（在高维稀疏的情况下这种情况很常见）。 我们都知道在这种情况下，树模型很容易优化出一个使用f1特征作为重要分裂节点的树，因为这个节点直接能够将训练数据划分的很好，但是当测的时候，却会发现效果很差，因为这个特征f1只是刚好偶然间跟y拟合到了这个规律，这也就是我们常说的过拟合。<br>（2）那么这种情况下， 如果采用lr的话，应该也会出现类似过拟合的情况呀。<code>y=w1*f1 + w2*f2+ w3*f3 +.....</code>  其中，w1特别大足够拟合这10个样本，为什么此时树模型就过拟合的更严重呢？因为现在的模型普遍都会带着正则项，而LR等线性模型的正则项是对<strong>权重</strong>的惩罚，也就是W1一旦变大，惩罚就会很大，进一步压缩w1的值，使他不至于过大，但是，树模型则不一样，树模型的惩罚通常为<strong>叶子节点数</strong>和<strong>深度</strong>等。而我们知道，对于上面的case,树只需要一个节点就可以完美分割9990和10个样本，一个节点，最终产生的惩罚项及其之小。<br><strong>这也就是为什么在高维稀疏特征的时候，线性模型会比非线性模型好的原因了：带正则化项的线性模型比较不容易对稀疏特征过拟合。</strong></p>
<h2 id="RF和GBDT的区别？"><a href="#RF和GBDT的区别？" class="headerlink" title="RF和GBDT的区别？"></a>RF和GBDT的区别？</h2><p><strong>随机森林</strong>：基于bagging思想，cart决策树作为基学习器。可并行化。随机选择样本（放回抽样）；随机选择特征(计算增益)；构建决策树；随机森林投票/平均。<br>优点：易于并行化，在大数据集上有很大的优势；能够处理高维度数据，不用做特征选择。</p>
<p><strong>GBDT</strong>：基于boosting思想，cart决策树作为基学习器，使用。串行。GBDT中的树都是回归树，每次学习上一个基分类器的残差，每一步残差计算其实变相地增大了被分错样本的权重，而对于分对样本的权重趋于0，这样后面的树就能专注于那些被分错的样本。<br>优点：可以自动进行特征组合，拟合非线性数据；可以灵活处理各种类型的数据。<br>缺点：对异常点敏感。回归类的损失函数会用<strong>绝对损失</strong>或者<strong>Huber损失</strong>函数来代替平方损失函数。</p>
<p>相同点：都是由多棵树组成，最终的结果都是由多棵树一起决定。<br>不同点：<br>集成学习： RF属于bagging 思想，而GBDT属于boosting思想。<br>偏差-方差衡量：RF不断的降低模型的方差，而GBDT不断的降低模型的偏差。<br>训练样本：RF每次迭代的样本是从全部训练集中有放回抽样形成的，而GBDT每次使用全部样本。<br>并行性：RF的树可以并行生成，而GBDT只能顺序生成（需要等上一棵树完全生成）。<br>最终结果： RF最终是多棵树进行多数表决（回归问题是取平均）， 而GBDT是加权融合。<br>数据敏感度：RF对异常值不敏感，而GBDT对异常值比较敏感。<br>泛化能力： RF不易过拟合，而GBDT容易过拟合。</p>
<h2 id="GBDT和xgboost区别？"><a href="#GBDT和xgboost区别？" class="headerlink" title="GBDT和xgboost区别？"></a>GBDT和xgboost区别？</h2><p><img src="/images/面试/GBDT和Xgboost.jpg" width="80%"><br><img src="/images/面试/GBDT和Xgboost2.jpg" width="60%"></p>
<h2 id="简单介绍下xgboost？"><a href="#简单介绍下xgboost？" class="headerlink" title="简单介绍下xgboost？"></a>简单介绍下xgboost？</h2><p>（1）首先需要说一下GBDT，它是一种基于boosting的增强策略的加法模型，训练的时候才用前向分布算法进行贪婪的学习，每次迭代都学习一颗cart树来拟合前t-1颗树的预测结果与训练样本真实值的差值。<br>（2）xbgoost对gbdt进行了一系列的优化，比如损失函数进行了二阶泰勒展开，目标函数加入正则项，支持并行和默认缺失值处理等等，在可扩展性和训练速度上有了巨大的提升，但其核心思想多大的变化。</p>
<h2 id="xgboost为什么使用泰勒二阶展开？"><a href="#xgboost为什么使用泰勒二阶展开？" class="headerlink" title="xgboost为什么使用泰勒二阶展开？"></a>xgboost为什么使用泰勒二阶展开？</h2><p>精准性：相对于GBDT的一阶展开，XGBOOST采用二阶泰勒展开，可以更为精准的逼近真实的损失函数。<br>扩展性：损失函数支持自定义，只需要新的损失函数二阶可导即可。</p>
<h2 id="XGBoost为什么可以并行训练？-如何寻找最佳分裂点？"><a href="#XGBoost为什么可以并行训练？-如何寻找最佳分裂点？" class="headerlink" title="XGBoost为什么可以并行训练？/如何寻找最佳分裂点？"></a>XGBoost为什么可以并行训练？/如何寻找最佳分裂点？</h2><p>（1）xgboost的并行，并不是说每棵树可以并行训练，XGB本质上仍然采用boosting的思想，每棵树训练前需要等前面的树训练完成以后才能开始训练。<br>（2）xgboost的并行，指的是特征维度的并行。在训练之前，每个特征按特征值对样本进行预排序，并存储为block结构，在后面查找特征分割点时可以重复使用，而且特征已经被存储为一个个block结构，那么在寻找每个特征的最佳分割点的时候，可以利用多线程对每个block并行计算。</p>
<h2 id="XGBOOST为什么快？"><a href="#XGBOOST为什么快？" class="headerlink" title="XGBOOST为什么快？"></a>XGBOOST为什么快？</h2><p>（1）分块并行：训练前每个特征按值进行排序并存储为block结构，后面查找特征分割点时重复使用，并且支持并行查找每个特征的分割点。<br>（2）候选分位点：每个特征采用常数个分位点作为候选分割点。<br>（3）CPU cache命中优化：使用缓存预取的方法，对每个线程分配一个连续的buffer,读取每个block中样本的梯度信息并存入连续的buffer中。<span class="exturl" data-url="aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2RwZW5nd2FuZy9hcnRpY2xlL2RldGFpbHMvOTU5MTM0MTc=">https://blog.csdn.net/dpengwang/article/details/95913417<i class="fa fa-external-link-alt"></i></span><br>（4）block 处理优化： block预先存入内存，block按列进行解压缩，将block划分到不同硬盘来提高吞吐。</p>
<h2 id="XGBOOST防止过拟合的方法？"><a href="#XGBOOST防止过拟合的方法？" class="headerlink" title="XGBOOST防止过拟合的方法？"></a>XGBOOST防止过拟合的方法？</h2><p>（1）正则项：叶子节点个数+叶子节点权重的L2正则化。<br>（2）列抽样：训练的时候只用一部分特征（不考虑剩余的block块即可）。<br>（3）子采样：每轮计算可以不使用全部样本，使算法更加保守。<br>（4）shrinkage: 可以叫做学习率或者步长，为了给后面的训练留出更多的学习空间。</p>
<h2 id="XGBOOST如何处理缺失值？"><a href="#XGBOOST如何处理缺失值？" class="headerlink" title="XGBOOST如何处理缺失值？"></a>XGBOOST如何处理缺失值？</h2><p>（1）在特征k上寻找最佳split point的时候，不会对该列特征missing的样本进行遍历，而只对该列特征值为non-missing的样本上对应的特征值进行遍历，通过这个技巧来减少为系数离散特征寻找split point的时间开销。<br>（2）在逻辑实现上，为了保证完备性，会将该特征值missing的样本分别分配到左叶子节点和右叶子节点，两种情况都计算一遍后，选择分裂后增益最大的那个方向（左分支或者右分支），<strong>作为预测时特征值却是样本的默认分支方向</strong>。<br>（3）如果在训练中没有缺失值而在预测中出现缺失，那么会自动将缺失值的划分方向放到右子节点。</p>
<h2 id="XBGOOST中叶子节点的权重如何计算出来的？"><a href="#XBGOOST中叶子节点的权重如何计算出来的？" class="headerlink" title="XBGOOST中叶子节点的权重如何计算出来的？"></a>XBGOOST中叶子节点的权重如何计算出来的？</h2><p>目标函数：<a href="https://soundmemories.github.io/2021/06/25/Paper/02.XGBoost%20A%20Scalable%20Tree%20Boosting%20System/">XGBoost</a></p>
<h2 id="XGBOOST中的一棵树的停止生长条件？"><a href="#XGBOOST中的一棵树的停止生长条件？" class="headerlink" title="XGBOOST中的一棵树的停止生长条件？"></a>XGBOOST中的一棵树的停止生长条件？</h2><p>（1）当新引入的一次分类所带来的增益Gain &lt; 0 时，放弃当前的分裂。这是训练损失和模型结构复杂度的博弈过程。<br>（2）当树达到最大深度时，停止建树，因为树深度太深容易出现过拟合，这里需要设置一个超参数max_depth。<br>（3）当引入一次分裂后，重新计算新生产的左右两个叶子节点的样本权重和。如果任一个叶子节点的样本权重低于某一个阈值，也会放弃此次分裂。这涉及到一个超参数： 最小样本权重和，是指如果一个叶子节点包含的样本数量太少也会放弃分裂，防止树分的太细。</p>
<h2 id="xgboost如何处理不平衡数据？"><a href="#xgboost如何处理不平衡数据？" class="headerlink" title="xgboost如何处理不平衡数据？"></a>xgboost如何处理不平衡数据？</h2><p>（1）如果你在意AUC，采用AUC来评估模型的性能，那你可以通过设置 scale_pos_weight来平衡正样本和负样本的权重。例如，当正负样本比例为1：10的，scale_pos_weigth可以设置为10；<br>（2）如果你在意概率（预测得分的合理性），你不能重新平衡数据集（会破坏数据的真实分布），应该设置max_delta_step为一个有限数字来帮助收敛（基模型为LR时有效）。<br>那么源码到底是怎么利用scale_pos_weight来平衡样本的呢， 是调节权重还是过采样呢？ 源码 <code>if (info.labels[i] == 1.0f)  w *= param_.scale_pos_weight</code> 可以看出，应该是增大了少数样本的权重。<br>除此之外，还可以通过上采样，下采样，SMOTE算法或者自定义代价函数的方式解决正负样本不平衡的问题。</p>
<h2 id="XGBOOS中如何对树进行剪枝？"><a href="#XGBOOS中如何对树进行剪枝？" class="headerlink" title="XGBOOS中如何对树进行剪枝？"></a>XGBOOS中如何对树进行剪枝？</h2><p><span class="exturl" data-url="aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2Fuc2h1YWlfYXcxL2FydGljbGUvZGV0YWlscy84NTA5MzEwNg==">https://blog.csdn.net/anshuai_aw1/article/details/85093106<i class="fa fa-external-link-alt"></i></span><br>（1）在目标函数中增加了正则项。适用叶子节点的数目和叶子节点权重的L2模的平方，控制树的复杂度。<br>（2）在结点分裂时，定义了一个阈值，如何分裂后目标函数的增益小于该阈值，则不分裂。<br>（3）当引入一次分裂后，重新计算新生成的左右两个叶子节点的样本权重和。如果任一个叶子节点的样本权重低于 某一个阈值（最小样本权重值min_child_weight），也会放弃该次分裂。<br>（4）XGBOOST先从顶到底建立树直到最大深度，再从低到顶反向检查是否有不满足分裂条件的结点，进行剪枝。比起GBM，这样不容易陷入局部最优解。</p>
<h2 id="XGBoost的Scalable-可扩展性-性如何体现？"><a href="#XGBoost的Scalable-可扩展性-性如何体现？" class="headerlink" title="XGBoost的Scalable(可扩展性)性如何体现？"></a>XGBoost的Scalable(可扩展性)性如何体现？</h2><p>基分类器: 弱分类器可以支持cart决策树，也可以支持LR和Linear.<br>目标函数: 支持自定义loss function. 只需要其一阶，二阶可导。有这个特性是因为泰勒二阶展开，得到通用的目标函数形式。<br>学习方法: block结构支持并行化，支持out-of-core计算。</p>
<h2 id="XGBOOST如何评价特征的重要性？"><a href="#XGBOOST如何评价特征的重要性？" class="headerlink" title="XGBOOST如何评价特征的重要性？"></a>XGBOOST如何评价特征的重要性？</h2><p>（1）权重：该特征在所有树中被用作分割样本的特征的总次数。<br>（2）增益：该特征在其出现过的所有树中产生的平均增益。<br>（3）覆盖程度：该特征在其出现过的所有树中的平均覆盖范围。 注意： 覆盖范围这里指的是 一个特征用作分割点后，其影响的样本数量， 即有多少样本经过该特征分割为两个子节点。这个是通过被分到该节点的样本的二阶导数之和。举个例子来说，某个特征作为结点的对应分割样本的数目为10， 那么此特征在这棵树上的覆盖度就是这10个样本的二阶导数之和。</p>
<p>很多时候，特征重要度只是给我们一些关于业务场景的信息，同时告诉我们下一步特征工程的方向。只关心哪些更重要，哪些更弱，不是绝对序。不同方式top部分的feature应该基本一致，哪有那么大差异。</p>
<h2 id="XGBOOST参数调优的一般步骤？"><a href="#XGBOOST参数调优的一般步骤？" class="headerlink" title="XGBOOST参数调优的一般步骤？"></a>XGBOOST参数调优的一般步骤？</h2><p>(1) learning rate，estimator<br>    learning rate可以先用0.1， 用cv来寻找最优的estimator（树）的数量</p>
<p>(2) max_depth 和 min_child_weght<br>    这两个参数对输出结果的影响很大，我们首先将这两个参数设置为较大的数，然后通过迭代的方式不断修正，缩小范围。<br>    max_depth  每棵子树的最大深度， check from range(3,10, 2)<br>    min_child_weight, 子节点的权重阈值， check from range(1, 6, 2)<br>    如果一个结点分裂后，他的所有子节点的权重之和都大于该阈值，该叶子节点才可以划分。</p>
<p>(3) gamma<br>    也称做最小划分损失 min_split_loss， check from 0.1 to 0.5,指的是，对于一个叶子节点，当对它采取划分之后，损失函数的降低值的阈值。<br>    如果大于该阈值，则该叶子节点值得继续划分<br>    如果小于该阈值，则该叶子节点不值得继续划分</p>
<p>(4)subsample, colsamle_bytree<br>    subsample是对训练的样本的采样比例<br>    colsample_bytree是对特征的采样比例<br>    both check from 0.6 to 0.9</p>
<p>(5)正则化系数<br>    alpha 是 L1正则化系数， try 1e~5  1e~2  0.1  1 100<br>    lambda是L2正则化系数</p>
<p>(6)降低学习率<br>    降低学习率的同时增加树的数量， 通常最后设置学习率为0.01-0.1</p>
<h2 id="XGBOOST模型如果过拟合了怎么解决？"><a href="#XGBOOST模型如果过拟合了怎么解决？" class="headerlink" title="XGBOOST模型如果过拟合了怎么解决？"></a>XGBOOST模型如果过拟合了怎么解决？</h2><p>当出现过拟合时，有两类参数可以缓解：<br>（1）第一类参数：用于直接控制模型的复杂度。包括 max_depth, min_child_weight, gamma 等参数。<br>（2）第二类参数： 用于增加随机性，从而使得模型在训练时对于噪音不敏感，包括 subsample, colsample_bytree。<br>（3）还有就是直接减小 learning_rate, 但需要同时增加estimator参数。</p>
<h2 id="XGBOOST和lightGBM的区别？"><a href="#XGBOOST和lightGBM的区别？" class="headerlink" title="XGBOOST和lightGBM的区别？"></a>XGBOOST和lightGBM的区别？</h2><p>（1）树生长策略： XGB采用<strong>level-wise</strong>的分裂策略，LGB采用<strong>leaf-wise</strong>的分类策略。<br>XGB的level-wise，对每一层的所有节点做无差别分类，但是可能有些节点的增益非常小，对结果影响不大，带来不必要的开销。leaf-wise是在所有叶子节点中选取分裂收益最大的节点进行的，但是很容易出现过拟合问题，所以需要对最大深度做限制。</p>
<blockquote>
<p>level-wise过一次数据可以同时分裂同一层的叶子，容易进行多线程优化，也好控制模型复杂度，不容易过拟合。但实际上level-wise是一种低效的算法，因为他不加区分的对待同一层的叶子，带来了很多不必要的开销，因为实际上很多叶子的分裂增益较低，没必要进行搜索和分裂。<br>leaf-wise 则是一种更为高效的策略，每次从当前所有叶子中，找到分割增益最大的一个叶子，然后分类，如此循环。因此同level-wise相比，leaf-wise可以降低更多的误差，得到更多的精度。leaf-wise的缺点是可能会长出比较深的决策树，产生过拟合。因此LightGBM在leaf-wise之上增加了一个最大深度的限制，以保证高效率的同时防止过拟合。</p>
</blockquote>
<p>（2）分割点查找算法：XGB使用<strong>特征预排序</strong>算法，LGB使用<strong>直方图的切分点</strong>算法。</p>
<blockquote>
<p>直方图的切分点算法优势：减少内存占用，比如离散为256个bin时，只需要用8位整形就可以保存一个样本被映射为哪个bin(这个bin 可以说就是转换后的特征)，对比预排序的exect greedy 算法来说（用int_32来存储索引+ 用float_3保存特征值）， 可以节省7/8的空间。</p>
<p>当然，Histogram算法不是完美的，由于特征呗离散化后，找到的并不是很精确的分割点，所以会对结果产生影响。但是在不同的数据集上的结果表明，离散化的分割点对最终的精度影响并不是很大，甚至有时候会更好一点。原因就是决策树本来就是弱模型，分割点是不是精确并不是太重要，较粗的分割点也有正则化的效果，可以有效地防止过拟合；即使单颗树的训练误差比精确分割的算法稍大，但在梯度提升的框架下没有太大的影响。</p>
<p>计算效率提高，预排序的Exact greedy 对每个特征都需要遍历一遍数据，并计算增益，复杂度为𝑂(#𝑓𝑒𝑎𝑡𝑢𝑟𝑒×#𝑑𝑎𝑡𝑎)。而直方图算法在建立完直方图后，只需要对每个特征遍历直方图即可，复杂度为𝑂(#𝑓𝑒𝑎𝑡𝑢𝑟𝑒×#𝑏𝑖𝑛𝑠)。</p>
<p>LGB还可以使用直方图做差加速，一个节点的直方图可以通过父节点的直方图减去兄弟节点的直方图得到，从而加速计算。但实际上XGBOOST的近似直方图算法也类似于lightGBM这里的直方图算法，为什么xgboost近似算法比lightGBM还是慢很多呢？</p>
<blockquote>
<p>XGBOOST在每一层都动态构建直方图，因为xgboost的直方图算法不是针对某个特定的feature，而是所有feature共享一个直方图（每个样本的权重是二阶导），所以每一层都要重新构建直方图，而lightgbm中对每个特征都有一个直方图，所以构建一次直方图就够了。</p>
</blockquote>
</blockquote>
<p>（3）支持离散变量：xgb无法直接输入类别型变量，因此需要事先对类别型变量进行编码(例如独热编码)，而lightGBM可以直接处理类别型变量。<br>（4）缓存命中率：xgb使用block结构的一个缺点就是取梯度的时候，是通过索引来获取的，而这些梯度的获取顺序是按照特征的大小排序的，这将导致非连续的内存访问，可能使得cpu cache缓存命中率低，从而影响算法效率。而LGB是基于直方图分裂特征的，梯度信息都存储在一个个bin中，所以访问梯度是连续的，缓存命中率高。</p>
<blockquote>
<p>解释： 直方图算法的基本思想，先把连续的浮点特征值离散化成为k个整数，同时构造一个宽度为k的直方图。遍历数据时，根据离散后的值作为索引在直方图中累计统计量，当遍历一次数据后，直方图累积了需要的统计量，然后根据直方图的离散值，遍历寻找最后的分割点。</p>
</blockquote>
<p>（5）lightGBM在训练过程中采用单边梯度算法过滤掉梯度小的样本，减少了大量的计算。</p>
<p>（6）lightGBM和XGBOOST的并行策略不同：<br><strong>特征并行</strong>：<br>LGB特征并行：前提是每个worker留有一份完整的数据集，但是每个worker仅在特征子集上进行最佳切分点的寻找；workder之间需要互相通信，通过对比损失来确定最佳切分点；然后将这个最佳切分点的位置进行全局广播，每个work进行切分即可。<br>XBG的特征并行：XGB每个worker节点中仅有部分的列数据，也就是垂直切分，每个worker寻找局部最佳切分点，worker之间相互通信，然后在具有最佳切分点的worker上进行节点分裂，再由这个节点广播一下被切分到左右节点的样本索引号，其他workder才能开始分裂。两者的区别就导致了LGB的worker间通信成本明显降低，只需要通信一个特征分裂点即可，而XBG中要广播样本索引。</p>
<blockquote>
<p>特征并行的主要思想是在不同机器在不同的特征集合上分别寻找最优的分割点，然后在机器间同步最优的分割点。<br>XGBOOST特征并行算法</p>
<ol>
<li>根据不同的特征子集，将数据集进行垂直切分。（不同机器worker有不同的特征子集）</li>
<li>每个worker寻找局部的最优分裂特征以及分裂点。</li>
<li>不同worker之间进行网络传输，交换最优分裂信息，最终得到最优的分裂信息。</li>
<li>具有最优分类特征的worker,局部进行分裂，并将分裂结果广播到其他worker。（这里解释下，比如找到特征1的最佳划分，即将样本分为两部分，比如第一个样本在左子树，第二个样本在右子树，然后把这个信息广播到其他worker）</li>
<li>其他worker根据接收到的数据进行切分数据（比如第一个样本在左子树，第二个样本在右子树）</li>
</ol>
<p>LGB的特征并行算法。LGB并没有垂直的切分数据集，而是每个worker都有全量的训练数据，因此最优的特征分裂结果不需要传输到其他worker中，只需要将最优特征以及分裂点告诉其他worker,worker随后本地自己处理。处理过程如下：</p>
<ol>
<li>每个worker在基于局部的特征集合找到最优分裂特征。（每个worker只处理几个特征）</li>
<li>worker间传输最优分裂信息，并得到全局最优分裂信息。</li>
<li>每个worker基于全局最优分裂信息，在本地进行数据分裂，生成决策树。<br>然后，当数据量很大时，特征并行方法还是受限于特征分裂效率。因此，当数据量大时，推荐使用数据并行算法。</li>
</ol>
</blockquote>
<p><strong>数据并行</strong>：当数据量很大，特征相对较少时，可采用数据并行策略。<br>LGB：先对数据水平切分，每个worker上的数据先建立起局部的直方图，然后合并成全局的直方图，采用直方图相减的方式，先计算样本量少的节点的样本索引，然后直接相减得到另一子节点的样本索引。这个直方图算法使得worker间的通信成本降低一倍，因为只通信样本量少的节点。<br>XGB：也是水平切分，然后单个worker建立局部直方图，再合并为全局，不同在于根据全局直方图，各个worker进行节点分裂时会单独计算子节点的样本索引，因此效率贼低，每个worker间的通信量也就变的很大。</p>
<blockquote>
<p>数据并行 则是让不同的机器先在 本地构造直方图，然后进行全局的合并，最后在合并的直方图上面寻找最优分割点。<br>在数据并行中使用分散规约（reduce scatter）把直方图合并的任务分摊到不同的机器，降低通信和计算，并利用直方图做差，进一步减小了一般的通信量。基于投票的数据并行则进一步优化数据并行中通信代价，使通信代价变成常数级别。在数据量很大的时候，使用投票并行可以得到非常好的加速效果。<br>XGBOOST算法里，数据并行目标是并行化整个决策学习的过程：</p>
<ol>
<li>水平切分数据，不同的worker拥有部分数据</li>
<li>每个worker根据本地数据构建局部直方图</li>
<li>合并所有的局部直方图得到全部直方图</li>
<li>根据全局直方图找到最优切分点并进行分裂<br>在第3步中，有两种合并的方式：(1) 采用点对点方式(point-to-point communication algorithm)进行通讯，每个worker通讯量为O(#machine∗#feature∗#bin)。 (2) 采用collective communication algorithm(如“All Reduce”)进行通讯（相当于有一个中心节点，通讯后再返回结果），每个worker的通讯量为O(2∗#feature∗#bin) 可以看出通信的代价是很高的，这也是数据并行的缺点。</li>
</ol>
<p>LightGBM中的数据并行：<br>使用“Reduce Scatter”将不同worker的不同特征的直方图合并，然后workers在局部合并的直方图中找到局部最优划分，最后同步全局最优划分。前面提到过，可以通过直方图作差法得到兄弟节点的直方图，因此只需要通信一个节点的直方图。通过上述两点做法，通信开销降为O(0.5∗#feature∗#bin) 。</p>
<p>投票并行（LGB）：当数据量和维度都很大时，选用投票并行，该方法是 数据并行的一个改进。数据并行中的合并直方图的代价相对较大，尤其是当特征维度很大时，大致思想是 每个worker首先会找到本地的一些优秀的特征，然后进行全局投票，根据投票结果，选择top的特征进行直方图的合并，再寻求全局的最优分割点。</p>
</blockquote>
<p>lightGBM其他注意事项：<br>当生长相同的叶子时，leaf-wise比level-wise减少更多的损失。<br>高速 高效处理大数据，运行时需要更低的内存，支持GPU。<br>不要在少量数据上使用，会过拟合，建议10000+行记录时 使用。</p>
<h2 id="超参数调优？"><a href="#超参数调优？" class="headerlink" title="超参数调优？"></a>超参数调优？</h2><p>网格搜索，随机搜索，贝叶斯优化。</p>
<h2 id="模型常用超参数？"><a href="#模型常用超参数？" class="headerlink" title="模型常用超参数？"></a>模型常用超参数？</h2><p><img src="/images/面试/超参数调优.jpg" width="80%"></p>
<h2 id="熵的概念，交叉熵？"><a href="#熵的概念，交叉熵？" class="headerlink" title="熵的概念，交叉熵？"></a>熵的概念，交叉熵？</h2><h2 id="交叉熵损失函数是什么，在什么地方使用过？"><a href="#交叉熵损失函数是什么，在什么地方使用过？" class="headerlink" title="交叉熵损失函数是什么，在什么地方使用过？"></a>交叉熵损失函数是什么，在什么地方使用过？</h2><h1 id="深度学习"><a href="#深度学习" class="headerlink" title="深度学习"></a>深度学习</h1><h2 id="LSTM和RNN区别，有什么优化？RNN为什么会有梯度消失，LSTM如何解决梯度消失？"><a href="#LSTM和RNN区别，有什么优化？RNN为什么会有梯度消失，LSTM如何解决梯度消失？" class="headerlink" title="LSTM和RNN区别，有什么优化？RNN为什么会有梯度消失，LSTM如何解决梯度消失？"></a>LSTM和RNN区别，有什么优化？RNN为什么会有梯度消失，LSTM如何解决梯度消失？</h2><h2 id="CNN为什么能做文本分类，超参数怎么确定？"><a href="#CNN为什么能做文本分类，超参数怎么确定？" class="headerlink" title="CNN为什么能做文本分类，超参数怎么确定？"></a>CNN为什么能做文本分类，超参数怎么确定？</h2><h2 id="详细讲一下不同的文本表征向量化方法，对比区别？"><a href="#详细讲一下不同的文本表征向量化方法，对比区别？" class="headerlink" title="详细讲一下不同的文本表征向量化方法，对比区别？"></a>详细讲一下不同的文本表征向量化方法，对比区别？</h2><h1 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h1><h2 id="reducebykey如何运行，和groupbykey相比高效在哪？"><a href="#reducebykey如何运行，和groupbykey相比高效在哪？" class="headerlink" title="reducebykey如何运行，和groupbykey相比高效在哪？"></a>reducebykey如何运行，和groupbykey相比高效在哪？</h2><h1 id="word2vec的原理，负例采样怎么做，word2vec有什么特点，CBOW和Skip-gram区别？"><a href="#word2vec的原理，负例采样怎么做，word2vec有什么特点，CBOW和Skip-gram区别？" class="headerlink" title="word2vec的原理，负例采样怎么做，word2vec有什么特点，CBOW和Skip-gram区别？"></a>word2vec的原理，负例采样怎么做，word2vec有什么特点，CBOW和Skip-gram区别？</h1><h1 id="attention机制有什么特别的地方？了解哪些不同的attention，self-attention为什么有效？"><a href="#attention机制有什么特别的地方？了解哪些不同的attention，self-attention为什么有效？" class="headerlink" title="attention机制有什么特别的地方？了解哪些不同的attention，self-attention为什么有效？"></a>attention机制有什么特别的地方？了解哪些不同的attention，self-attention为什么有效？</h1><h1 id="seq2seq是什么结构，transformer呢？"><a href="#seq2seq是什么结构，transformer呢？" class="headerlink" title="seq2seq是什么结构，transformer呢？"></a>seq2seq是什么结构，transformer呢？</h1><h1 id="NLP的数据预处理通常做什么？"><a href="#NLP的数据预处理通常做什么？" class="headerlink" title="NLP的数据预处理通常做什么？"></a>NLP的数据预处理通常做什么？</h1><h1 id="Fasttext和word2vec区别？"><a href="#Fasttext和word2vec区别？" class="headerlink" title="Fasttext和word2vec区别？"></a>Fasttext和word2vec区别？</h1><h1 id="QA建模方法有哪些？"><a href="#QA建模方法有哪些？" class="headerlink" title="QA建模方法有哪些？"></a>QA建模方法有哪些？</h1><h1 id="Bert做QA系统，数据输入和下游任务如何改造？"><a href="#Bert做QA系统，数据输入和下游任务如何改造？" class="headerlink" title="Bert做QA系统，数据输入和下游任务如何改造？"></a>Bert做QA系统，数据输入和下游任务如何改造？</h1><h1 id="某个类别效果很差，怎么办？"><a href="#某个类别效果很差，怎么办？" class="headerlink" title="某个类别效果很差，怎么办？"></a>某个类别效果很差，怎么办？</h1><h1 id="如何找到对分类影响最大的关键词？"><a href="#如何找到对分类影响最大的关键词？" class="headerlink" title="如何找到对分类影响最大的关键词？"></a>如何找到对分类影响最大的关键词？</h1><h1 id="深度学习如何提取query特征，如何利用深度学习计算语义相似度？"><a href="#深度学习如何提取query特征，如何利用深度学习计算语义相似度？" class="headerlink" title="深度学习如何提取query特征，如何利用深度学习计算语义相似度？"></a>深度学习如何提取query特征，如何利用深度学习计算语义相似度？</h1><h1 id="Dropout原理？"><a href="#Dropout原理？" class="headerlink" title="Dropout原理？"></a>Dropout原理？</h1><h1 id="神经网络训练出现nan，应该从哪些方面去查？"><a href="#神经网络训练出现nan，应该从哪些方面去查？" class="headerlink" title="神经网络训练出现nan，应该从哪些方面去查？"></a>神经网络训练出现nan，应该从哪些方面去查？</h1><p>nan的错误多源于<strong>学习率太大</strong>或者<strong>batch size太大</strong>，可以不断的10倍减小学习率直到nan错误不出现。</p>
<p><strong>梯度爆炸</strong>：这个错误是因为logits输出太大变成INF，对这个取log就会在求梯度就会变成nan。<strong>梯度截断</strong>来解决。<br><strong>不当的损失函数</strong>：有时候损失层中loss的计算可能导致NaN的出现。比如，给信息熵损失的输入没有归一化的值，使用带有bug的自定义损失层，0作为除数。观测训练产生的log时一开始并不能看到异常，loss也在逐步的降低，但突然之间NaN就出现了。<br><strong>输入中有nan</strong>：输入中就含有NaN，每当学习的过程中碰到这个错误的输入，就会变成NaN。观察log的时候也许不能察觉任何异常，loss逐步的降低，但突然间就变成NaN了。</p>
<h1 id="loss不下降可能是什么情况？"><a href="#loss不下降可能是什么情况？" class="headerlink" title="loss不下降可能是什么情况？"></a>loss不下降可能是什么情况？</h1><p>loss不降的原因可能数据里有错误，数据本身就含有了nan的数据，错误的数据导致网络无法收敛。</p>
<h1 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h1><p><span class="exturl" data-url="aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2xpYW5nZG9uZzIwMTQvYXJ0aWNsZS9kZXRhaWxzLzc5NTE3NjM4">为什么L1正则项产生稀疏的权重，L2正则项产生相对平滑的权重<i class="fa fa-external-link-alt"></i></span><br><span class="exturl" data-url="aHR0cHM6Ly96aHVhbmxhbi56aGlodS5jb20vcC84NTczMTIwNg==">【机器学习】决策树（上）——ID3、C4.5、CART<i class="fa fa-external-link-alt"></i></span><br><span class="exturl" data-url="aHR0cHM6Ly96aHVhbmxhbi56aGlodS5jb20vcC84NjI2Mzc4Ng==">【机器学习】决策树（中）——Random Forest、Adaboost、GBDT<i class="fa fa-external-link-alt"></i></span><br><span class="exturl" data-url="aHR0cHM6Ly96aHVhbmxhbi56aGlodS5jb20vcC84Nzg4NTY3OA==">【机器学习】决策树（下）——XGBoost、LightGBM<i class="fa fa-external-link-alt"></i></span><br><span class="exturl" data-url="aHR0cHM6Ly9tdWJ1LmNvbS9kb2MvY1VOOHBiRTdNMA==">总结<i class="fa fa-external-link-alt"></i></span><br><a href="随机森林(Random Forest">随机森林(Random Forest)</a>)</p>

    </div>

    
    
    
      
  <div class="popular-posts-header">相关文章</div>
  <ul class="popular-posts">
    <li class="popular-posts-item">
      <div class="popular-posts-title"><a href="\2019\01\31\Other\算法题合集\" rel="bookmark">算法题</a></div>
    </li>
  </ul>


    <footer class="post-footer">
          

<div class="post-copyright">
<ul>
  <li class="post-copyright-author">
      <strong>本文作者： </strong>SoundMemories
  </li>
  <li class="post-copyright-link">
      <strong>本文链接：</strong>
      <a href="https://soundmemories.github.io/2019/01/21/Other/%E9%9D%A2%E8%AF%95%E9%A2%98%E9%9B%86/" title="面试题集">https://soundmemories.github.io/2019/01/21/Other/面试题集/</a>
  </li>
  <li class="post-copyright-license">
    <strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <span class="exturl" data-url="aHR0cHM6Ly9jcmVhdGl2ZWNvbW1vbnMub3JnL2xpY2Vuc2VzL2J5LW5jLXNhLzQuMC8="><i class="fab fa-fw fa-creative-commons"></i>BY-NC-SA</span> 许可协议。转载请注明出处！
  </li>
</ul>
</div>

          <div class="post-tags">
              <a href="/tags/Other/" rel="tag"><i class="fa fa-tag"></i> Other</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2019/01/21/Python/10.Python-%E9%AB%98%E9%98%B6%E5%87%BD%E6%95%B0%E3%80%81%E8%A3%85%E9%A5%B0%E5%99%A8%E3%80%81functools%E6%BA%90%E7%A0%81/" rel="prev" title="Python-高阶函数、装饰器、functools源码">
                  <i class="fa fa-chevron-left"></i> Python-高阶函数、装饰器、functools源码
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2019/01/22/Python/11.Python-%E7%B1%BB%E5%9E%8B%E6%B3%A8%E9%87%8A/" rel="next" title="Python-类型注解">
                  Python-类型注解 <i class="fa fa-chevron-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






      

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      const activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      const commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

    </div>
  </main>

  <footer class="footer">
    <div class="footer-inner">
      

      

<div class="copyright">
  
  &copy; 2019 – 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">SoundMemories</span>
</div>
  <div class="powered-by">由 <span class="exturl theme-link" data-url="aHR0cHM6Ly9oZXhvLmlv">Hexo</span> & <span class="exturl theme-link" data-url="aHR0cHM6Ly90aGVtZS1uZXh0LmpzLm9yZy9tdXNlLw==">NexT.Muse</span> 强力驱动
  </div>

    </div>
  </footer>

  
  <script src="//cdn.jsdelivr.net/npm/animejs@3.2.1/lib/anime.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/@next-theme/pjax@0.4.0/pjax.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/jquery@3.5.1/dist/jquery.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js"></script>
<script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/muse.js"></script><script src="/js/next-boot.js"></script><script src="/js/bookmark.js"></script>
  <script>
var pjax = new Pjax({
  selectors: [
    'head title',
    '.page-configurations',
    '.main-inner',
    '.post-toc-wrap',
    '.languages',
    '.pjax'
  ],
  analytics: false,
  cacheBust: false,
  scrollRestoration: false,
  scrollTo: !CONFIG.bookmark.enable
});

document.addEventListener('pjax:success', () => {
  pjax.executeScripts(document.querySelectorAll('script[data-pjax], .pjax script'));
  NexT.boot.refresh();
  // Define Motion Sequence & Bootstrap Motion.
  if (CONFIG.motion.enable) {
    NexT.motion.integrator
      .init()
      .add(NexT.motion.middleWares.subMenu)
      .add(NexT.motion.middleWares.postList)
      .bootstrap();
  }
  const hasTOC = document.querySelector('.post-toc');
  document.querySelector('.sidebar-inner').classList.toggle('sidebar-nav-active', hasTOC);
  document.querySelector(hasTOC ? '.sidebar-nav-toc' : '.sidebar-nav-overview').click();
  NexT.utils.updateSidebarPosition();
});
</script>


  




  <script src="/js/local-search.js"></script>








<script data-pjax>
document.querySelectorAll('.pdfobject-container').forEach(element => {
  const url = element.dataset.target;
  const pdfOpenParams = {
    navpanes : 0,
    toolbar  : 0,
    statusbar: 0,
    pagemode : 'thumbs',
    view     : 'FitH'
  };
  const pdfOpenFragment = '#' + Object.entries(pdfOpenParams).map(([key, value]) => `${key}=${encodeURIComponent(value)}`).join('&');
  const fullURL = `/lib/pdf/web/viewer.html?file=${encodeURIComponent(url)}${pdfOpenFragment}`;

  if (NexT.utils.supportsPDFs()) {
    element.innerHTML = `<embed class="pdfobject" src="${url + pdfOpenFragment}" type="application/pdf" style="height: ${element.dataset.height};">`;
  } else {
    element.innerHTML = `<iframe src="${fullURL}" style="height: ${element.dataset.height};" frameborder="0"></iframe>`;
  }
});
</script>


<script data-pjax>
if (document.querySelectorAll('.mermaid').length) {
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/mermaid@8.8.2/dist/mermaid.min.js', () => {
    mermaid.init({
      theme    : 'neutral',
      logLevel : 3,
      flowchart: { curve     : 'linear' },
      gantt    : { axisFormat: '%m/%d/%Y' },
      sequence : { actorMargin: 50 }
    }, '.mermaid');
  }, window.mermaid);
}
</script>





  








    <div class="pjax">
  

  
      <script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              const target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    const script = document.createElement('script');
    script.src = '//cdn.jsdelivr.net/npm/mathjax@3.1.2/es5/tex-mml-chtml.js';
    script.defer = true;
    document.head.appendChild(script);
  } else {
    MathJax.startup.document.state(0);
    MathJax.typesetClear();
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  
  <script src="//cdn.jsdelivr.net/npm/quicklink@2.0.0/dist/quicklink.umd.js"></script>
  <script>
      window.addEventListener('load', () => {
      quicklink.listen({
        timeout : 3000,
        priority: true,
        ignores : [uri => uri.includes('#'),uri => uri === 'https://soundmemories.github.io/2019/01/21/Other/%E9%9D%A2%E8%AF%95%E9%A2%98%E9%9B%86/',]
      });
      });
  </script>

    </div>
</body>
</html>
