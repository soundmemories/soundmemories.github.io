<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.2.0">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="https://fonts.loli.net/css?family=Monda:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">

<link rel="stylesheet" href="//cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.1/css/all.min.css">
  <link rel="stylesheet" href="//cdn.jsdelivr.net/npm/animate.css@3.1.1/animate.min.css">
  <link rel="stylesheet" href="//cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.css">

<script class="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"soundmemories.github.io","root":"/","images":"/images","scheme":"Muse","version":"8.0.2","exturl":true,"sidebar":{"position":"left","display":"always","padding":18,"offset":12},"copycode":true,"bookmark":{"enable":true,"color":"#222","save":"auto"},"fancybox":true,"mediumzoom":true,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"},"path":"/search.xml","localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false}};
  </script>

  <meta name="description" content="图论最开始是1707年由 Seven bridges 问题而来的，欧拉把这个问题转成图解决。近代图论1959年 Erdos 和 Renyi 发表的 Random Graph 一系列论文建立起来的全新领域。目前学术界和工业界是基于1999年 brabasi 提出的 Scale-free Network 进行的。 degree-distribution度分布，即每个节点按边的数量分类，每类节点占总结点">
<meta property="og:type" content="article">
<meta property="og:title" content="GCN">
<meta property="og:url" content="https://soundmemories.github.io/2021/02/05/Graph/01.GCN/index.html">
<meta property="og:site_name" content="SoundMemories">
<meta property="og:description" content="图论最开始是1707年由 Seven bridges 问题而来的，欧拉把这个问题转成图解决。近代图论1959年 Erdos 和 Renyi 发表的 Random Graph 一系列论文建立起来的全新领域。目前学术界和工业界是基于1999年 brabasi 提出的 Scale-free Network 进行的。 degree-distribution度分布，即每个节点按边的数量分类，每类节点占总结点">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://soundmemories.github.io/images/GCN/随机网络.png">
<meta property="og:image" content="https://soundmemories.github.io/images/GCN/无标度网络.png">
<meta property="og:image" content="https://soundmemories.github.io/images/GCN/邻接矩阵.png">
<meta property="og:image" content="https://soundmemories.github.io/images/GCN/邻接矩阵2.png">
<meta property="og:image" content="https://soundmemories.github.io/images/GCN/LM.png">
<meta property="article:published_time" content="2021-02-04T16:00:00.000Z">
<meta property="article:modified_time" content="2021-09-14T08:16:14.377Z">
<meta property="article:author" content="SoundMemories">
<meta property="article:tag" content="Graph">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://soundmemories.github.io/images/GCN/随机网络.png">


<link rel="canonical" href="https://soundmemories.github.io/2021/02/05/Graph/01.GCN/">


<script data-pjax class="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>GCN | SoundMemories</title>
  






  <noscript>
  <style>
  body { margin-top: 2rem; }

  .use-motion .menu-item,
  .use-motion .sidebar,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header {
    visibility: visible;
  }

  .use-motion .header,
  .use-motion .site-brand-container .toggle,
  .use-motion .footer { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle,
  .use-motion .custom-logo-image {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line {
    transform: scaleX(1);
  }

  .search-pop-overlay, .sidebar-nav { display: none; }
  .sidebar-panel { display: block; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <h1 class="site-title">SoundMemories</h1>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu">
        <li class="menu-item menu-item-主页">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>主页</a>

  </li>
        <li class="menu-item menu-item-分类">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a>

  </li>
        <li class="menu-item menu-item-标签">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a>

  </li>
        <li class="menu-item menu-item-归档">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
        <li class="menu-item menu-item-关于">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" maxlength="80"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="search-result-container no-result">
  <div class="search-result-icon">
    <i class="fa fa-spinner fa-pulse fa-5x"></i>
  </div>
</div>

    </div>
  </div>

</div>
        
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>

  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>
	   
		  
      <div class="sidebar-panel-container">
        <!--noindex-->
        <section class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%9B%BE%E8%AE%BA"><span class="nav-number">1.</span> <span class="nav-text">图论</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#degree-distribution"><span class="nav-number">1.1.</span> <span class="nav-text">degree-distribution</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Distance-Hilbert-Space"><span class="nav-number">1.2.</span> <span class="nav-text">Distance(Hilbert Space)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#adjacency-matrix"><span class="nav-number">1.3.</span> <span class="nav-text">adjacency matrix</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Clustering-coefficient"><span class="nav-number">1.4.</span> <span class="nav-text">Clustering coefficient</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Betweenness"><span class="nav-number">1.5.</span> <span class="nav-text">Betweenness</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#GCN"><span class="nav-number">2.</span> <span class="nav-text">GCN</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Fourier-transform"><span class="nav-number">2.1.</span> <span class="nav-text">Fourier transform</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Laplacian-operater"><span class="nav-number">2.2.</span> <span class="nav-text">Laplacian operater</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Graph-Laplacian-operater"><span class="nav-number">2.3.</span> <span class="nav-text">Graph Laplacian operater</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Laplacian-Matrix"><span class="nav-number">2.4.</span> <span class="nav-text">Laplacian Matrix</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Graph-Fourier-transform"><span class="nav-number">2.5.</span> <span class="nav-text">Graph Fourier transform</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Graph-Convolution"><span class="nav-number">2.6.</span> <span class="nav-text">Graph Convolution</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Graph-Convolution-Networks"><span class="nav-number">2.7.</span> <span class="nav-text">Graph Convolution Networks</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Code"><span class="nav-number">3.</span> <span class="nav-text">Code</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%8F%82%E8%80%83%E6%96%87%E7%8C%AE"><span class="nav-number">4.</span> <span class="nav-text">参考文献</span></a></li></ol></div>
        </section>
        <!--/noindex-->

        <section class="site-overview-wrap sidebar-panel">
          <div class="site-author site-overview-item animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="SoundMemories"
      src="https://i.loli.net/2020/11/04/6JhNuwtBe4adylS.png">
  <p class="site-author-name" itemprop="name">SoundMemories</p>
  <div class="site-description" itemprop="description">今日事，今日毕</div>
</div>
<div class="site-state-wrap site-overview-item animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives">
          <span class="site-state-item-count">117</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
        <span class="site-state-item-count">8</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
        <span class="site-state-item-count">8</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author site-overview-item animated">
      <span class="links-of-author-item">
        <span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL3NvdW5kbWVtb3JpZXM=" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;soundmemories"><i class="fab fa-github fa-fw"></i>GitHub</span>
      </span>
      <span class="links-of-author-item">
        <span class="exturl" data-url="bWFpbHRvOnNvdW5kbWVtb3JpZXNAMTYzLmNvbQ==" title="E-Mail → mailto:soundmemories@163.com"><i class="fa fa-envelope fa-fw"></i>E-Mail</span>
      </span>
  </div>
  <div class="cc-license site-overview-item animated" itemprop="license">
    <span class="exturl cc-opacity" data-url="aHR0cHM6Ly9jcmVhdGl2ZWNvbW1vbnMub3JnL2xpY2Vuc2VzL2J5LW5jLXNhLzQuMC8="><img src="/images/cc-by-nc-sa.svg" alt="Creative Commons"></span>
  </div>



        </section>
      </div>
    </div>
  </aside>
  <div class="sidebar-dimmer"></div>


    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>
  <a role="button" class="book-mark-link book-mark-link-fixed"></a>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


    <div class="main-inner post posts-expand">
      

      

  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://soundmemories.github.io/2021/02/05/Graph/01.GCN/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="https://i.loli.net/2020/11/04/6JhNuwtBe4adylS.png">
      <meta itemprop="name" content="SoundMemories">
      <meta itemprop="description" content="今日事，今日毕">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="SoundMemories">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          GCN
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2021-02-05 00:00:00" itemprop="dateCreated datePublished" datetime="2021-02-05T00:00:00+08:00">2021-02-05</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Graph/" itemprop="url" rel="index"><span itemprop="name">Graph</span></a>
        </span>
    </span>

  
      </div>
      <div class="post-meta">
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>19k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>17 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
        <h1 id="图论"><a href="#图论" class="headerlink" title="图论"></a>图论</h1><p>最开始是1707年由 Seven bridges 问题而来的，欧拉把这个问题转成图解决。近代图论1959年 Erdos 和 Renyi 发表的 Random Graph 一系列论文建立起来的全新领域。目前学术界和工业界是基于1999年 brabasi 提出的 Scale-free Network 进行的。</p>
<h2 id="degree-distribution"><a href="#degree-distribution" class="headerlink" title="degree-distribution"></a>degree-distribution</h2><p><strong>度分布，即每个节点按边的数量分类，每类节点占总结点比例</strong>。一般 Random Graph 的度分布是 <strong>泊松分布</strong>（poisson，类似正态分布）。但基于 Scale-free Network 的提出，我们发现很多网络结构是 <strong>幂律分布</strong>（power-law）。</p>
<p><img src="/images/GCN/随机网络.png" width="80%"><br><img src="/images/GCN/无标度网络.png" width="80%"></p>
<h2 id="Distance-Hilbert-Space"><a href="#Distance-Hilbert-Space" class="headerlink" title="Distance(Hilbert Space)"></a>Distance(Hilbert Space)</h2><p>每个节点具有多维features，可以看成多维空间，计算Distance也可看作Similarity。为什么在 Hilbert Space ，因为要方便后面约束优化。</p>
<p>GCN要求：<br>（1）weights $\geq$ 0<br>（2）linear calculation<br>（3）Inner product</p>
<p>Hilbert Space（括号表示内积）：<br>（1）对称性：$(\vec{y}, \vec{x})$ = $(\vec{x}, \vec{y})$<br>（2）线性：$(a\vec{x_1}+b \vec{x_2}, \vec{y})$ = $a(\vec{x_1}, \vec{y})+b(\vec{x_2}, \vec{y})$<br>（3）半正定性：$(\vec{x}, \vec{x}) \geq 0$，if $\vec{x}=\vec{0}$，$(\vec{x}, \vec{x}) = 0$</p>
<p>Distance(Hilbert Space)：$d(\vec{x}, \vec{y})=||\vec{x}-\vec{y}||=\sqrt{(\vec{x}-\vec{y}, \vec{x}-\vec{y})}$</p>
<h2 id="adjacency-matrix"><a href="#adjacency-matrix" class="headerlink" title="adjacency matrix"></a>adjacency matrix</h2><p>邻接矩阵。阶为$n$的图$G$的邻接矩阵$A$是$n\times n$的。将$G$的顶点标签为$v_{1},v_{2},…,v_{n}$。若$(v_{i},v_{j})\in E(G)$，$A_{ij}=1$，否则$A_{ij}=0$。也可以用大于0的值表示边的权值，例如可以用边权值表示一个点到另一个点的距离。</p>
<p>无向图的邻接矩阵计算方法是每条边为对应的单元加上1，而每个自环加上2。这样让某一节点的度数可以通过邻接矩阵的对应行或者列求和得到。<br><img src="/images/GCN/邻接矩阵.png" width="50%"></p>
<p>有向图的邻接矩阵可以是不对称的。我们可以定义有向图的邻接矩阵中的某个元素 $A_{ij}$ 代表：<br>（1）从 $i$ 指向 $j$ 的边数目。<br>（2）从 $j$ 指向 $i$ 的边数目。<br>在第一种定义下，有向图的某个节点的入度可以通过对应的列（column）求和而得，出度可以通过对应的行（row）求和而得。在第二种定义下，入度可以通过对应的行（row）求和而得，出度可以通过对应的列（column）求和而得。<br><img src="/images/GCN/邻接矩阵2.png" width="50%"></p>
<h2 id="Clustering-coefficient"><a href="#Clustering-coefficient" class="headerlink" title="Clustering coefficient"></a>Clustering coefficient</h2><p>聚类系数，一个图中的顶点之间结集成团的程度的系数。集聚系数分为整体与局部两种。整体集聚系数可以给出一个图中整体的集聚程度的评估，而局部集聚系数则可以测量图中每一个结点附近的集聚程度。</p>
<p><strong>分子</strong>：闭三点组（邻近三点组成“三角形”）数量。<br><strong>分母</strong>：闭三点组（邻近三点组成“三角形”）数量 + 开三点组（邻近三点组成“缺一条边的三角形”）数量。</p>
<p>整体聚类系数：对每个节点的聚类系数求和取均值。<strong>如果该值很大，表示图是很稠密的，节点和节点之间联系紧密</strong>。</p>
<h2 id="Betweenness"><a href="#Betweenness" class="headerlink" title="Betweenness"></a>Betweenness</h2><p>也叫Betweenness centrality，分node和edge两种计算方法：<br>（1）<strong>node Betweenness</strong>：计算经过一个点的最短路径的数量占所有最短路径数量比例。两点一组，遍历所有组，计算每组中经过一个点的最短路径的数量占该组最短路径数量比例，最后求和。<br>（2）<strong>edge Betweenness</strong>：node Betweenness 换成边即可。</p>
<p><strong>这个值很大，表示此node或edge很重要，因为很多最短路径都要经过它，表现流通性</strong>。</p>
<p>缺点：需要遍历所有点找到所有最短路径，一般的算法有 迪杰斯特拉算法（Dijkstra） 和 弗洛伊德算法（Floyd），时间复杂度都为 $O(n^3)$。</p>
<h1 id="GCN"><a href="#GCN" class="headerlink" title="GCN"></a>GCN</h1><p>GCN（Graph Convolutional Networks，图卷积神经网络），实际上跟CNN的作用一样，就是一个特征提取器，只不过它的对象是图数据。<br>GCN精妙地设计了一种从图数据中提取特征的方法，从而让我们可以使用这些特征去对图数据进行<strong>节点分类</strong>（node classification）、<strong>图分类</strong>（graph classification）、<strong>边预测</strong>（link prediction），还可以顺便得到<strong>图的嵌入表示</strong>（graph embedding）。</p>
<p>GCN发展历史，那么肯定绕不过下面三篇论文：<br><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzEzMTIuNjIwMy5wZGY=">Spectral Networks and Deep Locally Connected Networks on Graphs<i class="fa fa-external-link-alt"></i></span> 2014年<br><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzE2MDYuMDkzNzUucGRm">Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering<i class="fa fa-external-link-alt"></i></span> 2016年<br><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzE2MDkuMDI5MDcucGRm">Semi-Supervised Classification with Graph Convolutional Networks<i class="fa fa-external-link-alt"></i></span> 2017年</p>
<p>在计算机科学领域、理论物理复杂网络领域的研究者在图（Graph）的空间域（spatial domain）和频谱域（spectral domain）分别提出了不同形式的图神经网络，并最终在2017年实现了空间域模型和频谱域模型的融合，即目前我们使用的第三代GCN。</p>
<p>对于其中理论和公式非常感兴趣的参考<span class="exturl" data-url="aHR0cDovL3h0ZjYxNS5jb20vMjAxOS8wMi8yNC9nY24v">图卷积神经网络理论基础<i class="fa fa-external-link-alt"></i></span>。</p>
<h2 id="Fourier-transform"><a href="#Fourier-transform" class="headerlink" title="Fourier transform"></a>Fourier transform</h2><script type="math/tex; mode=display">
F(w)=\frac{1}{2\pi}\int_{-\infty}^{+\infty} f(t)e^{-j\omega t} {\rm d}t</script><p>$F(\omega)$ 就是<strong>傅里叶变换</strong>，得到的就是<strong>频域曲线</strong>。每个频率$\omega$下都有对应的振幅$F(\omega)$。从几何上来看，$f(t)$ 以 $e^{-j\omega t}$ 为基函数投影，$F(w)$ 就是以频率 $\omega$ 对应基上的投影的坐标。</p>
<p>从数学角度来看，$f(x)$ 是函数 $f$ 在 $t$ 处的取值，所有基都对该处取值有贡献，即把每个$F(w)$ 投影到 $e^{-j\omega t}$ 基方向上分量累加起来，得到的就是该点处的函数值。</p>
<script type="math/tex; mode=display">
f(t) = \int_{-\infty}^{+\infty}F(w)e^{-j\omega t}\, {\rm d}\omega=\sum_{\omega}F(w)e^{-j\omega t}</script><p>上面简化了一下，用 $w$ 代表频率。这个公式也叫做<strong>逆傅里叶变换</strong>。</p>
<h2 id="Laplacian-operater"><a href="#Laplacian-operater" class="headerlink" title="Laplacian operater"></a>Laplacian operater</h2><script type="math/tex; mode=display">
\Delta f = \Delta^2 f = \sum_{i=1}^{n}\frac{\partial^2 f}{\partial x_i^2}</script><p>$f$ 是拉普拉斯算子作用的函数，求函数各向二阶导数再求和，定义为 $f$ 上的拉普拉斯算子。<br>可以理解为：<strong>二阶导数等于其在所有自由度上微扰之后获得的增益</strong>。<br>更形象的理解：<strong>拉普拉斯算子就是在所有自由度上进行微小变化后获得的增益</strong>。</p>
<p>求 $e^{-j\omega t}$ 上的 Laplacian operater：</p>
<script type="math/tex; mode=display">
\Delta f = \Delta e^{-j\omega t} = \sum_{i=1}^{n}\frac{\partial^2 e^{-j\omega t}}{\partial t^2}=-\omega^2 e^{-j\omega t}</script><p>由此可知，$e^{-j\omega t}$ 是 <strong>Laplacian operater 的特征向量</strong>（满足特征方程 $A\vec{x}=\lambda \vec{x}$）。</p>
<h2 id="Graph-Laplacian-operater"><a href="#Graph-Laplacian-operater" class="headerlink" title="Graph Laplacian operater"></a>Graph Laplacian operater</h2><p>Laplacian operater 推广到 Graph：假设<strong>图是一个完全图，即任意两个节点之间都有一条边，那么对一个节点进行微扰，它可能变成任意一个节点</strong>。即：</p>
<script type="math/tex; mode=display">
f=(f_1,f_2...f_N)</script><p>是函数 $f$ 在节点 $1..N$ 上的函数值，代表<strong>跟节点相关的信息</strong>，如节点属性等，此时可看作每一个节点是一个向量。</p>
<p>假设一个节点 $f_i$ ，其一阶邻域节点集合为 $N_i$ ，$f_j$为 $N_i$ 集合的一个节点，对于任意节点 $f_i$ ，对 $f_i$ 节点进行微扰，它可能变为任意一个与他相邻的节点 $f_j \in N_i$。前面提到，拉普拉斯算子就是在所有自由度上进行微小变化后获得的增益。对于 Graph 而言，从节点 $i$ 变化到节点 $j$ 增益是 $f_i−f_j$，即节点 $f_i$ 的 <strong>Graph Laplacian operater</strong>：</p>
<script type="math/tex; mode=display">
\Delta f_i =\sum_{j \in N_i} (f_i - f_j)</script><p>通俗理解，当前节点的 Graph Laplacian operater 就是 <strong>当前节点和所有邻接节点的差值</strong>。</p>
<h2 id="Laplacian-Matrix"><a href="#Laplacian-Matrix" class="headerlink" title="Laplacian Matrix"></a>Laplacian Matrix</h2><p>把 Graph Laplacian operater 公式变换一下，<strong>考虑权重</strong>：$w_{ij}=0$ 表示  $i,j$ 不相邻，$w_{ij}=1$ 表示  $i,j$ 相邻，那么上面公式可以转换为：</p>
<script type="math/tex; mode=display">
\begin{aligned}
\Delta f_i &=\sum_{j \in N} w_{ij}(f_i - f_j)\\
&=\sum_{j \in N} w_{ij}f_i - \sum_{j \in N} w_{ij}f_j\\
&=d_if_i-w_if
\end{aligned}</script><p>其中：令 $d_i=\sum_{j \in N}w_{ij}$ ，表示节点 $i$ 的度。令 $w_i=[w_{i1},…,w_{iN}]$ 行。令 $f=[f_1,…f_N]^T$ 列。<br>对于所有节点：</p>
<script type="math/tex; mode=display">
\begin{aligned}
\Delta f&=
\begin{bmatrix}
   d_1f_1-w_1f \\
   d_2f_2-w_2f \\
   \cdots \\
   d_Nf_N-w_Nf
\end{bmatrix}\\
&=
\begin{equation*}
    \begin{bmatrix} 
    d_1 &0&\cdots&0 \\
    0&d_2&\cdots&0\\
    \vdots&\vdots& \ddots&\vdots \\
    0&0&\cdots&d_N 
    \end{bmatrix}
    \end{equation*}f-\begin{bmatrix}
   w_1\\
   w_2\\
   \cdots \\
   w_N
\end{bmatrix}f\\ 
&=(D-W)f
\end{aligned}</script><p>$D$ 就是<strong>度矩阵</strong>（dgree matrix），$W$ 就是<strong>邻接矩阵</strong>（adjacency matrix），$L=D-W$ 就是<strong>拉普拉斯矩阵</strong>（Laplacian Matrix）。</p>
<p><img src="/images/GCN/LM.png" width="80%"></p>
<p>根据$\Delta f = Lf$，那么可以看作 <strong>Laplacian operater 等于 Laplacian Matrix</strong> ，即$\Delta=L$。这样<strong>求 Graph Laplacian operater 等价于求 Laplacian Matrix</strong>。</p>
<p>Laplacian Matrix 是<strong>半正定对称矩阵</strong>，因此拥有诸多优秀性质：</p>
<ul>
<li>对称矩阵一定n个线性无关的特征向量</li>
<li>半正定矩阵的特征值一定非负</li>
<li>对阵矩阵的特征向量相互正交，即所有特征向量构成的矩阵为正交矩阵</li>
</ul>
<p>对 Laplacian Matrix 进行特征分解：</p>
<script type="math/tex; mode=display">
\Delta=L=U\Lambda U^T</script><p>其中，$U$的每一列为$L$的<strong>特征向量</strong>，$\Lambda$ 是$L$的<strong>特征值矩阵</strong>，$U^T$的每一行为$L$的<strong>特征向量</strong>。</p>
<h2 id="Graph-Fourier-transform"><a href="#Graph-Fourier-transform" class="headerlink" title="Graph Fourier transform"></a>Graph Fourier transform</h2><p>前面提到，$e^{-j\omega t}$ 是 $\Delta$ 的<strong>特征向量</strong>，而后推导出：$\Delta=L=U\Lambda U^T$ ，$U^T$的每一行为$L$的<strong>特征向量</strong>$\phi_w$，因此我们可得到：</p>
<ul>
<li>频率$w$ $\to$ 特征值$\lambda_w$</li>
<li>正弦函数 $e^{-j\omega t}$ $\to$ 特征向量$\phi_w$</li>
<li>振幅$F(w)$ $\to$ 振幅$F(\lambda_w)$</li>
</ul>
<p>这样就把传统傅里叶变换推广到了图傅里叶变换。推广到矩阵形式：</p>
<script type="math/tex; mode=display">
\hat{f} = U^Tf</script><p>逆变换：</p>
<script type="math/tex; mode=display">
f = U\hat{f}</script><h2 id="Graph-Convolution"><a href="#Graph-Convolution" class="headerlink" title="Graph Convolution"></a>Graph Convolution</h2><p><strong>卷积定理：函数卷积的傅里叶变换是函数傅立叶变换的乘积，即对于函数 $f$ 与 $g$ 两者的卷积是其函数傅立叶变换乘积的逆变换</strong>。时域上的卷积-&gt;频域上的相乘后逆变换。从而方便计算，可以看作一种Mapping方式，把时域信号转成频域信号处理。</p>
<script type="math/tex; mode=display">
f*g = \mathcal{F}^{-1}\{\mathcal{F}\{f\} \cdot \mathcal{F}\{g\}\}</script><p>其中，$f$ 是图信号，$g$ 是卷积核。通过 Graph Fourier transform：</p>
<script type="math/tex; mode=display">
f*g=U(U^Tg\cdot U^Tf)</script><p>由于对 $g$ 和 $f$ 进行傅里叶变换的结果为 $U^Tg$ 和 $U^Tf$ 都是一个列向量，所以也可以写成：</p>
<script type="math/tex; mode=display">
f*g=U(U^Tg\odot U^Tf)</script><p>$\odot$表示哈达马积，对于两个向量，就是进行内积运算；对于维度相同的两个矩阵，就是对应元素的乘积运算。</p>
<p>通常把 $U^Tg$ 整体看作可学习的卷积核，这里把它写作 $g_{\theta}$（由参数 $\theta$ 构成的对角矩阵 $diag(\theta)$）。最终图上的卷积公式：</p>
<script type="math/tex; mode=display">
f*g=Ug_{\theta}U^Tf</script><p>由于参数 $\theta$ 的确定与 $L$ 的特征值有关，可把 $g_{\theta}$ 看作是特征值 $\Lambda$ 的一个函数，那么可把 $g_{\theta}$ 看成是拉普拉斯矩阵 $L$ 的一系列特征值组成的对角矩阵的形式，即定义$g_{\theta}=diag(U^Tg)=g_{\theta}(\Lambda)$：</p>
<script type="math/tex; mode=display">
f*g=Ug_{\theta}(\Lambda)U^Tf=U
\begin{equation*}
    \begin{bmatrix} 
    \hat{g}(\lambda_1) &   &\\
    & \ddots & \\
    & & \hat{g}(\lambda_N)
    \end{bmatrix}
    \end{equation*}
U^Tf</script><h2 id="Graph-Convolution-Networks"><a href="#Graph-Convolution-Networks" class="headerlink" title="Graph Convolution Networks"></a>Graph Convolution Networks</h2><p><strong>第一代GCN</strong>（Spectral CNN）：简单的把 $g_{\theta}$（由参数 $\theta$ 构成的对角矩阵 $diag(\theta)$）看作是一个可学习参数的集合，其中 $x$ 是节点特征向量：</p>
<script type="math/tex; mode=display">
f*g=x*g_{\theta}=Ug_{\theta}U^Tx</script><p>第一代GCN缺点：<br>（1）计算复杂度高$O(n^2)$，每次计算都需要特征分解求U；每一次前向传播，都要计算$U,g_{\theta},U^T$ 三者的乘积。<br>（2）没有正则化（no normalization）。<br>（3）没有考虑自身权重（no self-weight）。</p>
<hr>
<p><strong>第二代GCN</strong>（ChebNet）：定义特征向量对角矩阵的切比雪夫多项式为滤波器：</p>
<script type="math/tex; mode=display">
g_{\theta'}(\Lambda) \approx \sum_{k=0}^{K}\theta_{k}^{'}\Lambda^k=\sum_{k=0}^{K}\theta_{k}^{'}T_{k}(\tilde{\Lambda})</script><p>其中：</p>
<ul>
<li>$\tilde{\Lambda}=\frac{2}{\lambda_{max}}\Lambda-I_N$，$\lambda_{max}$是L的最大特征值。</li>
<li>$\theta \in \mathbb{R}^K$ 是切比雪夫系数的向量。</li>
<li>切比雪夫多项式（类似泰勒展开）定义为：$T_{k}(x)=2xT_{k-1}(x)-T_{k-2}(x)$，其中 $T_0(x)=1,T_1(x)=x$。</li>
</ul>
<p>就是利用Chebyshev多项式拟合卷积核的方法，来降低计算复杂度。但首先提出Chebyshev多项式K阶截断展开来拟合，并对 $\Lambda$ 进行归一化使其元素位于[-1,1]之间的是<span class="exturl" data-url="aHR0cHM6Ly9oYWwuaW5yaWEuZnIvaW5yaWEtMDA1NDE4NTUvZG9jdW1lbnQ=">Hammond et al.(2011) ：Wavelets on graphs via spectral graph theory<i class="fa fa-external-link-alt"></i></span>，二代GCN借鉴了这一方法。</p>
<p>回到 $g_{\theta}$ 和输入 $x$ 的卷积：</p>
<script type="math/tex; mode=display">
\begin{aligned}
g_{\theta}*x &= U \sum_{k=0}^{K}\theta_{k}^{'}\Lambda^k U^Tx\\
&=\sum_{k=0}^{K}\theta_{k}^{'}(U \Lambda^kU^T) x\\
&=\sum_{k=0}^{K}\theta_{k}^{'}(U \Lambda U^T)^k x\\
&=\sum_{k=0}^{K}\theta_{k}^{'}L^{k}x
\end{aligned}</script><p>这里面就用到拉普拉斯矩阵 $L$。计算复杂度为 $O(kn^2)$。使用切比雪夫展开，其中 $\tilde{L}=\frac{2}{\lambda_{max}}L-I_N$：</p>
<script type="math/tex; mode=display">
g_{\theta^{'}}*x=\sum_{k=0}^{K}\theta_{k}^{'}T_{x}(\tilde{L})x</script><hr>
<p><strong>第三代GCN</strong>（一阶ChebNet）：只对切比雪夫展开到一阶，即 $K=1,\lambda_{max}=2$，那么 $\tilde{L}=L-I_N$，且 $T_0(\tilde{L})=1,T_1(\tilde{L})=\tilde{L}$ ，第二代公式可简化为：</p>
<script type="math/tex; mode=display">
\begin{aligned}
g_{\theta^{'}}*x &= \theta_0^{'}T_0(\tilde{L})x+\theta_1^{'}T_1(\tilde{L})x\\
&=\theta_0^{'}x+\theta_1^{'}(L-I_N)x\\
\end{aligned}</script><p>对$L$做归一化处理：</p>
<script type="math/tex; mode=display">
\begin{aligned}
\hat{L}&=D^{-\frac{1}{2}}(L)D^{-\frac{1}{2}}\\
&=D^{-\frac{1}{2}}(D-W)D^{-\frac{1}{2}}\\
&=I_N-D^{-\frac{1}{2}}WD^{-\frac{1}{2}}
\end{aligned}</script><p>代入到前式中得到：</p>
<script type="math/tex; mode=display">
\theta_0^{\prime}x+\theta_1^{\prime}(L-I_N)x=\theta_0^{'}x+(-\theta_1^{'}(D^{-\frac{1}{2}}WD^{-\frac{1}{2}})x)</script><p>由于不希望 $\theta_0^{\prime}$ 和 $\theta_1^{\prime}$ 出现，所以假设 $\theta_0^{\prime}=-\theta_1^{\prime}=\theta$：</p>
<script type="math/tex; mode=display">
g_{\theta^{'}}*x=\theta(I_N+D^{-\frac{1}{2}}WD^{-\frac{1}{2}})x</script><p>注意 $I_N+D^{-\frac{1}{2}}WD^{-\frac{1}{2}}$ 的特征值被限制在了[0,2]中。由于这一步输出可能作为下一层的输入，会再次与 $I_N+D^{-\frac{1}{2}}WD^{-\frac{1}{2}}$ 相乘重复这样的操作将会导致数值不稳定、梯度消失/爆炸等问题。</p>
<p>为了解决该问题，引入renormalization（就是加了自环）：令 $\tilde{W}=W+I_N, \tilde{D}_i=\sum_j \tilde{W}_{ij}$：</p>
<script type="math/tex; mode=display">
I_N+D^{-\frac{1}{2}}WD^{-\frac{1}{2}} \approx \tilde{D}^{-\frac{1}{2}}\tilde{W}\tilde{D}^{-\frac{1}{2}}</script><p>那么，带入之前的公式得到：</p>
<script type="math/tex; mode=display">
\underbrace{\boldsymbol{g}_{\boldsymbol{\theta^{\prime}}} * \boldsymbol{x}}_{\mathbb{R}^{n \times n}} = \theta(\underbrace{\tilde{\boldsymbol{D}}^{-1/2}\tilde{\boldsymbol{W}} \tilde{\boldsymbol{D}}^{-1/2}}_{\mathbb{R}^{n \times n}}) \underbrace{\boldsymbol{x}}_{\mathbb{R}^{n \times 1}}</script><p>推广到多通道和多卷积，则卷积结果写作矩阵形式如下：</p>
<script type="math/tex; mode=display">
\underbrace{\boldsymbol{Z}}_{\mathbb{R}^{N \times F}} = \underbrace{\tilde{\boldsymbol{D}}^{-1/2}\tilde{\boldsymbol{W}} \tilde{\boldsymbol{D}}^{-1/2}}_{\mathbb{R}^{N \times N}} \underbrace{\boldsymbol{X}}_{\mathbb{R}^{N \times C}} \ \ \underbrace{\boldsymbol{\Theta}}_{\mathbb{R}^{C \times F}}</script><p>其中，$N$ 是<strong>节点数量</strong>，$C$ 是通道数或者称作节点的<strong>特征维度</strong>，$F$ 为<strong>卷积核数量</strong>。$D$ 就是<strong>度矩阵</strong>，$W$ 就是<strong>邻接矩阵</strong>，$X$ 是节点的<strong>特征矩阵</strong>，$\Theta$ 是<strong>卷积核参数矩阵</strong>，最终得到的卷积结果 $\boldsymbol{Z} \in \mathbb{R}^{N \times F}$，即每个节点的卷积结果的维数等于卷积核数量。上述操作可以叠加多层，对 $Z$ 激活一下，然后将激活后的 $Z$ 作为下一层的节点的特征矩阵。</p>
<p>第三代GCN特点总结：</p>
<ul>
<li>解决了计算复杂度高的问题：复杂度为$O(E)$ (稀疏矩阵优化的话)，$E$ 是图中边的几何。</li>
<li>只考虑1-hop，若要建模多hop，通过叠加层数，获得更大的感受野。（联想NLP中使用卷积操作语句序列时，也是通过叠加多层来达到获取长依赖的目的）。</li>
</ul>
<h1 id="Code"><a href="#Code" class="headerlink" title="Code"></a>Code</h1><p>作者给出了源码，分两个版本：</p>
<ul>
<li>tensorflow：<span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL3RraXBmL2djbg==">gcn<i class="fa fa-external-link-alt"></i></span></li>
<li>pytorch：<span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL3RraXBmL3B5Z2Nu">pygcn<i class="fa fa-external-link-alt"></i></span></li>
<li>数据集地址：<span class="exturl" data-url="aHR0cHM6Ly9saW5xcy1kYXRhLnNvZS51Y3NjLmVkdS9wdWJsaWMvbGJjL2NvcmEudGd6">cora<i class="fa fa-external-link-alt"></i></span></li>
</ul>
<p>cora数据集有2708个样本，每个样本由1433维特征表示，每个样本是一篇科学论文，每篇论文可能为7个类别，样本和样本之间包括了5429个连接。</p>
<p>模型输入：<br><strong>X</strong>：N×D的特征矩阵，N表示节点数量（cora数据集就是2708），D表示输入特征（cora数据集就是1433）。<br><strong>A</strong>：邻接矩阵。<br>模型输出：<br><strong>Z</strong>：N×F的特征矩阵，F是每个输出节点的特征维度（这个维度自己设置）。</p>
<p>使用的公式：<br>$H^{(l+1)}=f(H^{(l)},A),\qquad H^{(0)}=X, H^{(L)}=Z$<br>$f(H^{(l)},A)=\sigma(AH^{(l)}W^{(l)}),\qquad f(H^{(l)},A)=\sigma(\hat{D}^{-\frac{1}{2}}\hat{A}\hat{D}^{-\frac{1}{2}}H^{(l)}W^{(l)})$</p>
<p>其中，$\hat{A}=A+I$，$I$是对角矩阵(自环)，$\hat{A}$ 是加上自环(节点本身信息)后的邻接矩阵。如果一个节点有非常多的邻居，那么函数$f$就会越来越大，所以加上一个归一化$\hat{D}$是$\hat{A}$的度矩阵，有两种方法$\hat{D}^{-1}A$和$\hat{D}^{-\frac{1}{2}}\hat{A}\hat{D}^{-\frac{1}{2}}$。</p>
<p>以pytorch版本为例：<br>文件结构：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">.</span><br><span class="line">├── data      &#x2F;&#x2F; 图数据</span><br><span class="line">├── pygcn</span><br><span class="line">    ├── inits    &#x2F;&#x2F; 初始化的一些公用函数</span><br><span class="line">    ├── layers     &#x2F;&#x2F; GCN层的定义</span><br><span class="line">        ├── class GraphConvolution</span><br><span class="line">        ├── reset parameters</span><br><span class="line">        ├── forward</span><br><span class="line">    ├── models     &#x2F;&#x2F; 模型结构定义</span><br><span class="line">        ├── class GCN</span><br><span class="line">        ├── forward</span><br><span class="line">    ├── train    &#x2F;&#x2F; 训练</span><br><span class="line">        ├── def train</span><br><span class="line">        ├── def test </span><br><span class="line">    └── utils    &#x2F;&#x2F;  工具函数的定义</span><br><span class="line">        ├── encode_onehot</span><br><span class="line">        ├── load_data</span><br><span class="line">        ├── normazlize</span><br><span class="line">        ├── accuracy</span><br><span class="line">        ├── sparse mx to torch sparse tensor</span><br><span class="line">├── setup.py &#x2F;&#x2F;启动函数</span><br></pre></td></tr></table></figure></p>
<figure class="highlight python"><figcaption><span>model.py</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">from</span> pygcn.layers <span class="keyword">import</span> GraphConvolution</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">GCN</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, nfeat, nhid, nclass, dropout</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(GCN, self).__init__()</span><br><span class="line"></span><br><span class="line">        self.gc1 = GraphConvolution(nfeat, nhid)  <span class="comment"># nfeat：N×D的D</span></span><br><span class="line">        self.gc2 = GraphConvolution(nhid, nclass) <span class="comment"># nclass：类别，这里是7类</span></span><br><span class="line">        self.dropout = dropout</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x, adj</span>):</span></span><br><span class="line">        x = F.relu(self.gc1(x, adj)) <span class="comment"># 第一层输出+relu</span></span><br><span class="line">        x = F.dropout(x, self.dropout, training=self.training)</span><br><span class="line">        x = self.gc2(x, adj)  <span class="comment"># 第二层输出</span></span><br><span class="line">        <span class="keyword">return</span> F.log_softmax(x, dim=<span class="number">1</span>) <span class="comment"># 第二层输出+log_softmax</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><figcaption><span>layers.py</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> math</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> torch.nn.parameter <span class="keyword">import</span> Parameter</span><br><span class="line"><span class="keyword">from</span> torch.nn.modules.module <span class="keyword">import</span> Module</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">GraphConvolution</span>(<span class="params">Module</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Simple GCN layer, similar to https://arxiv.org/abs/1609.02907</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, in_features, out_features, bias=<span class="literal">True</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>(GraphConvolution, self).__init__()</span><br><span class="line">        self.in_features = in_features  <span class="comment"># 每层的输入维度</span></span><br><span class="line">        self.out_features = out_features   <span class="comment"># 每层的输出维度</span></span><br><span class="line">        self.weight = Parameter(torch.FloatTensor(in_features, out_features))</span><br><span class="line">        <span class="keyword">if</span> bias:</span><br><span class="line">            self.bias = Parameter(torch.FloatTensor(out_features))</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.register_parameter(<span class="string">&#x27;bias&#x27;</span>, <span class="literal">None</span>)</span><br><span class="line">        self.reset_parameters()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">reset_parameters</span>(<span class="params">self</span>):</span> <span class="comment"># 参数初始化方法</span></span><br><span class="line">        stdv = <span class="number">1.</span> / math.sqrt(self.weight.size(<span class="number">1</span>))</span><br><span class="line">        self.weight.data.uniform_(-stdv, stdv)</span><br><span class="line">        <span class="keyword">if</span> self.bias <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            self.bias.data.uniform_(-stdv, stdv)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, <span class="built_in">input</span>, adj</span>):</span> <span class="comment"># 实现AHW，第一次时H是X</span></span><br><span class="line">        support = torch.mm(<span class="built_in">input</span>, self.weight) <span class="comment"># 实现XW</span></span><br><span class="line">        <span class="comment"># Sparse matrix multiplication, https://github.com/tkipf/pygcn/issues/19</span></span><br><span class="line">        <span class="comment"># output = torch.spmm(adj, support) # spmm后续版本被移除了，使用sparse.mm替代</span></span><br><span class="line">        output = torch.sparse.mm(adj, support) <span class="comment"># 实现AXW</span></span><br><span class="line">        <span class="keyword">if</span> self.bias <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">return</span> output + self.bias</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> output</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__repr__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">return</span> self.__class__.__name__ + <span class="string">&#x27; (&#x27;</span> \</span><br><span class="line">               + <span class="built_in">str</span>(self.in_features) + <span class="string">&#x27; -&gt; &#x27;</span> \</span><br><span class="line">               + <span class="built_in">str</span>(self.out_features) + <span class="string">&#x27;)&#x27;</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><figcaption><span>util.py</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> scipy.sparse <span class="keyword">as</span> sp</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="comment"># focus</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">encode_onehot</span>(<span class="params">labels</span>):</span></span><br><span class="line">    classes = <span class="built_in">set</span>(labels)</span><br><span class="line">    classes_dict = &#123;c: np.identity(<span class="built_in">len</span>(classes))[i, :] <span class="keyword">for</span> i, c <span class="keyword">in</span></span><br><span class="line">                    <span class="built_in">enumerate</span>(classes)&#125;</span><br><span class="line">    labels_onehot = np.array(<span class="built_in">list</span>(<span class="built_in">map</span>(classes_dict.get, labels)),</span><br><span class="line">                             dtype=np.int32)</span><br><span class="line">    <span class="keyword">return</span> labels_onehot</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_data</span>(<span class="params">path=<span class="string">&quot;../data/cora/&quot;</span>, dataset=<span class="string">&quot;cora&quot;</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Load citation network dataset (cora only for now)&quot;&quot;&quot;</span></span><br><span class="line">    print(<span class="string">&#x27;Loading &#123;&#125; dataset...&#x27;</span>.<span class="built_in">format</span>(dataset))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 读进来的数据为：idx，features，labels</span></span><br><span class="line">    idx_features_labels = np.genfromtxt(<span class="string">&quot;&#123;&#125;&#123;&#125;.content&quot;</span>.<span class="built_in">format</span>(path, dataset),</span><br><span class="line">                                        dtype=np.dtype(<span class="built_in">str</span>))</span><br><span class="line">    <span class="comment"># csr_matrix数据存储成稀疏方式，格式为csr</span></span><br><span class="line">    features = sp.csr_matrix(idx_features_labels[:, <span class="number">1</span>:<span class="number">-1</span>], dtype=np.float32)</span><br><span class="line">    labels = encode_onehot(idx_features_labels[:, <span class="number">-1</span>]) <span class="comment"># 使用onehot编码类别 (2708, 7)</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># build graph</span></span><br><span class="line">    idx = np.array(idx_features_labels[:, <span class="number">0</span>], dtype=np.int32)</span><br><span class="line">    idx_map = &#123;j: i <span class="keyword">for</span> i, j <span class="keyword">in</span> <span class="built_in">enumerate</span>(idx)&#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 读进来的数据为：edges，unordered</span></span><br><span class="line">    <span class="comment"># [[     35,    1033],</span></span><br><span class="line">    <span class="comment">#  [     35,  103482],...]</span></span><br><span class="line">    edges_unordered = np.genfromtxt(<span class="string">&quot;&#123;&#125;&#123;&#125;.cites&quot;</span>.<span class="built_in">format</span>(path, dataset),</span><br><span class="line">                                    dtype=np.int32)</span><br><span class="line">    <span class="comment"># 转成对应map编号</span></span><br><span class="line">    <span class="comment"># [[ 163,  402],</span></span><br><span class="line">    <span class="comment">#  [ 163,  659],...]</span></span><br><span class="line">    edges = np.array(<span class="built_in">list</span>(<span class="built_in">map</span>(idx_map.get, edges_unordered.flatten())),</span><br><span class="line">                     dtype=np.int32).reshape(edges_unordered.shape)</span><br><span class="line">    <span class="comment"># (edges[:, 0], edges[:, 1])坐标点，(np.ones(edges.shape[0])每个坐标位置的值为1</span></span><br><span class="line">    <span class="comment"># 此步得到的是有向图邻接矩阵</span></span><br><span class="line">    adj = sp.coo_matrix((np.ones(edges.shape[<span class="number">0</span>]), (edges[:, <span class="number">0</span>], edges[:, <span class="number">1</span>])),</span><br><span class="line">                        shape=(labels.shape[<span class="number">0</span>], labels.shape[<span class="number">0</span>]),</span><br><span class="line">                        dtype=np.float32)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># build symmetric adjacency matrix !</span></span><br><span class="line">    <span class="comment"># 无向图，邻接矩阵是对称的，https://zhuanlan.zhihu.com/p/78191258</span></span><br><span class="line">    <span class="comment"># see https://github.com/tkipf/pygcn/issues/3</span></span><br><span class="line">    adj = adj + adj.T.multiply(adj.T &gt; adj) - adj.multiply(adj.T &gt; adj)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># see https://github.com/tkipf/pygcn/issues/47</span></span><br><span class="line">    <span class="comment"># 归一化防止梯度消失</span></span><br><span class="line">    features = normalize(features)</span><br><span class="line">    adj = normalize(adj + sp.eye(adj.shape[<span class="number">0</span>])) <span class="comment"># 加对角矩阵I，即A+I=\hat&#123;A&#125;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 切分数据</span></span><br><span class="line">    idx_train = <span class="built_in">range</span>(<span class="number">140</span>)</span><br><span class="line">    idx_val = <span class="built_in">range</span>(<span class="number">200</span>, <span class="number">500</span>)</span><br><span class="line">    idx_test = <span class="built_in">range</span>(<span class="number">500</span>, <span class="number">1500</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 转成Tensor</span></span><br><span class="line">    features = torch.FloatTensor(np.array(features.todense()))</span><br><span class="line">    labels = torch.LongTensor(np.where(labels)[<span class="number">1</span>])</span><br><span class="line">    adj = sparse_mx_to_torch_sparse_tensor(adj)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 转成Tensor</span></span><br><span class="line">    idx_train = torch.LongTensor(idx_train)</span><br><span class="line">    idx_val = torch.LongTensor(idx_val)</span><br><span class="line">    idx_test = torch.LongTensor(idx_test)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> adj, features, labels, idx_train, idx_val, idx_test</span><br><span class="line"></span><br><span class="line"><span class="comment"># focus</span></span><br><span class="line"><span class="comment"># 对特征矩阵features和邻接矩阵adj做标准化，防止梯度消失</span></span><br><span class="line"><span class="comment"># 每个值除以它所在行的和</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">normalize</span>(<span class="params">mx</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Row-normalize sparse matrix&quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># see https://github.com/tkipf/gcn/blob/master/gcn/utils.py#L122</span></span><br><span class="line">    <span class="comment"># 对每行求和得到rowsum</span></span><br><span class="line">    rowsum = np.array(mx.<span class="built_in">sum</span>(<span class="number">1</span>))</span><br><span class="line">    <span class="comment"># 求逆得到r_inv</span></span><br><span class="line">    r_inv = np.power(rowsum, <span class="number">-1</span>).flatten()</span><br><span class="line">    <span class="comment"># 如果某一行全为0，则r_inv算出来会等于无穷大，将这些行的r_inv置为0</span></span><br><span class="line">    r_inv[np.isinf(r_inv)] = <span class="number">0.</span></span><br><span class="line">    r_mat_inv = sp.diags(r_inv)</span><br><span class="line">    mx = r_mat_inv.dot(mx)</span><br><span class="line">    <span class="keyword">return</span> mx</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">accuracy</span>(<span class="params">output, labels</span>):</span></span><br><span class="line">    preds = output.<span class="built_in">max</span>(<span class="number">1</span>)[<span class="number">1</span>].type_as(labels)</span><br><span class="line">    correct = preds.eq(labels).double()</span><br><span class="line">    correct = correct.<span class="built_in">sum</span>()</span><br><span class="line">    <span class="keyword">return</span> correct / <span class="built_in">len</span>(labels)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sparse_mx_to_torch_sparse_tensor</span>(<span class="params">sparse_mx</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Convert a scipy sparse matrix to a torch sparse tensor.&quot;&quot;&quot;</span></span><br><span class="line">    sparse_mx = sparse_mx.tocoo().astype(np.float32)</span><br><span class="line">    indices = torch.from_numpy(</span><br><span class="line">        np.vstack((sparse_mx.row, sparse_mx.col)).astype(np.int64))</span><br><span class="line">    values = torch.from_numpy(sparse_mx.data)</span><br><span class="line">    shape = torch.Size(sparse_mx.shape)</span><br><span class="line">    <span class="keyword">return</span> torch.sparse.FloatTensor(indices, values, shape)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><figcaption><span>train.py</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> division</span><br><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> print_function</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> pygcn.utils <span class="keyword">import</span> load_data, accuracy</span><br><span class="line"><span class="keyword">from</span> pygcn.models <span class="keyword">import</span> GCN</span><br><span class="line"></span><br><span class="line"><span class="comment"># Training settings</span></span><br><span class="line">parser = argparse.ArgumentParser()</span><br><span class="line">parser.add_argument(<span class="string">&#x27;--no-cuda&#x27;</span>, action=<span class="string">&#x27;store_true&#x27;</span>, default=<span class="literal">False</span>,</span><br><span class="line">                    <span class="built_in">help</span>=<span class="string">&#x27;Disables CUDA training.&#x27;</span>)</span><br><span class="line">parser.add_argument(<span class="string">&#x27;--fastmode&#x27;</span>, action=<span class="string">&#x27;store_true&#x27;</span>, default=<span class="literal">False</span>,</span><br><span class="line">                    <span class="built_in">help</span>=<span class="string">&#x27;Validate during training pass.&#x27;</span>)</span><br><span class="line">parser.add_argument(<span class="string">&#x27;--seed&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">int</span>, default=<span class="number">42</span>, <span class="built_in">help</span>=<span class="string">&#x27;Random seed.&#x27;</span>)</span><br><span class="line">parser.add_argument(<span class="string">&#x27;--epochs&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">int</span>, default=<span class="number">200</span>,</span><br><span class="line">                    <span class="built_in">help</span>=<span class="string">&#x27;Number of epochs to train.&#x27;</span>)</span><br><span class="line">parser.add_argument(<span class="string">&#x27;--lr&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">float</span>, default=<span class="number">0.01</span>,</span><br><span class="line">                    <span class="built_in">help</span>=<span class="string">&#x27;Initial learning rate.&#x27;</span>)</span><br><span class="line">parser.add_argument(<span class="string">&#x27;--weight_decay&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">float</span>, default=<span class="number">5e-4</span>,</span><br><span class="line">                    <span class="built_in">help</span>=<span class="string">&#x27;Weight decay (L2 loss on parameters).&#x27;</span>)</span><br><span class="line">parser.add_argument(<span class="string">&#x27;--hidden&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">int</span>, default=<span class="number">16</span>,</span><br><span class="line">                    <span class="built_in">help</span>=<span class="string">&#x27;Number of hidden units.&#x27;</span>)</span><br><span class="line">parser.add_argument(<span class="string">&#x27;--dropout&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">float</span>, default=<span class="number">0.5</span>,</span><br><span class="line">                    <span class="built_in">help</span>=<span class="string">&#x27;Dropout rate (1 - keep probability).&#x27;</span>)</span><br><span class="line"></span><br><span class="line">args = parser.parse_args()</span><br><span class="line">args.cuda = <span class="keyword">not</span> args.no_cuda <span class="keyword">and</span> torch.cuda.is_available()</span><br><span class="line"></span><br><span class="line">np.random.seed(args.seed)</span><br><span class="line">torch.manual_seed(args.seed)</span><br><span class="line"><span class="keyword">if</span> args.cuda:</span><br><span class="line">    torch.cuda.manual_seed(args.seed)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Load data</span></span><br><span class="line">adj, features, labels, idx_train, idx_val, idx_test = load_data()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Model and optimizer</span></span><br><span class="line">model = GCN(nfeat=features.shape[<span class="number">1</span>],</span><br><span class="line">            nhid=args.hidden,</span><br><span class="line">            nclass=labels.<span class="built_in">max</span>().item() + <span class="number">1</span>,</span><br><span class="line">            dropout=args.dropout)</span><br><span class="line">optimizer = optim.Adam(model.parameters(),</span><br><span class="line">                       lr=args.lr, weight_decay=args.weight_decay)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> args.cuda:</span><br><span class="line">    model.cuda()</span><br><span class="line">    features = features.cuda()</span><br><span class="line">    adj = adj.cuda()</span><br><span class="line">    labels = labels.cuda()</span><br><span class="line">    idx_train = idx_train.cuda()</span><br><span class="line">    idx_val = idx_val.cuda()</span><br><span class="line">    idx_test = idx_test.cuda()</span><br><span class="line"></span><br><span class="line"><span class="comment"># focus</span></span><br><span class="line"><span class="comment"># 这里训练时给140个带标签，输入的是全部数据特征，整体是个半监督的任务</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span>(<span class="params">epoch</span>):</span></span><br><span class="line">    t = time.time()</span><br><span class="line">    model.train()</span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    output = model(features, adj)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># classify each node</span></span><br><span class="line">    <span class="comment"># 只考虑 train ids 计算 loss</span></span><br><span class="line">    <span class="comment"># see https://github.com/tkipf/pygcn/issues/50</span></span><br><span class="line">    <span class="comment"># 如果输出用softmax，这里就用交叉熵损失cross_entropy</span></span><br><span class="line">    <span class="comment"># 这里使用负对数似然损失nll_loss，因为前面输出用的是log_softmax</span></span><br><span class="line">    <span class="comment"># torch.nn.CrossEntropyLoss、cross_entropy都是上面两个函数的组合nll_loss(log_softmax(input))</span></span><br><span class="line">    loss_train = F.nll_loss(output[idx_train], labels[idx_train])</span><br><span class="line">    acc_train = accuracy(output[idx_train], labels[idx_train])</span><br><span class="line">    loss_train.backward()</span><br><span class="line">    optimizer.step()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> args.fastmode:</span><br><span class="line">        <span class="comment"># Evaluate validation set performance separately,</span></span><br><span class="line">        <span class="comment"># deactivates dropout during validation run.</span></span><br><span class="line">        model.<span class="built_in">eval</span>()</span><br><span class="line">        output = model(features, adj)</span><br><span class="line"></span><br><span class="line">    loss_val = F.nll_loss(output[idx_val], labels[idx_val])</span><br><span class="line">    acc_val = accuracy(output[idx_val], labels[idx_val])</span><br><span class="line">    print(<span class="string">&#x27;Epoch: &#123;:04d&#125;&#x27;</span>.<span class="built_in">format</span>(epoch+<span class="number">1</span>),</span><br><span class="line">          <span class="string">&#x27;loss_train: &#123;:.4f&#125;&#x27;</span>.<span class="built_in">format</span>(loss_train.item()),</span><br><span class="line">          <span class="string">&#x27;acc_train: &#123;:.4f&#125;&#x27;</span>.<span class="built_in">format</span>(acc_train.item()),</span><br><span class="line">          <span class="string">&#x27;loss_val: &#123;:.4f&#125;&#x27;</span>.<span class="built_in">format</span>(loss_val.item()),</span><br><span class="line">          <span class="string">&#x27;acc_val: &#123;:.4f&#125;&#x27;</span>.<span class="built_in">format</span>(acc_val.item()),</span><br><span class="line">          <span class="string">&#x27;time: &#123;:.4f&#125;s&#x27;</span>.<span class="built_in">format</span>(time.time() - t))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test</span>():</span></span><br><span class="line">    model.<span class="built_in">eval</span>()</span><br><span class="line">    output = model(features, adj)</span><br><span class="line">    loss_test = F.nll_loss(output[idx_test], labels[idx_test])</span><br><span class="line">    acc_test = accuracy(output[idx_test], labels[idx_test])</span><br><span class="line">    print(<span class="string">&quot;Test set results:&quot;</span>,</span><br><span class="line">          <span class="string">&quot;loss= &#123;:.4f&#125;&quot;</span>.<span class="built_in">format</span>(loss_test.item()),</span><br><span class="line">          <span class="string">&quot;accuracy= &#123;:.4f&#125;&quot;</span>.<span class="built_in">format</span>(acc_test.item()))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Train model</span></span><br><span class="line">t_total = time.time()</span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(args.epochs):</span><br><span class="line">    train(epoch)</span><br><span class="line">print(<span class="string">&quot;Optimization Finished!&quot;</span>)</span><br><span class="line">print(<span class="string">&quot;Total time elapsed: &#123;:.4f&#125;s&quot;</span>.<span class="built_in">format</span>(time.time() - t_total))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Testing</span></span><br><span class="line">test()</span><br></pre></td></tr></table></figure>
<h1 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h1><p><span class="exturl" data-url="aHR0cHM6Ly96aC53aWtpcGVkaWEub3JnL3dpa2kvJUU5JTlBJThGJUU2JTlDJUJBJUU1JTlCJUJF">随机图<i class="fa fa-external-link-alt"></i></span><br><span class="exturl" data-url="aHR0cHM6Ly96aC53aWtpcGVkaWEub3JnL3dpa2kvJUU2JTk3JUEwJUU1JUIwJUJBJUU1JUJBJUE2JUU3JUJEJTkxJUU3JUJCJTlD">无尺度网络<i class="fa fa-external-link-alt"></i></span>，<span class="exturl" data-url="aHR0cHM6Ly93d3cuY25ibG9ncy5jb20vbGVlengvcC85NDM2ODIwLmh0bWw=">Scale Free Network<i class="fa fa-external-link-alt"></i></span></p>
<p><span class="exturl" data-url="aHR0cHM6Ly96aC53aWtpcGVkaWEub3JnL3dpa2kvJUU1JUJBJUE2JUU1JTg4JTg2JUU1JUI4JTgz">度分布<i class="fa fa-external-link-alt"></i></span><br><span class="exturl" data-url="aHR0cHM6Ly96aC53aWtpcGVkaWEub3JnL3dpa2kvJUU5JTgyJUJCJUU2JThFJUE1JUU3JTlGJUE5JUU5JTk4JUI1">邻接矩阵<i class="fa fa-external-link-alt"></i></span><br><span class="exturl" data-url="aHR0cHM6Ly9lbi53aWtpcGVkaWEub3JnL3dpa2kvQ2x1c3RlcmluZ19jb2VmZmljaWVudA==">Clustering coefficient<i class="fa fa-external-link-alt"></i></span><br><span class="exturl" data-url="aHR0cHM6Ly9lbi53aWtpcGVkaWEub3JnL3dpa2kvQmV0d2Vlbm5lc3M=">Betweenness<i class="fa fa-external-link-alt"></i></span><br><span class="exturl" data-url="aHR0cHM6Ly93d3cuemhpaHUuY29tL3F1ZXN0aW9uLzE5OTY3Nzc4">如何理解希尔伯特空间？<i class="fa fa-external-link-alt"></i></span>，<span class="exturl" data-url="aHR0cHM6Ly96aC53aWtpcGVkaWEub3JnL3dpa2kvJUU1JUI4JThDJUU1JUIwJTk0JUU0JUJDJUFGJUU3JTg5JUI5JUU3JUE5JUJBJUU5JTk3JUI0">希尔伯特空间<i class="fa fa-external-link-alt"></i></span><br><span class="exturl" data-url="aHR0cHM6Ly9lbi53aWtpcGVkaWEub3JnL3dpa2kvRm91cmllcl90cmFuc2Zvcm0=">Fourier transform<i class="fa fa-external-link-alt"></i></span>，<span class="exturl" data-url="aHR0cHM6Ly96aC53aWtpcGVkaWEub3JnL3dpa2kvJUU1JTgyJTg1JUU5JTg3JThDJUU1JThGJUI2JUU1JThGJTk4JUU2JThEJUEy">傅里叶变换<i class="fa fa-external-link-alt"></i></span></p>
<p><span class="exturl" data-url="aHR0cDovL3RraXBmLmdpdGh1Yi5pby9ncmFwaC1jb252b2x1dGlvbmFsLW5ldHdvcmtzLw==">GRAPH CONVOLUTIONAL NETWORKS<i class="fa fa-external-link-alt"></i></span><br><span class="exturl" data-url="aHR0cHM6Ly93d3cuc29odS5jb20vYS8zNDI2MzQyOTFfNjUxODkz">跳出公式，看清全局，图神经网络（GCN）原理详解<i class="fa fa-external-link-alt"></i></span><br><span class="exturl" data-url="aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3l5bDQyNDUyNS9hcnRpY2xlL2RldGFpbHMvMTAwMDU4MjY0I0dDTl84Mjg=">图卷积网络 GCN Graph Convolutional Network（谱域GCN）的理解和详细推导<i class="fa fa-external-link-alt"></i></span><br><span class="exturl" data-url="aHR0cDovL3h0ZjYxNS5jb20vMjAxOS8wMi8yNC9nY24v">图卷积神经网络理论基础<i class="fa fa-external-link-alt"></i></span><br><span class="exturl" data-url="aHR0cHM6Ly96aHVhbmxhbi56aGlodS5jb20vcC83NjAwMTA4MA==">GNN综述——从入门到入门<i class="fa fa-external-link-alt"></i></span><br><span class="exturl" data-url="aHR0cHM6Ly93d3cuY25ibG9ncy5jb20vbnhmLXJhYmJpdDc1L3AvMTEzMDYxOTguaHRtbA==">GCN总结<i class="fa fa-external-link-alt"></i></span></p>

    </div>

    
    
    
      
  <div class="popular-posts-header">相关文章</div>
  <ul class="popular-posts">
    <li class="popular-posts-item">
      <div class="popular-posts-title"><a href="\2021\02\03\Graph\00.图深度表示\" rel="bookmark">图深度表示</a></div>
    </li>
    <li class="popular-posts-item">
      <div class="popular-posts-title"><a href="\2021\02\10\Graph\02.GAT\" rel="bookmark">GAT</a></div>
    </li>
    <li class="popular-posts-item">
      <div class="popular-posts-title"><a href="\2021\02\12\Graph\03.GraphSAGE\" rel="bookmark">GraphSAGE</a></div>
    </li>
  </ul>


    <footer class="post-footer">
          

<div class="post-copyright">
<ul>
  <li class="post-copyright-author">
      <strong>本文作者： </strong>SoundMemories
  </li>
  <li class="post-copyright-link">
      <strong>本文链接：</strong>
      <a href="https://soundmemories.github.io/2021/02/05/Graph/01.GCN/" title="GCN">https://soundmemories.github.io/2021/02/05/Graph/01.GCN/</a>
  </li>
  <li class="post-copyright-license">
    <strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <span class="exturl" data-url="aHR0cHM6Ly9jcmVhdGl2ZWNvbW1vbnMub3JnL2xpY2Vuc2VzL2J5LW5jLXNhLzQuMC8="><i class="fab fa-fw fa-creative-commons"></i>BY-NC-SA</span> 许可协议。转载请注明出处！
  </li>
</ul>
</div>

          <div class="post-tags">
              <a href="/tags/Graph/" rel="tag"><i class="fa fa-tag"></i> Graph</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2021/02/03/Graph/00.%E5%9B%BE%E6%B7%B1%E5%BA%A6%E8%A1%A8%E7%A4%BA/" rel="prev" title="图深度表示">
                  <i class="fa fa-chevron-left"></i> 图深度表示
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2021/02/10/Graph/02.GAT/" rel="next" title="GAT">
                  GAT <i class="fa fa-chevron-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






      

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      const activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      const commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

    </div>
  </main>

  <footer class="footer">
    <div class="footer-inner">
      

      

<div class="copyright">
  
  &copy; 2019 – 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">SoundMemories</span>
</div>
  <div class="powered-by">由 <span class="exturl theme-link" data-url="aHR0cHM6Ly9oZXhvLmlv">Hexo</span> & <span class="exturl theme-link" data-url="aHR0cHM6Ly90aGVtZS1uZXh0LmpzLm9yZy9tdXNlLw==">NexT.Muse</span> 强力驱动
  </div>

    </div>
  </footer>

  
  <script src="//cdn.jsdelivr.net/npm/animejs@3.2.1/lib/anime.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/@next-theme/pjax@0.4.0/pjax.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/jquery@3.5.1/dist/jquery.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js"></script>
<script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/muse.js"></script><script src="/js/next-boot.js"></script><script src="/js/bookmark.js"></script>
  <script>
var pjax = new Pjax({
  selectors: [
    'head title',
    '.page-configurations',
    '.main-inner',
    '.post-toc-wrap',
    '.languages',
    '.pjax'
  ],
  analytics: false,
  cacheBust: false,
  scrollRestoration: false,
  scrollTo: !CONFIG.bookmark.enable
});

document.addEventListener('pjax:success', () => {
  pjax.executeScripts(document.querySelectorAll('script[data-pjax], .pjax script'));
  NexT.boot.refresh();
  // Define Motion Sequence & Bootstrap Motion.
  if (CONFIG.motion.enable) {
    NexT.motion.integrator
      .init()
      .add(NexT.motion.middleWares.subMenu)
      .add(NexT.motion.middleWares.postList)
      .bootstrap();
  }
  const hasTOC = document.querySelector('.post-toc');
  document.querySelector('.sidebar-inner').classList.toggle('sidebar-nav-active', hasTOC);
  document.querySelector(hasTOC ? '.sidebar-nav-toc' : '.sidebar-nav-overview').click();
  NexT.utils.updateSidebarPosition();
});
</script>


  




  <script src="/js/local-search.js"></script>








<script data-pjax>
document.querySelectorAll('.pdfobject-container').forEach(element => {
  const url = element.dataset.target;
  const pdfOpenParams = {
    navpanes : 0,
    toolbar  : 0,
    statusbar: 0,
    pagemode : 'thumbs',
    view     : 'FitH'
  };
  const pdfOpenFragment = '#' + Object.entries(pdfOpenParams).map(([key, value]) => `${key}=${encodeURIComponent(value)}`).join('&');
  const fullURL = `/lib/pdf/web/viewer.html?file=${encodeURIComponent(url)}${pdfOpenFragment}`;

  if (NexT.utils.supportsPDFs()) {
    element.innerHTML = `<embed class="pdfobject" src="${url + pdfOpenFragment}" type="application/pdf" style="height: ${element.dataset.height};">`;
  } else {
    element.innerHTML = `<iframe src="${fullURL}" style="height: ${element.dataset.height};" frameborder="0"></iframe>`;
  }
});
</script>


<script data-pjax>
if (document.querySelectorAll('.mermaid').length) {
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/mermaid@8.8.2/dist/mermaid.min.js', () => {
    mermaid.init({
      theme    : 'neutral',
      logLevel : 3,
      flowchart: { curve     : 'linear' },
      gantt    : { axisFormat: '%m/%d/%Y' },
      sequence : { actorMargin: 50 }
    }, '.mermaid');
  }, window.mermaid);
}
</script>





  








    <div class="pjax">
  

  
      <script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              const target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    const script = document.createElement('script');
    script.src = '//cdn.jsdelivr.net/npm/mathjax@3.1.2/es5/tex-mml-chtml.js';
    script.defer = true;
    document.head.appendChild(script);
  } else {
    MathJax.startup.document.state(0);
    MathJax.typesetClear();
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  
  <script src="//cdn.jsdelivr.net/npm/quicklink@2.0.0/dist/quicklink.umd.js"></script>
  <script>
      window.addEventListener('load', () => {
      quicklink.listen({
        timeout : 3000,
        priority: true,
        ignores : [uri => uri.includes('#'),uri => uri === 'https://soundmemories.github.io/2021/02/05/Graph/01.GCN/',]
      });
      });
  </script>

    </div>
</body>
</html>
