<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222" media="(prefers-color-scheme: light)">
<meta name="theme-color" content="#222" media="(prefers-color-scheme: dark)"><meta name="generator" content="Hexo 6.3.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="https://fonts.loli.net/css?family=Monda:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">

<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css" integrity="sha256-HtsXJanqjKTc8vVQjO4YMhiqFoXkfBsjBWcX91T1jr8=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.css" integrity="sha256-Vzbj7sDDS/woiFS3uNKo8eIuni59rjyNGtXfstRzStA=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"soundmemories.github.io","root":"/","images":"/images","scheme":"Muse","darkmode":true,"version":"8.17.1","exturl":true,"sidebar":{"position":"left","display":"always","padding":18,"offset":12},"copycode":{"enable":true,"style":null},"bookmark":{"enable":true,"color":"#222","save":"auto"},"mediumzoom":true,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"},"path":"/search.xml","localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false}}</script><script src="/js/config.js"></script>

    <meta name="description" content="本项目使用了word2vec的中文预训练向量 模型分别有BiLSTM-attention和普通的LSTM两种，自行选择 使用说明： 1、在Config中配置相关参数 2、然后运行DataProcess.py，生成相应的word2id，word2vec等文件 3、运行主函数main.py，得到训练好的模型，并保存模型 4、运行eval.py，读取模型，并得到评价 5、模型准确率平均85">
<meta property="og:type" content="article">
<meta property="og:title" content="基于LSTM的情感分类">
<meta property="og:url" content="https://soundmemories.github.io/2021/07/15/NLP/02.%E5%9F%BA%E4%BA%8ELSTM%E7%9A%84%E6%83%85%E6%84%9F%E5%88%86%E7%B1%BB/index.html">
<meta property="og:site_name" content="SoundMemories">
<meta property="og:description" content="本项目使用了word2vec的中文预训练向量 模型分别有BiLSTM-attention和普通的LSTM两种，自行选择 使用说明： 1、在Config中配置相关参数 2、然后运行DataProcess.py，生成相应的word2id，word2vec等文件 3、运行主函数main.py，得到训练好的模型，并保存模型 4、运行eval.py，读取模型，并得到评价 5、模型准确率平均85">
<meta property="og:locale" content="zh_CN">
<meta property="article:published_time" content="2021-07-14T16:00:00.000Z">
<meta property="article:modified_time" content="2023-05-25T15:55:48.609Z">
<meta property="article:author" content="SoundMemories">
<meta property="article:tag" content="NLP">
<meta name="twitter:card" content="summary">


<link rel="canonical" href="https://soundmemories.github.io/2021/07/15/NLP/02.%E5%9F%BA%E4%BA%8ELSTM%E7%9A%84%E6%83%85%E6%84%9F%E5%88%86%E7%B1%BB/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-CN","comments":true,"permalink":"https://soundmemories.github.io/2021/07/15/NLP/02.%E5%9F%BA%E4%BA%8ELSTM%E7%9A%84%E6%83%85%E6%84%9F%E5%88%86%E7%B1%BB/","path":"2021/07/15/NLP/02.基于LSTM的情感分类/","title":"基于LSTM的情感分类"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>基于LSTM的情感分类 | SoundMemories</title>
  








  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">SoundMemories</p>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="搜索" role="button">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签<span class="badge">10</span></a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类<span class="badge">10</span></a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档<span class="badge">126</span></a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup"><div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" maxlength="80"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close" role="button">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="search-result-container no-result">
  <div class="search-result-icon">
    <i class="fa fa-spinner fa-pulse fa-5x"></i>
  </div>
</div>

    </div>
  </div>

</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%8F%82%E8%80%83%E6%96%87%E7%8C%AE"><span class="nav-number">1.</span> <span class="nav-text">参考文献</span></a></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="SoundMemories"
      src="/images/avstar.png">
  <p class="site-author-name" itemprop="name">SoundMemories</p>
  <div class="site-description" itemprop="description">今日事，今日毕</div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">126</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">10</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">10</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author animated">
      <span class="links-of-author-item">
        <span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL3NvdW5kbWVtb3JpZXM=" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;soundmemories"><i class="fab fa-github fa-fw"></i>GitHub</span>
      </span>
      <span class="links-of-author-item">
        <span class="exturl" data-url="bWFpbHRvOnNvdW5kbWVtb3JpZXNAMTYzLmNvbQ==" title="E-Mail → mailto:soundmemories@163.com"><i class="fa fa-envelope fa-fw"></i>E-Mail</span>
      </span>
  </div>
  <div class="cc-license animated" itemprop="license">
    <span class="exturl cc-opacity" data-url="aHR0cHM6Ly9jcmVhdGl2ZWNvbW1vbnMub3JnL2xpY2Vuc2VzL2J5LW5jLXNhLzQuMC8="><img src="https://cdnjs.cloudflare.com/ajax/libs/creativecommons-vocabulary/2020.11.3/assets/license_badges/small/by_nc_sa.svg" alt="Creative Commons"></span>
  </div>

        </div>
      </div>
    </div>

    
        <div class="pjax">
        </div>
  </aside>


    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://soundmemories.github.io/2021/07/15/NLP/02.%E5%9F%BA%E4%BA%8ELSTM%E7%9A%84%E6%83%85%E6%84%9F%E5%88%86%E7%B1%BB/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avstar.png">
      <meta itemprop="name" content="SoundMemories">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="SoundMemories">
      <meta itemprop="description" content="今日事，今日毕">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="基于LSTM的情感分类 | SoundMemories">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          基于LSTM的情感分类
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2021-07-15 00:00:00" itemprop="dateCreated datePublished" datetime="2021-07-15T00:00:00+08:00">2021-07-15</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/NLP/" itemprop="url" rel="index"><span itemprop="name">NLP</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>2.5k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>9 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody"><p><strong>本项目使用了word2vec的中文预训练向量</strong><br />
<strong>模型分别有BiLSTM-attention和普通的LSTM两种，自行选择</strong></p>
<p><strong>使用说明</strong>：<br />
1、在<strong>Config</strong>中配置相关参数</p>
<p>2、然后运行<strong>DataProcess.py</strong>，生成相应的word2id，word2vec等文件</p>
<p>3、运行主函数<strong>main.py</strong>，得到训练好的模型，并保存模型</p>
<p>4、运行<strong>eval.py</strong>，读取模型，并得到评价</p>
<p>5、模型<strong>准确率平均85%左右</strong></p>
<figure class="highlight python"><figcaption><span>Sentiment_Analysis_Config.py</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Config</span>():</span><br><span class="line">    update_w2v = <span class="literal">True</span>          <span class="comment"># 是否在训练中更新w2v</span></span><br><span class="line">    vocab_size = <span class="number">54848</span>          <span class="comment"># 词汇量，与word2id中的词汇量一致</span></span><br><span class="line">    n_class = <span class="number">2</span>                 <span class="comment"># 分类数：分别为pos和neg</span></span><br><span class="line">    max_sen_len = <span class="number">65</span>           <span class="comment"># 句子最大长度</span></span><br><span class="line">    embedding_dim = <span class="number">50</span>          <span class="comment"># 词向量维度</span></span><br><span class="line">    batch_size =<span class="number">64</span>            <span class="comment"># 批处理尺寸</span></span><br><span class="line">    hidden_dim=<span class="number">100</span>           <span class="comment"># 隐藏层节点数</span></span><br><span class="line">    n_epoch = <span class="number">30</span>            <span class="comment"># 训练迭代周期，即遍历整个训练样本的次数</span></span><br><span class="line">    lr = <span class="number">0.0001</span>               <span class="comment"># 学习率；若opt=‘adadelta&#x27;，则不需要定义学习率</span></span><br><span class="line">    drop_keep_prob = <span class="number">0.2</span>        <span class="comment"># dropout层，参数keep的比例</span></span><br><span class="line">    num_layers = <span class="number">2</span>              <span class="comment"># LSTM层数</span></span><br><span class="line">    bidirectional=<span class="literal">True</span>         <span class="comment">#是否使用双向LSTM</span></span><br><span class="line">    train_path = <span class="string">&#x27;./word2vec_data/train.txt&#x27;</span></span><br><span class="line">    val_path = <span class="string">&#x27;./word2vec_data/validation.txt&#x27;</span></span><br><span class="line">    test_path = <span class="string">&#x27;./word2vec_data/test.txt&#x27;</span></span><br><span class="line">    pre_path =<span class="string">&#x27;./word2vec_data/pre.txt&#x27;</span></span><br><span class="line">    word2id_path = <span class="string">&#x27;./word2vec_data/word2id.txt&#x27;</span></span><br><span class="line">    pre_word2vec_path = <span class="string">&#x27;./word2vec_data/wiki_word2vec_50.bin&#x27;</span></span><br><span class="line">    corpus_word2vec_path = <span class="string">&#x27;./word2vec_data/word_vec.txt&#x27;</span></span><br><span class="line">    model_state_dict_path=<span class="string">&#x27;./word2vec_data/sen_model.pkl&#x27;</span><span class="comment"># 训练模型保存的地址</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><figcaption><span>Sentiment_Analysis_DataProcess.py</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> unicode_literals, print_function, division</span><br><span class="line"><span class="keyword">from</span> io <span class="keyword">import</span> <span class="built_in">open</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">import</span>  numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> gensim</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataset</span><br><span class="line"><span class="keyword">from</span> Sentiment_Analysis_Config <span class="keyword">import</span> Config</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Data_set</span>(<span class="title class_ inherited__">Dataset</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, Data, Label</span>):</span><br><span class="line">        self.Data = Data</span><br><span class="line">        <span class="keyword">if</span> Label <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:<span class="comment">#考虑对测试集的使用</span></span><br><span class="line">            self.Label = Label</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.Data)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, index</span>):</span><br><span class="line">        <span class="keyword">if</span> self.Label <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            data = torch.from_numpy(self.Data[index])</span><br><span class="line">            label = torch.from_numpy(self.Label[index])</span><br><span class="line">            <span class="keyword">return</span> data, label</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            data = torch.from_numpy(self.Data[index])</span><br><span class="line">            <span class="keyword">return</span> data</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">stopwordslist</span>():<span class="comment">#创建停用词表</span></span><br><span class="line">    stopwords = [line.strip() <span class="keyword">for</span> line <span class="keyword">in</span> <span class="built_in">open</span>(<span class="string">&#x27;word2vec_data/stopword.txt&#x27;</span>,encoding=<span class="string">&#x27;UTF-8&#x27;</span>).readlines()]</span><br><span class="line">    <span class="keyword">return</span> stopwords</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">build_word2id</span>(<span class="params">file</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    :param file: word2id保存地址</span></span><br><span class="line"><span class="string">    :return: None</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment">#num_50=0#统计长度大于50的句子数</span></span><br><span class="line">    stopwords = stopwordslist()</span><br><span class="line">    word2id = &#123;<span class="string">&#x27;_PAD_&#x27;</span>: <span class="number">0</span>&#125;</span><br><span class="line">    path = [Config.train_path, Config.val_path]</span><br><span class="line">    <span class="comment">#print(path)</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> _path <span class="keyword">in</span> path:</span><br><span class="line">        <span class="keyword">with</span> <span class="built_in">open</span>(_path, encoding=<span class="string">&#x27;utf-8&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">            <span class="keyword">for</span> line <span class="keyword">in</span> f.readlines():</span><br><span class="line">                out_list = []</span><br><span class="line">                <span class="comment"># 去停用词</span></span><br><span class="line">                sp = line.strip().split()</span><br><span class="line">                <span class="keyword">for</span> word <span class="keyword">in</span> sp[<span class="number">1</span>:]:</span><br><span class="line">                    <span class="keyword">if</span> word <span class="keyword">not</span> <span class="keyword">in</span> stopwords:</span><br><span class="line">                        rt = re.findall(<span class="string">&#x27;[a-zA-Z]+&#x27;</span>, word)</span><br><span class="line">                        <span class="keyword">if</span> word != <span class="string">&#x27;\t&#x27;</span>:</span><br><span class="line">                            <span class="comment"># if is_number(word):</span></span><br><span class="line">                            <span class="comment"># continue</span></span><br><span class="line">                            <span class="keyword">if</span> <span class="built_in">len</span>(rt) == <span class="number">1</span>:</span><br><span class="line">                                <span class="keyword">continue</span></span><br><span class="line">                            <span class="keyword">else</span>:</span><br><span class="line">                                out_list.append(word)</span><br><span class="line">                <span class="keyword">for</span> word <span class="keyword">in</span> out_list:</span><br><span class="line">                    <span class="keyword">if</span> word <span class="keyword">not</span> <span class="keyword">in</span> word2id.keys():</span><br><span class="line">                        word2id[word] = <span class="built_in">len</span>(word2id)</span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(file, <span class="string">&#x27;w&#x27;</span>, encoding=<span class="string">&#x27;utf-8&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">        <span class="keyword">for</span> w <span class="keyword">in</span> word2id:</span><br><span class="line">            f.write(w+<span class="string">&#x27;\t&#x27;</span>)</span><br><span class="line">            f.write(<span class="built_in">str</span>(word2id[w]))</span><br><span class="line">            f.write(<span class="string">&#x27;\n&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">build_word2vec</span>(<span class="params">fname, word2id, save_to_path=<span class="literal">None</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    :param fname: 预训练的word2vec.</span></span><br><span class="line"><span class="string">    :param word2id: 语料文本中包含的词汇集.</span></span><br><span class="line"><span class="string">    :param save_to_path: 保存训练语料库中的词组对应的word2vec到本地</span></span><br><span class="line"><span class="string">    :return: 语料文本中词汇集对应的word2vec向量&#123;id: word2vec&#125;.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    n_words = <span class="built_in">max</span>(word2id.values()) + <span class="number">1</span></span><br><span class="line">    model = gensim.models.KeyedVectors.load_word2vec_format(fname, binary=<span class="literal">True</span>)</span><br><span class="line">    word_vecs = np.array(np.random.uniform(-<span class="number">1.</span>, <span class="number">1.</span>, [n_words, model.vector_size]))</span><br><span class="line">    <span class="keyword">for</span> word <span class="keyword">in</span> word2id.keys():</span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            word_vecs[word2id[word]] = model[word]</span><br><span class="line">        <span class="keyword">except</span> KeyError:</span><br><span class="line">            <span class="keyword">pass</span></span><br><span class="line">    <span class="keyword">if</span> save_to_path:</span><br><span class="line">        <span class="keyword">with</span> <span class="built_in">open</span>(save_to_path, <span class="string">&#x27;w&#x27;</span>, encoding=<span class="string">&#x27;utf-8&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">            <span class="keyword">for</span> vec <span class="keyword">in</span> word_vecs:</span><br><span class="line">                vec = [<span class="built_in">str</span>(w) <span class="keyword">for</span> w <span class="keyword">in</span> vec]</span><br><span class="line">                f.write(<span class="string">&#x27; &#x27;</span>.join(vec))</span><br><span class="line">                f.write(<span class="string">&#x27;\n&#x27;</span>)</span><br><span class="line">    <span class="keyword">return</span> word_vecs</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">text_to_array</span>(<span class="params">word2id,seq_lenth ,path</span>):  <span class="comment"># 文本转为索引数字模式,</span></span><br><span class="line"></span><br><span class="line">    lable_array=[]</span><br><span class="line">    i=<span class="number">0</span></span><br><span class="line">    sa=[]</span><br><span class="line">    <span class="comment">#获取句子个数</span></span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(path, encoding=<span class="string">&#x27;utf-8&#x27;</span>) <span class="keyword">as</span> f1:</span><br><span class="line">        <span class="keyword">for</span> l1 <span class="keyword">in</span> f1.readlines():</span><br><span class="line">            s= l1.strip().split()</span><br><span class="line">            s1=s[<span class="number">1</span>:]</span><br><span class="line">            new_s = [word2id.get(word, <span class="number">0</span>) <span class="keyword">for</span> word <span class="keyword">in</span> s1]  <span class="comment"># 单词转索引数字</span></span><br><span class="line">            sa.append(new_s)</span><br><span class="line">        <span class="comment">#print(len(sa))</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(path, encoding=<span class="string">&#x27;utf-8&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">        sentences_array=np.zeros(shape=(<span class="built_in">len</span>(sa),seq_lenth))<span class="comment">#行：句子个数 列：句子长度</span></span><br><span class="line">        <span class="keyword">for</span> line <span class="keyword">in</span> f.readlines():</span><br><span class="line">            sl1 = line.strip().split()</span><br><span class="line">            sen=sl1[<span class="number">1</span>:]</span><br><span class="line">            new_sen = [word2id.get(word, <span class="number">0</span>) <span class="keyword">for</span> word <span class="keyword">in</span> sen]  <span class="comment"># 单词转索引数字,不存在则为0</span></span><br><span class="line">            new_sen_np=np.array(new_sen).reshape(<span class="number">1</span>,-<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">            <span class="comment">#补齐每个句子长度，多余补零，少了就直接赋值,0填在前面。</span></span><br><span class="line">            <span class="keyword">if</span> np.size(new_sen_np,<span class="number">1</span>)&lt;seq_lenth:</span><br><span class="line">                sentences_array[i,seq_lenth-np.size(new_sen_np,<span class="number">1</span>):]=new_sen_np[<span class="number">0</span>,:]</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                sentences_array[i, <span class="number">0</span>:seq_lenth]=new_sen_np[<span class="number">0</span>,<span class="number">0</span>:seq_lenth]</span><br><span class="line"></span><br><span class="line">            i=i+<span class="number">1</span></span><br><span class="line">            lable=<span class="built_in">int</span>(sl1[<span class="number">0</span>])<span class="comment">#标签</span></span><br><span class="line">            lable_array.append(lable)</span><br><span class="line">    <span class="keyword">return</span> np.array(sentences_array),lable_array</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">text_to_array_nolable</span>(<span class="params">word2id,seq_lenth ,path</span>):  <span class="comment"># 文本转为索引数字模式,</span></span><br><span class="line"></span><br><span class="line">    i=<span class="number">0</span></span><br><span class="line">    sa=[]</span><br><span class="line">    <span class="comment">#获取句子个数</span></span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(path, encoding=<span class="string">&#x27;utf-8&#x27;</span>) <span class="keyword">as</span> f1:</span><br><span class="line">        <span class="keyword">for</span> l1 <span class="keyword">in</span> f1.readlines():</span><br><span class="line">            s= l1.strip().split()</span><br><span class="line">            s1=s[<span class="number">1</span>:]</span><br><span class="line">            new_s = [word2id.get(word, <span class="number">0</span>) <span class="keyword">for</span> word <span class="keyword">in</span> s1]  <span class="comment"># 单词转索引数字</span></span><br><span class="line">            sa.append(new_s)</span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(path, encoding=<span class="string">&#x27;utf-8&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">        sentences_array=np.zeros(shape=(<span class="built_in">len</span>(sa),seq_lenth))<span class="comment">#行：句子个数 列：句子长度</span></span><br><span class="line">        <span class="keyword">for</span> line <span class="keyword">in</span> f.readlines():</span><br><span class="line">            sl1 = line.strip().split()</span><br><span class="line">            sen=sl1[<span class="number">1</span>:]</span><br><span class="line">            new_sen = [word2id.get(word, <span class="number">0</span>) <span class="keyword">for</span> word <span class="keyword">in</span> sen]  <span class="comment"># 单词转索引数字,不存在则为0</span></span><br><span class="line">            new_sen_np=np.array(new_sen).reshape(<span class="number">1</span>,-<span class="number">1</span>)</span><br><span class="line">            <span class="keyword">if</span> np.size(new_sen_np,<span class="number">1</span>)&lt;seq_lenth:</span><br><span class="line">                sentences_array[i,seq_lenth-np.size(new_sen_np,<span class="number">1</span>):]=new_sen_np[<span class="number">0</span>,:]</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                sentences_array[i, <span class="number">0</span>:seq_lenth]=new_sen_np[<span class="number">0</span>,<span class="number">0</span>:seq_lenth]</span><br><span class="line">            i=i+<span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> np.array(sentences_array)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">to_categorical</span>(<span class="params">y, num_classes=<span class="literal">None</span></span>):<span class="comment">#将类别转化为one-hot编码</span></span><br><span class="line">    y = np.array(y, dtype=<span class="string">&#x27;int&#x27;</span>)</span><br><span class="line">    input_shape = y.shape</span><br><span class="line">    <span class="keyword">if</span> input_shape <span class="keyword">and</span> input_shape[-<span class="number">1</span>] == <span class="number">1</span> <span class="keyword">and</span> <span class="built_in">len</span>(input_shape) &gt; <span class="number">1</span>:</span><br><span class="line">        input_shape = <span class="built_in">tuple</span>(input_shape[:-<span class="number">1</span>])</span><br><span class="line">    y = y.ravel()</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> num_classes:</span><br><span class="line">        num_classes = np.<span class="built_in">max</span>(y) + <span class="number">1</span></span><br><span class="line">    n = y.shape[<span class="number">0</span>]</span><br><span class="line">    categorical = np.zeros((n, num_classes))</span><br><span class="line">    categorical[np.arange(n), y] = <span class="number">1</span></span><br><span class="line">    output_shape = input_shape + (num_classes,)</span><br><span class="line">    categorical = np.reshape(categorical, output_shape)</span><br><span class="line">    <span class="keyword">return</span> categorical</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">prepare_data</span>(<span class="params">w2id, train_path,val_path,test_path,seq_lenth</span>):<span class="comment">#得到数字索引表示的句子和标签</span></span><br><span class="line">    train_array,train_lable = text_to_array(w2id,seq_lenth= seq_lenth,path=train_path)</span><br><span class="line">    val_array,val_lable  = text_to_array(w2id,seq_lenth=seq_lenth,path= val_path)</span><br><span class="line">    test_array,test_lable=text_to_array(w2id,seq_lenth=seq_lenth,path=test_path)</span><br><span class="line"></span><br><span class="line">    <span class="comment">#标签为[1, 1, 1, 1, 1, 1, 1, 1, 0, 0...]将标签转为onehot</span></span><br><span class="line">    <span class="comment">#train_lable=to_categorical(train_lable,num_classes=2)</span></span><br><span class="line">    <span class="comment">#val_lable=to_categorical(val_lable,num_classes=2)</span></span><br><span class="line"></span><br><span class="line">    <span class="string">&quot;&quot;&quot;for i in train_lable:</span></span><br><span class="line"><span class="string">        np.array([i])&quot;&quot;&quot;</span></span><br><span class="line">    train_lable=np.array([train_lable]).T</span><br><span class="line">    val_lable=np.array([val_lable]).T</span><br><span class="line">    test_lable=np.array([test_lable]).T</span><br><span class="line">    <span class="string">&quot;&quot;&quot;转换后标签</span></span><br><span class="line"><span class="string">            [[0. 1.]</span></span><br><span class="line"><span class="string">            [0. 1.]</span></span><br><span class="line"><span class="string">            [0. 1.]</span></span><br><span class="line"><span class="string">            ...</span></span><br><span class="line"><span class="string">            [1. 0.]</span></span><br><span class="line"><span class="string">            [1. 0.]</span></span><br><span class="line"><span class="string">            [1. 0.]]&quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment">#print(train_lab,&quot;\nval\n&quot;,val_lab)</span></span><br><span class="line">    <span class="keyword">return</span> train_array ,train_lable,val_array,val_lable,test_array,test_lable</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#建立word2id</span></span><br><span class="line">build_word2id(<span class="string">&#x27;./word2vec_data/word2id.txt&#x27;</span>)<span class="comment">#建立词toid</span></span><br><span class="line">splist=[]</span><br><span class="line">word2id=&#123;&#125;</span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;./word2vec_data/word2id.txt&#x27;</span>, encoding=<span class="string">&#x27;utf-8&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">        <span class="keyword">for</span> line <span class="keyword">in</span> f.readlines():</span><br><span class="line">            sp = line.strip().split()<span class="comment">#去掉\n \t 等</span></span><br><span class="line">            splist.append(sp)</span><br><span class="line">        word2id=<span class="built_in">dict</span>(splist)<span class="comment">#转成字典</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> key <span class="keyword">in</span> word2id:<span class="comment"># 将字典的值，从str转成int</span></span><br><span class="line">    word2id[key]=<span class="built_in">int</span>(word2id[key])</span><br><span class="line"></span><br><span class="line">id2word=&#123;&#125;<span class="comment">#得到id2word</span></span><br><span class="line"><span class="keyword">for</span> key,val <span class="keyword">in</span> word2id.items():</span><br><span class="line">    id2word[val]=key</span><br><span class="line"><span class="comment">#建立word2vec</span></span><br><span class="line">w2vec=build_word2vec(Config.pre_word2vec_path,word2id,Config.corpus_word2vec_path)</span><br><span class="line"></span><br><span class="line"><span class="comment">#得到句子id表示和标签</span></span><br><span class="line">train_array,train_lable,val_array,val_lable,test_array,test_label=prepare_data(word2id,</span><br><span class="line">                                                         train_path=Config.train_path,</span><br><span class="line">                                                         val_path=Config.val_path,</span><br><span class="line">                                                         test_path=Config.test_path,seq_lenth=Config.max_sen_len)</span><br><span class="line"></span><br><span class="line">np.savetxt(<span class="string">&#x27;./word2vec_data/train_data.txt&#x27;</span>, train_array,fmt=<span class="string">&#x27;%d&#x27;</span>)</span><br><span class="line">np.savetxt(<span class="string">&#x27;./word2vec_data/val_data.txt&#x27;</span>, val_array,fmt=<span class="string">&#x27;%d&#x27;</span>)</span><br><span class="line">np.savetxt(<span class="string">&#x27;./word2vec_data/test_data.txt&#x27;</span>, test_array,fmt=<span class="string">&#x27;%d&#x27;</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><figcaption><span>Sentiment_model.py</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> unicode_literals, print_function, division</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">LSTMModel</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, vocab_size, embedding_dim,pretrained_weight, update_w2v,hidden_dim,</span></span><br><span class="line"><span class="params">                 num_layers,drop_keep_prob,n_class,bidirectional, **kwargs</span>):</span><br><span class="line">        <span class="built_in">super</span>(LSTMModel, self).__init__()</span><br><span class="line">        self.hidden_dim = hidden_dim</span><br><span class="line">        self.num_layers = num_layers</span><br><span class="line">        self.n_class = n_class</span><br><span class="line"></span><br><span class="line">        self.bidirectional = bidirectional</span><br><span class="line">        self.embedding = nn.Embedding.from_pretrained(pretrained_weight)</span><br><span class="line">        self.embedding.weight.requires_grad = update_w2v</span><br><span class="line">        self.encoder = nn.LSTM(input_size=embedding_dim, hidden_size=self.hidden_dim,</span><br><span class="line">                               num_layers=num_layers, bidirectional=self.bidirectional,</span><br><span class="line">                               dropout=drop_keep_prob)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.bidirectional:</span><br><span class="line">            self.decoder1 = nn.Linear(hidden_dim * <span class="number">4</span>, hidden_dim)</span><br><span class="line">            self.decoder2 = nn.Linear(hidden_dim,n_class)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.decoder1 = nn.Linear(hidden_dim * <span class="number">2</span>, hidden_dim)</span><br><span class="line">            self.decoder2 = nn.Linear(hidden_dim,n_class)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, inputs</span>):</span><br><span class="line">        embeddings = self.embedding(inputs)<span class="comment"># [batch, seq_len] =&gt; [batch, seq_len, embed_dim][64,75,50]</span></span><br><span class="line">        states, hidden = self.encoder(embeddings.permute([<span class="number">1</span>, <span class="number">0</span>, <span class="number">2</span>]))<span class="comment">#[75,32,50],[seq_len, batch, embed_dim]</span></span><br><span class="line"></span><br><span class="line">        encoding = torch.cat([states[<span class="number">0</span>], states[-<span class="number">1</span>]], dim=<span class="number">1</span>)<span class="comment">#张量拼接[32,512]</span></span><br><span class="line">        outputs = self.decoder1(encoding)</span><br><span class="line">        <span class="comment">#outputs = F.softmax(outputs, dim=1)</span></span><br><span class="line">        outputs=self.decoder2(outputs)</span><br><span class="line">        <span class="keyword">return</span> outputs</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">LSTM_attention</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, vocab_size, embedding_dim,pretrained_weight, update_w2v,hidden_dim,</span></span><br><span class="line"><span class="params">                 num_layers,drop_keep_prob,n_class,bidirectional, **kwargs</span>):</span><br><span class="line">        <span class="built_in">super</span>(LSTM_attention, self).__init__()</span><br><span class="line">        self.hidden_dim = hidden_dim</span><br><span class="line">        self.num_layers = num_layers</span><br><span class="line">        self.n_class = n_class</span><br><span class="line"></span><br><span class="line">        self.bidirectional = bidirectional</span><br><span class="line">        self.embedding = nn.Embedding.from_pretrained(pretrained_weight)</span><br><span class="line">        self.embedding.weight.requires_grad = update_w2v</span><br><span class="line">        self.encoder = nn.LSTM(input_size=embedding_dim, hidden_size=self.hidden_dim,</span><br><span class="line">                               num_layers=num_layers, bidirectional=self.bidirectional,</span><br><span class="line">                               dropout=drop_keep_prob)</span><br><span class="line"></span><br><span class="line">        <span class="comment">#TODO</span></span><br><span class="line">        <span class="comment"># What is nn. Parameter ? Explain</span></span><br><span class="line">        self.weight_W = nn.Parameter(torch.Tensor(<span class="number">2</span>*hidden_dim, <span class="number">2</span>*hidden_dim))</span><br><span class="line">        self.weight_proj = nn.Parameter(torch.Tensor(<span class="number">2</span>*hidden_dim, <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.bidirectional:</span><br><span class="line">            <span class="comment">#self.decoder1 = nn.Linear(hidden_dim * 2, n_class)</span></span><br><span class="line">            self.decoder1 = nn.Linear(hidden_dim * <span class="number">2</span>, hidden_dim)</span><br><span class="line">            self.decoder2 = nn.Linear(hidden_dim,n_class)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.decoder1 = nn.Linear(hidden_dim * <span class="number">2</span>, hidden_dim)</span><br><span class="line">            self.decoder2 = nn.Linear(hidden_dim,n_class)</span><br><span class="line"></span><br><span class="line">        nn.init.uniform_(self.weight_W, -<span class="number">0.1</span>, <span class="number">0.1</span>)</span><br><span class="line">        nn.init.uniform_(self.weight_proj, -<span class="number">0.1</span>, <span class="number">0.1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, inputs</span>):<span class="number">0</span></span><br><span class="line">        embeddings = self.embedding(inputs)<span class="comment"># [batch, seq_len] =&gt; [batch, seq_len, embed_dim][64,75,50]</span></span><br><span class="line"></span><br><span class="line">        states, hidden = self.encoder(embeddings.permute([<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>]))<span class="comment">#[batch, seq_len, embed_dim]</span></span><br><span class="line">        <span class="comment">#attention</span></span><br><span class="line"></span><br><span class="line">        u = torch.tanh(torch.matmul(states, self.weight_W))</span><br><span class="line">        att = torch.matmul(u, self.weight_proj)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        att_score = F.softmax(att, dim=<span class="number">1</span>)</span><br><span class="line">        scored_x = states * att_score</span><br><span class="line">        encoding = torch.<span class="built_in">sum</span>(scored_x, dim=<span class="number">1</span>)</span><br><span class="line">        outputs = self.decoder1(encoding)</span><br><span class="line">        outputs=self.decoder2(outputs)</span><br><span class="line">        <span class="keyword">return</span> outputs</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><figcaption><span>Sentiment_Analysis_main.py</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> unicode_literals, print_function, division</span><br><span class="line"><span class="keyword">from</span> io <span class="keyword">import</span> <span class="built_in">open</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> optim</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader,Dataset</span><br><span class="line"><span class="keyword">import</span> tqdm</span><br><span class="line"><span class="keyword">from</span> Sentiment_Analysis_DataProcess <span class="keyword">import</span> prepare_data,build_word2vec,Data_set</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> confusion_matrix,f1_score,recall_score</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">from</span> Sentiment_model <span class="keyword">import</span> LSTMModel,LSTM_attention</span><br><span class="line"><span class="keyword">from</span> Sentiment_Analysis_Config <span class="keyword">import</span> Config</span><br><span class="line"><span class="keyword">from</span> Sentiment_Analysis_eval <span class="keyword">import</span> val_accuary</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">train</span>(<span class="params">train_dataloader,model, device, epoches, lr</span>):</span><br><span class="line"></span><br><span class="line">        model.train()</span><br><span class="line">        model = model.to(device)</span><br><span class="line">        <span class="built_in">print</span>(model)</span><br><span class="line">        optimizer = optim.Adam(model.parameters(), lr=lr)</span><br><span class="line">        criterion = nn.CrossEntropyLoss()</span><br><span class="line">        <span class="comment"># scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.2)  # 学习率调整</span></span><br><span class="line">        best_acc = <span class="number">0.85</span></span><br><span class="line">        <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(epoches):  <span class="comment"># 一个epoch可以认为是一次训练循环</span></span><br><span class="line">            train_loss = <span class="number">0.0</span></span><br><span class="line">            correct = <span class="number">0</span></span><br><span class="line">            total = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">            train_dataloader = tqdm.tqdm(train_dataloader)</span><br><span class="line">            <span class="comment"># train_dataloader.set_description(&#x27;[%s%04d/%04d %s%f]&#x27; % </span></span><br><span class="line">            <span class="comment">#                                 (&#x27;Epoch:&#x27;, epoch + 1, epoches, &#x27;lr:&#x27;, scheduler.get_last_lr()[0]))</span></span><br><span class="line">            <span class="keyword">for</span> i, data_ <span class="keyword">in</span> (<span class="built_in">enumerate</span>(train_dataloader)):</span><br><span class="line"></span><br><span class="line">                optimizer.zero_grad()</span><br><span class="line">                input_, target = data_[<span class="number">0</span>], data_[<span class="number">1</span>]</span><br><span class="line">                input_=input_.<span class="built_in">type</span>(torch.LongTensor)</span><br><span class="line">                target=target.<span class="built_in">type</span>(torch.LongTensor)</span><br><span class="line">                input_=input_.to(device)</span><br><span class="line">                target=target.to(device)</span><br><span class="line">                output= model(input_)</span><br><span class="line">                <span class="comment"># 经过模型对象就产生了输出</span></span><br><span class="line">                target=target.squeeze(<span class="number">1</span>)</span><br><span class="line">                loss = criterion(output, target)</span><br><span class="line">                loss.backward()</span><br><span class="line">                optimizer.step()</span><br><span class="line">                train_loss+= loss.item()</span><br><span class="line">                _, predicted = torch.<span class="built_in">max</span>(output, <span class="number">1</span>)</span><br><span class="line">                <span class="comment">#print(predicted.shape)</span></span><br><span class="line">                total += target.size(<span class="number">0</span>)  <span class="comment"># 此处的size()类似numpy的shape: np.shape(train_images)[0]</span></span><br><span class="line">                <span class="comment">#print(target.shape)</span></span><br><span class="line">                correct += (predicted == target).<span class="built_in">sum</span>().item()</span><br><span class="line">                F1=f1_score(target.cpu(),predicted.cpu(),average=<span class="string">&#x27;weighted&#x27;</span>)</span><br><span class="line">                Recall=recall_score(target.cpu(),predicted.cpu(),average=<span class="string">&#x27;micro&#x27;</span>)</span><br><span class="line">                <span class="comment">#CM=confusion_matrix(target.cpu(),predicted.cpu())</span></span><br><span class="line">                postfix = &#123;<span class="string">&#x27;train_loss: &#123;:.5f&#125;,train_acc:&#123;:.3f&#125;%&#x27;</span></span><br><span class="line">                           <span class="string">&#x27;,F1: &#123;:.3f&#125;%,Recall:&#123;:.3f&#125;%&#x27;</span> .<span class="built_in">format</span>(train_loss / (i + <span class="number">1</span>),</span><br><span class="line">                                                                        <span class="number">100</span> * correct / total, <span class="number">100</span>*F1 , <span class="number">100</span>* Recall)&#125;</span><br><span class="line">                train_dataloader.set_postfix(log=postfix)</span><br><span class="line"></span><br><span class="line">            acc=val_accuary(model,val_dataloader,device,criterion)</span><br><span class="line">            <span class="keyword">if</span> acc&gt;best_acc:</span><br><span class="line">                best_acc = acc</span><br><span class="line">                <span class="keyword">if</span> os.path.exists(Config.model_state_dict_path) == <span class="literal">False</span>:</span><br><span class="line">                    os.mkdir(Config.model_state_dict_path)</span><br><span class="line">                torch.save(model,<span class="string">&#x27;./word2vec_data/sen_model_best.pkl&#x27;</span> )</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    splist=[]</span><br><span class="line">    word2id=&#123;&#125;</span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(Config.word2id_path, encoding=<span class="string">&#x27;utf-8&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">            <span class="keyword">for</span> line <span class="keyword">in</span> f.readlines():</span><br><span class="line">                sp = line.strip().split()<span class="comment">#去掉\n \t 等</span></span><br><span class="line">                splist.append(sp)</span><br><span class="line">            word2id=<span class="built_in">dict</span>(splist)<span class="comment">#转成字典</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> key <span class="keyword">in</span> word2id:<span class="comment"># 将字典的值，从str转成int</span></span><br><span class="line">        word2id[key]=<span class="built_in">int</span>(word2id[key])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    id2word=&#123;&#125;<span class="comment">#得到id2word</span></span><br><span class="line">    <span class="keyword">for</span> key,val <span class="keyword">in</span> word2id.items():</span><br><span class="line">        id2word[val]=key</span><br><span class="line"></span><br><span class="line">    device = torch.device(<span class="string">&quot;cuda&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    train_array,train_lable,val_array,val_lable,test_array,test_lable=prepare_data(word2id,</span><br><span class="line">                                                             train_path=Config.train_path,</span><br><span class="line">                                                             val_path=Config.val_path,</span><br><span class="line">                                                             test_path=Config.test_path,seq_lenth=Config.max_sen_len)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    train_loader = Data_set(train_array, train_lable)</span><br><span class="line">    train_dataloader = DataLoader(train_loader,</span><br><span class="line">                                 batch_size=Config.batch_size,</span><br><span class="line">                                 shuffle=<span class="literal">True</span>,</span><br><span class="line">                                 num_workers=<span class="number">0</span>)<span class="comment">#用了workers反而变慢了</span></span><br><span class="line"></span><br><span class="line">    val_loader = Data_set(val_array, val_lable)</span><br><span class="line">    val_dataloader = DataLoader(val_loader,</span><br><span class="line">                                 batch_size=Config.batch_size,</span><br><span class="line">                                 shuffle=<span class="literal">True</span>,</span><br><span class="line">                                 num_workers=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    test_loader = Data_set(test_array, test_lable)</span><br><span class="line">    test_dataloader = DataLoader(test_loader,</span><br><span class="line">                                 batch_size=Config.batch_size,</span><br><span class="line">                                 shuffle=<span class="literal">True</span>,</span><br><span class="line">                                 num_workers=<span class="number">0</span>)</span><br><span class="line">    w2vec=build_word2vec(Config.pre_word2vec_path,word2id,<span class="literal">None</span>)<span class="comment">#生成word2vec</span></span><br><span class="line">    w2vec=torch.from_numpy(w2vec)</span><br><span class="line">    w2vec=w2vec.<span class="built_in">float</span>()<span class="comment">#CUDA接受float32，不接受float64</span></span><br><span class="line">    model=LSTM_attention(Config.vocab_size,Config.embedding_dim,w2vec,Config.update_w2v,</span><br><span class="line">                    Config.hidden_dim,Config.num_layers,Config.drop_keep_prob,Config.n_class,Config.bidirectional)</span><br><span class="line"></span><br><span class="line"><span class="comment">#训练</span></span><br><span class="line">    train(train_dataloader,model=model,device=device,epoches=Config.n_epoch,lr=Config.lr)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#保存模型</span></span><br><span class="line">    <span class="keyword">if</span> os.path.exists(Config.model_state_dict_path) == <span class="literal">False</span>:</span><br><span class="line">           os.mkdir(Config.model_state_dict_path)</span><br><span class="line">    torch.save(model, Config.model_state_dict_path)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><figcaption><span>Sentiment_Analysis_eval.py</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> unicode_literals, print_function, division</span><br><span class="line"><span class="keyword">from</span> io <span class="keyword">import</span> <span class="built_in">open</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> confusion_matrix,f1_score,recall_score,precision_score</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">from</span> Sentiment_model <span class="keyword">import</span> LSTMModel,LSTM_attention</span><br><span class="line"><span class="keyword">from</span> Sentiment_Analysis_Config <span class="keyword">import</span> Config</span><br><span class="line"><span class="keyword">from</span> Sentiment_Analysis_DataProcess <span class="keyword">import</span> prepare_data,build_word2vec,text_to_array_nolable,Data_set</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">val_accuary</span>(<span class="params">model,val_dataloader,device,criterion</span>):</span><br><span class="line">    model = model.to(device)</span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        correct1 = <span class="number">0</span></span><br><span class="line">        total1 = <span class="number">0</span></span><br><span class="line">        val_loss=<span class="number">0.0</span></span><br><span class="line">        <span class="keyword">for</span> j, data_1 <span class="keyword">in</span> (<span class="built_in">enumerate</span>(val_dataloader, <span class="number">0</span>)):</span><br><span class="line">            input1, target1 = data_1[<span class="number">0</span>], data_1[<span class="number">1</span>]</span><br><span class="line">            input1= input1.<span class="built_in">type</span>(torch.LongTensor)</span><br><span class="line">            target1 = target1.<span class="built_in">type</span>(torch.LongTensor)</span><br><span class="line">            target1=target1.squeeze(<span class="number">1</span>)<span class="comment">#从[64,1]到[64]</span></span><br><span class="line">            input1 = input1.to(device)</span><br><span class="line">            target1 = target1.to(device)</span><br><span class="line">            output1 = model(input1)</span><br><span class="line">            loss1 = criterion(output1, target1)</span><br><span class="line">            val_loss += loss1.item()</span><br><span class="line">            _, predicted1 = torch.<span class="built_in">max</span>(output1, <span class="number">1</span>)</span><br><span class="line">            total1 += target1.size(<span class="number">0</span>)<span class="comment"># 此处的size()类似numpy的shape: np.shape(train_images)[0]</span></span><br><span class="line">            correct1 += (predicted1 == target1).<span class="built_in">sum</span>().item()</span><br><span class="line">            F1 = f1_score(target1.cpu(), predicted1.cpu(), average=<span class="string">&#x27;weighted&#x27;</span>)</span><br><span class="line">            Recall = recall_score(target1.cpu(), predicted1.cpu(), average=<span class="string">&#x27;micro&#x27;</span>)</span><br><span class="line">            <span class="comment">#CM = confusion_matrix(target1.cpu(), predicted1.cpu())</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;\nVal accuracy : &#123;:.3f&#125;%,val_loss:&#123;:.3f&#125;, F1_score：&#123;:.3f&#125;%, Recall：&#123;:.3f&#125;%&#x27;</span>.<span class="built_in">format</span>(<span class="number">100</span>*correct1/total1,val_loss,<span class="number">100</span>*F1,<span class="number">100</span>*Recall))</span><br><span class="line">        <span class="keyword">return</span> <span class="number">100</span>*correct1/total1</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">test_accuary</span>(<span class="params">model,test_dataloader,device</span>):</span><br><span class="line">    model = model.to(device)</span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        correct = <span class="number">0</span></span><br><span class="line">        total = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> k, data_test <span class="keyword">in</span> (<span class="built_in">enumerate</span>(test_dataloader, <span class="number">0</span>)):</span><br><span class="line">            input_test, target_ = data_test[<span class="number">0</span>], data_test[<span class="number">1</span>]</span><br><span class="line">            input_test= input_test.<span class="built_in">type</span>(torch.LongTensor)</span><br><span class="line">            target_ = target_.<span class="built_in">type</span>(torch.LongTensor)</span><br><span class="line">            target_=target_.squeeze(<span class="number">1</span>)<span class="comment">#从[64,1]到[64]</span></span><br><span class="line">            input_test = input_test.to(device)</span><br><span class="line">            target_ = target_.to(device)</span><br><span class="line">            output2 = model(input_test)</span><br><span class="line">            _, predicted_test = torch.<span class="built_in">max</span>(output2, <span class="number">1</span>)</span><br><span class="line">            total += target_.size(<span class="number">0</span>)<span class="comment"># 此处的size()类似numpy的shape: np.shape(train_images)[0]</span></span><br><span class="line">            correct += (predicted_test == target_).<span class="built_in">sum</span>().item()</span><br><span class="line">            F1 = f1_score(target_.cpu(), predicted_test.cpu(), average=<span class="string">&#x27;weighted&#x27;</span>)</span><br><span class="line">            Recall = recall_score(target_.cpu(), predicted_test.cpu(), average=<span class="string">&#x27;micro&#x27;</span>)</span><br><span class="line">            CM = confusion_matrix(target_.cpu(), predicted_test.cpu())</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;test accuracy : &#123;:.3f&#125;%, F1_score：&#123;:.3f&#125;%, Recall：&#123;:.3f&#125;%,Confusion_matrix：&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(<span class="number">100</span>*correct/total,<span class="number">100</span>*F1,<span class="number">100</span>*Recall,CM))</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">pre</span>(<span class="params">word2id,model,seq_lenth ,path</span>):</span><br><span class="line">    model.cpu()</span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        input_array=text_to_array_nolable(word2id,seq_lenth,path)</span><br><span class="line">        <span class="comment">#sen_p = sen_p.type(torch.LongTensor)</span></span><br><span class="line">        sen_p = torch.from_numpy(input_array)</span><br><span class="line">        sen_p=sen_p.<span class="built_in">type</span>(torch.LongTensor)</span><br><span class="line">        output_p = model(sen_p)</span><br><span class="line">        _, pred = torch.<span class="built_in">max</span>(output_p, <span class="number">1</span>)</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> pred:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&#x27;预测类别为&#x27;</span>,i.item())</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    splist = []</span><br><span class="line">    word2id = &#123;&#125;</span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(Config.word2id_path, encoding=<span class="string">&#x27;utf-8&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">        <span class="keyword">for</span> line <span class="keyword">in</span> f.readlines():</span><br><span class="line">            sp = line.strip().split()  <span class="comment"># 去掉\n \t 等</span></span><br><span class="line">            splist.append(sp)</span><br><span class="line">        word2id = <span class="built_in">dict</span>(splist)  <span class="comment"># 转成字典</span></span><br><span class="line">    <span class="keyword">for</span> key <span class="keyword">in</span> word2id:  <span class="comment"># 将字典的值，从str转成int</span></span><br><span class="line">        word2id[key] = <span class="built_in">int</span>(word2id[key])</span><br><span class="line"></span><br><span class="line">    device = torch.device(<span class="string">&quot;cuda&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span>)</span><br><span class="line">    train_array, train_lable, val_array, val_lable, test_array, test_lable = prepare_data(word2id,</span><br><span class="line">                                                                                          train_path=Config.train_path,</span><br><span class="line">                                                                                          val_path=Config.val_path,</span><br><span class="line">                                                                                          test_path=Config.test_path,seq_lenth=Config.max_sen_len)</span><br><span class="line">    test_loader = Data_set(test_array, test_lable)</span><br><span class="line">    test_dataloader = DataLoader(test_loader,</span><br><span class="line">                                 batch_size=Config.batch_size,</span><br><span class="line">                                 shuffle=<span class="literal">True</span>,</span><br><span class="line">                                 num_workers=<span class="number">0</span>)</span><br><span class="line">    w2vec = build_word2vec(Config.pre_word2vec_path,</span><br><span class="line">                           word2id,</span><br><span class="line">                          <span class="literal">None</span>)  <span class="comment"># 生成word2vec</span></span><br><span class="line">    w2vec = torch.from_numpy(w2vec)</span><br><span class="line">    w2vec = w2vec.<span class="built_in">float</span>()  <span class="comment"># CUDA接受float32，不接受float64</span></span><br><span class="line"></span><br><span class="line">    model=LSTM_attention(Config.vocab_size,Config.embedding_dim,w2vec,Config.update_w2v,</span><br><span class="line">                        Config.hidden_dim,Config.num_layers,Config.drop_keep_prob,Config.n_class,Config.bidirectional)</span><br><span class="line">    <span class="comment"># 读取模型</span></span><br><span class="line">    <span class="comment">#model1 = torch.load(Config.model_state_dict_path)</span></span><br><span class="line">    model = torch.load(<span class="string">&#x27;./word2vec_data/sen_model_best.pkl&#x27;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">#model.load_state_dict(torch.load(Config.model_state_dict_path)) #仅保存参数</span></span><br><span class="line">    <span class="comment">#验证</span></span><br><span class="line">    <span class="comment">#val_accuary(model1, val_dataloader, device)</span></span><br><span class="line">    <span class="comment">#测试</span></span><br><span class="line">    test_accuary(model,test_dataloader,device)</span><br><span class="line">    <span class="comment">#预测</span></span><br><span class="line">    pre(word2id,model,Config.max_sen_len,Config.pre_path)</span><br></pre></td></tr></table></figure>
<h1 id="参考文献">参考文献</h1>
<p><span class="exturl" data-url="aHR0cHM6Ly9naXRlZS5jb20vaHUteWFuZ2dhbmcvU2VudGltZW50LUFuYWx5c2lzLUNoaW5lc2UtcHl0b3JjaCMlRTUlOEUlOUYlRTUlODglOUIlRTQlQjglOEQlRTYlOTglOTMlRTUlQTYlODIlRTYlOUUlOUMlRTUlQTUlQkQlRTclOTQlQTglRTglQUYlQjclRTclQkIlOTklRTQlQjglQUFzdGFyJUU4JUIwJUEyJUU4JUIwJUEyJUU0JUJBJTg2LSVFNCVCRCU5QyVFOCU4MCU4NSVFNiU5RCU4RSVFNyU4QiU5NyVFNSU5NyVBOA==">Sentiment-Analysis-Chinese-pytorch<i class="fa fa-external-link-alt"></i></span></p>

    </div>

    
    
    

    <footer class="post-footer">
          

<div class="post-copyright">
<ul>
  <li class="post-copyright-author">
      <strong>本文作者： </strong>SoundMemories
  </li>
  <li class="post-copyright-link">
      <strong>本文链接：</strong>
      <a href="https://soundmemories.github.io/2021/07/15/NLP/02.%E5%9F%BA%E4%BA%8ELSTM%E7%9A%84%E6%83%85%E6%84%9F%E5%88%86%E7%B1%BB/" title="基于LSTM的情感分类">https://soundmemories.github.io/2021/07/15/NLP/02.基于LSTM的情感分类/</a>
  </li>
  <li class="post-copyright-license">
      <strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <span class="exturl" data-url="aHR0cHM6Ly9jcmVhdGl2ZWNvbW1vbnMub3JnL2xpY2Vuc2VzL2J5LW5jLXNhLzQuMC8="><i class="fab fa-fw fa-creative-commons"></i>BY-NC-SA</span> 许可协议。转载请注明出处！
  </li>
</ul>
</div>

          <div class="post-tags">
              <a href="/tags/NLP/" rel="tag"><i class="fa fa-tag"></i> NLP</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2021/06/25/Paper/02.XGBoost%20A%20Scalable%20Tree%20Boosting%20System/" rel="prev" title="XGBoost A Scalable Tree Boosting System">
                  <i class="fa fa-chevron-left"></i> XGBoost A Scalable Tree Boosting System
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2021/07/16/NLP/03.%E5%9F%BA%E4%BA%8ELSTM%E7%9A%84%E6%9C%BA%E5%99%A8%E7%BF%BB%E8%AF%91/" rel="next" title="基于LSTM的机器翻译">
                  基于LSTM的机器翻译 <i class="fa fa-chevron-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="copyright">
    &copy; 2019 – 
    <span itemprop="copyrightYear">2024</span>
    <span class="with-love">
      <i class="fa fa-heart"></i>
    </span>
    <span class="author" itemprop="copyrightHolder">SoundMemories</span>
  </div>
  <div class="powered-by">由 <span class="exturl" data-url="aHR0cHM6Ly9oZXhvLmlv">Hexo</span> & <span class="exturl" data-url="aHR0cHM6Ly90aGVtZS1uZXh0LmpzLm9yZy9tdXNlLw==">NexT.Muse</span> 强力驱动
  </div>

    </div>
  </footer>

  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>
  <div class="sidebar-dimmer"></div>
  <div class="back-to-top" role="button" aria-label="返回顶部">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>
  <a role="button" class="book-mark-link book-mark-link-fixed"></a>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/next-theme-pjax/0.6.0/pjax.min.js" integrity="sha256-vxLn1tSKWD4dqbMRyv940UYw4sXgMtYcK6reefzZrao=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.7.0/jquery.min.js" integrity="sha256-2Pmvv0kuTBOenSvLm6bvfBSSHrUJ+3A7x6P5Ebd07/g=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.js" integrity="sha256-yt2kYMy0w8AbtF89WXb2P1rfjcP/HTHLT7097U8Y5b8=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/medium-zoom/1.0.8/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/muse.js"></script><script src="/js/next-boot.js"></script><script src="/js/bookmark.js"></script><script src="/js/pjax.js"></script>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-generator-searchdb/1.4.1/search.js" integrity="sha256-1kfA5uHPf65M5cphT2dvymhkuyHPQp5A53EGZOnOLmc=" crossorigin="anonymous"></script>
<script src="/js/third-party/search/local-search.js"></script>

  <script class="next-config" data-name="pdf" type="application/json">{"object_url":{"url":"https://cdnjs.cloudflare.com/ajax/libs/pdfobject/2.2.12/pdfobject.min.js","integrity":"sha256-g2xji1rlE3KsGVClvuxTbcR0Kn2+wtQADSff2Tbb4zA="},"url":"/lib/pdf/web/viewer.html"}</script>
  <script src="/js/third-party/tags/pdf.js"></script>

  <script class="next-config" data-name="mermaid" type="application/json">{"enable":true,"theme":{"light":"neutral","dark":"neutral"},"js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mermaid/10.2.3/mermaid.min.js","integrity":"sha256-JFptYy4KzJ5OQP+Q9fubNf3cxpPPmZKqUOovyEONKrQ="}}</script>
  <script src="/js/third-party/tags/mermaid.js"></script>


  <script src="/js/third-party/fancybox.js"></script>



  




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"none","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>


  <script src="https://cdnjs.cloudflare.com/ajax/libs/quicklink/2.3.0/quicklink.umd.js" integrity="sha256-yvJQOINiH9fWemHn0vCA5lsHWJaHs6/ZmO+1Ft04SvM=" crossorigin="anonymous"></script>
  <script class="next-config" data-name="quicklink" type="application/json">{"enable":true,"home":true,"archive":true,"delay":true,"timeout":3000,"priority":true,"url":"https://soundmemories.github.io/2021/07/15/NLP/02.%E5%9F%BA%E4%BA%8ELSTM%E7%9A%84%E6%83%85%E6%84%9F%E5%88%86%E7%B1%BB/"}</script>
  <script src="/js/third-party/quicklink.js"></script>

</body>
</html>
