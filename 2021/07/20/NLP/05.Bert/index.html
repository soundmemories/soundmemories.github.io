<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222" media="(prefers-color-scheme: light)">
<meta name="theme-color" content="#222" media="(prefers-color-scheme: dark)"><meta name="generator" content="Hexo 6.3.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="https://fonts.loli.net/css?family=Monda:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">

<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css" integrity="sha256-HtsXJanqjKTc8vVQjO4YMhiqFoXkfBsjBWcX91T1jr8=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.css" integrity="sha256-Vzbj7sDDS/woiFS3uNKo8eIuni59rjyNGtXfstRzStA=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"soundmemories.github.io","root":"/","images":"/images","scheme":"Muse","darkmode":true,"version":"8.17.1","exturl":true,"sidebar":{"position":"left","display":"always","padding":18,"offset":12},"copycode":{"enable":true,"style":null},"bookmark":{"enable":true,"color":"#222","save":"auto"},"mediumzoom":true,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"},"path":"/search.xml","localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false}}</script><script src="/js/config.js"></script>

    <meta name="description" content="介绍 word2vec和GloVe都将相同的预训练向量分配给同一个词，而不考虑词的上下文。 由此出现考虑上下文的ELMo，通过将整个序列作为输入，ELMo是为输入序列中的每个单词分配一个表示的函数。具体来说，ELMo将来自预训练的双向长短期记忆网络的所有中间层表示组合为输出表示。然后，ELMo的表示将作为附加特征添加到下游任务的现有监督模型中，例如通过将ELMo的表示和现有模型中词元的原始表示">
<meta property="og:type" content="article">
<meta property="og:title" content="Bert">
<meta property="og:url" content="https://soundmemories.github.io/2021/07/20/NLP/05.Bert/index.html">
<meta property="og:site_name" content="SoundMemories">
<meta property="og:description" content="介绍 word2vec和GloVe都将相同的预训练向量分配给同一个词，而不考虑词的上下文。 由此出现考虑上下文的ELMo，通过将整个序列作为输入，ELMo是为输入序列中的每个单词分配一个表示的函数。具体来说，ELMo将来自预训练的双向长短期记忆网络的所有中间层表示组合为输出表示。然后，ELMo的表示将作为附加特征添加到下游任务的现有监督模型中，例如通过将ELMo的表示和现有模型中词元的原始表示">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://soundmemories.github.io/images/Bert/1.png">
<meta property="og:image" content="https://soundmemories.github.io/images/Bert/10.png">
<meta property="og:image" content="https://soundmemories.github.io/images/Bert/11.png">
<meta property="og:image" content="https://soundmemories.github.io/images/Bert/17.png">
<meta property="og:image" content="https://soundmemories.github.io/images/Bert/14.png">
<meta property="og:image" content="https://soundmemories.github.io/images/Bert/12.png">
<meta property="og:image" content="https://soundmemories.github.io/images/Bert/13.png">
<meta property="og:image" content="https://soundmemories.github.io/images/Bert/17.png">
<meta property="og:image" content="https://soundmemories.github.io/images/Bert/15.png">
<meta property="og:image" content="https://soundmemories.github.io/images/Bert/16.png">
<meta property="og:image" content="https://soundmemories.github.io/images/Bert/2.png">
<meta property="og:image" content="https://soundmemories.github.io/images/Bert/3.png">
<meta property="og:image" content="https://soundmemories.github.io/images/Bert/4.png">
<meta property="og:image" content="https://soundmemories.github.io/images/Bert/5.png">
<meta property="og:image" content="https://soundmemories.github.io/images/Bert/6.png">
<meta property="og:image" content="https://soundmemories.github.io/images/Bert/8.png">
<meta property="og:image" content="https://soundmemories.github.io/images/Bert/7.png">
<meta property="og:image" content="https://soundmemories.github.io/images/Bert/9.png">
<meta property="og:image" content="https://soundmemories.github.io/images/Bert/macbert1.jpg">
<meta property="og:image" content="https://soundmemories.github.io/images/Bert/ELECTRA1.jpg">
<meta property="article:published_time" content="2021-07-19T16:00:00.000Z">
<meta property="article:modified_time" content="2023-07-23T08:08:47.416Z">
<meta property="article:author" content="SoundMemories">
<meta property="article:tag" content="NLP">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://soundmemories.github.io/images/Bert/1.png">


<link rel="canonical" href="https://soundmemories.github.io/2021/07/20/NLP/05.Bert/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-CN","comments":"","permalink":"https://soundmemories.github.io/2021/07/20/NLP/05.Bert/","path":"2021/07/20/NLP/05.Bert/","title":"Bert"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>Bert | SoundMemories</title>
  








  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">SoundMemories</p>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="搜索" role="button">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签<span class="badge">9</span></a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类<span class="badge">9</span></a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档<span class="badge">126</span></a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup"><div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" maxlength="80"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close" role="button">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="search-result-container no-result">
  <div class="search-result-icon">
    <i class="fa fa-spinner fa-pulse fa-5x"></i>
  </div>
</div>

    </div>
  </div>

</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#%E4%BB%8B%E7%BB%8D"><span class="nav-number">1.</span> <span class="nav-text">介绍</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E9%A2%84%E8%AE%AD%E7%BB%83"><span class="nav-number">2.</span> <span class="nav-text">预训练</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#mlm"><span class="nav-number">2.1.</span> <span class="nav-text">MLM</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#nsp"><span class="nav-number">2.2.</span> <span class="nav-text">NSP</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E7%BB%93%E6%9E%84"><span class="nav-number">3.</span> <span class="nav-text">结构</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%BA%94%E7%94%A8"><span class="nav-number">4.</span> <span class="nav-text">应用</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%A6%82%E4%BD%95fine-tune"><span class="nav-number">5.</span> <span class="nav-text">如何fine-tune？</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#bert%E5%8F%98%E7%A7%8D"><span class="nav-number">6.</span> <span class="nav-text">Bert变种</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#bert-wwm"><span class="nav-number">6.1.</span> <span class="nav-text">BERT-WWM</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#ernie"><span class="nav-number">6.2.</span> <span class="nav-text">ERNIE</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#spanbert"><span class="nav-number">6.3.</span> <span class="nav-text">SpanBert</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#albert"><span class="nav-number">6.4.</span> <span class="nav-text">Albert</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#roberta"><span class="nav-number">6.5.</span> <span class="nav-text">RoBerta</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#macbert"><span class="nav-number">6.6.</span> <span class="nav-text">Macbert</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#electra"><span class="nav-number">6.7.</span> <span class="nav-text">ELECTRA</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#sbert"><span class="nav-number">6.8.</span> <span class="nav-text">SBERT</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#xlnet"><span class="nav-number">6.9.</span> <span class="nav-text">XLNet</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#gpt"><span class="nav-number">6.10.</span> <span class="nav-text">GPT</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#mass"><span class="nav-number">6.11.</span> <span class="nav-text">MASS</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#unilm"><span class="nav-number">6.12.</span> <span class="nav-text">UniLM</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%8F%82%E8%80%83%E6%96%87%E7%8C%AE"><span class="nav-number">7.</span> <span class="nav-text">参考文献</span></a></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="SoundMemories"
      src="/images/avstar.png">
  <p class="site-author-name" itemprop="name">SoundMemories</p>
  <div class="site-description" itemprop="description">今日事，今日毕</div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">126</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">9</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">9</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author animated">
      <span class="links-of-author-item">
        <span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL3NvdW5kbWVtb3JpZXM=" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;soundmemories"><i class="fab fa-github fa-fw"></i>GitHub</span>
      </span>
      <span class="links-of-author-item">
        <span class="exturl" data-url="bWFpbHRvOnNvdW5kbWVtb3JpZXNAMTYzLmNvbQ==" title="E-Mail → mailto:soundmemories@163.com"><i class="fa fa-envelope fa-fw"></i>E-Mail</span>
      </span>
  </div>
  <div class="cc-license animated" itemprop="license">
    <span class="exturl cc-opacity" data-url="aHR0cHM6Ly9jcmVhdGl2ZWNvbW1vbnMub3JnL2xpY2Vuc2VzL2J5LW5jLXNhLzQuMC8="><img src="https://cdnjs.cloudflare.com/ajax/libs/creativecommons-vocabulary/2020.11.3/assets/license_badges/small/by_nc_sa.svg" alt="Creative Commons"></span>
  </div>

        </div>
      </div>
    </div>

    
        <div class="pjax">
        </div>
  </aside>


    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://soundmemories.github.io/2021/07/20/NLP/05.Bert/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avstar.png">
      <meta itemprop="name" content="SoundMemories">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="SoundMemories">
      <meta itemprop="description" content="今日事，今日毕">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="Bert | SoundMemories">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Bert
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2021-07-20 00:00:00" itemprop="dateCreated datePublished" datetime="2021-07-20T00:00:00+08:00">2021-07-20</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/NLP/" itemprop="url" rel="index"><span itemprop="name">NLP</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>9.6k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>35 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody"><h1 id="介绍">介绍</h1>
<p>word2vec和GloVe都将相同的预训练向量分配给同一个词，而不考虑词的上下文。</p>
<p>由此出现考虑上下文的ELMo，通过将整个序列作为输入，ELMo是为输入序列中的每个单词分配一个表示的函数。具体来说，ELMo将来自预训练的双向长短期记忆网络的所有中间层表示组合为输出表示。然后，ELMo的表示将作为附加特征添加到下游任务的现有监督模型中，例如通过将ELMo的表示和现有模型中词元的原始表示（例如GloVe）连结起来。一方面，在加入ELMo表示后，冻结了预训练的双向LSTM模型中的所有权值。另一方面，现有的监督模型是专门为给定的任务定制的。利用当时不同任务的不同最佳模型，添加ELMo改进了六种自然语言处理任务的技术水平：情感分析、自然语言推理、语义角色标注、共指消解、命名实体识别和问答。</p>
<p>尽管ELMo显著改进了各种自然语言处理任务的解决方案，但每个解决方案仍然依赖于一个特定任务的结构。然而，为每一个自然语言处理任务设计一个特定的结构实际上并不是一件容易的事。GPT（Generative
Pre
Training）模型为上下文的敏感表示设计了通用的任务无关模型。GPT建立在Transformer解码器的基础上，预训练了一个用于表示文本序列的语言模型。当将GPT应用于下游任务时，语言模型的输出将被送到一个附加的线性输出层，以预测任务的标签。与ELMo冻结预训练模型的参数不同，GPT在下游任务的监督学习过程中对预训练Transformer解码器中的所有参数进行微调。GPT在自然语言推理、问答、句子相似性和分类等12项任务上进行了评估，并在对模型结构进行最小更改的情况下改善了其中9项任务的最新水平。</p>
<p>然而，由于语言模型的自回归特性，GPT只能向前看（从左到右）。在“i went
to the bank to deposit cash”（我去银行存现金）和“i went to the bank to
sit
down”（我去河岸边坐下）的上下文中，由于“bank”对其左边的上下文敏感，GPT将返回“bank”的相同表示，尽管它有不同的含义。</p>
<p>BERT：把两个最好的结合起来。ELMo对上下文进行双向编码，但使用特定于任务的结构；而GPT是任务无关的，但是从左到右编码上下文。BERT（来自Transformers的双向编码器表示）结合了这两个方面的优点。它对上下文进行双向编码，并且对于大多数的自然语言处理任务，只需要最少的结构改变。通过使用预训练的Transformer编码器，BERT能够基于其双向上下文表示任何词元。在下游任务的监督学习过程中，BERT在两个方面与GPT相似。首先，BERT表示将被输入到一个添加的输出层中，根据任务的性质对模型结构进行最小的更改，例如预测每个词元与预测整个序列。其次，对预训练Transformer编码器的所有参数进行微调，而额外的输出层将从头开始训练。</p>
<h1 id="预训练">预训练</h1>
<p>1、将预训练模型用于下游任务有两种策略：</p>
<ul>
<li>基于微调的策略。如
GPT，通过简单微调预训练模型的参数来训练下游任务。该策略在预训练期间通过单向语言模型来学习通用语言representation，而单向语言模型严重限制了预训练模型的表达能力。例如，在token
级别的任务（如：词性标注任务），结合两个方向的上下文对模型性能非常重要。</li>
<li>基于特征的策略。如 ELMo ，将预训练模型的representation
作为下游任务模型的额外特征。该策略虽然是双向语言模型，但是该模型是浅层的。</li>
</ul>
<p>与它们不同，BERT是一个<strong>同时利用了左右双向上下文的、深度的</strong>预训练模型，它在11项
nlp 任务中取得最领先的结果。</p>
<p>2、基于特征的方法具有一定优势：</p>
<ul>
<li>并不是所有的 NLP 任务都可以很容易地用 Transformer encoder
架构来表示，因此需要添加特定于任务的模型架构。</li>
<li>如果通过训练数据预先计算一次昂贵的数据表示，然后在该表示的基础上用廉价的模型来完成许多任务，这会带来很大的计算优势。</li>
</ul>
<p>3、单向语言模型可以是从左到右(LTR, Left to Right) 或者从右到左(RTL,
Right to Left)。BERT 也可以像 ELMO 一样训练独立的 LTR 和 RTL
模型后拼接在一起，但是这么做有两个问题：</p>
<ul>
<li>其训练代价是单个双向模型的两倍。</li>
<li>对于Question - Answer 之类的问题是反直觉的，因为 RTL
模型需要根据答案来反推问题。</li>
<li>BERT 可以自由的组合左侧上下文和右侧上下文。</li>
</ul>
<p>4、BERT
预训练模型包含两个预训练任务：预测被屏蔽的单词(MLM)、预测下一个句子是否和上一个句子相接(NSP)。</p>
<p>5、BERT 的预训练语料库必须使用
document-level的语料库，而不是经过混洗的 sentence-level
的语料库。因为混洗句子会破坏句子预测预训练任务。这里的 “句子”
不一定是真实的句子，而是一段话或者几段话，代表了一个 token 序列。</p>
<ul>
<li>BERT 预训练时，每个 ”句子“ 的 token 长度小于等于 512 。</li>
<li>BERT 的训练语料库经过了 WordPiece 词干化。如：engineer-&gt;engine
er</li>
</ul>
<p>6、BERT 预训练采用 gelu 激活函数，训练一百万步，bath size = 256
。</p>
<h2 id="mlm">MLM</h2>
<p>1、受完形填空任务启发，BERT
通过提出一个新的预训练目标来解决前面提到的单向限制：掩码语言模型<strong>Masked
language model (MLM)</strong>。</p>
<p>2、从直觉上，深度双向模型要比深度单向模型、单层双向模型表达能力更强。<br />
（1）标准的条件语言模型只能从左到右或者从右到左训练，因为双向条件作用允许每个单词在多层上下文中间接“看到自己”，与单向语言模型不同，MLM
结合了左右两侧上下文。<br />
（2）MLM
模型从输入中随机屏蔽一些token，目标是基于上下文预测被屏蔽单词。方法是：将被屏蔽的单词替换为
[MASK] 标记，然后被屏蔽的单词作为真实 label 。</p>
<p>3、为了训练 MLM，模型随机屏蔽15%的token，然后仅预测那些被屏蔽的 token
。这种方式有两个缺陷：<br />
（1）预训练和微调之间的不匹配。因为在微调阶段，模型永远不会看到 [MASK]
标记。为了缓解这种状况，MLM 在预训练时并不总是用真的 [MASK]
标记，而是从输入种<strong>随机选择 15% 的 token（80% 替换为 [MASK]
标记，10% 替换为一个随机单词，10% 保持原样）</strong>。<br />
（2）MLM
并不知道哪些词被替换，因此它总是努力的学习每个单词的正确表达。每个 batch
预测的 token 只有
15%，这意味着模型需要更多的训练步才可能收敛。实验证明MLM
的收敛速度确实比单向语言模型稍慢，但是相对于 MLM
的泛化能力的提升，这种代价是值得的。</p>
<h2 id="nsp">NSP</h2>
<p>许多重要的下游任务，例如知识问答和自然语言推理，都是基于理解两个句子之间的关系，而这种关系不是由语言模型直接捕获的，为了训练理解句子关系的模型，BERT
训练一个二元化的句子预测任务，称作<strong>Next Sentence Prediction
(NSP)</strong>：<br />
（1）每个训练样本由一对句子 A 和 B 组成：50% 的样本中 B 是紧跟在 A
之后的句子，50%的样本中 B 是随机在训练样本中挑选的。<br />
（2）模型需要预测的是： B 是否是 A 的下一个句子？</p>
<p>重要理解点：<br />
（1）正负样本对的构造，类似word2vec的负采样，但是句子级的负采样，如上（1）。<br />
（2）nsp任务的潜在弊端，由于在选择负样本时是随机选择训练样本的句子，导致AB的负样本比如AC或AD的Topic与AB并不一致，模型可能学习的并不是句间上下关系的判断，而是句子是否为同一个Topic的判断，这也是后续有些Bert模型把NSP任务去掉效果反而更好的原因。</p>
<h1 id="结构">结构</h1>
<p>1、参考Transformer的encoder部分，Bert是一个多层双向Transformer编码器。双向
self-attention 的 Transformer 也称作 Transformer encoder，而单向
self-attention 的 Transformer 被称作 Transformer decoder 。</p>
<p>2、作者指明L表示层数，H表示每个隐藏单元的维数大小，A表示self-attention头数。BERT有2种大小的模型，分别是BERT_base(L=12,
H=768, A=12, Total Parameters=110M)、BERT_large(L=24, H=1024, A=16,
Total Parameters=340M)。BERT_base设定为和OpenAI
GPT的模型大小相同，以便作比较。需要重点说明的是，BERT
Transformer使用双向self-attention，而GPT Transformer
使用带约束的self-attention，每个token只能注意到它左边的上下文。</p>
<p>3、BERT 的模型输入能够表达单个句子或者一对句子。</p>
<ul>
<li>每个 token 的输入 representation 由三个部分相加而成：token
embedding、segment embedding、position embedding 。</li>
<li>每个序列的第一个 token
是一个特殊的[CLS]。网络最后一层对应于该位置的一个隐向量作为整个序列的
representation 来用于分类任务。对于非分类任务，该隐向量被忽略。</li>
<li>如果是一对句子，则在句子之间插入特殊的 token：[SEP]
。然后对句子的前后关系学习一个segment
embedding，通过这种方式，模型得以学习和区分两个句子的前后关系：
<ul>
<li>前一个句子的每个 token 学习和使用 A embedding，代表前后关系的
“前关系” 的表达。</li>
<li>后一个句子的每个 token 学习和使用 B embedding，代表前后关系的
“后关系” 的表达。</li>
</ul></li>
<li>对于单个句子，模型仅学习和使用 A embedding 。</li>
<li>position embedding 是模型学习和使用的 input
每个绝对位置的表达。这里不是正弦+余弦方式，而是参数化方式。</li>
<li>token embedding 是模型学习和使用的每个 token 的表达。</li>
</ul>
<p><img src="/images/Bert/1.png" width="90%"></p>
<h1 id="应用">应用</h1>
<p>从input(2种)和output(4种)看：</p>
<ul>
<li>input：
<ul>
<li>one sentence：输入为1个句子。</li>
<li>multiple sentences：输入为多个句子，用[SEP]分隔。</li>
</ul></li>
<li>output：
<ul>
<li>one
class：[CLS]预训练后的向量输入全连接层，直接判断分类结果（当然，你也可以输入sentence全部向量）。</li>
<li>class for each
token：输入的每个token经过预训练后的向量输入全连接层，对每个token分类。</li>
<li>copy from
input：经典Q-A问题，这里设计时使用限制的QA(Answer必须在document中)，输入2个向量（代表Answer在document的起始-结束位置），和预训练（输入Question+document）后的document
token向量点积+softmax，取最大值token位置即可。</li>
<li>general
sequence：作为seq2seq的encoder、作为encoder+decoder（decoder应使用单向Transformer）。</li>
</ul></li>
</ul>
<h1 id="如何fine-tune">如何fine-tune？</h1>
<ol type="1">
<li>固定Bert参数，只训练全连接层。</li>
<li>Bert+全连接层 全部一起训练。</li>
<li>Adaptor：对Bert中添加一部分Layer，训练时只调整这些Layer参数。</li>
</ol>
<p><strong>warmup</strong>：学习率热身。<strong>规定前多少个热身步骤内，对学习率采取逐步递增的过程</strong>。热身步骤之后，会对学习率采用衰减策略。这样训练初期可以避免震荡，后期可以让loss降得更小。<strong>warmup抑制Layer
Normalization对学习率参数的敏感度。</strong></p>
<p>除了 batch size、学习率、训练 epoch
不同之外，<strong>其它训练参数与预训练阶段的训练参数相同</strong>。fine-tune阶段通常很快，因此建议<strong>对超参数进行彻底搜索</strong>并选择在验证集上表现最好的模型。<br />
论文发现：数据集越小，模型性能对超参数的选择越敏感。大数据集（超过10万的标记样本）对超参数的敏感性要低的多。</p>
<p>对于文本分类，详情参考<span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzE5MDUuMDU1ODMucGRm">How to Fine-Tune BERT for
Text Classification?<i class="fa fa-external-link-alt"></i></span>。<br />
BERT在NLP任务中效果十分优秀，这篇文章对于BERT在文本分类的应用上做了非常丰富的实验，介绍了一些调参以及改进的经验，进一步挖掘BERT的潜力。</p>
<ul>
<li>微调（fine-tune）策略
<ul>
<li>对于长文本，尝试了（1）取头部510 tokens（2）尾部510
tokens（3）头部128 tokens + 尾部382
tokens（4）分片并进行最大池化、平均池化、attention。发现方法（3）最好。因为文章的关键信息一般在开头和结尾。</li>
<li>分层训练，上层对fine-tune更加重要，这部分可随机初始化。</li>
<li>灾难性遗忘：在下游finetune可能会遗忘预训练的知识。需要设置较小的学习率，如2e-5.</li>
<li>分层衰减学习率（Layer-wise Decreasing Layer
Rate），对下层设置更小的学习率可以得到更高的准确率，在lr=2e-5，衰减率<span
class="math inline">\(\xi\)</span>=0.95</li>
</ul></li>
<li>继续预训练（Further Pretraining）
<ul>
<li>任务内（within-task）
和同领域（in-domain）的继续预训练可以大大提高准确率，In-domain比within-task要好。</li>
</ul></li>
<li>多任务微调（Multi-task Finetuning）
<ul>
<li>在单任务微调之前的多任务微调有帮助，但是提升效果小于Further
pretraining。</li>
</ul></li>
<li>小数据集
<ul>
<li>BERT对小数据集提升很大，这个大家都知道的。Further
pretraining对小数据集也有帮助。</li>
</ul></li>
</ul>
<p><span class="exturl" data-url="aHR0cHM6Ly96aHVhbmxhbi56aGlodS5jb20vcC81MjQwMzYwODc=">Bert在fine-tune时训练的5种技巧<i class="fa fa-external-link-alt"></i></span><br />
https://github.com/shuxinyin/Chinese-Text-Classification<br />
<span class="exturl" data-url="aHR0cHM6Ly96aHVhbmxhbi56aGlodS5jb20vcC84Njk2NTU5NQ==">深入理解NLP
Subword算法：BPE、WordPiece、ULM<i class="fa fa-external-link-alt"></i></span></p>
<h1 id="bert变种">Bert变种</h1>
<h2 id="bert-wwm">BERT-WWM</h2>
<p><strong>BERT-WWM</strong>：始版本的 BERT 采用了WordPiece tokenize
来预处理，即把每个单词拆解一些 WordPiece token（比如，loved-&gt;lov ed）
，最初是为了解决谷歌语音识别系统遇到的日语/韩语分割问题。该模型是数据驱动，并且确保任何可能的字符序列的确定性分割。这种预处理为
BERT 带来一个严重的问题：<strong>有可能仅仅对一个单词的某个部分
wordpiece token 执行了 mask。此时 MLM
模型预测的只是单词的一部分，相当于模型对一个错误的目标进行预测。这非常不利于模型的学习</strong>。有鉴于此，谷歌后续发布了
BERT 的 Whole Word Masking:WWM 版本
<strong>BERT-WWM</strong>。在该版本中，一个单词要么没有被
mask、要么该单词所有的 workpiece token 都被 mask
。类似的想法还有ERNIE模型的phrase-level mask和entity-level mask。</p>
<p><strong>BERT-wwm-ext</strong>：是 BERT-WWM
的中文版本(哈工大讯飞联合实验室发布)，该模型对中文的<strong>整个单词</strong>而不是单个字进行mask
，在最新的中文维基百科上训练。</p>
<h2 id="ernie">ERNIE</h2>
<p><strong>ERNIE</strong>：BERT 的 MLM 任务在执行 mask
的时候，未能考虑先验知识，如果有学习到与Mask的单词相关的先验知识，则无需借助很长的上下文就可以很容易的预测出Mask的单词。但是
ERNIE 并没有直接添加先验知识，而是通过引入 <strong>entity-level
mask</strong> 和 <strong>phrase-level mask</strong>
来引入先验知识，隐式的学习实体间的关系、实体的属性等知识：</p>
<ul>
<li><strong>phrase-level mask</strong>：将一个 phrase
作为一个单元，每个单元通常由几个 token
组成。在训练期间，同一个单元中的所有 token 都会被屏蔽，而不是只有一个
token 被屏蔽。</li>
<li><strong>entity-level mask</strong>：将一个 entity
作为一个单元，...。</li>
</ul>
<p>对于英语，论文使用单词分析和分块工具来获取短语的边界；对于中文，论文使用<strong>分词工具</strong>来获取词的边界。<br />
命名实体 entity
包括人名、地名、组织名、产品名等。需要使用NER的方法。</p>
<p><strong>ERNIE</strong> 通过三阶段学习来学得短语或实体的先验知识：</p>
<ul>
<li>第一阶段：Basic-level masking，使用基本的掩码策略，做法与 BERT
完全相同。这个阶段是在基本语言单元的随机 mask
上训练，因此很难对高级语义知识建模。对于英文，基本语言单元是词word；对于中文，基本语言单元是汉字
char 。</li>
<li>第二阶段：Phrase-level
masking，使用基本语言单元作为训练输入，但是使用 phrase-level
的掩码策略。这个阶段模型屏蔽和预测同一个短语的所有基本语言单元。</li>
<li>第三阶段：Entity-level
masking，使用基本语言单元作为训练输入，但是使用 entity-level
的掩码策略。这个阶段模型屏蔽和预测同一个命名实体的所有基本语言单元
。</li>
</ul>
<p>ERNIE 编码器和 BERT 相同，但是对于中文语料，ERNIE 把 CJK
编码范围内的字符都添加空格来分隔，然后使用 WordPiece（loved-&gt;lov ed）
来对中文语句词干化，WordPiece参考<span class="exturl" data-url="aHR0cHM6Ly93d3cuY25ibG9ncy5jb20vaHVhbmd5Yy9wLzEwMjIzMDc1Lmh0bWw=">一文读懂BERT中的WordPiece<i class="fa fa-external-link-alt"></i></span>。</p>
<h2 id="spanbert">SpanBert</h2>
<p><strong>SpanBert</strong>：一个新的分词级别的预训练方法，提出基于Bert掩码方式，采用不同长度token不同概率的方式（越短的token概率越大）Mask。
其在现有任务中的表现优于 BERT
，并在问答、指代消解等分词选择任务中取得了较大的进展。<br />
论文地址：<span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzE5MDcuMTA1MjkucGRm">SpanBERT:
Improving Pre-training by Representing and Predicting Spans<i class="fa fa-external-link-alt"></i></span></p>
<p>主要优化点：</p>
<ul>
<li>Span Masking。</li>
<li>MLM + SBO。</li>
<li>Single-Sequence Training。</li>
</ul>
<p>模型原理如图所示：<br />
<img src="/images/Bert/10.png" width="90%"></p>
<p><strong>Span Masking</strong><br />
提出了更好的 Span Mask 方案，SpanBERT 不再对随机的单个 token 添加
mask，而是对随机对邻接分词添加mask。根据几何分布，先随机选择一段（span）的长度，之后再根据均匀分布随机选择这一段的起始位置，最后按照长度mask。作者设定几何分布取
p=0.2，并裁剪最大长度只能是 10（不应当是长度 10
以上修剪，而应当为丢弃），利用此方案获得平均采样长度分布。因此分词的平均长度为
3.8
。作者还测量了词语（word）中的分词程度，使得添加mask的分词更长。如图，展示了分词mask长度的分布情况。<br />
<img src="/images/Bert/11.png" width="40%"><br />
和在 BERT 中一样，作者将 Y 的规模设定为 X 的15%，其中 80% 使用 [MASK]
进行替换，10%
使用随机单词替换，10%保持不变。与之不同的是，作者是在分词级别进行的这一替换，而非将每个单词单独替换。</p>
<p>不同mask方案对比：<br />
<img src="/images/Bert/17.png" width="80%"></p>
<p><strong>SBO</strong><br />
Span Boundary
Objective。分词选择模型一般使用其边界词创建一个固定长度的分词表示。为了于该模型相适应，作者希望结尾分词的表示的总和与中间分词的内容尽量相同。为此，作者引入了
SBO ，其仅使用观测到的边界词来预测带msak的分词的内容。</p>
<p>具体做法是，在训练时取 Span 前后边界的两个词，值得指出，这两个词不在
Span 内，然后用这<strong>两个词向量</strong>加上 Span
中被mask掉词的<strong>位置向量</strong>，来预测当前被mask的词。详细做法是将<strong>词向量</strong>和<strong>位置向量</strong>拼接起来，作者使用一个两层的前馈神经网络作为表示函数，该网络使用
GeLu 激活函数，并使用层正则化。和 MLM 一样使用交叉熵作为损失函数，就是
SBO 目标的损失，之后将这个损失和 BERT 的 MLM
的损失加起来，一起用于训练模型。<br />
<img src="/images/Bert/14.png" width="20%"><br />
<img src="/images/Bert/12.png" width="30%"><br />
<img src="/images/Bert/13.png" width="30%"></p>
<p><strong>Single-Sequence Training</strong><br />
没有segment embedding，只有一个长的句子，类似RoBERTa。它没用 Next
Sentence Prediction (NSP) 任务，而是直接用 Single-Sequence
Training，也就是根本不加入 NSP
任务来判断是否两句是上下句，直接用一句来训练。作者推测其可能原因如下：（a）更长的语境对模型更有利，模型可以获得更长上下文（类似
XLNet 的一部分效果；（b）加入另一个文本的语境信息会给MLM
语言模型带来噪音。</p>
<p>因此，SpanBERT 就没采用 NSP
任务，仅采样一个单独的邻接片段，该片段长度最多为512个单词，其长度与 BERT
使用的两片段的最大长度总和相同，然后 MLM 加上 SBO 任务来进行预训练。</p>
<p>其中主要训练细节是：</p>
<ul>
<li>训练时用了 Dynamic Masking 而不是像 BERT 在预处理时做 Mask；</li>
<li>取消 BERT 中随机采样短句的策略；</li>
<li>对 Adam 优化器中一些参数改变。</li>
</ul>
<p><img src="/images/Bert/17.png" width="80%"></p>
<p>实验结果：<br />
<img src="/images/Bert/15.png" width="40%"><br />
<img src="/images/Bert/16.png" width="80%"></p>
<h2 id="albert">Albert</h2>
<p>Albert主要对Bert参数进行优化，在任务结果下降很低的情况的下，使模型更小更适合部署。<br />
论文地址：<span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzE5MDkuMTE5NDIucGRm">ALBERT: A LITE
BERT FOR SELF-SUPERVISED LEARNING OF LANGUAGE REPRESENTATIONS<i class="fa fa-external-link-alt"></i></span></p>
<p>主要优化点：</p>
<ul>
<li>Token Embedding projection block
<ul>
<li>Parameter size <span class="math inline">\(O(V \times
E)\)</span></li>
</ul></li>
<li>Attention feed-forward block
<ul>
<li>Parameter size <span class="math inline">\(O(12 \times L \times H
\times H)\)</span></li>
</ul></li>
</ul>
<p><strong>优化一：Embedding维度分解</strong><br />
Token Embedding 采用 <span class="math inline">\(O(V\times E + E \times
H)\)</span> 且 <span class="math inline">\(E \ll
H\)</span>。由于模型结构的限制，WordePiece embedding的大小 <span
class="math inline">\(E\)</span> 总是与隐层大小 <span
class="math inline">\(H\)</span>
相同。从建模的角度考虑，词嵌入学习的是单词与上下文无关的表示，而隐层则是学习与上下文相关的表示。显然后者更加复杂，需要更多的参数，也就是说模型应当增大隐层大小
<span class="math inline">\(H\)</span> ，或者说满足 <span
class="math inline">\(H \gg E\)</span> 。但实际上词汇表的大小 <span
class="math inline">\(V\)</span> 通常非常大，如果 <span
class="math inline">\(E=H\)</span> 的话，增加隐层大小 <span
class="math inline">\(H\)</span> 后将会使embedding matrix的维度 <span
class="math inline">\(V \times E\)</span> 非常巨大。</p>
<p>因此Albert想要打破 <span class="math inline">\(E\)</span> 与 <span
class="math inline">\(H\)</span>
之间的绑定关系，从而减小模型的参数量，同时提升模型表现。具体做法是将embedding
matrix分解为两个大小分别为 <span class="math inline">\(V \times
E\)</span> 和 <span class="math inline">\(E \times H\)</span>
矩阵，也就是说先将单词投影到一个低维的embedding空间 <span
class="math inline">\(E\)</span> ，再将其投影到高维的隐藏空间 <span
class="math inline">\(H\)</span> 。这使得embedding matrix的维度从 <span
class="math inline">\(O(V \times E)\)</span> 减小到 <span
class="math inline">\(O(V\times E + E \times H)\)</span> 。当 <span
class="math inline">\(E \ll H\)</span>
时，参数量减少非常明显。在实现时，随机初始化 <span
class="math inline">\(V \times E\)</span> 和 <span
class="math inline">\(E \times H\)</span>
两个矩阵，计算某个单词的表示需用一个单词的one-hot向量乘以 <span
class="math inline">\(V \times E\)</span>
维的矩阵（也就是lookup），再用得到的结果乘 <span class="math inline">\(E
\times H\)</span> 维的矩阵即可。两个矩阵的参数通过模型学习。</p>
<p>从下图实验结果可见，对于不共享参数的情况， <span
class="math inline">\(E\)</span> 几乎是越大越好；而共享参数之后， <span
class="math inline">\(E\)</span> 太大反而会使模型表现变差， <span
class="math inline">\(E=128\)</span>
模型表现最好，因此ALBERT的默认参数设置为此。<br />
<img src="/images/Bert/2.png" width="90%"></p>
<p><strong>优化二：层权重共享</strong><br />
另一个减少参数量的方法就是层之间的参数共享，即多个层使用相同的参数。因为attention-feedforward操作是重复的，都是自注意力机制，所以考虑使用相同的权重来减少参数量，使得block参数变为<span
class="math inline">\(O(12 \times H \times H)\)</span>。</p>
<p>参数共享有三种方式：</p>
<ul>
<li>只共享feed-forward network的参数。</li>
<li>只共享attention的参数。</li>
<li>共享全部参数。</li>
</ul>
<p>ALBERT默认是共享全部参数的，在后续实验结果中我们可以看到几种方式的模型表现。</p>
<p>如下图所示，实验表明加入参数共享之后，每一层的输入embedding和输出embedding的L2距离和余弦相似度都比BERT稳定了很多，ALBERT
的结果更加平滑。这证明参数共享能够使模型参数更加稳定。<br />
<img src="/images/Bert/3.png" width="90%"></p>
<p>如下图所示，可以看出参数共享几乎也是对模型结果有负面影响的。但是考虑到其能够大幅削减参数量，并且对结果影响不是特别大，因此权衡之下选择了参数共享。<br />
<img src="/images/Bert/4.png" width="90%"></p>
<p><strong>优化三：SOP替代NSP</strong><br />
除了减少模型参数外，本外还对BERT的预训练任务<strong>Next-sentence
prediction</strong>(NSP)进行了改进。在BERT中，NSP任务的正例是文章中连续的两个句子，而负例则是从两篇文档中各选一个句子构造而成。在先前的研究中，已经证明NSP是并不是一个合适的预训练任务。本文推测其原因是模型在判断两个句子的关系时不仅考虑了两个句子之间的连贯性（coherence），还会考虑到两个句子的话题（topic）。而两篇文档的话题通常不同，模型会更多的通过话题去分析两个句子的关系，而不是句子间的连贯性，这使得NSP任务变成了一个相对简单的任务。</p>
<p>因此本文提出了<strong>Sentence-order
prediction</strong>(SOP)来取代NSP。具体来说，其正例与NSP相同，但负例是通过选择一篇文档中的两个连续的句子并将它们的顺序交换构造的。这样两个句子就会有相同的话题，模型学习到的就更多是句子间的连贯性。</p>
<p>如下图实验结果所示，如果不使用NSP或SOP作为预训练任务的话，模型在NSP和SOP两个任务上表现都很差；如果使用NSP作为预训练任务的话，模型确实能很好的解决NSP问题，但是在SOP问题上表现却很差，几乎相当于随机猜，因此说明NSP任务确实很难学到句子间的连贯性；而如果用SOP作为预训练任务，则模型也可以较好的解决NSP问题，同时模型在下游任务上表现也更好。说明SOP确实是更好的预训练任务。<br />
<img src="/images/Bert/5.png" width="90%"></p>
<h2 id="roberta">RoBerta</h2>
<p>论⽂地址：<span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzE5MDcuMTE2OTIucGRm">RoBERTa: A
Robustly Optimized BERT Pretraining Approach<i class="fa fa-external-link-alt"></i></span></p>
<p>预训练模型能够显著的提升任务效果，但是不同预训练模型的比较非常困难。首先，每个预训练模型的训练成本很高，无法一一训练并进行比较。其次，不同预训练模型通常是在不同规模大小的数据集上训练的，难以评估效果的好坏是预训练模型引起的还是预训练数据引起的。最后，超参数的选择对预训练模型的表现影响很大。同一个预训练模型的不同超参数，其比较结果会有很大不同。</p>
<p>RoBERTa 主要工作是复现 BERT，然后对 BERT
的模型架构、训练目标、训练细节（如数据集大小、训练时间）的重要性进行探索，从而提出了改进方案，这个改进方案称为
RoBERTa 。主要修改：</p>
<ul>
<li>更大的 batch
size、更多的数据、更长的输入序列、更长的预训练时间。</li>
<li>移除 NSP 任务。</li>
<li><strong>使用动态mask(dynamic masking)</strong>。</li>
<li>使用BPE方式，减少UNK单词出现次数（英文）。</li>
<li>调整Adam优化器的参数，<span
class="math inline">\(\epsilon\)</span>由1e-6改成1e-8，<span
class="math inline">\(\beta_2\)</span>由0.999改成0.98。</li>
</ul>
<p><strong>更大的 batch size</strong><br />
原本的BERTbase 的batch size是256，训练1M个steps。RoBERTa的batch
size为8k。为什么要用更大的batch
size呢？（除了因为他们有钱玩得起外）作者借鉴了在机器翻译中，用更大的batch
size配合更大学习率能提升模型优化速率和模型性能的现象，并且也用实验证明了确实Bert还能用更大的batch
size。<br />
<img src="/images/Bert/6.png" width="40%"></p>
<p><strong>更多的数据、更长的预训练时间</strong><br />
借鉴XLNet用了比Bert多10倍的数据，RoBERTa也用了更多的数据。性能确实再次彪升。当然，也需要配合更长时间的训练。<br />
<img src="/images/Bert/8.png" width="80%"></p>
<p><strong>移除 NSP 任务 + 使用更长的输入序列</strong><br />
原本的Bert为了捕捉句子之间的关系，使用了NSP任务进行预训练，就是输入一对句子A和B，判断这两个句子是否是连续的。在训练的数据中，50%的B是A的下一个句子，50%的B是随机抽取的。</p>
<p>而RoBERTa去除了NSP，而是每次输入连续的多个句子，直到最大长度512（可以跨文章）。这种训练方式叫做（FULL
-
SENTENCES），而原来的Bert每次只输入两个句子。实验表明在MNLI这种推断句子关系的任务上RoBERTa也能有更好性能。<br />
<img src="/images/Bert/7.png" width="80%"></p>
<p><strong>使用动态mask</strong><br />
原来Bert对每一个序列随机选择15%的Tokens替换成[MASK]，为了消除与下游任务的不匹配，还对这15%的Tokens进行（1）80%的时间替换成[MASK]；（2）10%的时间不变；（3）10%的时间替换成其他词。但整个训练过程，这15%的Tokens一旦被选择就不再改变，也就是说从一开始随机选择了这15%的Tokens，之后的N个epoch里都不再改变了。这就叫做静态Masking。</p>
<p>而RoBERTa一开始把预训练的数据复制10份，每一份都随机选择15%的Tokens进行Masking，也就是说，同样的一句话有10种不同的mask方式。然后每份数据都训练N/10个epoch。这就相当于在这N个epoch的训练中，每个序列的被mask的tokens是会变化的。这就叫做动态Masking。</p>
<p>那么这样改变是否真的有效果？作者在只将静态Masking改成动态Masking，其他参数不变的情况下做了实验，动态Masking确实能提高性能。<br />
<img src="/images/Bert/9.png" width="40%"></p>
<p><strong>使用BPE方式</strong><br />
针对的是英文，BERT原型使用的是 character-level BPE vocabulary of size
30K, RoBERTa使用了GPT2的 byte BPE 实现，使用的是byte而不是unicode
characters作为subword的单位。</p>
<p>这些针对Bert的预训练优化都使用起来，最终在GLUE, RACE,
SQuAD上都达到了SOTA的性能。</p>
<p>RoBERTa 采用 160 G 训练文本，远超 BERT 的 16G 文本，其中包括：</p>
<ul>
<li>BOOKCORPUS 和英文维基百科：原始 BERT 的训练集，大小 16GB 。</li>
<li>CC-NEWS：包含2016年9月到2019年2月爬取的6300万篇英文新闻，大小 76
GB（经过过滤之后）。</li>
<li>OPENWEBTEXT：从 Reddit 上共享的 URL
（至少3个点赞）中提取的网页内容，大小 38 GB 。</li>
<li>STORIES：CommonCrawl 数据集的一个子集，包含 Winograd
模式的故事风格，大小 31GB 。</li>
</ul>
<h2 id="macbert">Macbert</h2>
<p>MLM as
correction，使用校正作为Mask的语言模型，通过<strong>相似的单词mask</strong>而不是mask标记，减轻了预训练和微调阶段两者之间的差距。<br />
论文地址：<span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzIwMDQuMTM5MjIucGRm">Revisiting
Pre-trained Models for Chinese Natural Language Processing<i class="fa fa-external-link-alt"></i></span></p>
<p><img src="/images/Bert/macbert1.jpg" width="90%"></p>
<p>使用不同的mask策略，常见的有wwm和n-gram，而Macbert使用相似词作为mask，缓解预训练和微调阶段之间任务不一致导致的差距。</p>
<p>总结一下mask类任务依赖三点：<br />
（1）选择被mask的token：随机、针对特殊词、组合形式比如bert的80%+10%+10%。<br />
（2）替换方法：中文bert是字、还有词比如wwm，词组比如n-gram等方式，相似词也可以。<br />
（3）mask时机：静态或动态mask策略，静态是在训练前就处理好的，动态是每个epoch都重新处理一次。<br />
根据下游任务设计合适的策略，进行定制化是一项重要能力，比如n-gram在文本分类更有效，相似词对阅读理解效果更好。</p>
<h2 id="electra">ELECTRA</h2>
<p>ELECTRA最主要的贡献是提出了新的预训练任务和框架，把这种Masked
language model(MLM)预训练任务改成了判别式的Replaced token
detection(RTD)任务，判断当前token是否被语⾔模型替换过。<br />
论⽂地址：<span class="exturl" data-url="aHR0cHM6Ly9vcGVucmV2aWV3Lm5ldC9wZGY/aWQ9cjF4TUgxQnR2Qg==">ELECTRA:
PRE-TRAINING TEXT ENCODERS AS DISCRIMINATORS RATHER THAN
GENERATORS<i class="fa fa-external-link-alt"></i></span></p>
<p><img src="/images/Bert/ELECTRA1.jpg" width="90%"></p>
<p>ELECTRA提出了⼀套新的预训练框架，其中包括两个部分：Generator 和
Discriminator。可看作两个Bert模型。<br />
<strong>Generator</strong>：⼀个⼩的MLM模型，在[MASK]的位置预测原来的词。Generator将⽤来把输⼊⽂本做部分词的替换，<br />
<strong>Discriminator</strong>：判断输⼊句⼦中的每个词是否被替换，即使⽤Replaced
Token Detection (RTD)预训练任务，取代了BERT原始的Masked Language Model
(MLM)。需要注意的是这⾥并没有使⽤Next Sentence Prediction
(NSP)任务，在预训练阶段结束之后，只使⽤Discriminator作为下游任务精调的基模型。</p>
<p>需要注意的是：<br />
（1）由于Generator生成的词是离散的，所以梯度到它俩之间是断掉的，Discriminator的梯度无法传递到Generator，所以Generator还是训练MLM任务，Discriminator训练的是序列标注任务（判断每个token是真是假），两者同时训练但Discriminator的梯度不会传给Generator。<br />
（2）因为Discriminator任务相对容易，所以的到的loss会比Generator小，因此在Discriminator的loss前乘系数50，最终loss是它俩相加。<br />
（3）在优化Discriminator时计算了所有token上的loss，而以往计算BERT的MLM
loss时会忽略没被mask的token。</p>
<p>这个模型在同级别的参数量下，阅读理解上任务非常优秀！</p>
<h2 id="sbert">SBERT</h2>
<p>虽然BERT和RoBERTa在很多句子对形式的回归任务（例如文本语义相似度）上达到了SOTA效果，但是它们还存在一些缺点：在这些任务中，它们均需要将比较的两个句子都传入到模型中计算，计算开销过大。BERT模型在一个1W句子集合中，找出最相近的一个句子对，需要5千万次推断计算（约65小时）才能完成，所以BERT并不适合语义相似度搜索等任务。</p>
<p>在该论文中，作者提出了一个新的模型，Sentence-BERT（简称SBERT）。SBERT采用双重或三重BERT网络结构，具体结构介绍会在后文中详细介绍。如果使用的是基于RoBERTa模型，则改造后的模型简称为SRoBERTa。</p>
<p>通过SBERT模型获取到的句子embedding，可以直接通过cos相似度计算两个句子的相似度，这样就大大减少了计算量。因为在使用BERT模型进行句子间相似度的判断时，需要从句子集合中，选出两个句子进行组合，传入BERT中进行计算，而使用SBERT模型，只需要将集合中每个句子单独传入到模型中，得到每个句子的embeding，计算相似度只需要使用cos函数计算两两embeding的cos距离即可。因此，使用BERT/RoBERTa模型需要65h才能完成的寻找最相似句子对任务，SBERT模型完成仅需5s。</p>
<p>作者在一些STS任务和迁移学习任务上评估SBERT模型，该模型达到了新的SOTA水平。</p>
<p>论文地址：<span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzE5MDguMTAwODQucGRm">Sentence-BERT: Sentence
Embeddings using Siamese BERT-Networks<i class="fa fa-external-link-alt"></i></span>。<br />
模型地址：<span class="exturl" data-url="aHR0cHM6Ly93d3cuc2JlcnQubmV0Lw==">Sentence-Transformers<i class="fa fa-external-link-alt"></i></span>。<br />
详解参考：<span class="exturl" data-url="aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2Zlbmd4aW5saW51eC9hcnRpY2xlL2RldGFpbHMvMTA5MTk1NzYy">论文阅读Sentence-BERT:
Sentence Embeddings using Siamese BERT-Networks<i class="fa fa-external-link-alt"></i></span>。</p>
<h2 id="xlnet">XLNet</h2>
<p><strong>XLNet</strong>：使用Transformer-XL模型。</p>
<p>当Bert作为seq2seq的encoder时，输入也要Mask：</p>
<ul>
<li>随机Mask。</li>
<li>删除Delete。</li>
<li>多个句子，换顺序Permuttion。</li>
<li>打乱token，比如后面词放在前面。</li>
<li>插入Mask，Text Infilling。这个效果最好。</li>
</ul>
<h2 id="gpt">GPT</h2>
<p>只使用Transformer-Decoder端进行序列生成(预测下一个词)。<br />
训练使用Few-shot
Learning：先给出问题描述和部分示例学习，再给出prompt(引子/问题)，能给出解答。<br />
训练时只给出一个示例(one-shot)或不给示例(zero-shot)进行学习。</p>
<p>GPT-2的学习目标是使用无监督的预训练模型做有监督的任务。作者认为，当一个语言模型的容量足够大时，它就足以覆盖所有的有监督任务，也就是说所有的有监督学习都是无监督语言模型的一个子集。GPT-2去掉了fine-tuning层。</p>
<p>GPT-3沿用了GPT-2的结构，但是在网络容量上做了很大的提升。</p>
<p><span class="exturl" data-url="aHR0cHM6Ly96aHVhbmxhbi56aGlodS5jb20vcC8zNTAwMTc0NDM=">词向量之GPT-1，GPT-2和GPT-3<i class="fa fa-external-link-alt"></i></span><br />
<span class="exturl" data-url="aHR0cHM6Ly96aHVhbmxhbi56aGlodS5jb20vcC8yMDA5Nzg1Mzg=">GPT-3阅读笔记：Language
Models are Few-Shot Learners<i class="fa fa-external-link-alt"></i></span></p>
<h2 id="mass">MASS</h2>
<p>MASS针对的是seq2seq任务。<br />
MASS在encoder端对句子随机mask一个长度为k的连续片段，然后通过decoder预测生被mask片段。<br />
mask方式：随机删除、打乱词序、旋转词序、插入mask等方式。</p>
<h2 id="unilm">UniLM</h2>
<p>UniLM
1.0通过设计不同掩码，支持4种不同的训练目标：从左往右单向LM，从右往左单向LM，双向LM，序列到序列LM。<br />
UniLM 2.0支持更多样的factorization
order且无需重复构建训练实体。训练时部分自回归使用pseudo mask LM
(PMLM)，直译为伪掩码，作为部分自回归训练时占位符，和自编码的<code>[M]</code>任务作为区分。</p>
<p><span class="exturl" data-url="aHR0cHM6Ly96aHVhbmxhbi56aGlodS5jb20vcC8xMjI5OTkxNTM=">微软统一预训练语言模型UniLM
2.0解读<i class="fa fa-external-link-alt"></i></span></p>
<h1 id="参考文献">参考文献</h1>
<p><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzE4MTAuMDQ4MDU=">BERT: Pre-training of Deep
Bidirectional Transformers for Language Understanding<i class="fa fa-external-link-alt"></i></span><br />
<span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzE5MDUuMDU1ODMucGRm">How to Fine-Tune BERT for
Text Classification?<i class="fa fa-external-link-alt"></i></span><br />
<span class="exturl" data-url="aHR0cHM6Ly9jbG91ZC50ZW5jZW50LmNvbS9kZXZlbG9wZXIvYXJ0aWNsZS8xNTE5MTgw">BERT论文解读<i class="fa fa-external-link-alt"></i></span><br />
<span class="exturl" data-url="aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2RlbmRpX2h1c3QvYXJ0aWNsZS9kZXRhaWxzLzEwNDQ2NTMzNw==">【调优方法】——warmup<i class="fa fa-external-link-alt"></i></span><br />
<span class="exturl" data-url="aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2xpb24xOTkzMDkyNC9hcnRpY2xlL2RldGFpbHMvMTA0NDY5OTQ0">Bert微调技巧实验大全-How
to Fine-Tune BERT for Text Classification<i class="fa fa-external-link-alt"></i></span><br />
<span class="exturl" data-url="aHR0cHM6Ly96aHVhbmxhbi56aGlodS5jb20vcC84NzU2MjkyNg==">【论文阅读】ALBERT<i class="fa fa-external-link-alt"></i></span><br />
<span class="exturl" data-url="aHR0cHM6Ly93d3cuamlhbnNodS5jb20vcC9lZGRmMDRiYTg1NDU=">改进版的RoBERTa到底改进了什么？<i class="fa fa-external-link-alt"></i></span><br />
<span class="exturl" data-url="aHR0cHM6Ly9tcC53ZWl4aW4ucXEuY29tL3MvTUhtN0F4bWN1RWdGUl9vTmJOcUZrUQ==">BERT模型的优化改进方法！<i class="fa fa-external-link-alt"></i></span></p>

    </div>

    
    
    

    <footer class="post-footer">
          

<div class="post-copyright">
<ul>
  <li class="post-copyright-author">
      <strong>本文作者： </strong>SoundMemories
  </li>
  <li class="post-copyright-link">
      <strong>本文链接：</strong>
      <a href="https://soundmemories.github.io/2021/07/20/NLP/05.Bert/" title="Bert">https://soundmemories.github.io/2021/07/20/NLP/05.Bert/</a>
  </li>
  <li class="post-copyright-license">
      <strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <span class="exturl" data-url="aHR0cHM6Ly9jcmVhdGl2ZWNvbW1vbnMub3JnL2xpY2Vuc2VzL2J5LW5jLXNhLzQuMC8="><i class="fab fa-fw fa-creative-commons"></i>BY-NC-SA</span> 许可协议。转载请注明出处！
  </li>
</ul>
</div>

          <div class="post-tags">
              <a href="/tags/NLP/" rel="tag"><i class="fa fa-tag"></i> NLP</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2021/07/19/NLP/04.Transformer/" rel="prev" title="Transformer">
                  <i class="fa fa-chevron-left"></i> Transformer
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2021/07/21/NLP/06.%E5%AF%B9%E8%AF%9D%E7%B3%BB%E7%BB%9F/" rel="next" title="对话系统简介">
                  对话系统简介 <i class="fa fa-chevron-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="copyright">
    &copy; 2019 – 
    <span itemprop="copyrightYear">2023</span>
    <span class="with-love">
      <i class="fa fa-heart"></i>
    </span>
    <span class="author" itemprop="copyrightHolder">SoundMemories</span>
  </div>
  <div class="powered-by">由 <span class="exturl" data-url="aHR0cHM6Ly9oZXhvLmlv">Hexo</span> & <span class="exturl" data-url="aHR0cHM6Ly90aGVtZS1uZXh0LmpzLm9yZy9tdXNlLw==">NexT.Muse</span> 强力驱动
  </div>

    </div>
  </footer>

  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>
  <div class="sidebar-dimmer"></div>
  <div class="back-to-top" role="button" aria-label="返回顶部">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>
  <a role="button" class="book-mark-link book-mark-link-fixed"></a>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/next-theme-pjax/0.6.0/pjax.min.js" integrity="sha256-vxLn1tSKWD4dqbMRyv940UYw4sXgMtYcK6reefzZrao=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.7.0/jquery.min.js" integrity="sha256-2Pmvv0kuTBOenSvLm6bvfBSSHrUJ+3A7x6P5Ebd07/g=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.js" integrity="sha256-yt2kYMy0w8AbtF89WXb2P1rfjcP/HTHLT7097U8Y5b8=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/medium-zoom/1.0.8/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/muse.js"></script><script src="/js/next-boot.js"></script><script src="/js/bookmark.js"></script><script src="/js/pjax.js"></script>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-generator-searchdb/1.4.1/search.js" integrity="sha256-1kfA5uHPf65M5cphT2dvymhkuyHPQp5A53EGZOnOLmc=" crossorigin="anonymous"></script>
<script src="/js/third-party/search/local-search.js"></script>

  <script class="next-config" data-name="pdf" type="application/json">{"object_url":{"url":"https://cdnjs.cloudflare.com/ajax/libs/pdfobject/2.2.12/pdfobject.min.js","integrity":"sha256-g2xji1rlE3KsGVClvuxTbcR0Kn2+wtQADSff2Tbb4zA="},"url":"/lib/pdf/web/viewer.html"}</script>
  <script src="/js/third-party/tags/pdf.js"></script>

  <script class="next-config" data-name="mermaid" type="application/json">{"enable":true,"theme":{"light":"neutral","dark":"neutral"},"js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mermaid/10.2.3/mermaid.min.js","integrity":"sha256-JFptYy4KzJ5OQP+Q9fubNf3cxpPPmZKqUOovyEONKrQ="}}</script>
  <script src="/js/third-party/tags/mermaid.js"></script>


  <script src="/js/third-party/fancybox.js"></script>



  




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"none","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>


  <script src="https://cdnjs.cloudflare.com/ajax/libs/quicklink/2.3.0/quicklink.umd.js" integrity="sha256-yvJQOINiH9fWemHn0vCA5lsHWJaHs6/ZmO+1Ft04SvM=" crossorigin="anonymous"></script>
  <script class="next-config" data-name="quicklink" type="application/json">{"enable":true,"home":true,"archive":true,"delay":true,"timeout":3000,"priority":true,"url":"https://soundmemories.github.io/2021/07/20/NLP/05.Bert/"}</script>
  <script src="/js/third-party/quicklink.js"></script>

</body>
</html>
