<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.2.0">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="https://fonts.loli.net/css?family=Monda:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">

<link rel="stylesheet" href="//cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.1/css/all.min.css">
  <link rel="stylesheet" href="//cdn.jsdelivr.net/npm/animate.css@3.1.1/animate.min.css">
  <link rel="stylesheet" href="//cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.css">

<script class="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"soundmemories.github.io","root":"/","images":"/images","scheme":"Muse","version":"8.0.2","exturl":true,"sidebar":{"position":"left","display":"always","padding":18,"offset":12},"copycode":true,"bookmark":{"enable":true,"color":"#222","save":"auto"},"fancybox":true,"mediumzoom":true,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"},"path":"/search.xml","localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false}};
  </script>

  <meta name="description" content="张量类型张量的数据类型和numpy.array基本一一对应，但是不支持str类型。123456789torch.float64&#x2F;torch.doubletorch.float32&#x2F;torch.float  # 一般神经网络建模使用的都是torch.float32类型。torch.float16torch.int64&#x2F;torch.longtorch.int32&#x2F;torch.inttorch.int1">
<meta property="og:type" content="article">
<meta property="og:title" content="Pytorch">
<meta property="og:url" content="https://soundmemories.github.io/2021/06/10/NLP/00.Pytorch/index.html">
<meta property="og:site_name" content="SoundMemories">
<meta property="og:description" content="张量类型张量的数据类型和numpy.array基本一一对应，但是不支持str类型。123456789torch.float64&#x2F;torch.doubletorch.float32&#x2F;torch.float  # 一般神经网络建模使用的都是torch.float32类型。torch.float16torch.int64&#x2F;torch.longtorch.int32&#x2F;torch.inttorch.int1">
<meta property="og:locale" content="zh_CN">
<meta property="article:published_time" content="2021-06-09T16:00:00.000Z">
<meta property="article:modified_time" content="2022-03-05T06:47:45.125Z">
<meta property="article:author" content="SoundMemories">
<meta property="article:tag" content="NLP">
<meta name="twitter:card" content="summary">


<link rel="canonical" href="https://soundmemories.github.io/2021/06/10/NLP/00.Pytorch/">


<script data-pjax class="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>Pytorch | SoundMemories</title>
  






  <noscript>
  <style>
  body { margin-top: 2rem; }

  .use-motion .menu-item,
  .use-motion .sidebar,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header {
    visibility: visible;
  }

  .use-motion .header,
  .use-motion .site-brand-container .toggle,
  .use-motion .footer { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle,
  .use-motion .custom-logo-image {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line {
    transform: scaleX(1);
  }

  .search-pop-overlay, .sidebar-nav { display: none; }
  .sidebar-panel { display: block; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <h1 class="site-title">SoundMemories</h1>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu">
        <li class="menu-item menu-item-主页">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>主页</a>

  </li>
        <li class="menu-item menu-item-分类">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a>

  </li>
        <li class="menu-item menu-item-标签">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a>

  </li>
        <li class="menu-item menu-item-归档">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" maxlength="80"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="search-result-container no-result">
  <div class="search-result-icon">
    <i class="fa fa-spinner fa-pulse fa-5x"></i>
  </div>
</div>

    </div>
  </div>

</div>
        
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>

  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>
	   
		  
      <div class="sidebar-panel-container">
        <!--noindex-->
        <section class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%BC%A0%E9%87%8F%E7%B1%BB%E5%9E%8B"><span class="nav-number">1.</span> <span class="nav-text">张量类型</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%BC%A0%E9%87%8F%E5%B0%BA%E5%AF%B8"><span class="nav-number">2.</span> <span class="nav-text">张量尺寸</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%BC%A0%E9%87%8F%E5%92%8Cnumpy%E6%95%B0%E7%BB%84"><span class="nav-number">3.</span> <span class="nav-text">张量和numpy数组</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%BC%A0%E9%87%8F%E6%93%8D%E4%BD%9C"><span class="nav-number">4.</span> <span class="nav-text">张量操作</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E8%87%AA%E5%8A%A8%E5%BE%AE%E5%88%86"><span class="nav-number">5.</span> <span class="nav-text">自动微分</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%8A%A8%E6%80%81%E8%AE%A1%E7%AE%97%E5%9B%BE"><span class="nav-number">6.</span> <span class="nav-text">动态计算图</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Dataset%E5%92%8CDataLoader"><span class="nav-number">7.</span> <span class="nav-text">Dataset和DataLoader</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#nn-functional%E5%92%8C-nn-Module"><span class="nav-number">8.</span> <span class="nav-text">nn.functional和 nn.Module</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#containers"><span class="nav-number">9.</span> <span class="nav-text">containers</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#save"><span class="nav-number">10.</span> <span class="nav-text">save</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#layers"><span class="nav-number">11.</span> <span class="nav-text">layers</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#losses"><span class="nav-number">12.</span> <span class="nav-text">losses</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#TensorBoard%E5%8F%AF%E8%A7%86%E5%8C%96"><span class="nav-number">13.</span> <span class="nav-text">TensorBoard可视化</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99"><span class="nav-number">14.</span> <span class="nav-text">参考资料</span></a></li></ol></div>
        </section>
        <!--/noindex-->

        <section class="site-overview-wrap sidebar-panel">
          <div class="site-author site-overview-item animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="SoundMemories"
      src="https://i.loli.net/2020/11/04/6JhNuwtBe4adylS.png">
  <p class="site-author-name" itemprop="name">SoundMemories</p>
  <div class="site-description" itemprop="description">今日事，今日毕</div>
</div>
<div class="site-state-wrap site-overview-item animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives">
          <span class="site-state-item-count">118</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
        <span class="site-state-item-count">8</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
        <span class="site-state-item-count">8</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author site-overview-item animated">
      <span class="links-of-author-item">
        <span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL3NvdW5kbWVtb3JpZXM=" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;soundmemories"><i class="fab fa-github fa-fw"></i>GitHub</span>
      </span>
      <span class="links-of-author-item">
        <span class="exturl" data-url="bWFpbHRvOnNvdW5kbWVtb3JpZXNAMTYzLmNvbQ==" title="E-Mail → mailto:soundmemories@163.com"><i class="fa fa-envelope fa-fw"></i>E-Mail</span>
      </span>
  </div>
  <div class="cc-license site-overview-item animated" itemprop="license">
    <span class="exturl cc-opacity" data-url="aHR0cHM6Ly9jcmVhdGl2ZWNvbW1vbnMub3JnL2xpY2Vuc2VzL2J5LW5jLXNhLzQuMC8="><img src="/images/cc-by-nc-sa.svg" alt="Creative Commons"></span>
  </div>



        </section>
      </div>
    </div>
  </aside>
  <div class="sidebar-dimmer"></div>


    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>
  <a role="button" class="book-mark-link book-mark-link-fixed"></a>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


    <div class="main-inner post posts-expand">
      

      

  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://soundmemories.github.io/2021/06/10/NLP/00.Pytorch/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="https://i.loli.net/2020/11/04/6JhNuwtBe4adylS.png">
      <meta itemprop="name" content="SoundMemories">
      <meta itemprop="description" content="今日事，今日毕">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="SoundMemories">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Pytorch
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2021-06-10 00:00:00" itemprop="dateCreated datePublished" datetime="2021-06-10T00:00:00+08:00">2021-06-10</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/NLP/" itemprop="url" rel="index"><span itemprop="name">NLP</span></a>
        </span>
    </span>

  
      </div>
      <div class="post-meta">
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>71k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>1:04</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
        <h1 id="张量类型"><a href="#张量类型" class="headerlink" title="张量类型"></a>张量类型</h1><p>张量的数据类型和numpy.array基本一一对应，但是不支持str类型。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">torch.float64/torch.double</span><br><span class="line">torch.float32/torch.<span class="built_in">float</span>  <span class="comment"># 一般神经网络建模使用的都是torch.float32类型。</span></span><br><span class="line">torch.float16</span><br><span class="line">torch.int64/torch.long</span><br><span class="line">torch.int32/torch.<span class="built_in">int</span></span><br><span class="line">torch.int16</span><br><span class="line">torch.int8</span><br><span class="line">torch.uint8</span><br><span class="line">torch.<span class="built_in">bool</span></span><br></pre></td></tr></table></figure></p>
<p>张量常用属性：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">a = torch.rand([<span class="number">2</span>,<span class="number">2</span>])</span><br><span class="line">a.dtype <span class="comment"># 元素类型</span></span><br><span class="line">a.shape/size() <span class="comment"># 形状</span></span><br><span class="line">a.device <span class="comment"># 设备</span></span><br><span class="line"><span class="comment"># 不同类型的数据可以用不同维度(dimension)的张量来表示，一般有几层中括号，就是多少维的张量。</span></span><br><span class="line">a.dim()</span><br></pre></td></tr></table></figure></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch </span><br><span class="line"></span><br><span class="line"><span class="comment"># 自动推断数据类型</span></span><br><span class="line">i = torch.tensor(<span class="number">1</span>); print(i, i.dtype) <span class="comment"># type(i)是&lt;class &#x27;torch.Tensor&#x27;&gt;</span></span><br><span class="line">x = torch.tensor(<span class="number">2.0</span>); print(x, x.dtype)</span><br><span class="line">b = torch.tensor(<span class="literal">True</span>); print(b, b.dtype)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">tensor(1) torch.int64</span></span><br><span class="line"><span class="string">tensor(2.) torch.float32</span></span><br><span class="line"><span class="string">tensor(True) torch.bool</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 指定数据类型</span></span><br><span class="line">i = torch.tensor(<span class="number">1</span>, dtype=torch.int32); print(i, i.dtype)</span><br><span class="line">x = torch.tensor(<span class="number">2.0</span>, dtype=torch.double); print(x, x.dtype)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">tensor(1, dtype=torch.int32) torch.int32</span></span><br><span class="line"><span class="string">tensor(2., dtype=torch.float64) torch.float64</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用特定类型构造函数</span></span><br><span class="line">i = torch.IntTensor(<span class="number">1</span>); print(i,i.dtype) <span class="comment"># 这里1是容量，不是数据</span></span><br><span class="line">x = torch.Tensor(np.array(<span class="number">2.0</span>)); print(x,x.dtype) <span class="comment"># 等价于torch.FloatTensor</span></span><br><span class="line">b = torch.BoolTensor(np.array([<span class="number">1</span>,<span class="number">0</span>,<span class="number">2</span>,<span class="number">0</span>])); print(b,b.dtype)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">tensor([5], dtype=torch.int32) torch.int32</span></span><br><span class="line"><span class="string">tensor(2.) torch.float32</span></span><br><span class="line"><span class="string">tensor([ True, False,  True, False]) torch.bool</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 不同类型进行转换</span></span><br><span class="line">i = torch.tensor(<span class="number">1</span>); print(i,i.dtype)</span><br><span class="line">x = i.<span class="built_in">float</span>(); print(x,x.dtype) <span class="comment"># 调用 float方法转换成浮点类型</span></span><br><span class="line">y = i.<span class="built_in">type</span>(torch.<span class="built_in">float</span>); print(y,y.dtype) <span class="comment"># 使用type函数转换成浮点类型</span></span><br><span class="line">z = i.type_as(x);print(z,z.dtype) <span class="comment"># 使用type_as方法转换成某个Tensor相同类型</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">tensor(1) torch.int64</span></span><br><span class="line"><span class="string">tensor(1.) torch.float32</span></span><br><span class="line"><span class="string">tensor(1.) torch.float32</span></span><br><span class="line"><span class="string">tensor(1.) torch.float32</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>
<h1 id="张量尺寸"><a href="#张量尺寸" class="headerlink" title="张量尺寸"></a>张量尺寸</h1><p>可以使用shape属性或者size()方法查看张量在每一维的长度。<br>可以使用view/reshape方法改变张量的尺寸，view需张量连续，reshape会重新复制一份数据，所以不需要数据连续。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># shape/size()查看维度长度</span></span><br><span class="line">vector = torch.tensor([<span class="number">1.0</span>,<span class="number">2.0</span>,<span class="number">3.0</span>,<span class="number">4.0</span>])</span><br><span class="line">print(vector.size())</span><br><span class="line">print(vector.shape)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">torch.Size([4])</span></span><br><span class="line"><span class="string">torch.Size([4])</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用view可以改变张量尺寸</span></span><br><span class="line">vector = torch.arange(<span class="number">0</span>,<span class="number">12</span>)</span><br><span class="line">print(vector)</span><br><span class="line">print(vector.shape)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11])</span></span><br><span class="line"><span class="string">torch.Size([12])</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">matrix34 = vector.view(<span class="number">3</span>,<span class="number">4</span>)</span><br><span class="line">print(matrix34)</span><br><span class="line">print(matrix34.shape)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">tensor([[ 0,  1,  2,  3],</span></span><br><span class="line"><span class="string">        [ 4,  5,  6,  7],</span></span><br><span class="line"><span class="string">        [ 8,  9, 10, 11]])</span></span><br><span class="line"><span class="string">torch.Size([3, 4])</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 有些操作会让张量存储结构扭曲(如：转置操作)，直接使用view会失败，可以用reshape方法</span></span><br><span class="line">matrix26 = torch.arange(<span class="number">0</span>,<span class="number">12</span>).view(<span class="number">2</span>,<span class="number">6</span>)</span><br><span class="line">print(matrix26)</span><br><span class="line">print(matrix26.shape)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">tensor([[ 0,  1,  2,  3,  4,  5],</span></span><br><span class="line"><span class="string">        [ 6,  7,  8,  9, 10, 11]])</span></span><br><span class="line"><span class="string">torch.Size([2, 6])</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span> </span><br><span class="line">matrix62 = matrix26.t() <span class="comment"># 转置操作让张量存储结构扭曲</span></span><br><span class="line">print(matrix62.is_contiguous())</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">False</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="comment"># 直接使用view方法会失败，可以使用reshape方法</span></span><br><span class="line"><span class="comment">#matrix34 = matrix62.view(3,4) #error!</span></span><br><span class="line">matrix34 = matrix62.reshape(<span class="number">3</span>,<span class="number">4</span>) <span class="comment">#等价于matrix34 = matrix62.contiguous().view(3,4)</span></span><br><span class="line">print(matrix34)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">tensor([[ 0,  6,  1,  7],</span></span><br><span class="line"><span class="string">        [ 2,  8,  3,  9],</span></span><br><span class="line"><span class="string">        [ 4, 10,  5, 11]])</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure></p>
<h1 id="张量和numpy数组"><a href="#张量和numpy数组" class="headerlink" title="张量和numpy数组"></a>张量和numpy数组</h1><p>可以用numpy方法从Tensor得到numpy数组，也可以用torch.from_numpy从numpy数组得到Tensor。<strong>这两种方法关联的Tensor和numpy数组是共享数据内存的，如果改变其中一个，另外一个的值也会发生改变</strong>。如果有需要，可以用张量的clone方法拷贝张量，中断这种关联。<br>此外，还可以使用item方法从标量张量得到对应的Python数值。使用tolist方法从张量得到对应的Python数值列表。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 从numpy数组得到Tensor</span></span><br><span class="line">np_array = np.array([[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">4</span>]])</span><br><span class="line">x_np = torch.from_numpy(np_array) </span><br><span class="line">print(x_np)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">tensor([[1, 2],</span></span><br><span class="line"><span class="string">        [3, 4]], dtype=torch.int32)</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="comment"># 改变np_array，x_np也随之改变。</span></span><br><span class="line">np.add(np_array,<span class="number">1</span>,out= np_array)</span><br><span class="line">print(x_np)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">tensor([[2, 3],</span></span><br><span class="line"><span class="string">        [4, 5]], dtype=torch.int32)</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="comment"># 同理x_np改变，np_array也会随之改变。</span></span><br><span class="line">x_np.add_(<span class="number">1</span>)</span><br><span class="line">print(np_array)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">[[3 4]</span></span><br><span class="line"><span class="string"> [5 6]]</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="comment"># 可以用 clone() 方法拷贝张量，独立中断这种关联，拷贝后的张量和原始张量内存。</span></span><br><span class="line">x_np_c = x_np.clone().numpy() <span class="comment"># 中断了和x_np关联</span></span><br><span class="line">x_np.add_(<span class="number">1</span>)</span><br><span class="line">print(x_np_c)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">[[3 4]</span></span><br><span class="line"><span class="string"> [5 6]]</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="comment"># item方法和tolist方法可以将张量转换成Python数值和数值列表。</span></span><br><span class="line">i = torch.tensor(<span class="number">1.0</span>)</span><br><span class="line">print(i.item())</span><br><span class="line">x = torch.tensor([<span class="number">1.0</span>,<span class="number">2.0</span>])</span><br><span class="line">print(x.tolist())</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">1.0</span></span><br><span class="line"><span class="string">[1.0, 2.0]</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>
<p>张量创建参考了很多numpy方法，比如ones_like，rand_like，rand，ones，zeros等等，可参考<a href="https://soundmemories.github.io/2020/04/22/Machine%20Learning/02.ML-NumPy/">numpy创建方法</a>。</p>
<h1 id="张量操作"><a href="#张量操作" class="headerlink" title="张量操作"></a>张量操作</h1><p>张量具有100多种操作，包含了算数、线性代数、矩阵运算（转置/索引/切片）、采样等，详情见<span class="exturl" data-url="aHR0cHM6Ly9weXRvcmNoLm9yZy9kb2NzL3N0YWJsZS90b3JjaC5odG1s">官网介绍<i class="fa fa-external-link-alt"></i></span>。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 数据量大需要用GPU提高速度，把数据移动到GPU计算</span></span><br><span class="line">tensor = torch.rand(<span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line"><span class="keyword">if</span> torch.cuda.is_available():</span><br><span class="line">  tensor = tensor.to(<span class="string">&#x27;cuda&#x27;</span>)</span><br><span class="line">  print(<span class="string">f&quot;Device tensor is stored on: <span class="subst">&#123;tensor.device&#125;</span>&quot;</span>)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">Device tensor is stored on: cuda:0</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><br>常用方法：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br></pre></td><td class="code"><pre><span class="line">torch.is_tensor(obj) <span class="comment"># 是否为tensor？</span></span><br><span class="line">torch.is_complex(<span class="built_in">input</span>) <span class="comment"># 是否为复数类型？只支持torch.complex64和torch.complex128。</span></span><br><span class="line">torch.is_floating_point(<span class="built_in">input</span>) <span class="comment"># 是否为float类型？</span></span><br><span class="line">torch.is_nonzero(<span class="built_in">input</span>) <span class="comment"># 是否为&gt;0的值？只能是一个元素。</span></span><br><span class="line">torch.numel(<span class="built_in">input</span>) <span class="comment"># 返回张量的元素数量。</span></span><br><span class="line">torch.set_default_tensor_type(t) <span class="comment"># 设置全局默认张量类型，为t类型。</span></span><br><span class="line"></span><br><span class="line">torch.tensor、as_tensor、from_numpy <span class="comment"># 创建张量，建议用tensor创建。</span></span><br><span class="line">torch.zeros(*size,...,requires_grad=<span class="literal">False</span>) <span class="comment"># 创建全0张量，可选择梯度是否计算。</span></span><br><span class="line">torch.zeros_like(<span class="built_in">input</span>,...)  <span class="comment"># 创建类似input形状的全0张量。</span></span><br><span class="line">torch.ones(*size,...) <span class="comment"># 创建全1张量，可选择梯度是否计算。</span></span><br><span class="line">torch.ones_like(<span class="built_in">input</span>,...)  <span class="comment"># 创建类似input形状的全1张量。</span></span><br><span class="line">torch.arange(start=<span class="number">0</span>, end, step=<span class="number">1</span>,...) <span class="comment"># 创建张量列表，[start, end)，默认int32。建议用此方法。</span></span><br><span class="line">torch.<span class="built_in">range</span>(start=<span class="number">0</span>, end, step=<span class="number">1</span>,...) <span class="comment"># 创建张量列表，[start, end]，默认float32。</span></span><br><span class="line">torch.linspace(start, end, steps...) <span class="comment"># 创建线性张量，steps个元素，自动线性增长。</span></span><br><span class="line">torch.logspace(start, end, steps, base=<span class="number">10.0</span>,,,) <span class="comment"># 创建对数张量，steps个元素，自动对数(10为底)增长。</span></span><br><span class="line">torch.eye(n, m=<span class="literal">None</span>,...) <span class="comment"># 创建2维张量，主对角线全为1，其余为0，只传入一个值为方阵。</span></span><br><span class="line">torch.full(size, fill_value,...) <span class="comment"># 创建size形状张量，用fill_value(列表/元组等)数据填充。</span></span><br><span class="line"></span><br><span class="line">torch.cat(tensors, dim=<span class="number">0</span>,...) <span class="comment"># 按dim维度拼接张量，tensors为多个张量序列(每个张量dim一定要相同)。</span></span><br><span class="line">torch.concat() <span class="comment"># torch.cat的别名。</span></span><br><span class="line">torch.chunk(<span class="built_in">input</span>, chunks, dim=<span class="number">0</span>) <span class="comment"># 将张量按dim维度拆分成chunks块(均分)。</span></span><br><span class="line">torch.gather(<span class="built_in">input</span>, dim, index...) <span class="comment"># 按dim和index从input取值。</span></span><br><span class="line">torch.hstack(tensors,...) <span class="comment"># 沿着列堆叠。</span></span><br><span class="line">torch.reshape(<span class="built_in">input</span>, shape) <span class="comment"># 改变张量形状，返回新张量。view就地改变，需张量contiguous。</span></span><br><span class="line">torch.scatter(<span class="built_in">input</span>, dim, index, src) <span class="comment"># 将src中数据根据index中的索引按照dim的方向填进input中，根据src下标找index值，</span></span><br><span class="line">                                        <span class="comment"># 和scatter_()作用相同，但不会改变原来张量(带_为就地修改)。</span></span><br><span class="line">torch.split(tensor, split_size, dim=<span class="number">0</span>) <span class="comment"># 按split_size切分张量，split_size整数为每块元素个数，列表时为指定每块元素个数。</span></span><br><span class="line">torch.squeeze(<span class="built_in">input</span>, dim=<span class="literal">None</span>,...) <span class="comment"># 对dim维度压缩，未指定时压缩所有为1的维度。</span></span><br><span class="line">torch.stack(tensors, dim=<span class="number">0.</span>..) <span class="comment"># 按dim维度堆叠张量，会增加维度，cat不会增加维度。</span></span><br><span class="line">torch.take(<span class="built_in">input</span>, index) <span class="comment"># 把张量铺平看成一维数组，按照index索引取值。</span></span><br><span class="line">torch.tile(<span class="built_in">input</span>, dims) <span class="comment"># 复制dims-1次数据，dims形状小于input形状时，dims前面补1。</span></span><br><span class="line">                        <span class="comment">#  (8, 6, 4, 2) and dims is (2, 2), then dims is treated as (1, 1, 2, 2)</span></span><br><span class="line">torch.transpose(<span class="built_in">input</span>, dim0, dim1) <span class="comment"># dim0和dim1交换维度。</span></span><br><span class="line">torch.unbind(<span class="built_in">input</span>, dim=<span class="number">0</span>) <span class="comment"># 沿着dim维度切片得到多个张量，返回所有张量元组。</span></span><br><span class="line">torch.unsqueeze(<span class="built_in">input</span>, dim) <span class="comment"># 在dim维度增加一维，和squeeze相反。</span></span><br><span class="line">torch.where(condition, x, y) <span class="comment"># 根据condition条件判断，成立为x，否则为y。</span></span><br><span class="line"></span><br><span class="line">torch.manual_seed(seed) <span class="comment"># 设定随机数种子，包括cpu和GPU，不包括numpy。</span></span><br><span class="line">torch.bernoulli(<span class="built_in">input</span>,...) <span class="comment"># 伯努利次采样，input为每个位置的概率，返回每个位置伯努利采样结果(0/1)。</span></span><br><span class="line">torch.normal(mean, std, size,...) <span class="comment"># 返回正态分布(高斯分布)，mean和std可以为整数/序列。</span></span><br><span class="line">torch.rand(*size, ...) <span class="comment"># 生成随机数，[0,1)区间均匀分布采样。</span></span><br><span class="line">torch.randint(low=<span class="number">0</span>, high, size,...) <span class="comment"># 生成随机整数，[low,high)区间均匀分布采样。</span></span><br><span class="line">torch.randn(*size,...) <span class="comment"># 生成随机数，正态分布采样(均值0，标准差1)。</span></span><br><span class="line">torch.randperm(n, ...) <span class="comment"># 随机组合，[0,n)区间随机打乱组合。</span></span><br><span class="line"></span><br><span class="line">torch.<span class="built_in">abs</span>(<span class="built_in">input</span>,...) <span class="comment"># 绝对值。</span></span><br><span class="line">torch.add(<span class="built_in">input</span>, other, *, alpha=<span class="number">1</span>,...)  <span class="comment"># 相加，input + alpha*other。</span></span><br><span class="line">torch.ceil(<span class="built_in">input</span>,...) <span class="comment"># 取上界。</span></span><br><span class="line">torch.div(<span class="built_in">input</span>, other,...) <span class="comment"># 相除，input/other。</span></span><br><span class="line">torch.exp(<span class="built_in">input</span>,...) <span class="comment"># 指数，e^input。</span></span><br><span class="line">torch.floor(<span class="built_in">input</span>,...) <span class="comment"># 取下界。</span></span><br><span class="line">torch.log(<span class="built_in">input</span>,...) <span class="comment"># 对数,log_e(input)。</span></span><br><span class="line">torch.mul(<span class="built_in">input</span>, other,...) <span class="comment"># 相乘，input*other。</span></span><br><span class="line">torch.neg(<span class="built_in">input</span>,...) <span class="comment"># 取负，-1*input。</span></span><br><span class="line">torch.<span class="built_in">pow</span>(<span class="built_in">input</span>, exponent,...) <span class="comment"># 幂指数，input^exponent。</span></span><br><span class="line">torch.sqrt(<span class="built_in">input</span>,...) <span class="comment"># 开方。</span></span><br><span class="line">torch.<span class="built_in">round</span>(<span class="built_in">input</span>,...) <span class="comment"># 保留整数部分，四舍五入。</span></span><br><span class="line">torch.trunc(<span class="built_in">input</span>,...) <span class="comment"># 保留整数部分，向0归整。fix别名。</span></span><br><span class="line">torch.sub(<span class="built_in">input</span>, other, *, alpha=<span class="number">1</span>,...) <span class="comment"># 相减，input - alpha*other。</span></span><br><span class="line">torch.clamp(<span class="built_in">input</span>, <span class="built_in">min</span>=<span class="literal">None</span>, <span class="built_in">max</span>=<span class="literal">None</span>,...) <span class="comment"># 限制范围[min,max]。</span></span><br><span class="line"></span><br><span class="line">torch.argmax(<span class="built_in">input</span>, dim, keepdim=<span class="literal">False</span>) <span class="comment"># input最大值索引，不写dim将按input展平比较。</span></span><br><span class="line">torch.argmin(<span class="built_in">input</span>, dim=<span class="literal">None</span>, keepdim=<span class="literal">False</span>) <span class="comment"># input最小值索引，不写dim将按input展平比较。</span></span><br><span class="line">torch.<span class="built_in">all</span>(<span class="built_in">input</span>, dim, keepdim=<span class="literal">False</span>,...) <span class="comment"># input元素全为True？不写dim将按input展平比较。</span></span><br><span class="line">torch.<span class="built_in">any</span>(<span class="built_in">input</span>, dim, keepdim=<span class="literal">False</span>,...) <span class="comment"># input元素不全为True？不写dim将按input展平比较。</span></span><br><span class="line">torch.<span class="built_in">max</span>(<span class="built_in">input</span>, dim, keepdim=<span class="literal">False</span>,...) <span class="comment"># input最大值, 不写dim将按input展平比较。</span></span><br><span class="line">torch.<span class="built_in">min</span>(<span class="built_in">input</span>, dim, keepdim=<span class="literal">False</span>,...) <span class="comment"># input最小值, 不写dim将按input展平比较。</span></span><br><span class="line">torch.mean(<span class="built_in">input</span>, dim, keepdim=<span class="literal">False</span>,...) <span class="comment"># input均值, 不写dim将按input展平比较。</span></span><br><span class="line">torch.median(<span class="built_in">input</span>, dim, keepdim=<span class="literal">False</span>,...) <span class="comment"># input中位数, 不写dim将按input展平比较。</span></span><br><span class="line">torch.<span class="built_in">sum</span>(<span class="built_in">input</span>, dim, keepdim=<span class="literal">False</span>,...) <span class="comment"># input求和, 不写dim将按input展平比较。</span></span><br><span class="line"></span><br><span class="line">torch.argsort(<span class="built_in">input</span>, dim=<span class="number">-1</span>, descending=<span class="literal">False</span>) <span class="comment"># 给定维度升序排序后的索引。</span></span><br><span class="line">torch.equal(<span class="built_in">input</span>, other) <span class="comment"># input和other形状和元素相同为True。</span></span><br><span class="line">torch.ge(<span class="built_in">input</span>, other,...) <span class="comment"># 大于等于。</span></span><br><span class="line">torch.gt(<span class="built_in">input</span>, other,...) <span class="comment"># 大于。</span></span><br><span class="line">torch.isin(elements, test_elements,...) <span class="comment"># elements元素在test_elements中出现为True，形状同elements。</span></span><br><span class="line">torch.isnan(<span class="built_in">input</span>) <span class="comment"># nan？</span></span><br><span class="line">torch.le(<span class="built_in">input</span>, other,...) <span class="comment"># 小于等于。</span></span><br><span class="line">torch.lt(<span class="built_in">input</span>, other,...) <span class="comment"># 小于。</span></span><br><span class="line">torch.maximum(<span class="built_in">input</span>, other,...) <span class="comment"># 留下input和other相同位置最大值。</span></span><br><span class="line">torch.minimum(<span class="built_in">input</span>, other,...) <span class="comment"># 留下input和other相同位置最小值。</span></span><br><span class="line">torch.ne(<span class="built_in">input</span>, other,...) <span class="comment"># 不等于。</span></span><br><span class="line">torch.sort(<span class="built_in">input</span>, dim=<span class="number">-1</span>, descending=<span class="literal">False</span>,...) <span class="comment"># 按dim排序，默认升序。</span></span><br><span class="line">torch.topk(<span class="built_in">input</span>, k, dim=<span class="literal">None</span>, largest=<span class="literal">True</span>, <span class="built_in">sorted</span>=<span class="literal">True</span>,...) <span class="comment"># topk个元素，默认最大k个。</span></span><br><span class="line"></span><br><span class="line">torch.mm(mat1, mat2, out=<span class="literal">None</span>) <span class="comment"># 两个二维矩阵的矩阵乘法，并且不支持broadcast操作。</span></span><br><span class="line"><span class="comment"># mat1 x mat2 -&gt; n×m × m×d = n×d</span></span><br><span class="line">torch.bmm(bmat1, bmat2, out=<span class="literal">None</span>) <span class="comment"># 三维带batch的矩阵乘法。</span></span><br><span class="line"><span class="comment"># bmat1 x bmat2 -&gt; b×n×m × b×m×d = b×n×d</span></span><br><span class="line">torch.matmul(<span class="built_in">input</span>, other, out=<span class="literal">None</span>) <span class="comment"># 支持broadcast操作，使用起来比较复杂。</span></span><br><span class="line"><span class="comment"># 针对多维数据 matmul() 乘法，可以认为该乘法使用使用两个参数的后两个维度来计算，其他的维度都可以认为是batch维度。</span></span><br><span class="line">torch.mul(mat1, other, out=<span class="literal">None</span>) <span class="comment"># 矩阵逐元素乘法。</span></span><br><span class="line"><span class="comment"># 其中 other 乘数可以是标量，也可以是任意维度的矩阵， 只要满足最终相乘是可以broadcast的即可。</span></span><br></pre></td></tr></table></figure><br>不规则取值示例：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 班级成绩册的例子，有4个班级，每个班级10个学生，每个学生7门科目成绩。可以用一个4×10×7的张量来表示。</span></span><br><span class="line">minval=<span class="number">0</span></span><br><span class="line">maxval=<span class="number">100</span></span><br><span class="line">scores = torch.floor(minval + (maxval-minval)*torch.rand([<span class="number">4</span>,<span class="number">10</span>,<span class="number">7</span>])).<span class="built_in">int</span>()</span><br><span class="line"><span class="comment"># 抽取每个班级第0个学生，第5个学生，第9个学生的全部成绩</span></span><br><span class="line">torch.index_select(scores,dim = <span class="number">1</span>,index = torch.tensor([<span class="number">0</span>,<span class="number">5</span>,<span class="number">9</span>]))</span><br><span class="line"><span class="comment"># 抽取每个班级第0个学生，第5个学生，第9个学生的第1门课程，第3门课程，第6门课程成绩</span></span><br><span class="line">torch.index_select(torch.index_select(scores, dim=<span class="number">1</span>, index=torch.tensor([<span class="number">0</span>,<span class="number">5</span>,<span class="number">9</span>]))</span><br><span class="line">                   ,dim=<span class="number">2</span>,index=torch.tensor([<span class="number">1</span>,<span class="number">3</span>,<span class="number">6</span>]))</span><br><span class="line"><span class="comment"># 抽取第0个班级第0个学生的第0门课程，第2个班级的第4个学生的第1门课程，第3个班级的第9个学生第6门课程成绩</span></span><br><span class="line"><span class="comment"># take将输入看成一维数组，输出和index同形状</span></span><br><span class="line">torch.take(scores,torch.tensor([<span class="number">0</span>*<span class="number">10</span>*<span class="number">7</span>+<span class="number">0</span>,<span class="number">2</span>*<span class="number">10</span>*<span class="number">7</span>+<span class="number">4</span>*<span class="number">7</span>+<span class="number">1</span>,<span class="number">3</span>*<span class="number">10</span>*<span class="number">7</span>+<span class="number">9</span>*<span class="number">7</span>+<span class="number">6</span>]))</span><br><span class="line"><span class="comment"># 抽取分数大于等于80分的分数（布尔索引）</span></span><br><span class="line"><span class="comment"># 结果是1维张量</span></span><br><span class="line">torch.masked_select(scores, scores&gt;=<span class="number">80</span>)</span><br></pre></td></tr></table></figure><br>如果要通过修改张量的部分元素值得到新的张量，可以使用torch.where，torch.index_fill 和 torch.masked_fill。<br>torch.where可以理解为if的张量版本。<br>torch.index_fill的选取元素逻辑和torch.index_select相同。<br>torch.masked_fill的选取元素逻辑和torch.masked_select相同。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 如果分数大于60分，赋值成1，否则赋值成0</span></span><br><span class="line">ifpass = torch.where(scores&gt;<span class="number">60</span>,torch.tensor(<span class="number">1</span>),torch.tensor(<span class="number">0</span>))</span><br><span class="line"><span class="comment"># 将每个班级第0个学生，第5个学生，第9个学生的全部成绩赋值成满分</span></span><br><span class="line"><span class="comment"># 等价于 scores.index_fill(dim = 1,index = torch.tensor([0,5,9]),value = 100)</span></span><br><span class="line">torch.index_fill(scores,dim = <span class="number">1</span>,index = torch.tensor([<span class="number">0</span>,<span class="number">5</span>,<span class="number">9</span>]),value = <span class="number">100</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将分数小于60分的分数赋值成60分</span></span><br><span class="line"><span class="comment"># 等价于scores.masked_fill(scores&lt;60,60)</span></span><br><span class="line">torch.masked_fill(scores,scores&lt;<span class="number">60</span>,<span class="number">60</span>)</span><br></pre></td></tr></table></figure></p>
<h1 id="自动微分"><a href="#自动微分" class="headerlink" title="自动微分"></a>自动微分</h1><p>关于自动微分的论文：<span class="exturl" data-url="aHR0cHM6Ly93d3cuam1sci5vcmcvcGFwZXJzL3ZvbHVtZTE4LzE3LTQ2OC8xNy00NjgucGRm">Automatic Differentiation in Machine Learning: a Survey<i class="fa fa-external-link-alt"></i></span>。<br>为什么深度学习框架都用反向传播计算梯度？<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">假设 a=f(x), b=g(a), y=h(b)</span><br><span class="line"></span><br><span class="line">p_y/p_x = p_h/p_b * p_g/p_a * p_f/p_x (链式法则+雅可比)</span><br><span class="line">雅可比矩阵大小：|y|*|b|, |b|*|a|, |a|*|x|</span><br><span class="line"></span><br><span class="line">统计计算量：</span><br><span class="line">forward mode <span class="keyword">for</span> AD：</span><br><span class="line">|y|*|b| (|b|*|a|, |a|*|x|) = bax+ybx</span><br><span class="line"></span><br><span class="line">reverse mode <span class="keyword">for</span> AD:</span><br><span class="line">(|y|*|b|, |b|*|a|) |a|*|x| = yba+yax</span><br><span class="line"></span><br><span class="line">假设a=b，变成x跟y的比较：</span><br><span class="line">当x&gt;y, 输入特征大于输出特征, reverse mode计算量比较小(深度网络一般输入维度x&gt;y的，所以用方向传播省计算量)</span><br><span class="line">当x&lt;y, 输入特征小于输出特征, forward mode计算量比较小</span><br></pre></td></tr></table></figure><br>Pytorch一般通过反向传播<code>backward()</code>方法实现这种求梯度计算。该方法求得的梯度将存在对应自变量张量的grad属性下。<br>除此之外，也能够调用<code>torch.autograd.grad()</code>函数来实现求梯度计算。这就是Pytorch的自动微分机制。<br>注意，Pytorch梯度会自动累加，不想累加使用<code>.grad_zero_()</code>清除当前梯度。<br>利用backward方法求导数：<br><div class="tabs" id="one"><ul class="nav-tabs"><li class="tab active"><a href="#one-1">标量反向传播</a></li><li class="tab"><a href="#one-2">非标量反向传播</a></li><li class="tab"><a href="#one-3">非标量用标量反向传播</a></li></ul><div class="tab-content"><div class="tab-pane active" id="one-1"><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np </span><br><span class="line"><span class="keyword">import</span> torch </span><br><span class="line"></span><br><span class="line"><span class="comment"># f(x) = a*x^2 + b*x + c的导数</span></span><br><span class="line">x = torch.tensor(<span class="number">0.0</span>, requires_grad=<span class="literal">True</span>) <span class="comment"># x需要被求导</span></span><br><span class="line">a = torch.tensor(<span class="number">1.0</span>)</span><br><span class="line">b = torch.tensor(<span class="number">-2.0</span>)</span><br><span class="line">c = torch.tensor(<span class="number">1.0</span>)</span><br><span class="line">y = a*torch.<span class="built_in">pow</span>(x,<span class="number">2</span>) + b*x + c </span><br><span class="line"></span><br><span class="line">y.backward()</span><br><span class="line">dy_dx = x.grad</span><br><span class="line">print(dy_dx)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">tensor(-2.)</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure></div><div class="tab-pane" id="one-2"><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np </span><br><span class="line"><span class="keyword">import</span> torch </span><br><span class="line"></span><br><span class="line"><span class="comment"># f(x) = a*x^2 + b*x + c</span></span><br><span class="line">x = torch.tensor([[<span class="number">0.0</span>,<span class="number">0.0</span>],[<span class="number">1.0</span>,<span class="number">2.0</span>]], requires_grad=<span class="literal">True</span>) <span class="comment"># x需要被求导</span></span><br><span class="line">a = torch.tensor(<span class="number">1.0</span>)</span><br><span class="line">b = torch.tensor(<span class="number">-2.0</span>)</span><br><span class="line">c = torch.tensor(<span class="number">1.0</span>)</span><br><span class="line">y = a*torch.<span class="built_in">pow</span>(x,<span class="number">2</span>) + b*x + c </span><br><span class="line"></span><br><span class="line">gradient = torch.tensor([[<span class="number">1.0</span>,<span class="number">1.0</span>],[<span class="number">1.0</span>,<span class="number">1.0</span>]])</span><br><span class="line"></span><br><span class="line">print(<span class="string">&quot;x:\n&quot;</span>,x)</span><br><span class="line">print(<span class="string">&quot;y:\n&quot;</span>,y)</span><br><span class="line"><span class="comment"># 当loss非标量时，backward需要传入和结果形状一致的张量。loss对参数求完梯度后乘以这个张量作为结果。</span></span><br><span class="line"><span class="comment"># 官网自动微分：https://pytorch.org/tutorials/beginner/basics/autogradqs_tutorial.html</span></span><br><span class="line">y.backward(gradient=gradient)</span><br><span class="line">x_grad = x.grad</span><br><span class="line">print(<span class="string">&quot;x_grad:\n&quot;</span>,x_grad)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">x:</span></span><br><span class="line"><span class="string"> tensor([[0., 0.],</span></span><br><span class="line"><span class="string">        [1., 2.]], requires_grad=True)</span></span><br><span class="line"><span class="string">y:</span></span><br><span class="line"><span class="string"> tensor([[1., 1.],</span></span><br><span class="line"><span class="string">        [0., 1.]], grad_fn=&lt;AddBackward0&gt;)</span></span><br><span class="line"><span class="string">x_grad:</span></span><br><span class="line"><span class="string"> tensor([[-2., -2.],</span></span><br><span class="line"><span class="string">        [ 0.,  2.]])</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="comment"># 手动计算</span></span><br><span class="line"><span class="keyword">from</span> torch.autograd.functional <span class="keyword">import</span> jacobian</span><br><span class="line"><span class="comment"># jacobian需要输入一个函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">func</span>(<span class="params">x</span>):</span></span><br><span class="line">    <span class="keyword">return</span> a*torch.<span class="built_in">pow</span>(x,<span class="number">2</span>) + b*x + c</span><br><span class="line"><span class="comment"># 把矩阵运算转成向量运算</span></span><br><span class="line">x_1 = torch.ones_like(func(x[<span class="number">0</span>])) @ jacobian(func,x[<span class="number">0</span>])</span><br><span class="line">print(x_1)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">tensor([-2., -2.])</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">x_2 = torch.ones_like(func(x[<span class="number">1</span>])) @ jacobian(func,x[<span class="number">1</span>])</span><br><span class="line">print(x_2)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">tensor([0., 2.])</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure></div><div class="tab-pane" id="one-3"><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np </span><br><span class="line"><span class="keyword">import</span> torch </span><br><span class="line"></span><br><span class="line"><span class="comment"># f(x) = a*x^2 + b*x + c</span></span><br><span class="line">x = torch.tensor([[<span class="number">0.0</span>,<span class="number">0.0</span>],[<span class="number">1.0</span>,<span class="number">2.0</span>]], requires_grad=<span class="literal">True</span>) <span class="comment"># x需要被求导</span></span><br><span class="line">a = torch.tensor(<span class="number">1.0</span>)</span><br><span class="line">b = torch.tensor(<span class="number">-2.0</span>)</span><br><span class="line">c = torch.tensor(<span class="number">1.0</span>)</span><br><span class="line">y = a*torch.<span class="built_in">pow</span>(x,<span class="number">2</span>) + b*x + c </span><br><span class="line"></span><br><span class="line">gradient = torch.tensor([[<span class="number">1.0</span>,<span class="number">1.0</span>],[<span class="number">1.0</span>,<span class="number">1.0</span>]])</span><br><span class="line">z = torch.<span class="built_in">sum</span>(y*gradient)</span><br><span class="line"></span><br><span class="line">print(<span class="string">&quot;x:&quot;</span>,x)</span><br><span class="line">print(<span class="string">&quot;y:&quot;</span>,y)</span><br><span class="line">z.backward()</span><br><span class="line">x_grad = x.grad</span><br><span class="line">print(<span class="string">&quot;x_grad:\n&quot;</span>,x_grad)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">x: tensor([[0., 0.],</span></span><br><span class="line"><span class="string">        [1., 2.]], requires_grad=True)</span></span><br><span class="line"><span class="string">y: tensor([[1., 1.],</span></span><br><span class="line"><span class="string">        [0., 1.]], grad_fn=&lt;AddBackward0&gt;)</span></span><br><span class="line"><span class="string">x_grad:</span></span><br><span class="line"><span class="string"> tensor([[-2., -2.],</span></span><br><span class="line"><span class="string">        [ 0.,  2.]])</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure></div></div></div></p>
<p>利用autograd.grad方法求导数：<br><div class="tabs" id="two"><ul class="nav-tabs"><li class="tab active"><a href="#two-1">单个标量</a></li><li class="tab"><a href="#two-2">多个标量</a></li></ul><div class="tab-content"><div class="tab-pane active" id="two-1"><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np </span><br><span class="line"><span class="keyword">import</span> torch </span><br><span class="line"></span><br><span class="line"><span class="comment"># f(x) = a*x^2 + b*x + c的导数</span></span><br><span class="line">x = torch.tensor(<span class="number">0.0</span>, requires_grad=<span class="literal">True</span>) <span class="comment"># x需要被求导</span></span><br><span class="line">a = torch.tensor(<span class="number">1.0</span>)</span><br><span class="line">b = torch.tensor(<span class="number">-2.0</span>)</span><br><span class="line">c = torch.tensor(<span class="number">1.0</span>)</span><br><span class="line">y = a*torch.<span class="built_in">pow</span>(x,<span class="number">2</span>) + b*x + c</span><br><span class="line"></span><br><span class="line"><span class="comment"># create_graph 设置为 True 将允许创建更高阶的导数 </span></span><br><span class="line">dy_dx = torch.autograd.grad(y,x, create_graph=<span class="literal">True</span>)[<span class="number">0</span>]</span><br><span class="line">print(dy_dx.data)</span><br><span class="line"><span class="comment"># 求二阶导数</span></span><br><span class="line">dy2_dx2 = torch.autograd.grad(dy_dx,x)[<span class="number">0</span>] </span><br><span class="line">print(dy2_dx2.data)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">tensor(-2.)</span></span><br><span class="line"><span class="string">tensor(2.)</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure></div><div class="tab-pane" id="two-2"><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np </span><br><span class="line"><span class="keyword">import</span> torch </span><br><span class="line"></span><br><span class="line">x1 = torch.tensor(<span class="number">1.0</span>, requires_grad=<span class="literal">True</span>) <span class="comment"># x需要被求导</span></span><br><span class="line">x2 = torch.tensor(<span class="number">2.0</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">y1 = x1*x2</span><br><span class="line">y2 = x1+x2</span><br><span class="line"></span><br><span class="line"><span class="comment"># 允许同时对多个自变量求导数</span></span><br><span class="line">(dy1_dx1,dy1_dx2) = torch.autograd.grad(outputs=y1, inputs=[x1,x2], retain_graph=<span class="literal">True</span>)</span><br><span class="line">print(dy1_dx1,dy1_dx2)</span><br><span class="line"><span class="comment"># 如果有多个因变量，相当于把多个因变量的梯度结果求和</span></span><br><span class="line">(dy12_dx1,dy12_dx2) = torch.autograd.grad(outputs=[y1,y2], inputs=[x1,x2])</span><br><span class="line">print(dy12_dx1,dy12_dx2)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">tensor(2.) tensor(1.)</span></span><br><span class="line"><span class="string">tensor(3.) tensor(2.)</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure></div></div></div></p>
<p>利用自动微分和优化器求最小值：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np </span><br><span class="line"><span class="keyword">import</span> torch </span><br><span class="line"></span><br><span class="line"><span class="comment"># f(x) = a*x^2 + b*x + c的最小值</span></span><br><span class="line">x = torch.tensor(<span class="number">0.0</span>, requires_grad=<span class="literal">True</span>) <span class="comment"># x需要被求导</span></span><br><span class="line">a = torch.tensor(<span class="number">1.0</span>)</span><br><span class="line">b = torch.tensor(<span class="number">-2.0</span>)</span><br><span class="line">c = torch.tensor(<span class="number">1.0</span>)</span><br><span class="line"></span><br><span class="line">optimizer = torch.optim.SGD(params=[x], lr=<span class="number">0.01</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">f</span>(<span class="params">x</span>):</span></span><br><span class="line">    result = a*torch.<span class="built_in">pow</span>(x,<span class="number">2</span>) + b*x + c </span><br><span class="line">    <span class="keyword">return</span>(result)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">500</span>):</span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    y = f(x)</span><br><span class="line">    y.backward()</span><br><span class="line">    optimizer.step()</span><br><span class="line"></span><br><span class="line">print(<span class="string">&quot;y=&quot;</span>,f(x).data,<span class="string">&quot;;&quot;</span>,<span class="string">&quot;x=&quot;</span>,x.data)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">y= tensor(0.) ; x= tensor(1.0000)</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure></p>
<h1 id="动态计算图"><a href="#动态计算图" class="headerlink" title="动态计算图"></a>动态计算图</h1><p>Pytorch的计算图由<strong>节点</strong>和<strong>边</strong>组成，节点表示张量或者Function，边表示张量和Function之间的依赖关系。<br>Pytorch中的计算图是动态图。这里的动态主要有两重含义。<strong>第一层含义</strong>：计算图的<strong>正向传播是立即执行</strong>。无需等待完整的计算图创建完毕，每条语句都会在计算图中动态添加节点和边，并立即执行正向传播得到计算结果。<strong>第二层含义</strong>：计算图在<strong>反向传播后立即销毁</strong>。下次调用需要重新构建计算图。如果在程序中使用了backward方法执行了反向传播，或者利用torch.autograd.grad方法计算了梯度，那么创建的计算图会被立即销毁，释放存储空间，下次调用需要重新创建。<br><div class="tabs" id="ll"><ul class="nav-tabs"><li class="tab active"><a href="#ll-1">正向传播是立即执行</a></li><li class="tab"><a href="#ll-2">反向传播后立即销毁</a></li></ul><div class="tab-content"><div class="tab-pane active" id="ll-1"><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch </span><br><span class="line">w = torch.tensor([[<span class="number">3.0</span>,<span class="number">1.0</span>]],requires_grad=<span class="literal">True</span>)</span><br><span class="line">b = torch.tensor([[<span class="number">3.0</span>]],requires_grad=<span class="literal">True</span>)</span><br><span class="line">X = torch.tensor([[<span class="number">2.0</span>,<span class="number">4.0</span>]])</span><br><span class="line">Y = torch.tensor(<span class="number">1</span>)</span><br><span class="line">Y_hat = X@w.t() + b  <span class="comment"># Y_hat定义后其正向传播被立即执行，与其后面的loss创建语句无关</span></span><br><span class="line">loss = torch.mean(torch.<span class="built_in">pow</span>(Y_hat-Y,<span class="number">2</span>))</span><br><span class="line"></span><br><span class="line">print(loss.data)</span><br><span class="line">print(Y_hat.data)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">tensor(144.)</span></span><br><span class="line"><span class="string">tensor([[13.]])</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure></div><div class="tab-pane" id="ll-2"><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch </span><br><span class="line">w = torch.tensor([[<span class="number">3.0</span>,<span class="number">1.0</span>]],requires_grad=<span class="literal">True</span>)</span><br><span class="line">b = torch.tensor([[<span class="number">3.0</span>]],requires_grad=<span class="literal">True</span>)</span><br><span class="line">X = torch.tensor([[<span class="number">2.0</span>,<span class="number">4.0</span>]],requires_grad=<span class="literal">False</span>)</span><br><span class="line">Y = torch.tensor(<span class="number">1</span>,requires_grad=<span class="literal">False</span>)</span><br><span class="line">Y_hat = X@w.t() + b  <span class="comment"># Y_hat定义后其正向传播被立即执行，与其后面的loss创建语句无关</span></span><br><span class="line">loss = torch.mean(torch.<span class="built_in">pow</span>(Y_hat-Y,<span class="number">2</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment">#计算图在反向传播后立即销毁，如果需要保留计算图, 需要backward中设置retain_graph=True</span></span><br><span class="line">loss.backward(retain_graph=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#loss.backward() #若不加retain_graph，如果再次执行反向传播将报错</span></span><br></pre></td></tr></table></figure></div></div></div></p>
<p>计算图中的 张量我们已经比较熟悉了, 计算图中的另外一种节点是Function, 实际上就是Pytorch中各种对张量操作的函数。这些Function和我们Python中的函数有一个较大的区别，那就是它同时包括正向计算逻辑和反向传播的逻辑。我们可以通过继承torch.autograd.Function来创建这种支持反向传播的Function。<br><div class="tabs" id="s"><ul class="nav-tabs"><li class="tab active"><a href="#s-1">自定义Function</a></li><li class="tab"><a href="#s-2">调用自定义Function</a></li><li class="tab"><a href="#s-3">反向传播计算过程</a></li></ul><div class="tab-content"><div class="tab-pane active" id="s-1"><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyReLU</span>(<span class="params">torch.autograd.Function</span>):</span></span><br><span class="line">    <span class="comment">#正向传播逻辑，可以用ctx存储一些值，供反向传播使用。</span></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">ctx, <span class="built_in">input</span></span>):</span></span><br><span class="line">        ctx.save_for_backward(<span class="built_in">input</span>)</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">input</span>.clamp(<span class="built_in">min</span>=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">#反向传播逻辑</span></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">backward</span>(<span class="params">ctx, grad_output</span>):</span></span><br><span class="line">        <span class="built_in">input</span>, = ctx.saved_tensors</span><br><span class="line">        grad_input = grad_output.clone()</span><br><span class="line">        grad_input[<span class="built_in">input</span> &lt; <span class="number">0</span>] = <span class="number">0</span></span><br><span class="line">        <span class="keyword">return</span> grad_input</span><br></pre></td></tr></table></figure></div><div class="tab-pane" id="s-2"><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch </span><br><span class="line">w = torch.tensor([[<span class="number">3.0</span>,<span class="number">1.0</span>]],requires_grad=<span class="literal">True</span>)</span><br><span class="line">b = torch.tensor([[<span class="number">3.0</span>]],requires_grad=<span class="literal">True</span>)</span><br><span class="line">X = torch.tensor([[<span class="number">-1.0</span>,<span class="number">-1.0</span>],[<span class="number">1.0</span>,<span class="number">1.0</span>]])</span><br><span class="line">Y = torch.tensor([[<span class="number">2.0</span>,<span class="number">3.0</span>]])</span><br><span class="line"></span><br><span class="line">relu = MyReLU.apply <span class="comment"># relu现在也可以具有正向传播和反向传播功能</span></span><br><span class="line">Y_hat = relu(X@w.t() + b)</span><br><span class="line">loss = torch.mean(torch.<span class="built_in">pow</span>(Y_hat-Y,<span class="number">2</span>))</span><br><span class="line"></span><br><span class="line">loss.backward()</span><br><span class="line"></span><br><span class="line">print(w.grad)</span><br><span class="line">print(b.grad)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">tensor([[4.5000, 4.5000]])</span></span><br><span class="line"><span class="string">tensor([[4.5000]])</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="comment"># Y_hat的梯度函数即是我们自己所定义的 MyReLU.backward</span></span><br><span class="line">print(Y_hat.grad_fn)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">&lt;torch.autograd.function.MyReLUBackward object at 0x1205a46c8&gt;</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure></div><div class="tab-pane" id="s-3"><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch </span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">loss.backward()语句调用后，依次发生以下计算过程：</span></span><br><span class="line"><span class="string">(1) loss自己的grad梯度赋值为1，即对自身的梯度为1。</span></span><br><span class="line"><span class="string">(2) loss根据其自身梯度以及关联的backward方法，计算出其对应的自变量即y1和y2的梯度，将该值赋值到y1.grad和y2.grad。</span></span><br><span class="line"><span class="string">(3) y2和y1根据其自身梯度以及关联的backward方法, 分别计算出其对应的自变量x的梯度，x.grad将其收到的多个梯度值累加。</span></span><br><span class="line"><span class="string">（注意，1,2,3步骤的求梯度顺序和对多个梯度值的累加规则恰好是求导链式法则的程序表述）</span></span><br><span class="line"><span class="string">正因为求导链式法则衍生的梯度累加规则，张量的grad梯度不会自动清零，在需要的时候需要手动置零。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">x = torch.tensor(<span class="number">3.0</span>,requires_grad=<span class="literal">True</span>)</span><br><span class="line">y1 = x+<span class="number">1</span>  <span class="comment"># 梯度:dy1_dx=1</span></span><br><span class="line">y2 = <span class="number">2</span>*x  <span class="comment"># 梯度:dy2_dx=2</span></span><br><span class="line">loss = (y1-y2)**<span class="number">2</span>  </span><br><span class="line"></span><br><span class="line">loss.backward()</span><br><span class="line"><span class="comment"># 会出现UserWarning，y1和y2为非叶子节点，想看梯度需.retain_grad()</span></span><br><span class="line">print(<span class="string">&quot;loss.grad:&quot;</span>, loss.grad)</span><br><span class="line">print(<span class="string">&quot;y1.grad:&quot;</span>, y1.grad)</span><br><span class="line">print(<span class="string">&quot;y2.grad:&quot;</span>, y2.grad)</span><br><span class="line"><span class="comment"># 对x的梯度:2(y1-y2)*(dy1_dx-dy2_dx)=2(1-x)*(-1)，带入x=3得4</span></span><br><span class="line">print(x.grad)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">loss.grad: None</span></span><br><span class="line"><span class="string">y1.grad: None</span></span><br><span class="line"><span class="string">y2.grad: None</span></span><br><span class="line"><span class="string">tensor(4.)</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">print(loss.is_leaf)</span><br><span class="line">print(y1.is_leaf)</span><br><span class="line">print(y2.is_leaf)</span><br><span class="line">print(x.is_leaf)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">False</span></span><br><span class="line"><span class="string">False</span></span><br><span class="line"><span class="string">False</span></span><br><span class="line"><span class="string">True</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure></div></div></div><br>叶子节点和非叶子节点：<br>反向传播计算过程中，会发现loss.grad并不是我们期望的1，而是None，类似地 y1.grad 以及 y2.grad也是None。<br>这是为什么呢？这是由于它们不是叶子节点张量。</p>
<p>在反向传播过程中，只有 is_leaf=True 的叶子节点，需要求导的张量的导数结果才会被最后保留下来。<br>那么什么是叶子节点张量呢？叶子节点张量需要满足两个条件。<br>（1）叶子节点张量是由用户直接创建的张量，而非由某个Function通过计算得到的张量。<br>（2）叶子节点张量的 requires_grad 属性必须为True。<br>Pytorch设计这样的规则主要是为了节约内存或者显存空间，因为几乎所有的时候，用户只会关心他自己直接创建的张量的梯度。<br>所有依赖于叶子节点张量的张量, 其 requires_grad 属性必定是True的，但其梯度值只在计算过程中被用到，不会最终存储到grad属性中。</p>
<p>如果需要保留中间计算结果的梯度到grad属性中，可以使用<code>retain_grad</code>方法。<br>如果仅仅是为了调试代码查看梯度值，可以利用<code>register_hook</code>打印日志。<br>如果不想保留梯度，可使用<code>with torch.no_grad()</code>方法或Tensor的<code>detach()</code>方法。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#非叶子节点梯度显示控制</span></span><br><span class="line">y1.retain_grad()  <span class="comment"># y1.register_hook(lambda grad: print(&#x27;y1 grad: &#x27;, grad))</span></span><br><span class="line">y2.retain_grad()</span><br><span class="line">loss.retain_grad()</span><br><span class="line"></span><br><span class="line"><span class="comment">#反向传播</span></span><br><span class="line">loss.backward()</span><br><span class="line">print(<span class="string">&quot;loss.grad:&quot;</span>, loss.grad)</span><br><span class="line"><span class="comment"># 对y1的梯度:2(y1-y2)=2(x+1-2x)，带入x=3得-4</span></span><br><span class="line">print(<span class="string">&quot;y1.grad:&quot;</span>, y1.grad)</span><br><span class="line"><span class="comment"># 对y2的梯度:-2(y1-y2)=-2(x+1-2x)，带入x=3得4</span></span><br><span class="line">print(<span class="string">&quot;y2.grad:&quot;</span>, y2.grad)</span><br><span class="line"><span class="comment"># 对x的梯度:2(y1-y2)*(dy1_dx-dy2_dx)=2(1-x)*(-1)，带入x=3得4</span></span><br><span class="line">print(x.grad)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">loss.grad: tensor(1.)</span></span><br><span class="line"><span class="string">y1 grad:  tensor(-4.)</span></span><br><span class="line"><span class="string">y2 grad:  tensor(4.)</span></span><br><span class="line"><span class="string">x.grad: tensor(4.)</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="comment"># 不想保留梯度，torch.no_grad或detach</span></span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    loss = (y1-y2)**<span class="number">2</span> </span><br><span class="line">print(loss.requires_grad)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">False</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">loss = (y1-y2)**<span class="number">2</span> </span><br><span class="line">loss.detach()</span><br><span class="line">print(loss.requires_grad)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">False</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><br>计算图在TensorBoard中的可视化：<br>可以利用 torch.utils.tensorboard 将计算图导出到 TensorBoard进行可视化。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn </span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Net</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(Net, self).__init__()</span><br><span class="line">        self.w = nn.Parameter(torch.randn(<span class="number">2</span>,<span class="number">1</span>))</span><br><span class="line">        self.b = nn.Parameter(torch.zeros(<span class="number">1</span>,<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        y = x@self.w + self.b</span><br><span class="line">        <span class="keyword">return</span> y</span><br><span class="line"></span><br><span class="line">net = Net()</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> torch.utils.tensorboard <span class="keyword">import</span> SummaryWriter</span><br><span class="line">writer = SummaryWriter(<span class="string">&#x27;../data/tensorboard&#x27;</span>)</span><br><span class="line">writer.add_graph(net,input_to_model = torch.rand(<span class="number">10</span>,<span class="number">2</span>))</span><br><span class="line">writer.close()</span><br><span class="line"></span><br><span class="line">%load_ext tensorboard</span><br><span class="line"><span class="comment">#%tensorboard --logdir ../data/tensorboard</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> tensorboard <span class="keyword">import</span> notebook</span><br><span class="line">notebook.<span class="built_in">list</span>() </span><br><span class="line"></span><br><span class="line"><span class="comment">#在tensorboard中查看模型</span></span><br><span class="line">notebook.start(<span class="string">&quot;--logdir ../data/tensorboard&quot;</span>)</span><br></pre></td></tr></table></figure></p>
<h1 id="Dataset和DataLoader"><a href="#Dataset和DataLoader" class="headerlink" title="Dataset和DataLoader"></a>Dataset和DataLoader</h1><p>Pytorch通常使用Dataset和DataLoader这两个工具类来构建数据管道(<span class="exturl" data-url="aHR0cHM6Ly9weXRvcmNoLm9yZy90dXRvcmlhbHMvYmVnaW5uZXIvYmFzaWNzL2RhdGFfdHV0b3JpYWwuaHRtbA==">官方讲解<i class="fa fa-external-link-alt"></i></span>)。<br>Dataset定义了数据集的内容，它相当于一个类似列表的数据结构，具有确定的长度，能够用索引获取数据集中的元素。<br>DataLoader定义了按batch加载数据集的方法，它是一个实现了<code>__iter__</code>方法的可迭代对象，每次迭代输出一个batch的数据。它能够控制batch的大小，batch中元素的采样方法，以及将batch结果整理成模型所需输入形式的方法，能够使用多进程读取数据。<br>在绝大部分情况下，用户只需实现Dataset的<code>__init__</code>、<code>__len__</code>和<code>__getitem__</code>方法，就可以构建自己的数据集，并用默认数据管道进行加载。</p>
<p>获取一个batch数据的步骤(假定数据集的特征和标签分别表示为张量X和Y，数据集可以表示为(X,Y), 假定batch大小为m)：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">（1）首先确定数据集的长度n。</span><br><span class="line">（2）然后从0到n-1的范围中抽样出m个数(batch大小)。假定m&#x3D;4, 拿到的结果是一个列表，类似：indices&#x3D;[1,4,8,9]。</span><br><span class="line">（3）接着从数据集中去取这m个数对应下标的元素。拿到的结果是一个元组列表，</span><br><span class="line">	 类似：samples&#x3D;[(X[1],Y[1]),(X[4],Y[4]),(X[8],Y[8]),(X[9],Y[9])]。</span><br><span class="line">（4）最后将结果整理成两个张量作为输出。拿到的结果是两个张量：</span><br><span class="line">	 类似batch&#x3D;(features,labels)，其中features&#x3D;torch.stack([X[1],X[4],X[8],X[9]])，</span><br><span class="line">	 labels&#x3D;torch.stack([Y[1],Y[4],Y[8],Y[9]])。</span><br><span class="line"></span><br><span class="line">第（1）个步骤确定数据集的长度是由 Dataset 的__len__方法实现的。</span><br><span class="line">第（2）个步骤从0到n-1的范围中抽样出m个数的方法是由 DataLoader的 sampler 和 batch_sampler 参数指定的。</span><br><span class="line">  sampler参数指定单个元素抽样方法，一般无需用户设置，程序默认在DataLoader的参数shuffle&#x3D;True时采用随机抽样，</span><br><span class="line">  shuffle&#x3D;False时采用顺序抽样，</span><br><span class="line">  batch_sampler参数将多个抽样的元素整理成一个列表，一般无需用户设置，</span><br><span class="line">  默认方法在DataLoader的参数drop_last&#x3D;True时会丢弃数据集最后一个长度不能被batch大小整除的批次，</span><br><span class="line">  在drop_last&#x3D;False时保留最后一个批次。</span><br><span class="line">第（3）个步骤的核心逻辑根据下标取数据集中的元素，是由 Dataset 的__getitem__方法实现的。</span><br><span class="line">第（4）个步骤的逻辑由 DataLoader 的参数collate_fn指定。一般情况下也无需用户设置。</span><br></pre></td></tr></table></figure><br>以下是 Dataset和 DataLoader的核心接口逻辑伪代码，不完全和源码一致：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch </span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Dataset</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">raise</span> NotImplementedError</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span>(<span class="params">self,index</span>):</span></span><br><span class="line">        <span class="keyword">raise</span> NotImplementedError</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DataLoader</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self,dataset,batch_size,collate_fn,shuffle = <span class="literal">True</span>,drop_last = <span class="literal">False</span></span>):</span></span><br><span class="line">        self.dataset = dataset</span><br><span class="line">        self.sampler =torch.utils.data.RandomSampler <span class="keyword">if</span> shuffle <span class="keyword">else</span> \</span><br><span class="line">           torch.utils.data.SequentialSampler</span><br><span class="line">        self.batch_sampler = torch.utils.data.BatchSampler</span><br><span class="line">        self.sample_iter = self.batch_sampler(</span><br><span class="line">            self.sampler(<span class="built_in">range</span>(<span class="built_in">len</span>(dataset))),</span><br><span class="line">            batch_size = batch_size,drop_last = drop_last)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__next__</span>(<span class="params">self</span>):</span></span><br><span class="line">        indices = <span class="built_in">next</span>(self.sample_iter)</span><br><span class="line">        batch = self.collate_fn([self.dataset[i] <span class="keyword">for</span> i <span class="keyword">in</span> indices])</span><br><span class="line">        <span class="keyword">return</span> batch</span><br></pre></td></tr></table></figure><br>官网自定义数据集示例：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> torchvision.io <span class="keyword">import</span> read_image</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CustomImageDataset</span>(<span class="params">Dataset</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, annotations_file, img_dir, transform=<span class="literal">None</span>, target_transform=<span class="literal">None</span></span>):</span></span><br><span class="line">        self.img_labels = pd.read_csv(annotations_file) <span class="comment"># 标签地址</span></span><br><span class="line">        self.img_dir = img_dir  <span class="comment"># 文件地址</span></span><br><span class="line">        self.transform = transform  <span class="comment"># 文件预处理函数</span></span><br><span class="line">        self.target_transform = target_transform <span class="comment"># 标签预处理函数</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.img_labels)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span>(<span class="params">self, idx</span>):</span></span><br><span class="line">        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, <span class="number">0</span>])  <span class="comment"># 第0列为地址</span></span><br><span class="line">        image = read_image(img_path)</span><br><span class="line">        label = self.img_labels.iloc[idx, <span class="number">1</span>]  <span class="comment"># 第1列为标签</span></span><br><span class="line">        <span class="keyword">if</span> self.transform:</span><br><span class="line">            image = self.transform(image)</span><br><span class="line">        <span class="keyword">if</span> self.target_transform:</span><br><span class="line">            label = self.target_transform(label)</span><br><span class="line">        <span class="keyword">return</span> image, label</span><br></pre></td></tr></table></figure><br><strong>使用Dataset创建数据集</strong><br>（1）使用 torch.utils.data.TensorDataset 根据Tensor创建数据集（numpy的array，Pandas的DataFrame需要先转换成Tensor）。<br>（2）使用 torchvision.datasets.ImageFolder 根据图片目录创建图片数据集。<br>（3）继承 torch.utils.data.Dataset 创建自定义数据集。<br>torch.utils.data.random_split 将一个数据集分割成多份，常用于分割训练集，验证集和测试集。调用Dataset的加法运算符（+）将多个数据集合并成一个数据集。<br><div class="tabs" id="three"><ul class="nav-tabs"><li class="tab active"><a href="#three-1">根据Tensor创建数据集</a></li><li class="tab"><a href="#three-2">根据图片目录创建图片数据集</a></li><li class="tab"><a href="#three-3">自定义数据集</a></li></ul><div class="tab-content"><div class="tab-pane active" id="three-1"><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np </span><br><span class="line"><span class="keyword">import</span> torch </span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> TensorDataset,Dataset,DataLoader,random_split </span><br><span class="line"></span><br><span class="line"><span class="comment"># 根据Tensor创建数据集</span></span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> datasets </span><br><span class="line">iris = datasets.load_iris()</span><br><span class="line">ds_iris = TensorDataset(torch.tensor(iris.data),torch.tensor(iris.target))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 分割成训练集和预测集</span></span><br><span class="line">n_train = <span class="built_in">int</span>(<span class="built_in">len</span>(ds_iris)*<span class="number">0.8</span>)</span><br><span class="line">n_valid = <span class="built_in">len</span>(ds_iris) - n_train</span><br><span class="line">ds_train,ds_valid = random_split(ds_iris,[n_train,n_valid])</span><br><span class="line"></span><br><span class="line">print(<span class="built_in">type</span>(ds_iris))</span><br><span class="line">print(<span class="built_in">type</span>(ds_train))</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">torch.utils.data.dataset.TensorDataset</span></span><br><span class="line"><span class="string">torch.utils.data.dataset.Subset</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="comment"># 使用DataLoader加载数据集</span></span><br><span class="line">dl_train,dl_valid = DataLoader(ds_train,batch_size=<span class="number">8</span>),DataLoader(ds_valid,batch_size=<span class="number">8</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> features,labels <span class="keyword">in</span> dl_train:</span><br><span class="line">    print(features,labels)</span><br><span class="line">    <span class="keyword">break</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">tensor([[5.4000, 3.4000, 1.5000, 0.4000],</span></span><br><span class="line"><span class="string">        [7.2000, 3.0000, 5.8000, 1.6000],</span></span><br><span class="line"><span class="string">        [5.7000, 2.6000, 3.5000, 1.0000],</span></span><br><span class="line"><span class="string">        [6.1000, 2.6000, 5.6000, 1.4000],</span></span><br><span class="line"><span class="string">        [6.7000, 3.1000, 4.4000, 1.4000],</span></span><br><span class="line"><span class="string">        [5.8000, 2.8000, 5.1000, 2.4000],</span></span><br><span class="line"><span class="string">        [5.6000, 2.5000, 3.9000, 1.1000],</span></span><br><span class="line"><span class="string">        [6.2000, 2.2000, 4.5000, 1.5000]], </span></span><br><span class="line"><span class="string">        dtype=torch.float64) tensor([0, 2, 1, 2, 1, 2, 1, 1], dtype=torch.int32)</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="comment"># 演示加法运算符（`+`）的合并作用</span></span><br><span class="line">ds_data = ds_train + ds_valid</span><br><span class="line"></span><br><span class="line">print(<span class="string">&#x27;len(ds_train) = &#x27;</span>,<span class="built_in">len</span>(ds_train))</span><br><span class="line">print(<span class="string">&#x27;len(ds_valid) = &#x27;</span>,<span class="built_in">len</span>(ds_valid))</span><br><span class="line">print(<span class="string">&#x27;len(ds_train+ds_valid) = &#x27;</span>,<span class="built_in">len</span>(ds_data))</span><br><span class="line">print(<span class="built_in">type</span>(ds_data))</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">len(ds_train) =  120</span></span><br><span class="line"><span class="string">len(ds_valid) =  30</span></span><br><span class="line"><span class="string">len(ds_train+ds_valid) =  150</span></span><br><span class="line"><span class="string">torch.utils.data.dataset.ConcatDataset</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure></div><div class="tab-pane" id="three-2"><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np </span><br><span class="line"><span class="keyword">import</span> torch </span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms,datasets </span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line">img = Image.<span class="built_in">open</span>(<span class="string">&#x27;../data/cat.jpeg&#x27;</span>)</span><br><span class="line">img</span><br><span class="line"></span><br><span class="line"><span class="comment"># 一些常用的图片增强操作</span></span><br><span class="line"><span class="comment">#随机数值翻转</span></span><br><span class="line">transforms.RandomVerticalFlip()(img)</span><br><span class="line"><span class="comment">#随机旋转</span></span><br><span class="line">transforms.RandomRotation(<span class="number">45</span>)(img)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 自定义图片增强操作</span></span><br><span class="line">transform_train = transforms.Compose([</span><br><span class="line">   transforms.RandomHorizontalFlip(), <span class="comment">#随机水平翻转</span></span><br><span class="line">   transforms.RandomVerticalFlip(), <span class="comment">#随机垂直翻转</span></span><br><span class="line">   transforms.RandomRotation(<span class="number">45</span>),  <span class="comment">#随机在45度角度内旋转</span></span><br><span class="line">   transforms.ToTensor() <span class="comment">#转换成张量</span></span><br><span class="line">  ]</span><br><span class="line">) </span><br><span class="line"><span class="comment"># 验证集不需要增强</span></span><br><span class="line">transform_valid = transforms.Compose([</span><br><span class="line">    transforms.ToTensor()</span><br><span class="line">  ]</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 根据图片目录创建数据集</span></span><br><span class="line">ds_train = datasets.ImageFolder(<span class="string">&quot;../data/cifar2/train/&quot;</span>,</span><br><span class="line">            transform = transform_train,target_transform= <span class="keyword">lambda</span> t:torch.tensor([t]).<span class="built_in">float</span>())</span><br><span class="line">ds_valid = datasets.ImageFolder(<span class="string">&quot;../data/cifar2/test/&quot;</span>,</span><br><span class="line">            transform = transform_train,target_transform= <span class="keyword">lambda</span> t:torch.tensor([t]).<span class="built_in">float</span>())</span><br><span class="line"></span><br><span class="line">print(ds_train.class_to_idx)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">&#123;&#x27;0_airplane&#x27;: 0, &#x27;1_automobile&#x27;: 1&#125;</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="comment"># 使用DataLoader加载数据集</span></span><br><span class="line">dl_train = DataLoader(ds_train,batch_size = <span class="number">50</span>,shuffle = <span class="literal">True</span>,num_workers=<span class="number">3</span>)</span><br><span class="line">dl_valid = DataLoader(ds_valid,batch_size = <span class="number">50</span>,shuffle = <span class="literal">True</span>,num_workers=<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> features,labels <span class="keyword">in</span> dl_train:</span><br><span class="line">    print(features.shape)</span><br><span class="line">    print(labels.shape)</span><br><span class="line">    <span class="keyword">break</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">torch.Size([50, 3, 32, 32])</span></span><br><span class="line"><span class="string">torch.Size([50, 1])</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure></div><div class="tab-pane" id="three-3"><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 首先，对训练集文本分词构建词典。然后将训练集文本和测试集文本数据转换成token单词编码。</span></span><br><span class="line"><span class="comment"># 接着将转换成单词编码的训练集数据和测试集数据按样本分割成多个文件，一个文件代表一个样本。</span></span><br><span class="line"><span class="comment"># 最后，我们可以根据文件名列表获取对应序号的样本内容，从而构建Dataset数据集。</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np </span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd </span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> OrderedDict</span><br><span class="line"><span class="keyword">import</span> re,string</span><br><span class="line"></span><br><span class="line">MAX_WORDS = <span class="number">10000</span>  <span class="comment"># 仅考虑最高频的10000个词</span></span><br><span class="line">MAX_LEN = <span class="number">200</span>  <span class="comment"># 每个样本保留200个词的长度</span></span><br><span class="line">BATCH_SIZE = <span class="number">20</span> </span><br><span class="line"></span><br><span class="line">train_data_path = <span class="string">&#x27;data/imdb/train.tsv&#x27;</span></span><br><span class="line">test_data_path = <span class="string">&#x27;data/imdb/test.tsv&#x27;</span></span><br><span class="line">train_token_path = <span class="string">&#x27;data/imdb/train_token.tsv&#x27;</span></span><br><span class="line">test_token_path =  <span class="string">&#x27;data/imdb/test_token.tsv&#x27;</span></span><br><span class="line">train_samples_path = <span class="string">&#x27;data/imdb/train_samples/&#x27;</span></span><br><span class="line">test_samples_path =  <span class="string">&#x27;data/imdb/test_samples/&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 构建词典，并保留最高频的MAX_WORDS个词</span></span><br><span class="line"><span class="comment">#构建词典</span></span><br><span class="line">word_count_dict = &#123;&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">#清洗文本</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">clean_text</span>(<span class="params">text</span>):</span></span><br><span class="line">    lowercase = text.lower().replace(<span class="string">&quot;\n&quot;</span>,<span class="string">&quot; &quot;</span>)</span><br><span class="line">    stripped_html = re.sub(<span class="string">&#x27;&lt;br /&gt;&#x27;</span>, <span class="string">&#x27; &#x27;</span>,lowercase)</span><br><span class="line">    cleaned_punctuation = re.sub(<span class="string">&#x27;[%s]&#x27;</span>%re.escape(string.punctuation),<span class="string">&#x27;&#x27;</span>,stripped_html)</span><br><span class="line">    <span class="keyword">return</span> cleaned_punctuation</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(train_data_path,<span class="string">&quot;r&quot;</span>,encoding = <span class="string">&#x27;utf-8&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> f:</span><br><span class="line">        label,text = line.split(<span class="string">&quot;\t&quot;</span>)</span><br><span class="line">        cleaned_text = clean_text(text)</span><br><span class="line">        <span class="keyword">for</span> word <span class="keyword">in</span> cleaned_text.split(<span class="string">&quot; &quot;</span>):</span><br><span class="line">            word_count_dict[word] = word_count_dict.get(word,<span class="number">0</span>)+<span class="number">1</span> </span><br><span class="line"></span><br><span class="line">df_word_dict = pd.DataFrame(pd.Series(word_count_dict,name = <span class="string">&quot;count&quot;</span>))</span><br><span class="line">df_word_dict = df_word_dict.sort_values(by = <span class="string">&quot;count&quot;</span>,ascending =<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">df_word_dict = df_word_dict[<span class="number">0</span>:MAX_WORDS<span class="number">-2</span>]  </span><br><span class="line">df_word_dict[<span class="string">&quot;word_id&quot;</span>] = <span class="built_in">range</span>(<span class="number">2</span>,MAX_WORDS) <span class="comment"># 编号0和1分别留给未知词&lt;unkown&gt;和填充&lt;padding&gt;</span></span><br><span class="line"></span><br><span class="line">word_id_dict = df_word_dict[<span class="string">&quot;word_id&quot;</span>].to_dict()</span><br><span class="line"></span><br><span class="line">df_word_dict.head(<span class="number">10</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 利用构建好的词典，将文本转换成token序号</span></span><br><span class="line"><span class="comment">#转换token, 填充文本</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">pad</span>(<span class="params">data_list,pad_length</span>):</span></span><br><span class="line">    padded_list = data_list.copy()</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(data_list)&gt; pad_length:</span><br><span class="line">         padded_list = data_list[-pad_length:]</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(data_list)&lt; pad_length:</span><br><span class="line">         padded_list = [<span class="number">1</span>]*(pad_length-<span class="built_in">len</span>(data_list))+data_list</span><br><span class="line">    <span class="keyword">return</span> padded_list</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">text_to_token</span>(<span class="params">text_file,token_file</span>):</span></span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(text_file,<span class="string">&quot;r&quot;</span>,encoding = <span class="string">&#x27;utf-8&#x27;</span>) <span class="keyword">as</span> fin,\</span><br><span class="line">      <span class="built_in">open</span>(token_file,<span class="string">&quot;w&quot;</span>,encoding = <span class="string">&#x27;utf-8&#x27;</span>) <span class="keyword">as</span> fout:</span><br><span class="line">        <span class="keyword">for</span> line <span class="keyword">in</span> fin:</span><br><span class="line">            label,text = line.split(<span class="string">&quot;\t&quot;</span>)</span><br><span class="line">            cleaned_text = clean_text(text)</span><br><span class="line">            word_token_list = [word_id_dict.get(word, <span class="number">0</span>) <span class="keyword">for</span> word <span class="keyword">in</span> cleaned_text.split(<span class="string">&quot; &quot;</span>)]</span><br><span class="line">            pad_list = pad(word_token_list,MAX_LEN)</span><br><span class="line">            out_line = label+<span class="string">&quot;\t&quot;</span>+<span class="string">&quot; &quot;</span>.join([<span class="built_in">str</span>(x) <span class="keyword">for</span> x <span class="keyword">in</span> pad_list])</span><br><span class="line">            fout.write(out_line+<span class="string">&quot;\n&quot;</span>)</span><br><span class="line"></span><br><span class="line">text_to_token(train_data_path,train_token_path)</span><br><span class="line">text_to_token(test_data_path,test_token_path)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 接着将token文本按照样本分割，每个文件存放一个样本的数据</span></span><br><span class="line"><span class="comment">#分割样本</span></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(train_samples_path):</span><br><span class="line">    os.mkdir(train_samples_path)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(test_samples_path):</span><br><span class="line">    os.mkdir(test_samples_path)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">split_samples</span>(<span class="params">token_path,samples_dir</span>):</span></span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(token_path,<span class="string">&quot;r&quot;</span>, encoding=<span class="string">&#x27;utf-8&#x27;</span>) <span class="keyword">as</span> fin:</span><br><span class="line">        i = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> line <span class="keyword">in</span> fin:</span><br><span class="line">            <span class="keyword">with</span> <span class="built_in">open</span>(samples_dir+<span class="string">&quot;%d.txt&quot;</span>%i,<span class="string">&quot;w&quot;</span>, encoding=<span class="string">&quot;utf-8&quot;</span>) <span class="keyword">as</span> fout:</span><br><span class="line">                fout.write(line)</span><br><span class="line">            i = i+<span class="number">1</span></span><br><span class="line"></span><br><span class="line">split_samples(train_token_path,train_samples_path)</span><br><span class="line">split_samples(test_token_path,test_samples_path)</span><br><span class="line"></span><br><span class="line">print(os.listdir(train_samples_path)[<span class="number">0</span>:<span class="number">3</span>])</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">[&#x27;11303.txt&#x27;, &#x27;3644.txt&#x27;, &#x27;19987.txt&#x27;]</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="comment"># 创建数据集Dataset, 从文件名称列表中读取文件内容了</span></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">imdbDataset</span>(<span class="params">Dataset</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self,samples_dir</span>):</span></span><br><span class="line">        self.samples_dir = samples_dir</span><br><span class="line">        self.samples_paths = os.listdir(samples_dir)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.samples_paths)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span>(<span class="params">self,index</span>):</span></span><br><span class="line">        path = self.samples_dir + self.samples_paths[index]</span><br><span class="line">        <span class="keyword">with</span> <span class="built_in">open</span>(path,<span class="string">&quot;r&quot;</span>,encoding=<span class="string">&quot;utf-8&quot;</span>) <span class="keyword">as</span> f:</span><br><span class="line">            line = f.readline()</span><br><span class="line">            label,tokens = line.split(<span class="string">&quot;\t&quot;</span>)</span><br><span class="line">            label = torch.tensor([<span class="built_in">float</span>(label)], dtype=torch.<span class="built_in">float</span>)</span><br><span class="line">            feature = torch.tensor([<span class="built_in">int</span>(x) <span class="keyword">for</span> x <span class="keyword">in</span> tokens.split(<span class="string">&quot; &quot;</span>)], dtype=torch.long)</span><br><span class="line">            <span class="keyword">return</span>  (feature,label)</span><br><span class="line"></span><br><span class="line">ds_train = imdbDataset(train_samples_path)</span><br><span class="line">ds_test = imdbDataset(test_samples_path)</span><br><span class="line"></span><br><span class="line">print(<span class="built_in">len</span>(ds_train))</span><br><span class="line">print(<span class="built_in">len</span>(ds_test))</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">20000</span></span><br><span class="line"><span class="string">5000</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">dl_train = DataLoader(ds_train, batch_size=BATCH_SIZE, shuffle=<span class="literal">True</span>, num_workers=<span class="number">4</span>)</span><br><span class="line">dl_test = DataLoader(ds_test, batch_size=BATCH_SIZE, num_workers=<span class="number">4</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> features,labels <span class="keyword">in</span> dl_train:</span><br><span class="line">    print(features)</span><br><span class="line">    print(labels)</span><br><span class="line">    <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 最后构建模型测试一下数据集管道是否可用</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn </span><br><span class="line"><span class="keyword">import</span> importlib </span><br><span class="line"><span class="keyword">from</span> torchkeras <span class="keyword">import</span> Model,summary</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Net</span>(<span class="params">Model</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(Net, self).__init__()</span><br><span class="line"></span><br><span class="line">        <span class="comment">#设置padding_idx参数后将在训练过程中将填充的token始终赋值为0向量</span></span><br><span class="line">        self.embedding = nn.Embedding(num_embeddings=MAX_WORDS, embedding_dim=<span class="number">3</span>,padding_idx=<span class="number">1</span>)</span><br><span class="line">        self.conv = nn.Sequential()</span><br><span class="line">        self.conv.add_module(<span class="string">&quot;conv_1&quot;</span>,nn.Conv1d(in_channels=<span class="number">3</span>, out_channels=<span class="number">16</span>, kernel_size=<span class="number">5</span>))</span><br><span class="line">        self.conv.add_module(<span class="string">&quot;pool_1&quot;</span>,nn.MaxPool1d(kernel_size=<span class="number">2</span>))</span><br><span class="line">        self.conv.add_module(<span class="string">&quot;relu_1&quot;</span>,nn.ReLU())</span><br><span class="line">        self.conv.add_module(<span class="string">&quot;conv_2&quot;</span>,nn.Conv1d(in_channels=<span class="number">16</span>, out_channels=<span class="number">128</span>, kernel_size=<span class="number">2</span>))</span><br><span class="line">        self.conv.add_module(<span class="string">&quot;pool_2&quot;</span>,nn.MaxPool1d(kernel_size = <span class="number">2</span>))</span><br><span class="line">        self.conv.add_module(<span class="string">&quot;relu_2&quot;</span>,nn.ReLU())</span><br><span class="line"></span><br><span class="line">        self.dense = nn.Sequential()</span><br><span class="line">        self.dense.add_module(<span class="string">&quot;flatten&quot;</span>,nn.Flatten())</span><br><span class="line">        self.dense.add_module(<span class="string">&quot;linear&quot;</span>,nn.Linear(<span class="number">6144</span>,<span class="number">1</span>))</span><br><span class="line">        self.dense.add_module(<span class="string">&quot;sigmoid&quot;</span>,nn.Sigmoid())</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self,x</span>):</span></span><br><span class="line">        x = self.embedding(x).transpose(<span class="number">1</span>,<span class="number">2</span>)</span><br><span class="line">        x = self.conv(x)</span><br><span class="line">        y = self.dense(x)</span><br><span class="line">        <span class="keyword">return</span> y</span><br><span class="line"></span><br><span class="line">model = Net()</span><br><span class="line">print(model)</span><br><span class="line"></span><br><span class="line">model.summary(input_shape=(<span class="number">200</span>,),input_dtype=torch.LongTensor)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">Net(</span></span><br><span class="line"><span class="string">  (embedding): Embedding(10000, 3, padding_idx=1)</span></span><br><span class="line"><span class="string">  (conv): Sequential(</span></span><br><span class="line"><span class="string">    (conv_1): Conv1d(3, 16, kernel_size=(5,), stride=(1,))</span></span><br><span class="line"><span class="string">    (pool_1): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)</span></span><br><span class="line"><span class="string">    (relu_1): ReLU()</span></span><br><span class="line"><span class="string">    (conv_2): Conv1d(16, 128, kernel_size=(2,), stride=(1,))</span></span><br><span class="line"><span class="string">    (pool_2): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)</span></span><br><span class="line"><span class="string">    (relu_2): ReLU()</span></span><br><span class="line"><span class="string">  )</span></span><br><span class="line"><span class="string">  (dense): Sequential(</span></span><br><span class="line"><span class="string">    (flatten): Flatten()</span></span><br><span class="line"><span class="string">    (linear): Linear(in_features=6144, out_features=1, bias=True)</span></span><br><span class="line"><span class="string">    (sigmoid): Sigmoid()</span></span><br><span class="line"><span class="string">  )</span></span><br><span class="line"><span class="string">)</span></span><br><span class="line"><span class="string">----------------------------------------------------------------</span></span><br><span class="line"><span class="string">        Layer (type)               Output Shape         Param #</span></span><br><span class="line"><span class="string">================================================================</span></span><br><span class="line"><span class="string">         Embedding-1               [-1, 200, 3]          30,000</span></span><br><span class="line"><span class="string">            Conv1d-2              [-1, 16, 196]             256</span></span><br><span class="line"><span class="string">         MaxPool1d-3               [-1, 16, 98]               0</span></span><br><span class="line"><span class="string">              ReLU-4               [-1, 16, 98]               0</span></span><br><span class="line"><span class="string">            Conv1d-5              [-1, 128, 97]           4,224</span></span><br><span class="line"><span class="string">         MaxPool1d-6              [-1, 128, 48]               0</span></span><br><span class="line"><span class="string">              ReLU-7              [-1, 128, 48]               0</span></span><br><span class="line"><span class="string">           Flatten-8                 [-1, 6144]               0</span></span><br><span class="line"><span class="string">            Linear-9                    [-1, 1]           6,145</span></span><br><span class="line"><span class="string">          Sigmoid-10                    [-1, 1]               0</span></span><br><span class="line"><span class="string">================================================================</span></span><br><span class="line"><span class="string">Total params: 40,625</span></span><br><span class="line"><span class="string">Trainable params: 40,625</span></span><br><span class="line"><span class="string">Non-trainable params: 0</span></span><br><span class="line"><span class="string">----------------------------------------------------------------</span></span><br><span class="line"><span class="string">Input size (MB): 0.000763</span></span><br><span class="line"><span class="string">Forward/backward pass size (MB): 0.287796</span></span><br><span class="line"><span class="string">Params size (MB): 0.154972</span></span><br><span class="line"><span class="string">Estimated Total Size (MB): 0.443531</span></span><br><span class="line"><span class="string">----------------------------------------------------------------</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="comment"># 编译模型</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">accuracy</span>(<span class="params">y_pred,y_true</span>):</span></span><br><span class="line">    y_pred = torch.where(y_pred&gt;<span class="number">0.5</span>,torch.ones_like(y_pred, dtype=torch.float32),</span><br><span class="line">                      torch.zeros_like(y_pred,dtype=torch.float32))</span><br><span class="line">    acc = torch.mean(<span class="number">1</span>-torch.<span class="built_in">abs</span>(y_true-y_pred))</span><br><span class="line">    <span class="keyword">return</span> acc</span><br><span class="line"></span><br><span class="line">model.<span class="built_in">compile</span>(loss_func = nn.BCELoss(),optimizer= torch.optim.Adagrad(model.parameters(),lr=<span class="number">0.02</span>),</span><br><span class="line">             metrics_dict=&#123;<span class="string">&quot;accuracy&quot;</span>:accuracy&#125;)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练模型</span></span><br><span class="line">dfhistory = model.fit(<span class="number">10</span>,dl_train,dl_val=dl_test,log_step_freq=<span class="number">200</span>)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">Start Training ...</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">================================================================================2020-07-11 23:21:53</span></span><br><span class="line"><span class="string">&#123;&#x27;step&#x27;: 200, &#x27;loss&#x27;: 0.956, &#x27;accuracy&#x27;: 0.521&#125;</span></span><br><span class="line"><span class="string">&#123;&#x27;step&#x27;: 400, &#x27;loss&#x27;: 0.823, &#x27;accuracy&#x27;: 0.53&#125;</span></span><br><span class="line"><span class="string">&#123;&#x27;step&#x27;: 600, &#x27;loss&#x27;: 0.774, &#x27;accuracy&#x27;: 0.545&#125;</span></span><br><span class="line"><span class="string">&#123;&#x27;step&#x27;: 800, &#x27;loss&#x27;: 0.747, &#x27;accuracy&#x27;: 0.56&#125;</span></span><br><span class="line"><span class="string">&#123;&#x27;step&#x27;: 1000, &#x27;loss&#x27;: 0.726, &#x27;accuracy&#x27;: 0.572&#125;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"> +-------+-------+----------+----------+--------------+</span></span><br><span class="line"><span class="string">| epoch |  loss | accuracy | val_loss | val_accuracy |</span></span><br><span class="line"><span class="string">+-------+-------+----------+----------+--------------+</span></span><br><span class="line"><span class="string">|   1   | 0.726 |  0.572   |  0.661   |    0.613     |</span></span><br><span class="line"><span class="string">+-------+-------+----------+----------+--------------+</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">================================================================================2020-07-11 23:22:20</span></span><br><span class="line"><span class="string">&#123;&#x27;step&#x27;: 200, &#x27;loss&#x27;: 0.605, &#x27;accuracy&#x27;: 0.668&#125;</span></span><br><span class="line"><span class="string">&#123;&#x27;step&#x27;: 400, &#x27;loss&#x27;: 0.602, &#x27;accuracy&#x27;: 0.674&#125;</span></span><br><span class="line"><span class="string">&#123;&#x27;step&#x27;: 600, &#x27;loss&#x27;: 0.592, &#x27;accuracy&#x27;: 0.681&#125;</span></span><br><span class="line"><span class="string">&#123;&#x27;step&#x27;: 800, &#x27;loss&#x27;: 0.584, &#x27;accuracy&#x27;: 0.687&#125;</span></span><br><span class="line"><span class="string">&#123;&#x27;step&#x27;: 1000, &#x27;loss&#x27;: 0.575, &#x27;accuracy&#x27;: 0.696&#125;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"> +-------+-------+----------+----------+--------------+</span></span><br><span class="line"><span class="string">| epoch |  loss | accuracy | val_loss | val_accuracy |</span></span><br><span class="line"><span class="string">+-------+-------+----------+----------+--------------+</span></span><br><span class="line"><span class="string">|   2   | 0.575 |  0.696   |  0.553   |    0.716     |</span></span><br><span class="line"><span class="string">+-------+-------+----------+----------+--------------+</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">================================================================================2020-07-11 23:25:53</span></span><br><span class="line"><span class="string">&#123;&#x27;step&#x27;: 200, &#x27;loss&#x27;: 0.294, &#x27;accuracy&#x27;: 0.877&#125;</span></span><br><span class="line"><span class="string">&#123;&#x27;step&#x27;: 400, &#x27;loss&#x27;: 0.299, &#x27;accuracy&#x27;: 0.875&#125;</span></span><br><span class="line"><span class="string">&#123;&#x27;step&#x27;: 600, &#x27;loss&#x27;: 0.298, &#x27;accuracy&#x27;: 0.875&#125;</span></span><br><span class="line"><span class="string">&#123;&#x27;step&#x27;: 800, &#x27;loss&#x27;: 0.296, &#x27;accuracy&#x27;: 0.876&#125;</span></span><br><span class="line"><span class="string">&#123;&#x27;step&#x27;: 1000, &#x27;loss&#x27;: 0.298, &#x27;accuracy&#x27;: 0.875&#125;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"> +-------+-------+----------+----------+--------------+</span></span><br><span class="line"><span class="string">| epoch |  loss | accuracy | val_loss | val_accuracy |</span></span><br><span class="line"><span class="string">+-------+-------+----------+----------+--------------+</span></span><br><span class="line"><span class="string">|   10  | 0.298 |  0.875   |  0.464   |    0.795     |</span></span><br><span class="line"><span class="string">+-------+-------+----------+----------+--------------+</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">================================================================================2020-07-11 23:26:19</span></span><br><span class="line"><span class="string">Finished Training...</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure></div></div></div></p>
<p><strong>使用DataLoader加载数据集</strong><br>DataLoader能够控制batch的大小，batch中元素的采样方法，以及将batch结果整理成模型所需输入形式的方法，并且能够使用多进程读取数据。<br>DataLoader的函数签名如下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">DataLoader(</span><br><span class="line">    dataset,  <span class="comment"># 数据集</span></span><br><span class="line">    batch_size=<span class="number">1</span>,  <span class="comment"># 批次大小</span></span><br><span class="line">    shuffle=<span class="literal">False</span>,  <span class="comment"># 是否乱序，shuffle=True和sampler和batch_sampler自定义会冲突。</span></span><br><span class="line">    sampler=<span class="literal">None</span>,  <span class="comment"># 样本采样函数，一般无需设置。</span></span><br><span class="line">    batch_sampler=<span class="literal">None</span>,  <span class="comment"># 批次采样后处理函数,一般无需设置.若设置了那么batch_size,shuffle,sampler,drop_last都不用设置了。</span></span><br><span class="line">    num_workers=<span class="number">0</span>,  <span class="comment"># 使用多进程读取数据，设置的进程数。</span></span><br><span class="line">    collate_fn=<span class="literal">None</span>,  <span class="comment"># 处理一个批次数据的函数,对采样完的每一个batch数据进行处理,输入batch输出batch.常用于padding等操作。</span></span><br><span class="line">    pin_memory=<span class="literal">False</span>,  <span class="comment"># 是否设置为锁业内存。默认为False,锁业内存不会使用虚拟内存(硬盘),从锁业内存拷贝到GPU上速度会更快。</span></span><br><span class="line">    drop_last=<span class="literal">False</span>,  <span class="comment"># 是否丢弃最后一个样本数量不足batch_size批次数据。</span></span><br><span class="line">    timeout=<span class="number">0</span>,  <span class="comment"># 加载一个数据批次的最长等待时间，一般无需设置。</span></span><br><span class="line">    worker_init_fn=<span class="literal">None</span>,  <span class="comment"># 每个worker中dataset的初始化函数，常用于 IterableDataset。一般不使用。</span></span><br><span class="line">    multiprocessing_context=<span class="literal">None</span>,</span><br><span class="line">)</span><br><span class="line"><span class="comment"># 一般情况下，我们仅仅会配置 dataset, batch_size, shuffle, num_workers, drop_last这五个参数，其他参数使用默认值即可。</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># sampler源码中使用了RandomSample(shuffle=Ture时)和SequentialSample：</span></span><br><span class="line">  <span class="comment"># RandomSample中使用了torch.randperm方法。</span></span><br><span class="line">  <span class="comment"># SequentialSample直接是iter(range(len(self.data.source)))。</span></span><br><span class="line"><span class="comment"># batch_size!=None and batch_sampler==None时用BatchSampler(sample,batch_size,drop_last)。</span></span><br><span class="line"><span class="comment"># collate_fn=None时默认会用default_conllate(batch)，它可以看作什么也没处理。如果自定义要遵循输入batch输出batch的格式。</span></span><br><span class="line"><span class="comment"># 支持迭代方式，__iter__中调用了self._get_iterator(),它又调用了_SingleProcessDataLoaderIter(当num_workers=0时),</span></span><br><span class="line">  <span class="comment"># 多进程用_MultiProcessingDataLoaderIter，不详细介绍此方法了。</span></span><br><span class="line">  <span class="comment"># _SingleProcessDataLoaderIter定义了_next_data方法,调用了_DatasetKind.create_fetcher(...).fetch(index)获取batch数据。</span></span><br><span class="line">  <span class="comment"># _SingleProcessDataLoaderIter继承自_BaseDataLoaderIter类，该类定义了__next__方法调用了_next_data方法，</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><br>两种加载数据方式：DataLoader除了可以加载 torch.utils.data.Dataset(官网叫<span class="exturl" data-url="aHR0cHM6Ly9weXRvcmNoLm9yZy9kb2NzL3N0YWJsZS9kYXRhLmh0bWwjbWFwLXN0eWxlLWRhdGFzZXRz">map-style datasets<i class="fa fa-external-link-alt"></i></span>) 外，还能够加载另外一种数据集 torch.utils.data.IterableDataset(官网叫<span class="exturl" data-url="aHR0cHM6Ly9weXRvcmNoLm9yZy9kb2NzL3N0YWJsZS9kYXRhLmh0bWwjaXRlcmFibGUtc3R5bGUtZGF0YXNldHM=">iterable-style datasets<i class="fa fa-external-link-alt"></i></span>)。和Dataset数据集相当于一种列表结构不同，IterableDataset相当于一种迭代器结构， 它更加复杂，多用于流式读取数据，一般较少使用。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#构建输入数据管道</span></span><br><span class="line">ds = TensorDataset(torch.arange(<span class="number">1</span>,<span class="number">50</span>))</span><br><span class="line">dl = DataLoader(ds, batch_size=<span class="number">10</span>, shuffle=<span class="literal">True</span>, num_workers=<span class="number">2</span>, drop_last=<span class="literal">True</span>)</span><br><span class="line"><span class="comment">#迭代数据</span></span><br><span class="line"><span class="keyword">for</span> batch, <span class="keyword">in</span> dl:</span><br><span class="line">    print(batch)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">tensor([43, 44, 21, 36,  9,  5, 28, 16, 20, 14])</span></span><br><span class="line"><span class="string">tensor([23, 49, 35, 38,  2, 34, 45, 18, 15, 40])</span></span><br><span class="line"><span class="string">tensor([26,  6, 27, 39,  8,  4, 24, 19, 32, 17])</span></span><br><span class="line"><span class="string">tensor([ 1, 29, 11, 47, 12, 22, 48, 42, 10,  7])</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure></p>
<h1 id="nn-functional和-nn-Module"><a href="#nn-functional和-nn-Module" class="headerlink" title="nn.functional和 nn.Module"></a>nn.functional和 nn.Module</h1><p>Pytorch和神经网络相关的功能组件大多都封装在torch.nn模块下。这些功能组件的绝大部分既有函数形式实现，也有类形式实现。其中nn.functional（一般引入后改名为F）有各种功能组件的函数实现。为了便于对参数进行管理，一般通过继承 nn.Module 转换成为类的实现形式，并直接封装在 nn 模块下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 激活函数</span></span><br><span class="line">F.relu/nn.ReLU</span><br><span class="line">F.sigmoid/nn.Sigmoid</span><br><span class="line">F.tanh/nn.Tanh</span><br><span class="line">F.softmax/nn.Softmax</span><br><span class="line"></span><br><span class="line"><span class="comment"># 模型层</span></span><br><span class="line">F.linear/nn.Linear</span><br><span class="line">F.conv2d/nn.Conv2d</span><br><span class="line">F.max_pool2d/nn.MaxPool2d</span><br><span class="line">F.dropout2d/nn.Dropout2d</span><br><span class="line">F.embedding/nn.Embedding</span><br><span class="line"></span><br><span class="line"><span class="comment"># 损失函数</span></span><br><span class="line">F.binary_cross_entropy/nn.BCELoss</span><br><span class="line">F.mse_loss/nn.MSELoss</span><br><span class="line">F.cross_entropy/nn.CrossEntropyLoss</span><br></pre></td></tr></table></figure><br><strong>nn.Modules类的方法</strong>：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># nn.Modules类是一切模块的基类，创建层或者网络结构都要继承此类。</span></span><br><span class="line">add_module(name, module) <span class="comment"># 添加子模块。</span></span><br><span class="line">apply(fn) <span class="comment"># 对子模块(.children()返回的模块)应用fn函数，可用于初始化。</span></span><br><span class="line"><span class="comment"># fn实际上也作用于parameter和buffer上了。</span></span><br><span class="line"></span><br><span class="line">children() <span class="comment"># 返回所有子模块(迭代器)。</span></span><br><span class="line">modules() <span class="comment"># 返回所有层级模块(迭代器)，包括本身。</span></span><br><span class="line"><span class="comment"># 可用._moudles返回字典。</span></span><br><span class="line">parameters(recurse=<span class="literal">True</span>) <span class="comment"># 返回所有参数(可反向传播)(迭代器)。</span></span><br><span class="line"><span class="comment"># ._parameters只提供模型中显示创建的Parameter类变量(不会遍历子模块)，空字典不代表模型没有参数。</span></span><br><span class="line">buffers(recurse=<span class="literal">True</span>) <span class="comment"># 类似tf.constant，存储常量。Parameter和buffer关系：https://zhuanlan.zhihu.com/p/89442276</span></span><br><span class="line"><span class="comment"># ._buffers和._parameters一样，只提供模型中显示创建的buffers(不会遍历子模块)。</span></span><br><span class="line"></span><br><span class="line">named_children() <span class="comment"># 返回所有子模块名称和值。</span></span><br><span class="line">named_modules(memo=<span class="literal">None</span>, prefix=<span class="string">&#x27;&#x27;</span>, remove_duplicate=<span class="literal">True</span>) <span class="comment"># 返回所有层级模块名称和值，包括本身。</span></span><br><span class="line">named_parameters(prefix=<span class="string">&#x27;&#x27;</span>, recurse=<span class="literal">True</span>) <span class="comment"># 返回所有参数名称和值。</span></span><br><span class="line">named_buffers(prefix=<span class="string">&#x27;&#x27;</span>, recurse=<span class="literal">True</span>) <span class="comment"># 返回所有buffer名称和值。</span></span><br><span class="line"></span><br><span class="line">register_buffer(name, tensor, persistent=<span class="literal">True</span>) <span class="comment"># 注册buffer到model。</span></span><br><span class="line"><span class="comment"># persistent是否作为参数保存到磁盘上，在torch.save中调用model.state_dict()生效。</span></span><br><span class="line">register_parameter(name, param) <span class="comment"># 注册parameter到model。</span></span><br><span class="line"><span class="comment"># param必须是torch.nn.parameter.Parameter类的实例。</span></span><br><span class="line"></span><br><span class="line">get_buffer(target) <span class="comment"># 获取buffer，target为目标名称(字符串)。</span></span><br><span class="line">get_parameter(target) <span class="comment"># 获取parameter，target为目标名称(字符串)。</span></span><br><span class="line">get_submodule(target) <span class="comment"># 获取子模块，target为目标名称(字符串)。</span></span><br><span class="line"></span><br><span class="line">cpu() <span class="comment"># 模型(parameters and buffers)放入cpu运行。</span></span><br><span class="line">cuda(device=<span class="literal">None</span>) <span class="comment"># 模型(parameters and buffers)放入gpu运行。</span></span><br><span class="line"><span class="built_in">eval</span>() <span class="comment"># 设置成验证模式，不进行梯度计算。 </span></span><br><span class="line">train(mode=<span class="literal">True</span>) <span class="comment"># 设置成训练模式。</span></span><br><span class="line"></span><br><span class="line">load_state_dict(state_dict, strict=<span class="literal">True</span>) <span class="comment"># 加载parameter和buffer到模型中。</span></span><br><span class="line">state_dict(destination=<span class="literal">None</span>, prefix=<span class="string">&#x27;&#x27;</span>, keep_vars=<span class="literal">False</span>) <span class="comment"># 返回模型所有参数，字典格式。</span></span><br><span class="line">requires_grad_(requires_grad=<span class="literal">True</span>) <span class="comment"># 参数是否需要梯度更新？</span></span><br><span class="line">zero_grad(set_to_none=<span class="literal">False</span>) <span class="comment"># 参数梯度清零。</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 以下转换都是针对parameter和buffer。</span></span><br><span class="line">bfloat16() <span class="comment"># float转换成bfloat16(只对float有效)。</span></span><br><span class="line">half() <span class="comment"># float转换成半精度float(只对float有效)。</span></span><br><span class="line"><span class="built_in">float</span>() <span class="comment"># float转换成float(只对float有效)。</span></span><br><span class="line">double() <span class="comment"># # float转换成双精度(只对float有效)。</span></span><br><span class="line">to_empty(*, device) <span class="comment"># 拷贝parameter和buffer的空张量，到指定设备。</span></span><br><span class="line">to(*args, **kwargs)：<span class="comment"># 转换，多种选择。</span></span><br><span class="line">    <span class="comment">#to(device=None, dtype=None, non_blocking=False)</span></span><br><span class="line">    <span class="comment">#to(dtype, non_blocking=False)</span></span><br><span class="line">    <span class="comment">#to(tensor, non_blocking=False)</span></span><br><span class="line">    <span class="comment">#to(memory_format=torch.channels_last)</span></span><br></pre></td></tr></table></figure><br><strong>使用nn.Module来管理参数</strong>：<br>在Pytorch中，模型的参数是需要被优化器训练的，因此，通常要设置参数为 requires_grad=True 的张量。<br>同时，在一个模型中，往往有许多的参数，要手动管理这些参数并不是一件容易的事情。<br>Pytorch一般将参数用nn.Parameter来表示，并且用nn.Module来管理其结构下的所有参数。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch </span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn </span><br><span class="line"><span class="keyword">import</span> torch.nn.functional  <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment"># nn.Parameter 具有 requires_grad=True 属性</span></span><br><span class="line">w = nn.Parameter(torch.randn(<span class="number">2</span>,<span class="number">2</span>))</span><br><span class="line">print(w)</span><br><span class="line">print(w.requires_grad)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">Parameter containing:</span></span><br><span class="line"><span class="string">tensor([[ 0.3544, -1.1643],</span></span><br><span class="line"><span class="string">        [ 1.2302,  1.3952]], requires_grad=True)</span></span><br><span class="line"><span class="string">True</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="comment"># nn.ParameterList 可以将多个nn.Parameter组成一个列表</span></span><br><span class="line">params_list = nn.ParameterList([nn.Parameter(torch.rand(<span class="number">2</span>,i)) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>,<span class="number">3</span>)])</span><br><span class="line">print(params_list)</span><br><span class="line">print(params_list[<span class="number">0</span>].requires_grad)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">ParameterList(</span></span><br><span class="line"><span class="string">    (0): Parameter containing: [torch.FloatTensor of size 2x1]</span></span><br><span class="line"><span class="string">    (1): Parameter containing: [torch.FloatTensor of size 2x2]</span></span><br><span class="line"><span class="string">)</span></span><br><span class="line"><span class="string">True</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="comment"># nn.ParameterDict 可以将多个nn.Parameter组成一个字典</span></span><br><span class="line">params_dict = nn.ParameterDict(&#123;<span class="string">&quot;a&quot;</span>:nn.Parameter(torch.rand(<span class="number">2</span>,<span class="number">2</span>)),</span><br><span class="line">                                <span class="string">&quot;b&quot;</span>:nn.Parameter(torch.zeros(<span class="number">2</span>))&#125;)</span><br><span class="line">print(params_dict)</span><br><span class="line">print(params_dict[<span class="string">&quot;a&quot;</span>].requires_grad)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">ParameterDict(</span></span><br><span class="line"><span class="string">    (a): Parameter containing: [torch.FloatTensor of size 2x2]</span></span><br><span class="line"><span class="string">    (b): Parameter containing: [torch.FloatTensor of size 2]</span></span><br><span class="line"><span class="string">)</span></span><br><span class="line"><span class="string">True</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="comment"># 可以用Module将它们管理起来</span></span><br><span class="line"><span class="comment"># module.parameters()返回一个生成器，包括其结构下的所有parameters</span></span><br><span class="line">module = nn.Module()</span><br><span class="line">module.w = w</span><br><span class="line">module.params_list = params_list</span><br><span class="line">module.params_dict = params_dict</span><br><span class="line"></span><br><span class="line">num_param = <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> param <span class="keyword">in</span> module.parameters():</span><br><span class="line">    print(param,<span class="string">&quot;\n&quot;</span>)</span><br><span class="line">    num_param = num_param + <span class="number">1</span></span><br><span class="line">print(<span class="string">&quot;number of Parameters =&quot;</span>, num_param)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">Parameter containing:</span></span><br><span class="line"><span class="string">tensor([[ 0.3544, -1.1643],</span></span><br><span class="line"><span class="string">        [ 1.2302,  1.3952]], requires_grad=True) </span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Parameter containing:</span></span><br><span class="line"><span class="string">tensor([[0.9391],</span></span><br><span class="line"><span class="string">        [0.1353]], requires_grad=True) </span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Parameter containing:</span></span><br><span class="line"><span class="string">tensor([[0.8012, 0.9587],</span></span><br><span class="line"><span class="string">        [0.3418, 0.7291]], requires_grad=True) </span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Parameter containing:</span></span><br><span class="line"><span class="string">tensor([[0.7729, 0.2383],</span></span><br><span class="line"><span class="string">        [0.7054, 0.9937]], requires_grad=True) </span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Parameter containing:</span></span><br><span class="line"><span class="string">tensor([0., 0.], requires_grad=True) </span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">number of Parameters = 5</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#实践当中，一般通过继承nn.Module来构建模块类，并将所有含有需要学习的参数的部分放在构造函数中。</span></span><br><span class="line"><span class="comment">#以下范例为Pytorch中nn.Linear的源码的简化版本</span></span><br><span class="line"><span class="comment">#可以看到它将需要学习的参数放在了__init__构造函数中，并在forward中调用F.linear函数来实现计算逻辑。</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Linear</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    __constants__ = [<span class="string">&#x27;in_features&#x27;</span>, <span class="string">&#x27;out_features&#x27;</span>]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, in_features, out_features, bias=<span class="literal">True</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>(Linear, self).__init__()</span><br><span class="line">        self.in_features = in_features</span><br><span class="line">        self.out_features = out_features</span><br><span class="line">        self.weight = nn.Parameter(torch.Tensor(out_features, in_features))</span><br><span class="line">        <span class="keyword">if</span> bias:</span><br><span class="line">            self.bias = nn.Parameter(torch.Tensor(out_features))</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.register_parameter(<span class="string">&#x27;bias&#x27;</span>, <span class="literal">None</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, <span class="built_in">input</span></span>):</span></span><br><span class="line">        <span class="keyword">return</span> F.linear(<span class="built_in">input</span>, self.weight, self.bias)</span><br></pre></td></tr></table></figure><br><strong>使用nn.Module来管理子模块</strong>：<br>一般情况下，我们都很少直接使用nn.Parameter来定义参数构建模型，而是通过一些拼装一些常用的模型层来构造模型。这些模型层也是继承自nn.Module的对象，本身也包括参数，属于我们要定义的模块的子模块。nn.Module提供了一些方法可以管理这些子模块：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">children() <span class="comment"># 返回生成器，包括模块下的所有子模块。</span></span><br><span class="line">named_children() <span class="comment"># 返回一个生成器，包括模块下的所有子模块，以及它们的名字。</span></span><br><span class="line">modules() <span class="comment"># 返回一个生成器，包括模块下的所有各个层级的模块，包括模块本身。</span></span><br><span class="line">named_modules() <span class="comment"># 返回一个生成器，包括模块下的所有各个层级的模块以及它们的名字，包括模块本身。</span></span><br></pre></td></tr></table></figure><br>其中chidren()方法和named_children()方法较多使用。<br>modules()方法和named_modules()方法较少使用，其功能可以通过多个named_children()的嵌套使用实现。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Net</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(Net, self).__init__()</span><br><span class="line"></span><br><span class="line">        self.embedding = nn.Embedding(num_embeddings=<span class="number">10000</span>,embedding_dim=<span class="number">3</span>,padding_idx=<span class="number">1</span>)</span><br><span class="line">        self.conv = nn.Sequential()</span><br><span class="line">        self.conv.add_module(<span class="string">&quot;conv_1&quot;</span>,nn.Conv1d(in_channels=<span class="number">3</span>,out_channels=<span class="number">16</span>,kernel_size=<span class="number">5</span>))</span><br><span class="line">        self.conv.add_module(<span class="string">&quot;pool_1&quot;</span>,nn.MaxPool1d(kernel_size=<span class="number">2</span>))</span><br><span class="line">        self.conv.add_module(<span class="string">&quot;relu_1&quot;</span>,nn.ReLU())</span><br><span class="line">        self.conv.add_module(<span class="string">&quot;conv_2&quot;</span>,nn.Conv1d(in_channels=<span class="number">16</span>,out_channels=<span class="number">128</span>,kernel_size=<span class="number">2</span>))</span><br><span class="line">        self.conv.add_module(<span class="string">&quot;pool_2&quot;</span>,nn.MaxPool1d(kernel_size=<span class="number">2</span>))</span><br><span class="line">        self.conv.add_module(<span class="string">&quot;relu_2&quot;</span>,nn.ReLU())</span><br><span class="line"></span><br><span class="line">        self.dense = nn.Sequential()</span><br><span class="line">        self.dense.add_module(<span class="string">&quot;flatten&quot;</span>,nn.Flatten())</span><br><span class="line">        self.dense.add_module(<span class="string">&quot;linear&quot;</span>,nn.Linear(<span class="number">6144</span>,<span class="number">1</span>))</span><br><span class="line">        self.dense.add_module(<span class="string">&quot;sigmoid&quot;</span>,nn.Sigmoid())</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self,x</span>):</span></span><br><span class="line">        x = self.embedding(x).transpose(<span class="number">1</span>,<span class="number">2</span>)</span><br><span class="line">        x = self.conv(x)</span><br><span class="line">        y = self.dense(x)</span><br><span class="line">        <span class="keyword">return</span> y</span><br><span class="line"></span><br><span class="line">net = Net()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 打印子模块</span></span><br><span class="line">i = <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> child <span class="keyword">in</span> net.children():</span><br><span class="line">    i+=<span class="number">1</span></span><br><span class="line">    print(child,<span class="string">&quot;\n&quot;</span>)</span><br><span class="line">print(<span class="string">&quot;child number&quot;</span>,i)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">Embedding(10000, 3, padding_idx=1) </span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Sequential(</span></span><br><span class="line"><span class="string">  (conv_1): Conv1d(3, 16, kernel_size=(5,), stride=(1,))</span></span><br><span class="line"><span class="string">  (pool_1): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)</span></span><br><span class="line"><span class="string">  (relu_1): ReLU()</span></span><br><span class="line"><span class="string">  (conv_2): Conv1d(16, 128, kernel_size=(2,), stride=(1,))</span></span><br><span class="line"><span class="string">  (pool_2): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)</span></span><br><span class="line"><span class="string">  (relu_2): ReLU()</span></span><br><span class="line"><span class="string">) </span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Sequential(</span></span><br><span class="line"><span class="string">  (flatten): Flatten()</span></span><br><span class="line"><span class="string">  (linear): Linear(in_features=6144, out_features=1, bias=True)</span></span><br><span class="line"><span class="string">  (sigmoid): Sigmoid()</span></span><br><span class="line"><span class="string">) </span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">child number 3</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="comment"># 打印子模块名字和子模块</span></span><br><span class="line">i = <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> name,child <span class="keyword">in</span> net.named_children():</span><br><span class="line">    i+=<span class="number">1</span></span><br><span class="line">    print(name,<span class="string">&quot;:&quot;</span>,child,<span class="string">&quot;\n&quot;</span>)</span><br><span class="line">print(<span class="string">&quot;child number&quot;</span>,i)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">embedding : Embedding(10000, 3, padding_idx=1) </span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">conv : Sequential(</span></span><br><span class="line"><span class="string">  (conv_1): Conv1d(3, 16, kernel_size=(5,), stride=(1,))</span></span><br><span class="line"><span class="string">  (pool_1): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)</span></span><br><span class="line"><span class="string">  (relu_1): ReLU()</span></span><br><span class="line"><span class="string">  (conv_2): Conv1d(16, 128, kernel_size=(2,), stride=(1,))</span></span><br><span class="line"><span class="string">  (pool_2): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)</span></span><br><span class="line"><span class="string">  (relu_2): ReLU()</span></span><br><span class="line"><span class="string">) </span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">dense : Sequential(</span></span><br><span class="line"><span class="string">  (flatten): Flatten()</span></span><br><span class="line"><span class="string">  (linear): Linear(in_features=6144, out_features=1, bias=True)</span></span><br><span class="line"><span class="string">  (sigmoid): Sigmoid()</span></span><br><span class="line"><span class="string">) </span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">child number 3</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="comment"># 打印各级模块</span></span><br><span class="line">i = <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> module <span class="keyword">in</span> net.modules():</span><br><span class="line">    i+=<span class="number">1</span></span><br><span class="line">    print(module)</span><br><span class="line">print(<span class="string">&quot;module number:&quot;</span>,i)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">Net(</span></span><br><span class="line"><span class="string">  (embedding): Embedding(10000, 3, padding_idx=1)</span></span><br><span class="line"><span class="string">  (conv): Sequential(</span></span><br><span class="line"><span class="string">    (conv_1): Conv1d(3, 16, kernel_size=(5,), stride=(1,))</span></span><br><span class="line"><span class="string">    (pool_1): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)</span></span><br><span class="line"><span class="string">    (relu_1): ReLU()</span></span><br><span class="line"><span class="string">    (conv_2): Conv1d(16, 128, kernel_size=(2,), stride=(1,))</span></span><br><span class="line"><span class="string">    (pool_2): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)</span></span><br><span class="line"><span class="string">    (relu_2): ReLU()</span></span><br><span class="line"><span class="string">  )</span></span><br><span class="line"><span class="string">  (dense): Sequential(</span></span><br><span class="line"><span class="string">    (flatten): Flatten()</span></span><br><span class="line"><span class="string">    (linear): Linear(in_features=6144, out_features=1, bias=True)</span></span><br><span class="line"><span class="string">    (sigmoid): Sigmoid()</span></span><br><span class="line"><span class="string">  )</span></span><br><span class="line"><span class="string">)</span></span><br><span class="line"><span class="string">Embedding(10000, 3, padding_idx=1)</span></span><br><span class="line"><span class="string">Sequential(</span></span><br><span class="line"><span class="string">  (conv_1): Conv1d(3, 16, kernel_size=(5,), stride=(1,))</span></span><br><span class="line"><span class="string">  (pool_1): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)</span></span><br><span class="line"><span class="string">  (relu_1): ReLU()</span></span><br><span class="line"><span class="string">  (conv_2): Conv1d(16, 128, kernel_size=(2,), stride=(1,))</span></span><br><span class="line"><span class="string">  (pool_2): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)</span></span><br><span class="line"><span class="string">  (relu_2): ReLU()</span></span><br><span class="line"><span class="string">)</span></span><br><span class="line"><span class="string">Conv1d(3, 16, kernel_size=(5,), stride=(1,))</span></span><br><span class="line"><span class="string">MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)</span></span><br><span class="line"><span class="string">ReLU()</span></span><br><span class="line"><span class="string">Conv1d(16, 128, kernel_size=(2,), stride=(1,))</span></span><br><span class="line"><span class="string">MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)</span></span><br><span class="line"><span class="string">ReLU()</span></span><br><span class="line"><span class="string">Sequential(</span></span><br><span class="line"><span class="string">  (flatten): Flatten()</span></span><br><span class="line"><span class="string">  (linear): Linear(in_features=6144, out_features=1, bias=True)</span></span><br><span class="line"><span class="string">  (sigmoid): Sigmoid()</span></span><br><span class="line"><span class="string">)</span></span><br><span class="line"><span class="string">Flatten()</span></span><br><span class="line"><span class="string">Linear(in_features=6144, out_features=1, bias=True)</span></span><br><span class="line"><span class="string">Sigmoid()</span></span><br><span class="line"><span class="string">module number: 13</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><br>可通过named_children方法找到embedding层，并将其参数设置为不可训练(相当于冻结embedding层)。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line">children_dict = &#123;name:module <span class="keyword">for</span> name,module <span class="keyword">in</span> net.named_children()&#125;</span><br><span class="line"></span><br><span class="line">print(children_dict)</span><br><span class="line">embedding = children_dict[<span class="string">&quot;embedding&quot;</span>]</span><br><span class="line">embedding.requires_grad_(<span class="literal">False</span>) <span class="comment">#冻结其参数</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">&#123;&#x27;embedding&#x27;: Embedding(10000, 3, padding_idx=1), &#x27;conv&#x27;: Sequential(</span></span><br><span class="line"><span class="string">  (conv_1): Conv1d(3, 16, kernel_size=(5,), stride=(1,))</span></span><br><span class="line"><span class="string">  (pool_1): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)</span></span><br><span class="line"><span class="string">  (relu_1): ReLU()</span></span><br><span class="line"><span class="string">  (conv_2): Conv1d(16, 128, kernel_size=(2,), stride=(1,))</span></span><br><span class="line"><span class="string">  (pool_2): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)</span></span><br><span class="line"><span class="string">  (relu_2): ReLU()</span></span><br><span class="line"><span class="string">), &#x27;dense&#x27;: Sequential(</span></span><br><span class="line"><span class="string">  (flatten): Flatten()</span></span><br><span class="line"><span class="string">  (linear): Linear(in_features=6144, out_features=1, bias=True)</span></span><br><span class="line"><span class="string">  (sigmoid): Sigmoid()</span></span><br><span class="line"><span class="string">)&#125;</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="comment">#可以看到其第一层的参数已经不可以被训练了。</span></span><br><span class="line"><span class="keyword">for</span> param <span class="keyword">in</span> embedding.parameters():</span><br><span class="line">    print(param.requires_grad)</span><br><span class="line">    print(param.numel())</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">False</span></span><br><span class="line"><span class="string">30000</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="comment"># totchsummary包也实现了summary功能，用法一样</span></span><br><span class="line"><span class="keyword">from</span> torchkeras <span class="keyword">import</span> summary</span><br><span class="line">summary(net,input_shape=(<span class="number">200</span>,),input_dtype=torch.LongTensor)</span><br><span class="line"><span class="comment"># 不可训练参数数量增加</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">----------------------------------------------------------------</span></span><br><span class="line"><span class="string">        Layer (type)               Output Shape         Param #</span></span><br><span class="line"><span class="string">================================================================</span></span><br><span class="line"><span class="string">         Embedding-1               [-1, 200, 3]          30,000</span></span><br><span class="line"><span class="string">            Conv1d-2              [-1, 16, 196]             256</span></span><br><span class="line"><span class="string">         MaxPool1d-3               [-1, 16, 98]               0</span></span><br><span class="line"><span class="string">              ReLU-4               [-1, 16, 98]               0</span></span><br><span class="line"><span class="string">            Conv1d-5              [-1, 128, 97]           4,224</span></span><br><span class="line"><span class="string">         MaxPool1d-6              [-1, 128, 48]               0</span></span><br><span class="line"><span class="string">              ReLU-7              [-1, 128, 48]               0</span></span><br><span class="line"><span class="string">           Flatten-8                 [-1, 6144]               0</span></span><br><span class="line"><span class="string">            Linear-9                    [-1, 1]           6,145</span></span><br><span class="line"><span class="string">          Sigmoid-10                    [-1, 1]               0</span></span><br><span class="line"><span class="string">================================================================</span></span><br><span class="line"><span class="string">Total params: 40,625</span></span><br><span class="line"><span class="string">Trainable params: 10,625</span></span><br><span class="line"><span class="string">Non-trainable params: 30,000</span></span><br><span class="line"><span class="string">----------------------------------------------------------------</span></span><br><span class="line"><span class="string">Input size (MB): 0.000763</span></span><br><span class="line"><span class="string">Forward/backward pass size (MB): 0.287796</span></span><br><span class="line"><span class="string">Params size (MB): 0.154972</span></span><br><span class="line"><span class="string">Estimated Total Size (MB): 0.443531</span></span><br><span class="line"><span class="string">----------------------------------------------------------------</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><br><strong>创建基础的神经网络：</strong><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> datasets, transforms</span><br><span class="line"></span><br><span class="line">device = <span class="string">&#x27;cuda&#x27;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&#x27;cpu&#x27;</span></span><br><span class="line">print(<span class="string">f&#x27;Using <span class="subst">&#123;device&#125;</span> device&#x27;</span>)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">Using cuda device</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">NeuralNetwork</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(NeuralNetwork, self).__init__()</span><br><span class="line">        self.flatten = nn.Flatten()</span><br><span class="line">        self.linear_relu_stack = nn.Sequential(</span><br><span class="line">            nn.Linear(<span class="number">28</span>*<span class="number">28</span>, <span class="number">512</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Linear(<span class="number">512</span>, <span class="number">512</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Linear(<span class="number">512</span>, <span class="number">10</span>),</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        x = self.flatten(x)</span><br><span class="line">        logits = self.linear_relu_stack(x)</span><br><span class="line">        <span class="keyword">return</span> logits</span><br><span class="line"></span><br><span class="line">model = NeuralNetwork().to(device)</span><br><span class="line">print(model)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">NeuralNetwork(</span></span><br><span class="line"><span class="string">  (flatten): Flatten(start_dim=1, end_dim=-1)</span></span><br><span class="line"><span class="string">  (linear_relu_stack): Sequential(</span></span><br><span class="line"><span class="string">    (0): Linear(in_features=784, out_features=512, bias=True)</span></span><br><span class="line"><span class="string">    (1): ReLU()</span></span><br><span class="line"><span class="string">    (2): Linear(in_features=512, out_features=512, bias=True)</span></span><br><span class="line"><span class="string">    (3): ReLU()</span></span><br><span class="line"><span class="string">    (4): Linear(in_features=512, out_features=10, bias=True)</span></span><br><span class="line"><span class="string">  )</span></span><br><span class="line"><span class="string">)</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">X = torch.rand(<span class="number">1</span>, <span class="number">28</span>, <span class="number">28</span>, device=device)</span><br><span class="line">logits = model(X)</span><br><span class="line">pred_probab = nn.Softmax(dim=<span class="number">1</span>)(logits)</span><br><span class="line">y_pred = pred_probab.argmax(<span class="number">1</span>)</span><br><span class="line">print(<span class="string">f&quot;Predicted class: <span class="subst">&#123;y_pred&#125;</span>&quot;</span>)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">Predicted class: tensor([7], device=&#x27;cuda:0&#x27;)</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure></p>
<h1 id="containers"><a href="#containers" class="headerlink" title="containers"></a>containers</h1><p>Pytorch中除了torch.nn.Module，还有很多<span class="exturl" data-url="aHR0cHM6Ly9weXRvcmNoLm9yZy9kb2NzL3N0YWJsZS9ubi5odG1sI2NvbnRhaW5lcnM=">容器<i class="fa fa-external-link-alt"></i></span>：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Sequential具有forward方法，而ModuleList等只是容器，所以Sequential用的更多些。</span></span><br><span class="line">torch.nn.Sequential(*args) </span><br><span class="line">torch.nn.ModuleList(modules=<span class="literal">None</span>) <span class="comment"># 模块列表，和python列表不同的是继承了nn.Module方法。默认module名称是从0开始。</span></span><br><span class="line">torch.nn.ModuleDict(modules=<span class="literal">None</span>) <span class="comment"># 模块字典，可自定义模型名称了。</span></span><br><span class="line">torch.nn.ParameterList(parameters=<span class="literal">None</span>)</span><br><span class="line">torch.nn.ParameterDict(parameters=<span class="literal">None</span>)</span><br></pre></td></tr></table></figure></p>
<h1 id="save"><a href="#save" class="headerlink" title="save"></a>save</h1><p>Pytorch模型的保存：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># https://pytorch.org/tutorials/recipes/recipes/saving_and_loading_a_general_checkpoint.html</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Net</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(Net, self).__init__()</span><br><span class="line">        self.conv1 = nn.Conv2d(<span class="number">3</span>, <span class="number">6</span>, <span class="number">5</span>)</span><br><span class="line">        self.pool = nn.MaxPool2d(<span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">        self.conv2 = nn.Conv2d(<span class="number">6</span>, <span class="number">16</span>, <span class="number">5</span>)</span><br><span class="line">        self.fc1 = nn.Linear(<span class="number">16</span> * <span class="number">5</span> * <span class="number">5</span>, <span class="number">120</span>)</span><br><span class="line">        self.fc2 = nn.Linear(<span class="number">120</span>, <span class="number">84</span>)</span><br><span class="line">        self.fc3 = nn.Linear(<span class="number">84</span>, <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        x = self.pool(F.relu(self.conv1(x)))</span><br><span class="line">        x = self.pool(F.relu(self.conv2(x)))</span><br><span class="line">        x = x.view(<span class="number">-1</span>, <span class="number">16</span> * <span class="number">5</span> * <span class="number">5</span>)</span><br><span class="line">        x = F.relu(self.fc1(x))</span><br><span class="line">        x = F.relu(self.fc2(x))</span><br><span class="line">        x = self.fc3(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">net = Net()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Save the general checkpoint</span></span><br><span class="line">EPOCH = <span class="number">5</span></span><br><span class="line">PATH = <span class="string">&quot;model.pt&quot;</span></span><br><span class="line">LOSS = <span class="number">0.4</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 保存模型</span></span><br><span class="line">torch.save(&#123;</span><br><span class="line">            <span class="string">&#x27;epoch&#x27;</span>: EPOCH,</span><br><span class="line">            <span class="string">&#x27;model_state_dict&#x27;</span>: net.state_dict(), <span class="comment"># 包括parameter和buffer</span></span><br><span class="line">            <span class="string">&#x27;optimizer_state_dict&#x27;</span>: optimizer.state_dict(),</span><br><span class="line">            <span class="string">&#x27;loss&#x27;</span>: LOSS,</span><br><span class="line">            &#125;, PATH)</span><br><span class="line"><span class="comment"># Load the general checkpoint</span></span><br><span class="line">model = Net()</span><br><span class="line">optimizer = optim.SGD(net.parameters(), lr=<span class="number">0.001</span>, momentum=<span class="number">0.9</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载模型</span></span><br><span class="line">checkpoint = torch.load(PATH)</span><br><span class="line">model.load_state_dict(checkpoint[<span class="string">&#x27;model_state_dict&#x27;</span>])</span><br><span class="line">optimizer.load_state_dict(checkpoint[<span class="string">&#x27;optimizer_state_dict&#x27;</span>])</span><br><span class="line">epoch = checkpoint[<span class="string">&#x27;epoch&#x27;</span>]</span><br><span class="line">loss = checkpoint[<span class="string">&#x27;loss&#x27;</span>]</span><br></pre></td></tr></table></figure></p>
<h1 id="layers"><a href="#layers" class="headerlink" title="layers"></a>layers</h1><p>深度学习模型一般由各种模型层组合而成。torch.nn中内置了非常丰富的各种模型层。它们都属于nn.Module的子类，具备参数管理功能。如果这些内置模型层不能够满足需求，也可以通过继承nn.Module基类构建自定义的模型层。实际上，Pytorch不区分模型和模型层，都是通过继承nn.Module进行构建。因此，只要继承nn.Module基类并实现forward方法即可自定义模型层。更多层的定义参考<span class="exturl" data-url="aHR0cHM6Ly9weXRvcmNoLm9yZy9kb2NzL3N0YWJsZS9ubi5odG1s">torch.nn<i class="fa fa-external-link-alt"></i></span>。</p>
<p>基础层：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">nn.Linear <span class="comment"># 全连接层。参数个数=输入层特征数×输出层特征数(weight)＋输出层特征数(bias)</span></span><br><span class="line"></span><br><span class="line">nn.Flatten <span class="comment"># 压平层，用于将多维张量(从维度1开始)样本压成一维张量样本。</span></span><br><span class="line"></span><br><span class="line">nn.BatchNorm1d <span class="comment"># 一维批标准化层。通过线性变换将输入批次缩放平移到稳定的均值和标准差。</span></span><br><span class="line"><span class="comment"># 可以增强模型对输入不同分布的适应性，加快模型训练速度，有轻微正则化效果。</span></span><br><span class="line"><span class="comment"># 一般在激活函数之前使用。可以用afine参数设置该层是否含有可以训练的参数。</span></span><br><span class="line"></span><br><span class="line">nn.BatchNorm2d <span class="comment"># 二维批标准化层。</span></span><br><span class="line"></span><br><span class="line">nn.BatchNorm3d <span class="comment"># 三维批标准化层。</span></span><br><span class="line"></span><br><span class="line">nn.Dropout <span class="comment"># 一维随机丢弃层。一种正则化手段。</span></span><br><span class="line"></span><br><span class="line">nn.Dropout2d <span class="comment"># 二维随机丢弃层。</span></span><br><span class="line"></span><br><span class="line">nn.Dropout3d <span class="comment"># 三维随机丢弃层。</span></span><br><span class="line"></span><br><span class="line">nn.Threshold <span class="comment"># 限幅层。当输入大于或小于阈值范围时，截断之。</span></span><br><span class="line"></span><br><span class="line">nn.ConstantPad2d <span class="comment"># 二维常数填充层。对二维张量样本填充常数扩展长度。</span></span><br><span class="line"></span><br><span class="line">nn.ReplicationPad1d <span class="comment"># 一维复制填充层。对一维张量样本通过复制边缘值填充扩展长度。</span></span><br><span class="line"></span><br><span class="line">nn.ZeroPad2d <span class="comment"># 二维零值填充层。对二维张量样本在边缘填充0值.</span></span><br><span class="line"></span><br><span class="line">nn.GroupNorm <span class="comment"># 组归一化。一种替代批归一化的方法，将通道分成若干组进行归一。</span></span><br><span class="line"><span class="comment"># 不受batch大小限制，据称性能和效果都优于BatchNorm。</span></span><br><span class="line"></span><br><span class="line">nn.LayerNorm <span class="comment"># 层归一化。较少使用。</span></span><br><span class="line"></span><br><span class="line">nn.InstanceNorm2d <span class="comment"># 样本归一化。较少使用。</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 各种归一化技术参考如下知乎文章:https://zhuanlan.zhihu.com/p/34858971</span></span><br></pre></td></tr></table></figure><br>卷积网络相关层：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">nn.Conv1d <span class="comment"># 普通一维卷积，常用于文本。参数个数=输入通道数×卷积核尺寸(如3)×卷积核个数+卷积核尺寸(如3)。</span></span><br><span class="line"></span><br><span class="line">nn.Conv2d <span class="comment"># 普通二维卷积，常用于图像。参数个数=输入通道数×卷积核尺寸(如3乘3)×卷积核个数+卷积核尺寸(如3乘3)。</span></span><br><span class="line"><span class="comment"># 通过调整dilation参数大于1，可以变成空洞卷积，增大卷积核感受野。</span></span><br><span class="line"><span class="comment"># 通过调整groups参数不为1，可以变成分组卷积。分组卷积中不同分组使用相同的卷积核，显著减少参数数量。</span></span><br><span class="line"><span class="comment"># 当groups参数等于通道数时，相当于tensorflow中的二维深度卷积层tf.keras.layers.DepthwiseConv2D。</span></span><br><span class="line"><span class="comment"># 利用分组卷积和1乘1卷积的组合操作，可以构造相当于Keras中的二维深度可分离卷积层tf.keras.layers.SeparableConv2D。</span></span><br><span class="line"></span><br><span class="line">nn.Conv3d <span class="comment"># 普通三维卷积，常用于视频。参数个数=输入通道数×卷积核尺寸(如3乘3乘3)×卷积核个数+卷积核尺寸(如3乘3乘3)。</span></span><br><span class="line"></span><br><span class="line">nn.MaxPool1d <span class="comment"># 一维最大池化。</span></span><br><span class="line"></span><br><span class="line">nn.MaxPool2d <span class="comment"># 二维最大池化。一种下采样方式。没有需要训练的参数。</span></span><br><span class="line"></span><br><span class="line">nn.MaxPool3d <span class="comment"># 三维最大池化。</span></span><br><span class="line"></span><br><span class="line">nn.AdaptiveMaxPool2d <span class="comment"># 二维自适应最大池化。无论输入图像的尺寸如何变化，输出的图像尺寸是固定的。</span></span><br><span class="line"><span class="comment"># 该函数的实现原理，大概是通过输入图像的尺寸和要得到的输出图像的尺寸来反向推算池化算子的padding,stride等参数。</span></span><br><span class="line"></span><br><span class="line">nn.FractionalMaxPool2d <span class="comment"># 二维分数最大池化。普通最大池化通常输入尺寸是输出的整数倍。而分数最大池化则可以不必是整数。</span></span><br><span class="line"><span class="comment"># 分数最大池化使用了一些随机采样策略，有一定的正则效果，可以用它来代替普通最大池化和Dropout层。</span></span><br><span class="line"></span><br><span class="line">nn.AvgPool2d <span class="comment"># 二维平均池化。</span></span><br><span class="line"></span><br><span class="line">nn.AdaptiveAvgPool2d <span class="comment"># 二维自适应平均池化。无论输入的维度如何变化，输出的维度是固定的。</span></span><br><span class="line"></span><br><span class="line">nn.ConvTranspose2d <span class="comment"># 二维卷积转置层，俗称反卷积层。</span></span><br><span class="line"><span class="comment"># 并非卷积的逆操作，但在卷积核相同的情况下，当其输入尺寸是卷积操作输出尺寸的情况下，</span></span><br><span class="line"><span class="comment"># 卷积转置的输出尺寸恰好是卷积操作的输入尺寸。在语义分割中可用于上采样。</span></span><br><span class="line"></span><br><span class="line">nn.Upsample <span class="comment"># 上采样层，操作效果和池化相反。可以通过mode参数控制上采样策略为&quot;nearest&quot;最邻近策略或&quot;linear&quot;线性插值策略。</span></span><br><span class="line"></span><br><span class="line">nn.Unfold <span class="comment"># 滑动窗口提取层。其参数和卷积操作nn.Conv2d相同。</span></span><br><span class="line"><span class="comment"># 实际上，卷积操作可以等价于nn.Unfold和nn.Linear以及nn.Fold的一个组合。</span></span><br><span class="line"><span class="comment"># 其中nn.Unfold操作可以从输入中提取各个滑动窗口的数值矩阵，并将其压平成一维。</span></span><br><span class="line"><span class="comment"># 利用nn.Linear将nn.Unfold的输出和卷积核做乘法后，再使用nn.Fold操作将结果转换成输出图片形状。</span></span><br><span class="line"></span><br><span class="line">nn.Fold <span class="comment"># 逆滑动窗口提取层。</span></span><br></pre></td></tr></table></figure><br>循环网络相关层：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">nn.Embedding <span class="comment"># 嵌入层。一种比Onehot更加有效的对离散特征进行编码的方法。</span></span><br><span class="line"><span class="comment"># 一般用于将输入中的单词映射为稠密向量。嵌入层的参数需要学习。</span></span><br><span class="line"></span><br><span class="line">nn.LSTM <span class="comment"># 长短记忆循环网络层【支持多层】。最普遍使用的循环网络层。</span></span><br><span class="line"><span class="comment"># 具有携带轨道，遗忘门，更新门，输出门。可以较为有效地缓解梯度消失问题，从而能够适用长期依赖问题。</span></span><br><span class="line"><span class="comment"># 设置bidirectional=True时可以得到双向LSTM。需要注意的是，</span></span><br><span class="line"><span class="comment"># 默认的输入和输出形状是(seq,batch,feature), 如果需要将batch维度放在第0维，则要设置batch_first参数设置为True。</span></span><br><span class="line"></span><br><span class="line">nn.GRU <span class="comment"># 门控循环网络层【支持多层】。LSTM的低配版，不具有携带轨道，参数数量少于LSTM，训练速度更快。</span></span><br><span class="line"></span><br><span class="line">nn.RNN <span class="comment"># 简单循环网络层【支持多层】。容易存在梯度消失，不能够适用长期依赖问题。一般较少使用。</span></span><br><span class="line"></span><br><span class="line">nn.LSTMCell <span class="comment"># 长短记忆循环网络单元。和nn.LSTM在整个序列上迭代相比，它仅在序列上迭代一步。一般较少使用。</span></span><br><span class="line"></span><br><span class="line">nn.GRUCell <span class="comment"># 门控循环网络单元。和nn.GRU在整个序列上迭代相比，它仅在序列上迭代一步。一般较少使用。</span></span><br><span class="line"></span><br><span class="line">nn.RNNCell <span class="comment"># 简单循环网络单元。和nn.RNN在整个序列上迭代相比，它仅在序列上迭代一步。一般较少使用。</span></span><br></pre></td></tr></table></figure><br>Transformer相关层(它是目前NLP任务的主流模型的主要构成部分)：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Transformer网络结构是替代循环网络的一种结构，解决了循环网络难以并行，难以捕捉长期依赖的缺陷。</span></span><br><span class="line"><span class="comment"># Transformer网络结构由TransformerEncoder编码器和TransformerDecoder解码器组成。</span></span><br><span class="line"><span class="comment"># 编码器和解码器的核心是MultiheadAttention多头注意力层。</span></span><br><span class="line"><span class="comment"># Transformer原理介绍可以参考如下知乎文章：https://zhuanlan.zhihu.com/p/48508221</span></span><br><span class="line">nn.Transformer <span class="comment"># Transformer网络结构。</span></span><br><span class="line"></span><br><span class="line">nn.TransformerEncoder <span class="comment"># Transformer编码器结构。由多个 nn.TransformerEncoderLayer编码器层组成。</span></span><br><span class="line"></span><br><span class="line">nn.TransformerDecoder <span class="comment"># Transformer解码器结构。由多个 nn.TransformerDecoderLayer解码器层组成。</span></span><br><span class="line"></span><br><span class="line">nn.TransformerEncoderLayer <span class="comment"># Transformer的编码器层。</span></span><br><span class="line"></span><br><span class="line">nn.TransformerDecoderLayer <span class="comment"># Transformer的解码器层。</span></span><br><span class="line"></span><br><span class="line">nn.MultiheadAttention <span class="comment"># 多头注意力层。</span></span><br></pre></td></tr></table></figure></p>
<h1 id="losses"><a href="#losses" class="headerlink" title="losses"></a>losses</h1><p>一般来说，监督学习的目标函数由损失函数和正则化项组成。(Objective = Loss + Regularization)</p>
<p>Pytorch中的损失函数一般在训练模型时候指定。注意Pytorch中内置的损失函数的参数和tensorflow不同，是y_pred在前，y_true在后，而Tensorflow是y_true在前，y_pred在后。</p>
<p>对于回归模型，通常使用的内置损失函数是均方损失函数nn.MSELoss 。对于二分类模型，通常使用的是二元交叉熵损失函数nn.BCELoss (输入已经是sigmoid激活函数之后的结果) 或者 nn.BCEWithLogitsLoss (输入尚未经过nn.Sigmoid激活函数) 。</p>
<p>对于多分类模型，一般推荐使用交叉熵损失函数 nn.CrossEntropyLoss。(y_true需要是一维的，是类别编码。y_pred未经过nn.Softmax激活)。此外，如果多分类的y_pred经过了nn.LogSoftmax激活，可以使用nn.NLLLoss损失函数(The negative log likelihood loss)。这种方法和直接使用nn.CrossEntropyLoss等价。</p>
<p>也可以自定义损失函数，自定义损失函数需要接收两个张量y_pred，y_true作为输入参数，并输出一个标量作为损失函数值。Pytorch中的正则化项一般通过自定义的方式和损失函数一起添加作为目标函数。如果仅仅使用L2正则化，也可以利用优化器的weight_decay参数来实现相同的效果。</p>
<p>常用的一些内置损失函数：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 更多损失函数：https://zhuanlan.zhihu.com/p/61379965</span></span><br><span class="line"><span class="comment"># 官网：https://pytorch.org/docs/stable/nn.html#loss-functions</span></span><br><span class="line">nn.MSELoss <span class="comment"># 均方误差损失，也叫做L2损失，用于回归</span></span><br><span class="line"></span><br><span class="line">nn.L1Loss <span class="comment"># L1损失，也叫做绝对值误差损失，用于回归</span></span><br><span class="line"></span><br><span class="line">nn.SmoothL1Loss  <span class="comment"># 平滑L1损失，当输入在-1到1之间时，平滑为L2损失，用于回归</span></span><br><span class="line"></span><br><span class="line">nn.BCELoss <span class="comment"># 二元交叉熵，用于二分类，输入已经过nn.Sigmoid激活，对不平衡数据集可以用weigths参数调整类别权重</span></span><br><span class="line"></span><br><span class="line">nn.BCEWithLogitsLoss <span class="comment"># 二元交叉熵，用于二分类，输入未经过nn.Sigmoid激活</span></span><br><span class="line"></span><br><span class="line">nn.CrossEntropyLoss <span class="comment"># 交叉熵，用于多分类。</span></span><br><span class="line"><span class="comment"># 要求label为稀疏编码，输入未经过nn.Softmax激活，对不平衡数据集可以用weigths参数调整类别权重</span></span><br><span class="line"></span><br><span class="line">nn.NLLLoss <span class="comment"># 负对数似然损失，用于多分类，要求label为稀疏编码，输入经过nn.LogSoftmax激活</span></span><br><span class="line"></span><br><span class="line">nn.CosineSimilarity <span class="comment"># 余弦相似度，可用于多分类</span></span><br><span class="line"></span><br><span class="line">nn.AdaptiveLogSoftmaxWithLoss <span class="comment"># 一种适合非常多类别且类别分布很不均衡的损失函数，会自适应地将多个小类别合成一个cluster</span></span><br></pre></td></tr></table></figure></p>
<p><strong>自定义损失函数</strong>：<br>自定义损失函数接收两个张量 y_pred 和 y_true 作为输入参数，并输出一个标量作为损失函数值。也可以对nn.Module进行子类化，重写forward方法实现损失的计算逻辑，从而得到损失函数的类的实现。</p>
<p>下面是一个Focal Loss的自定义实现示范。Focal Loss是一种对binary_crossentropy的改进损失函数形式。它在样本不均衡和存在较多易分类的样本时相比binary_crossentropy具有明显的优势。它有两个可调参数，alpha和gamma。其中alpha参数主要用于衰减负样本的权重，gamma参数主要用于衰减容易训练样本的权重。从而让模型更加聚焦在正样本和困难样本上。这就是为什么这个损失函数叫做Focal Loss。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 详见https://zhuanlan.zhihu.com/p/80594704</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">FocalLoss</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self,gamma=<span class="number">2.0</span>,alpha=<span class="number">0.75</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.gamma = gamma</span><br><span class="line">        self.alpha = alpha</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self,y_pred,y_true</span>):</span></span><br><span class="line">        bce = torch.nn.BCELoss(reduction = <span class="string">&quot;none&quot;</span>)(y_pred,y_true)</span><br><span class="line">        p_t = (y_true * y_pred) + ((<span class="number">1</span> - y_true) * (<span class="number">1</span> - y_pred))</span><br><span class="line">        alpha_factor = y_true * self.alpha + (<span class="number">1</span> - y_true) * (<span class="number">1</span> - self.alpha)</span><br><span class="line">        modulating_factor = torch.<span class="built_in">pow</span>(<span class="number">1.0</span> - p_t, self.gamma)</span><br><span class="line">        loss = torch.mean(alpha_factor * modulating_factor * bce)</span><br><span class="line">        <span class="keyword">return</span> loss</span><br><span class="line"></span><br><span class="line"><span class="comment">#困难样本</span></span><br><span class="line">y_pred_hard = torch.tensor([[<span class="number">0.5</span>],[<span class="number">0.5</span>]])</span><br><span class="line">y_true_hard = torch.tensor([[<span class="number">1.0</span>],[<span class="number">0.0</span>]])</span><br><span class="line"></span><br><span class="line"><span class="comment">#容易样本</span></span><br><span class="line">y_pred_easy = torch.tensor([[<span class="number">0.9</span>],[<span class="number">0.1</span>]])</span><br><span class="line">y_true_easy = torch.tensor([[<span class="number">1.0</span>],[<span class="number">0.0</span>]])</span><br><span class="line"></span><br><span class="line">focal_loss = FocalLoss()</span><br><span class="line">bce_loss = nn.BCELoss()</span><br><span class="line"></span><br><span class="line">print(<span class="string">&quot;focal_loss(hard samples):&quot;</span>, focal_loss(y_pred_hard,y_true_hard))</span><br><span class="line">print(<span class="string">&quot;bce_loss(hard samples):&quot;</span>, bce_loss(y_pred_hard,y_true_hard))</span><br><span class="line">print(<span class="string">&quot;focal_loss(easy samples):&quot;</span>, focal_loss(y_pred_easy,y_true_easy))</span><br><span class="line">print(<span class="string">&quot;bce_loss(easy samples):&quot;</span>, bce_loss(y_pred_easy,y_true_easy))</span><br><span class="line"></span><br><span class="line"><span class="comment">#可见 focal_loss让容易样本的权重衰减到原来的 0.0005/0.1054 = 0.00474</span></span><br><span class="line"><span class="comment">#而让困难样本的权重只衰减到原来的 0.0866/0.6931=0.12496</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 因此相对而言，focal_loss可以衰减容易样本的权重。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">focal_loss(hard samples): tensor(0.0866)</span></span><br><span class="line"><span class="string">bce_loss(hard samples): tensor(0.6931)</span></span><br><span class="line"><span class="string">focal_loss(easy samples): tensor(0.0005)</span></span><br><span class="line"><span class="string">bce_loss(easy samples): tensor(0.1054)</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><br>FocalLoss的使用完整范例，可以参考下面中自定义L1和L2正则化项中的范例，该范例既演示了自定义正则化项的方法，也演示了FocalLoss的使用方法。</p>
<p><strong>自定义L1和L2正则化项</strong>：<br>通常认为 L1 正则化可以产生稀疏权值矩阵，即产生一个稀疏模型，可以用于特征选择。而 L2 正则化可以防止模型过拟合（overfitting）。一定程度上，L1也可以防止过拟合。</p>
<p>下面以一个二分类问题为例，演示给模型的目标函数添加自定义L1和L2正则化项的方法。这个范例同时演示了上一部分的FocalLoss的使用。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np </span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd </span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataset,DataLoader,TensorDataset</span><br><span class="line"><span class="keyword">import</span> torchkeras </span><br><span class="line">%matplotlib inline</span><br><span class="line">%config InlineBackend.figure_format = <span class="string">&#x27;svg&#x27;</span></span><br><span class="line"><span class="comment"># 1, 准备数据</span></span><br><span class="line"><span class="comment">#正负样本数量</span></span><br><span class="line">n_positive,n_negative = <span class="number">200</span>,<span class="number">6000</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#生成正样本, 小圆环分布</span></span><br><span class="line">r_p = <span class="number">5.0</span> + torch.normal(<span class="number">0.0</span>,<span class="number">1.0</span>,size = [n_positive,<span class="number">1</span>]) </span><br><span class="line">theta_p = <span class="number">2</span>*np.pi*torch.rand([n_positive,<span class="number">1</span>])</span><br><span class="line">Xp = torch.cat([r_p*torch.cos(theta_p),r_p*torch.sin(theta_p)],axis = <span class="number">1</span>)</span><br><span class="line">Yp = torch.ones_like(r_p)</span><br><span class="line"></span><br><span class="line"><span class="comment">#生成负样本, 大圆环分布</span></span><br><span class="line">r_n = <span class="number">8.0</span> + torch.normal(<span class="number">0.0</span>,<span class="number">1.0</span>,size = [n_negative,<span class="number">1</span>]) </span><br><span class="line">theta_n = <span class="number">2</span>*np.pi*torch.rand([n_negative,<span class="number">1</span>])</span><br><span class="line">Xn = torch.cat([r_n*torch.cos(theta_n),r_n*torch.sin(theta_n)],axis = <span class="number">1</span>)</span><br><span class="line">Yn = torch.zeros_like(r_n)</span><br><span class="line"></span><br><span class="line"><span class="comment">#汇总样本</span></span><br><span class="line">X = torch.cat([Xp,Xn],axis = <span class="number">0</span>)</span><br><span class="line">Y = torch.cat([Yp,Yn],axis = <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#可视化</span></span><br><span class="line">plt.figure(figsize = (<span class="number">6</span>,<span class="number">6</span>))</span><br><span class="line">plt.scatter(Xp[:,<span class="number">0</span>],Xp[:,<span class="number">1</span>],c = <span class="string">&quot;r&quot;</span>)</span><br><span class="line">plt.scatter(Xn[:,<span class="number">0</span>],Xn[:,<span class="number">1</span>],c = <span class="string">&quot;g&quot;</span>)</span><br><span class="line">plt.legend([<span class="string">&quot;positive&quot;</span>,<span class="string">&quot;negative&quot;</span>])</span><br><span class="line"></span><br><span class="line">ds = TensorDataset(X,Y)</span><br><span class="line"></span><br><span class="line">ds_train,ds_valid = torch.utils.data.random_split(ds,[<span class="built_in">int</span>(<span class="built_in">len</span>(ds)*<span class="number">0.7</span>),<span class="built_in">len</span>(ds)-<span class="built_in">int</span>(<span class="built_in">len</span>(ds)*<span class="number">0.7</span>)])</span><br><span class="line">dl_train = DataLoader(ds_train,batch_size = <span class="number">100</span>,shuffle=<span class="literal">True</span>,num_workers=<span class="number">2</span>)</span><br><span class="line">dl_valid = DataLoader(ds_valid,batch_size = <span class="number">100</span>,num_workers=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2, 定义模型</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DNNModel</span>(<span class="params">torchkeras.Model</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(DNNModel, self).__init__()</span><br><span class="line">        self.fc1 = nn.Linear(<span class="number">2</span>,<span class="number">4</span>)</span><br><span class="line">        self.fc2 = nn.Linear(<span class="number">4</span>,<span class="number">8</span>) </span><br><span class="line">        self.fc3 = nn.Linear(<span class="number">8</span>,<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self,x</span>):</span></span><br><span class="line">        x = F.relu(self.fc1(x))</span><br><span class="line">        x = F.relu(self.fc2(x))</span><br><span class="line">        y = nn.Sigmoid()(self.fc3(x))</span><br><span class="line">        <span class="keyword">return</span> y</span><br><span class="line"></span><br><span class="line">model = DNNModel()</span><br><span class="line"></span><br><span class="line">model.summary(input_shape =(<span class="number">2</span>,))</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">----------------------------------------------------------------</span></span><br><span class="line"><span class="string">        Layer (type)               Output Shape         Param #</span></span><br><span class="line"><span class="string">================================================================</span></span><br><span class="line"><span class="string">            Linear-1                    [-1, 4]              12</span></span><br><span class="line"><span class="string">            Linear-2                    [-1, 8]              40</span></span><br><span class="line"><span class="string">            Linear-3                    [-1, 1]               9</span></span><br><span class="line"><span class="string">================================================================</span></span><br><span class="line"><span class="string">Total params: 61</span></span><br><span class="line"><span class="string">Trainable params: 61</span></span><br><span class="line"><span class="string">Non-trainable params: 0</span></span><br><span class="line"><span class="string">----------------------------------------------------------------</span></span><br><span class="line"><span class="string">Input size (MB): 0.000008</span></span><br><span class="line"><span class="string">Forward/backward pass size (MB): 0.000099</span></span><br><span class="line"><span class="string">Params size (MB): 0.000233</span></span><br><span class="line"><span class="string">Estimated Total Size (MB): 0.000340</span></span><br><span class="line"><span class="string">----------------------------------------------------------------</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="comment"># 3，训练模型</span></span><br><span class="line"><span class="comment"># 准确率</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">accuracy</span>(<span class="params">y_pred,y_true</span>):</span></span><br><span class="line">    y_pred = torch.where(y_pred&gt;<span class="number">0.5</span>,torch.ones_like(y_pred,dtype = torch.float32),</span><br><span class="line">                      torch.zeros_like(y_pred,dtype = torch.float32))</span><br><span class="line">    acc = torch.mean(<span class="number">1</span>-torch.<span class="built_in">abs</span>(y_true-y_pred))</span><br><span class="line">    <span class="keyword">return</span> acc</span><br><span class="line"></span><br><span class="line"><span class="comment"># L2正则化</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">L2Loss</span>(<span class="params">model,alpha</span>):</span></span><br><span class="line">    l2_loss = torch.tensor(<span class="number">0.0</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">for</span> name, param <span class="keyword">in</span> model.named_parameters():</span><br><span class="line">        <span class="keyword">if</span> <span class="string">&#x27;bias&#x27;</span> <span class="keyword">not</span> <span class="keyword">in</span> name: <span class="comment">#一般不对偏置项使用正则</span></span><br><span class="line">            l2_loss = l2_loss + (<span class="number">0.5</span> * alpha * torch.<span class="built_in">sum</span>(torch.<span class="built_in">pow</span>(param, <span class="number">2</span>)))</span><br><span class="line">    <span class="keyword">return</span> l2_loss</span><br><span class="line"></span><br><span class="line"><span class="comment"># L1正则化</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">L1Loss</span>(<span class="params">model,beta</span>):</span></span><br><span class="line">    l1_loss = torch.tensor(<span class="number">0.0</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">for</span> name, param <span class="keyword">in</span> model.named_parameters():</span><br><span class="line">        <span class="keyword">if</span> <span class="string">&#x27;bias&#x27;</span> <span class="keyword">not</span> <span class="keyword">in</span> name:</span><br><span class="line">            l1_loss = l1_loss +  beta * torch.<span class="built_in">sum</span>(torch.<span class="built_in">abs</span>(param))</span><br><span class="line">    <span class="keyword">return</span> l1_loss</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将L2正则和L1正则添加到FocalLoss损失，一起作为目标函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">focal_loss_with_regularization</span>(<span class="params">y_pred,y_true</span>):</span></span><br><span class="line">    focal = FocalLoss()(y_pred,y_true) </span><br><span class="line">    l2_loss = L2Loss(model,<span class="number">0.001</span>) <span class="comment">#注意设置正则化项系数</span></span><br><span class="line">    l1_loss = L1Loss(model,<span class="number">0.001</span>)</span><br><span class="line">    total_loss = focal + l2_loss + l1_loss</span><br><span class="line">    <span class="keyword">return</span> total_loss</span><br><span class="line"></span><br><span class="line">model.<span class="built_in">compile</span>(loss_func =focal_loss_with_regularization,</span><br><span class="line">              optimizer= torch.optim.Adam(model.parameters(),lr = <span class="number">0.01</span>),</span><br><span class="line">             metrics_dict=&#123;<span class="string">&quot;accuracy&quot;</span>:accuracy&#125;)</span><br><span class="line"></span><br><span class="line">dfhistory = model.fit(<span class="number">30</span>,dl_train = dl_train,dl_val = dl_valid,log_step_freq = <span class="number">30</span>)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">Start Training ...</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">================================================================================2020-07-11 23:34:17</span></span><br><span class="line"><span class="string">&#123;&#x27;step&#x27;: 30, &#x27;loss&#x27;: 0.021, &#x27;accuracy&#x27;: 0.972&#125;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"> +-------+-------+----------+----------+--------------+</span></span><br><span class="line"><span class="string">| epoch |  loss | accuracy | val_loss | val_accuracy |</span></span><br><span class="line"><span class="string">+-------+-------+----------+----------+--------------+</span></span><br><span class="line"><span class="string">|   1   | 0.022 |  0.971   |  0.025   |     0.96     |</span></span><br><span class="line"><span class="string">+-------+-------+----------+----------+--------------+</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">================================================================================2020-07-11 23:34:27</span></span><br><span class="line"><span class="string">&#123;&#x27;step&#x27;: 30, &#x27;loss&#x27;: 0.016, &#x27;accuracy&#x27;: 0.984&#125;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"> +-------+-------+----------+----------+--------------+</span></span><br><span class="line"><span class="string">| epoch |  loss | accuracy | val_loss | val_accuracy |</span></span><br><span class="line"><span class="string">+-------+-------+----------+----------+--------------+</span></span><br><span class="line"><span class="string">|   30  | 0.016 |  0.981   |  0.017   |    0.983     |</span></span><br><span class="line"><span class="string">+-------+-------+----------+----------+--------------+</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">================================================================================2020-07-11 23:34:27</span></span><br><span class="line"><span class="string">Finished Training...</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="comment"># 结果可视化</span></span><br><span class="line">fig, (ax1,ax2) = plt.subplots(nrows=<span class="number">1</span>,ncols=<span class="number">2</span>,figsize = (<span class="number">12</span>,<span class="number">5</span>))</span><br><span class="line">ax1.scatter(Xp[:,<span class="number">0</span>],Xp[:,<span class="number">1</span>], c=<span class="string">&quot;r&quot;</span>)</span><br><span class="line">ax1.scatter(Xn[:,<span class="number">0</span>],Xn[:,<span class="number">1</span>],c = <span class="string">&quot;g&quot;</span>)</span><br><span class="line">ax1.legend([<span class="string">&quot;positive&quot;</span>,<span class="string">&quot;negative&quot;</span>]);</span><br><span class="line">ax1.set_title(<span class="string">&quot;y_true&quot;</span>);</span><br><span class="line"></span><br><span class="line">Xp_pred = X[torch.squeeze(model.forward(X)&gt;=<span class="number">0.5</span>)]</span><br><span class="line">Xn_pred = X[torch.squeeze(model.forward(X)&lt;<span class="number">0.5</span>)]</span><br><span class="line"></span><br><span class="line">ax2.scatter(Xp_pred[:,<span class="number">0</span>],Xp_pred[:,<span class="number">1</span>],c = <span class="string">&quot;r&quot;</span>)</span><br><span class="line">ax2.scatter(Xn_pred[:,<span class="number">0</span>],Xn_pred[:,<span class="number">1</span>],c = <span class="string">&quot;g&quot;</span>)</span><br><span class="line">ax2.legend([<span class="string">&quot;positive&quot;</span>,<span class="string">&quot;negative&quot;</span>]);</span><br><span class="line">ax2.set_title(<span class="string">&quot;y_pred&quot;</span>)</span><br></pre></td></tr></table></figure><br><strong>通过优化器实现L2正则化</strong>：<br>如果仅仅需要使用L2正则化，那么也可以利用优化器的weight_decay参数来实现。weight_decay参数可以设置参数在训练过程中的衰减，这和L2正则化的作用效果等价。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">before L2 regularization:</span><br><span class="line">gradient descent: w = w - lr * dloss_dw </span><br><span class="line"></span><br><span class="line">after L2 regularization:</span><br><span class="line">gradient descent: w = w - lr * (dloss_dw+beta*w) = (<span class="number">1</span>-lr*beta)*w - lr*dloss_dw</span><br><span class="line"></span><br><span class="line">so （<span class="number">1</span>-lr*beta）<span class="keyword">is</span> the weight decay ratio.</span><br></pre></td></tr></table></figure><br>Pytorch的优化器支持一种称之为Per-parameter options的操作，就是对每一个参数进行特定的学习率，权重衰减率指定，以满足更为细致的要求。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">weight_params = [param <span class="keyword">for</span> name, param <span class="keyword">in</span> model.named_parameters() <span class="keyword">if</span> <span class="string">&quot;bias&quot;</span> <span class="keyword">not</span> <span class="keyword">in</span> name]</span><br><span class="line">bias_params = [param <span class="keyword">for</span> name, param <span class="keyword">in</span> model.named_parameters() <span class="keyword">if</span> <span class="string">&quot;bias&quot;</span> <span class="keyword">in</span> name]</span><br><span class="line"></span><br><span class="line">optimizer = torch.optim.SGD([&#123;<span class="string">&#x27;params&#x27;</span>: weight_params, <span class="string">&#x27;weight_decay&#x27;</span>:<span class="number">1e-5</span>&#125;,</span><br><span class="line">                             &#123;<span class="string">&#x27;params&#x27;</span>: bias_params, <span class="string">&#x27;weight_decay&#x27;</span>:<span class="number">0</span>&#125;],</span><br><span class="line">                            lr=<span class="number">1e-2</span>, momentum=<span class="number">0.9</span>)</span><br></pre></td></tr></table></figure></p>
<h1 id="TensorBoard可视化"><a href="#TensorBoard可视化" class="headerlink" title="TensorBoard可视化"></a>TensorBoard可视化</h1><p>TensorBoard是炼丹可视化辅助工具。它原是TensorFlow的小弟，但它也能够很好地和Pytorch进行配合。甚至在Pytorch中使用TensorBoard比TensorFlow中使用TensorBoard还要来的更加简单和自然。Pytorch中利用TensorBoard可视化的大概过程如下：<br>（1）在Pytorch中指定一个目录创建一个torch.utils.tensorboard.SummaryWriter日志写入器。<br>（2）根据需要可视化的信息，利用日志写入器将相应信息日志写入我们指定的目录。<br>（3）传入日志目录作为参数启动TensorBoard，然后就可以在TensorBoard中愉快地看片了。</p>
<p>我们主要介绍Pytorch中利用TensorBoard进行如下方面信息的可视化的方法：<br>可视化模型结构：writer.add_graph<br>可视化指标变化：writer.add_scalar<br>可视化参数分布：writer.add_histogram<br>可视化原始图像：writer.add_image 或 writer.add_images<br>可视化人工绘图：writer.add_figure</p>
<p><strong>可视化模型结构</strong>：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch </span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.utils.tensorboard <span class="keyword">import</span> SummaryWriter</span><br><span class="line"><span class="keyword">from</span> torchkeras <span class="keyword">import</span> Model,summary</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Net</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(Net, self).__init__()</span><br><span class="line">        self.conv1 = nn.Conv2d(in_channels=<span class="number">3</span>,out_channels=<span class="number">32</span>,kernel_size = <span class="number">3</span>)</span><br><span class="line">        self.pool = nn.MaxPool2d(kernel_size = <span class="number">2</span>,stride = <span class="number">2</span>)</span><br><span class="line">        self.conv2 = nn.Conv2d(in_channels=<span class="number">32</span>,out_channels=<span class="number">64</span>,kernel_size = <span class="number">5</span>)</span><br><span class="line">        self.dropout = nn.Dropout2d(p = <span class="number">0.1</span>)</span><br><span class="line">        self.adaptive_pool = nn.AdaptiveMaxPool2d((<span class="number">1</span>,<span class="number">1</span>))</span><br><span class="line">        self.flatten = nn.Flatten()</span><br><span class="line">        self.linear1 = nn.Linear(<span class="number">64</span>,<span class="number">32</span>)</span><br><span class="line">        self.relu = nn.ReLU()</span><br><span class="line">        self.linear2 = nn.Linear(<span class="number">32</span>,<span class="number">1</span>)</span><br><span class="line">        self.sigmoid = nn.Sigmoid()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self,x</span>):</span></span><br><span class="line">        x = self.conv1(x)</span><br><span class="line">        x = self.pool(x)</span><br><span class="line">        x = self.conv2(x)</span><br><span class="line">        x = self.pool(x)</span><br><span class="line">        x = self.dropout(x)</span><br><span class="line">        x = self.adaptive_pool(x)</span><br><span class="line">        x = self.flatten(x)</span><br><span class="line">        x = self.linear1(x)</span><br><span class="line">        x = self.relu(x)</span><br><span class="line">        x = self.linear2(x)</span><br><span class="line">        y = self.sigmoid(x)</span><br><span class="line">        <span class="keyword">return</span> y</span><br><span class="line"></span><br><span class="line">net = Net()</span><br><span class="line">print(net)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">Net(</span></span><br><span class="line"><span class="string">  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1))</span></span><br><span class="line"><span class="string">  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)</span></span><br><span class="line"><span class="string">  (conv2): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1))</span></span><br><span class="line"><span class="string">  (dropout): Dropout2d(p=0.1, inplace=False)</span></span><br><span class="line"><span class="string">  (adaptive_pool): AdaptiveMaxPool2d(output_size=(1, 1))</span></span><br><span class="line"><span class="string">  (flatten): Flatten()</span></span><br><span class="line"><span class="string">  (linear1): Linear(in_features=64, out_features=32, bias=True)</span></span><br><span class="line"><span class="string">  (relu): ReLU()</span></span><br><span class="line"><span class="string">  (linear2): Linear(in_features=32, out_features=1, bias=True)</span></span><br><span class="line"><span class="string">  (sigmoid): Sigmoid()</span></span><br><span class="line"><span class="string">)</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">summary(net,input_shape= (<span class="number">3</span>,<span class="number">32</span>,<span class="number">32</span>))</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">----------------------------------------------------------------</span></span><br><span class="line"><span class="string">        Layer (type)               Output Shape         Param #</span></span><br><span class="line"><span class="string">================================================================</span></span><br><span class="line"><span class="string">            Conv2d-1           [-1, 32, 30, 30]             896</span></span><br><span class="line"><span class="string">         MaxPool2d-2           [-1, 32, 15, 15]               0</span></span><br><span class="line"><span class="string">            Conv2d-3           [-1, 64, 11, 11]          51,264</span></span><br><span class="line"><span class="string">         MaxPool2d-4             [-1, 64, 5, 5]               0</span></span><br><span class="line"><span class="string">         Dropout2d-5             [-1, 64, 5, 5]               0</span></span><br><span class="line"><span class="string"> AdaptiveMaxPool2d-6             [-1, 64, 1, 1]               0</span></span><br><span class="line"><span class="string">           Flatten-7                   [-1, 64]               0</span></span><br><span class="line"><span class="string">            Linear-8                   [-1, 32]           2,080</span></span><br><span class="line"><span class="string">              ReLU-9                   [-1, 32]               0</span></span><br><span class="line"><span class="string">           Linear-10                    [-1, 1]              33</span></span><br><span class="line"><span class="string">          Sigmoid-11                    [-1, 1]               0</span></span><br><span class="line"><span class="string">================================================================</span></span><br><span class="line"><span class="string">Total params: 54,273</span></span><br><span class="line"><span class="string">Trainable params: 54,273</span></span><br><span class="line"><span class="string">Non-trainable params: 0</span></span><br><span class="line"><span class="string">----------------------------------------------------------------</span></span><br><span class="line"><span class="string">Input size (MB): 0.011719</span></span><br><span class="line"><span class="string">Forward/backward pass size (MB): 0.359634</span></span><br><span class="line"><span class="string">Params size (MB): 0.207035</span></span><br><span class="line"><span class="string">Estimated Total Size (MB): 0.578388</span></span><br><span class="line"><span class="string">----------------------------------------------------------------</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">writer = SummaryWriter(<span class="string">&#x27;../data/tensorboard&#x27;</span>)</span><br><span class="line">writer.add_graph(net,input_to_model = torch.rand(<span class="number">1</span>,<span class="number">3</span>,<span class="number">32</span>,<span class="number">32</span>))</span><br><span class="line">writer.close()</span><br><span class="line"></span><br><span class="line">%load_ext tensorboard</span><br><span class="line"><span class="comment">#%tensorboard --logdir ../data/tensorboard</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> tensorboard <span class="keyword">import</span> notebook</span><br><span class="line"><span class="comment">#查看启动的tensorboard程序</span></span><br><span class="line">notebook.<span class="built_in">list</span>() </span><br><span class="line"></span><br><span class="line"><span class="comment">#启动tensorboard程序</span></span><br><span class="line">notebook.start(<span class="string">&quot;--logdir ../data/tensorboard&quot;</span>)</span><br><span class="line"><span class="comment">#等价于在命令行中执行 tensorboard --logdir ../data/tensorboard</span></span><br><span class="line"><span class="comment">#可以在浏览器中打开 http://localhost:6006/ 查看</span></span><br></pre></td></tr></table></figure><br><strong>可视化指标变化</strong>：<br>有时候在训练过程中，如果能够实时动态地查看loss和各种metric的变化曲线，那么无疑可以帮助我们更加直观地了解模型的训练情况。</p>
<p>注意，writer.add_scalar仅能对标量的值的变化进行可视化。因此它一般用于对loss和metric的变化进行可视化分析。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np </span><br><span class="line"><span class="keyword">import</span> torch </span><br><span class="line"><span class="keyword">from</span> torch.utils.tensorboard <span class="keyword">import</span> SummaryWriter</span><br><span class="line"></span><br><span class="line"><span class="comment"># f(x) = a*x**2 + b*x + c的最小值</span></span><br><span class="line">x = torch.tensor(<span class="number">0.0</span>,requires_grad = <span class="literal">True</span>) <span class="comment"># x需要被求导</span></span><br><span class="line">a = torch.tensor(<span class="number">1.0</span>)</span><br><span class="line">b = torch.tensor(<span class="number">-2.0</span>)</span><br><span class="line">c = torch.tensor(<span class="number">1.0</span>)</span><br><span class="line"></span><br><span class="line">optimizer = torch.optim.SGD(params=[x],lr = <span class="number">0.01</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">f</span>(<span class="params">x</span>):</span></span><br><span class="line">    result = a*torch.<span class="built_in">pow</span>(x,<span class="number">2</span>) + b*x + c </span><br><span class="line">    <span class="keyword">return</span>(result)</span><br><span class="line"></span><br><span class="line">writer = SummaryWriter(<span class="string">&#x27;../data/tensorboard&#x27;</span>)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">500</span>):</span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    y = f(x)</span><br><span class="line">    y.backward()</span><br><span class="line">    optimizer.step()</span><br><span class="line">    writer.add_scalar(<span class="string">&quot;x&quot;</span>,x.item(),i) <span class="comment">#日志中记录x在第step i 的值</span></span><br><span class="line">    writer.add_scalar(<span class="string">&quot;y&quot;</span>,y.item(),i) <span class="comment">#日志中记录y在第step i 的值</span></span><br><span class="line"></span><br><span class="line">writer.close()</span><br><span class="line"></span><br><span class="line">print(<span class="string">&quot;y=&quot;</span>,f(x).data,<span class="string">&quot;;&quot;</span>,<span class="string">&quot;x=&quot;</span>,x.data)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">y= tensor(0.) ; x= tensor(1.0000)</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><br><strong>可视化参数分布</strong>：<br>如果需要对模型的参数(一般非标量)在训练过程中的变化进行可视化，可以使用 writer.add_histogram。</p>
<p>它能够观测张量值分布的直方图随训练步骤的变化趋势。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np </span><br><span class="line"><span class="keyword">import</span> torch </span><br><span class="line"><span class="keyword">from</span> torch.utils.tensorboard <span class="keyword">import</span> SummaryWriter</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建正态分布的张量模拟参数矩阵</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">norm</span>(<span class="params">mean,std</span>):</span></span><br><span class="line">    t = std*torch.randn((<span class="number">100</span>,<span class="number">20</span>))+mean</span><br><span class="line">    <span class="keyword">return</span> t</span><br><span class="line"></span><br><span class="line">writer = SummaryWriter(<span class="string">&#x27;../data/tensorboard&#x27;</span>)</span><br><span class="line"><span class="keyword">for</span> step,mean <span class="keyword">in</span> <span class="built_in">enumerate</span>(<span class="built_in">range</span>(<span class="number">-10</span>,<span class="number">10</span>,<span class="number">1</span>)):</span><br><span class="line">    w = norm(mean,<span class="number">1</span>)</span><br><span class="line">    writer.add_histogram(<span class="string">&quot;w&quot;</span>,w, step)</span><br><span class="line">    writer.flush()</span><br><span class="line">writer.close()</span><br></pre></td></tr></table></figure><br><strong>可视化原始图像</strong>：<br>如果我们做图像相关的任务，也可以将原始的图片在tensorboard中进行可视化展示。</p>
<p>如果只写入一张图片信息，可以使用writer.add_image。<br>如果要写入多张图片信息，可以使用writer.add_images。<br>也可以用 torchvision.utils.make_grid将多张图片拼成一张图片，然后用writer.add_image写入。</p>
<p>注意，传入的是代表图片信息的Pytorch中的张量数据。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataset,DataLoader</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms,datasets </span><br><span class="line"></span><br><span class="line"></span><br><span class="line">transform_train = transforms.Compose(</span><br><span class="line">    [transforms.ToTensor()])</span><br><span class="line">transform_valid = transforms.Compose(</span><br><span class="line">    [transforms.ToTensor()])</span><br><span class="line"></span><br><span class="line">ds_train = datasets.ImageFolder(<span class="string">&quot;../data/cifar2/train/&quot;</span>,</span><br><span class="line">            transform = transform_train,target_transform= <span class="keyword">lambda</span> t:torch.tensor([t]).<span class="built_in">float</span>())</span><br><span class="line">ds_valid = datasets.ImageFolder(<span class="string">&quot;../data/cifar2/test/&quot;</span>,</span><br><span class="line">            transform = transform_train,target_transform= <span class="keyword">lambda</span> t:torch.tensor([t]).<span class="built_in">float</span>())</span><br><span class="line"></span><br><span class="line">print(ds_train.class_to_idx)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">&#123;&#x27;0_airplane&#x27;: 0, &#x27;1_automobile&#x27;: 1&#125;</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">dl_train = DataLoader(ds_train,batch_size = <span class="number">50</span>,shuffle = <span class="literal">True</span>,num_workers=<span class="number">3</span>)</span><br><span class="line">dl_valid = DataLoader(ds_valid,batch_size = <span class="number">50</span>,shuffle = <span class="literal">True</span>,num_workers=<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">dl_train_iter = <span class="built_in">iter</span>(dl_train)</span><br><span class="line">images, labels = dl_train_iter.<span class="built_in">next</span>()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 仅查看一张图片</span></span><br><span class="line">writer = SummaryWriter(<span class="string">&#x27;../data/tensorboard&#x27;</span>)</span><br><span class="line">writer.add_image(<span class="string">&#x27;images[0]&#x27;</span>, images[<span class="number">0</span>])</span><br><span class="line">writer.close()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将多张图片拼接成一张图片，中间用黑色网格分割</span></span><br><span class="line">writer = SummaryWriter(<span class="string">&#x27;../data/tensorboard&#x27;</span>)</span><br><span class="line"><span class="comment"># create grid of images</span></span><br><span class="line">img_grid = torchvision.utils.make_grid(images)</span><br><span class="line">writer.add_image(<span class="string">&#x27;image_grid&#x27;</span>, img_grid)</span><br><span class="line">writer.close()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将多张图片直接写入</span></span><br><span class="line">writer = SummaryWriter(<span class="string">&#x27;../data/tensorboard&#x27;</span>)</span><br><span class="line">writer.add_images(<span class="string">&quot;images&quot;</span>,images,global_step = <span class="number">0</span>)</span><br><span class="line">writer.close()</span><br></pre></td></tr></table></figure><br><strong>可视化人工绘图</strong>：<br>如果我们将matplotlib绘图的结果再 tensorboard中展示，可以使用 add_figure.</p>
<p>注意，和writer.add_image不同的是，writer.add_figure需要传入matplotlib的figure对象。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataset,DataLoader</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms,datasets </span><br><span class="line"></span><br><span class="line">transform_train = transforms.Compose(</span><br><span class="line">    [transforms.ToTensor()])</span><br><span class="line">transform_valid = transforms.Compose(</span><br><span class="line">    [transforms.ToTensor()])</span><br><span class="line"></span><br><span class="line">ds_train = datasets.ImageFolder(<span class="string">&quot;../data/cifar2/train/&quot;</span>,</span><br><span class="line">            transform = transform_train,target_transform= <span class="keyword">lambda</span> t:torch.tensor([t]).<span class="built_in">float</span>())</span><br><span class="line">ds_valid = datasets.ImageFolder(<span class="string">&quot;../data/cifar2/test/&quot;</span>,</span><br><span class="line">            transform = transform_train,target_transform= <span class="keyword">lambda</span> t:torch.tensor([t]).<span class="built_in">float</span>())</span><br><span class="line"></span><br><span class="line">print(ds_train.class_to_idx)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">&#123;&#x27;0_airplane&#x27;: 0, &#x27;1_automobile&#x27;: 1&#125;</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">%matplotlib inline</span><br><span class="line">%config InlineBackend.figure_format = <span class="string">&#x27;svg&#x27;</span></span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt </span><br><span class="line"></span><br><span class="line">figure = plt.figure(figsize=(<span class="number">8</span>,<span class="number">8</span>)) </span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">9</span>):</span><br><span class="line">    img,label = ds_train[i]</span><br><span class="line">    img = img.permute(<span class="number">1</span>,<span class="number">2</span>,<span class="number">0</span>)</span><br><span class="line">    ax=plt.subplot(<span class="number">3</span>,<span class="number">3</span>,i+<span class="number">1</span>)</span><br><span class="line">    ax.imshow(img.numpy())</span><br><span class="line">    ax.set_title(<span class="string">&quot;label = %d&quot;</span>%label.item())</span><br><span class="line">    ax.set_xticks([])</span><br><span class="line">    ax.set_yticks([]) </span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Tensorboard显示</span></span><br><span class="line">writer = SummaryWriter(<span class="string">&#x27;../data/tensorboard&#x27;</span>)</span><br><span class="line">writer.add_figure(<span class="string">&#x27;figure&#x27;</span>,figure,global_step=<span class="number">0</span>)</span><br><span class="line">writer.close()         </span><br></pre></td></tr></table></figure></p>
<h1 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h1><p><span class="exturl" data-url="aHR0cHM6Ly9weXRvcmNoLm9yZy90dXRvcmlhbHMvYmVnaW5uZXIvZGVlcF9sZWFybmluZ182MG1pbl9ibGl0ei5odG1s">Pytorch官网<i class="fa fa-external-link-alt"></i></span><br><span class="exturl" data-url="aHR0cHM6Ly96aC12Mi5kMmwuYWkvY2hhcHRlcl9wcmVsaW1pbmFyaWVzL25kYXJyYXkuaHRtbA==">动手学深度学习<i class="fa fa-external-link-alt"></i></span><br><span class="exturl" data-url="aHR0cHM6Ly9qYWNraWV4aWFvLmdpdGh1Yi5pby9lYXRfcHl0b3JjaF9pbl8yMF9kYXlzLzIuJUU2JUEwJUI4JUU1JUJGJTgzJUU2JUE2JTgyJUU1JUJGJUI1LzItMSUyQyVFNSVCQyVBMCVFOSU4NyU4RiVFNiU5NSVCMCVFNiU4RCVBRSVFNyVCQiU5MyVFNiU5RSU4NC8=">20天吃掉那只Pytorch<i class="fa fa-external-link-alt"></i></span><br><span class="exturl" data-url="aHR0cHM6Ly93d3cuYmlsaWJpbGkuY29tL3ZpZGVvL0JWMW92NDExTTd4TD9zcG1faWRfZnJvbT0zMzMuOTk5LjAuMA==">PyTorch视频精讲<i class="fa fa-external-link-alt"></i></span><br><span class="exturl" data-url="aHR0cHM6Ly93d3cuam1sci5vcmcvcGFwZXJzL3ZvbHVtZTE4LzE3LTQ2OC8xNy00NjgucGRm">Automatic Differentiation in Machine Learning: a Survey<i class="fa fa-external-link-alt"></i></span></p>

    </div>

    
    
    
      
  <div class="popular-posts-header">相关文章</div>
  <ul class="popular-posts">
    <li class="popular-posts-item">
      <div class="popular-posts-title"><a href="\2021\06\13\NLP\00.TensorFlow\" rel="bookmark">TensorFlow2.0</a></div>
    </li>
    <li class="popular-posts-item">
      <div class="popular-posts-title"><a href="\2021\07\16\NLP\03.基于LSTM的机器翻译\" rel="bookmark">基于LSTM的机器翻译</a></div>
    </li>
    <li class="popular-posts-item">
      <div class="popular-posts-title"><a href="\2021\07\19\NLP\04.Transformer\" rel="bookmark">Transformer</a></div>
    </li>
    <li class="popular-posts-item">
      <div class="popular-posts-title"><a href="\2021\07\21\NLP\06.对话系统\" rel="bookmark">对话系统简介</a></div>
    </li>
    <li class="popular-posts-item">
      <div class="popular-posts-title"><a href="\2021\06\19\NLP\00.NLP简介\" rel="bookmark">NLP简介</a></div>
    </li>
  </ul>


    <footer class="post-footer">
          

<div class="post-copyright">
<ul>
  <li class="post-copyright-author">
      <strong>本文作者： </strong>SoundMemories
  </li>
  <li class="post-copyright-link">
      <strong>本文链接：</strong>
      <a href="https://soundmemories.github.io/2021/06/10/NLP/00.Pytorch/" title="Pytorch">https://soundmemories.github.io/2021/06/10/NLP/00.Pytorch/</a>
  </li>
  <li class="post-copyright-license">
    <strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <span class="exturl" data-url="aHR0cHM6Ly9jcmVhdGl2ZWNvbW1vbnMub3JnL2xpY2Vuc2VzL2J5LW5jLXNhLzQuMC8="><i class="fab fa-fw fa-creative-commons"></i>BY-NC-SA</span> 许可协议。转载请注明出处！
  </li>
</ul>
</div>

          <div class="post-tags">
              <a href="/tags/NLP/" rel="tag"><i class="fa fa-tag"></i> NLP</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2021/02/12/Graph/03.GraphSAGE/" rel="prev" title="GraphSAGE">
                  <i class="fa fa-chevron-left"></i> GraphSAGE
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2021/06/13/NLP/00.TensorFlow/" rel="next" title="TensorFlow2.0">
                  TensorFlow2.0 <i class="fa fa-chevron-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






      

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      const activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      const commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

    </div>
  </main>

  <footer class="footer">
    <div class="footer-inner">
      

      

<div class="copyright">
  
  &copy; 2019 – 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">SoundMemories</span>
</div>
  <div class="powered-by">由 <span class="exturl theme-link" data-url="aHR0cHM6Ly9oZXhvLmlv">Hexo</span> & <span class="exturl theme-link" data-url="aHR0cHM6Ly90aGVtZS1uZXh0LmpzLm9yZy9tdXNlLw==">NexT.Muse</span> 强力驱动
  </div>

    </div>
  </footer>

  
  <script src="//cdn.jsdelivr.net/npm/animejs@3.2.1/lib/anime.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/@next-theme/pjax@0.4.0/pjax.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/jquery@3.5.1/dist/jquery.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js"></script>
<script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/muse.js"></script><script src="/js/next-boot.js"></script><script src="/js/bookmark.js"></script>
  <script>
var pjax = new Pjax({
  selectors: [
    'head title',
    '.page-configurations',
    '.main-inner',
    '.post-toc-wrap',
    '.languages',
    '.pjax'
  ],
  analytics: false,
  cacheBust: false,
  scrollRestoration: false,
  scrollTo: !CONFIG.bookmark.enable
});

document.addEventListener('pjax:success', () => {
  pjax.executeScripts(document.querySelectorAll('script[data-pjax], .pjax script'));
  NexT.boot.refresh();
  // Define Motion Sequence & Bootstrap Motion.
  if (CONFIG.motion.enable) {
    NexT.motion.integrator
      .init()
      .add(NexT.motion.middleWares.subMenu)
      .add(NexT.motion.middleWares.postList)
      .bootstrap();
  }
  const hasTOC = document.querySelector('.post-toc');
  document.querySelector('.sidebar-inner').classList.toggle('sidebar-nav-active', hasTOC);
  document.querySelector(hasTOC ? '.sidebar-nav-toc' : '.sidebar-nav-overview').click();
  NexT.utils.updateSidebarPosition();
});
</script>


  




  <script src="/js/local-search.js"></script>








<script data-pjax>
document.querySelectorAll('.pdfobject-container').forEach(element => {
  const url = element.dataset.target;
  const pdfOpenParams = {
    navpanes : 0,
    toolbar  : 0,
    statusbar: 0,
    pagemode : 'thumbs',
    view     : 'FitH'
  };
  const pdfOpenFragment = '#' + Object.entries(pdfOpenParams).map(([key, value]) => `${key}=${encodeURIComponent(value)}`).join('&');
  const fullURL = `/lib/pdf/web/viewer.html?file=${encodeURIComponent(url)}${pdfOpenFragment}`;

  if (NexT.utils.supportsPDFs()) {
    element.innerHTML = `<embed class="pdfobject" src="${url + pdfOpenFragment}" type="application/pdf" style="height: ${element.dataset.height};">`;
  } else {
    element.innerHTML = `<iframe src="${fullURL}" style="height: ${element.dataset.height};" frameborder="0"></iframe>`;
  }
});
</script>


<script data-pjax>
if (document.querySelectorAll('.mermaid').length) {
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/mermaid@8.8.2/dist/mermaid.min.js', () => {
    mermaid.init({
      theme    : 'neutral',
      logLevel : 3,
      flowchart: { curve     : 'linear' },
      gantt    : { axisFormat: '%m/%d/%Y' },
      sequence : { actorMargin: 50 }
    }, '.mermaid');
  }, window.mermaid);
}
</script>





  








    <div class="pjax">
  

  
      <script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              const target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    const script = document.createElement('script');
    script.src = '//cdn.jsdelivr.net/npm/mathjax@3.1.2/es5/tex-mml-chtml.js';
    script.defer = true;
    document.head.appendChild(script);
  } else {
    MathJax.startup.document.state(0);
    MathJax.typesetClear();
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  
  <script src="//cdn.jsdelivr.net/npm/quicklink@2.0.0/dist/quicklink.umd.js"></script>
  <script>
      window.addEventListener('load', () => {
      quicklink.listen({
        timeout : 3000,
        priority: true,
        ignores : [uri => uri.includes('#'),uri => uri === 'https://soundmemories.github.io/2021/06/10/NLP/00.Pytorch/',]
      });
      });
  </script>

    </div>
</body>
</html>
