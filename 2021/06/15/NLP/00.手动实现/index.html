<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222" media="(prefers-color-scheme: light)">
<meta name="theme-color" content="#222" media="(prefers-color-scheme: dark)"><meta name="generator" content="Hexo 6.3.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="https://fonts.loli.net/css?family=Monda:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">

<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css" integrity="sha256-HtsXJanqjKTc8vVQjO4YMhiqFoXkfBsjBWcX91T1jr8=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.css" integrity="sha256-Vzbj7sDDS/woiFS3uNKo8eIuni59rjyNGtXfstRzStA=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"soundmemories.github.io","root":"/","images":"/images","scheme":"Muse","darkmode":true,"version":"8.17.1","exturl":true,"sidebar":{"position":"left","display":"always","padding":18,"offset":12},"copycode":{"enable":true,"style":null},"bookmark":{"enable":true,"color":"#222","save":"auto"},"mediumzoom":true,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"},"path":"/search.xml","localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false}}</script><script src="/js/config.js"></script>

    <meta name="description" content="dropout dropoutR-Drop1234567891011121314151617181920212223242526272829303132333435363738394041424344454647# 基础dropoutimport numpy as npdef train(rate, x, w1, b1, w2, b2):    &amp;#x27;&amp;#x27;&amp;#x27;    rat">
<meta property="og:type" content="article">
<meta property="og:title" content="手动实现+部分源码">
<meta property="og:url" content="https://soundmemories.github.io/2021/06/15/NLP/00.%E6%89%8B%E5%8A%A8%E5%AE%9E%E7%8E%B0/index.html">
<meta property="og:site_name" content="SoundMemories">
<meta property="og:description" content="dropout dropoutR-Drop1234567891011121314151617181920212223242526272829303132333435363738394041424344454647# 基础dropoutimport numpy as npdef train(rate, x, w1, b1, w2, b2):    &amp;#x27;&amp;#x27;&amp;#x27;    rat">
<meta property="og:locale" content="zh_CN">
<meta property="article:published_time" content="2021-06-14T16:00:00.000Z">
<meta property="article:modified_time" content="2023-05-25T15:43:18.621Z">
<meta property="article:author" content="SoundMemories">
<meta property="article:tag" content="NLP">
<meta name="twitter:card" content="summary">


<link rel="canonical" href="https://soundmemories.github.io/2021/06/15/NLP/00.%E6%89%8B%E5%8A%A8%E5%AE%9E%E7%8E%B0/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-CN","comments":"","permalink":"https://soundmemories.github.io/2021/06/15/NLP/00.%E6%89%8B%E5%8A%A8%E5%AE%9E%E7%8E%B0/","path":"2021/06/15/NLP/00.手动实现/","title":"手动实现+部分源码"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>手动实现+部分源码 | SoundMemories</title>
  








  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">SoundMemories</p>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="搜索" role="button">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签<span class="badge">10</span></a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类<span class="badge">10</span></a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档<span class="badge">128</span></a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup"><div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" maxlength="80"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close" role="button">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="search-result-container no-result">
  <div class="search-result-icon">
    <i class="fa fa-spinner fa-pulse fa-5x"></i>
  </div>
</div>

    </div>
  </div>

</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#dropout"><span class="nav-number">1.</span> <span class="nav-text">dropout</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E7%AE%97%E5%AD%90%E8%9E%8D%E5%90%88"><span class="nav-number">2.</span> <span class="nav-text">算子融合</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#transformer"><span class="nav-number">3.</span> <span class="nav-text">Transformer</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99"><span class="nav-number">4.</span> <span class="nav-text">参考资料</span></a></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="SoundMemories"
      src="/images/avstar.png">
  <p class="site-author-name" itemprop="name">SoundMemories</p>
  <div class="site-description" itemprop="description">今日事，今日毕</div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">128</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">10</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">10</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author animated">
      <span class="links-of-author-item">
        <span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL3NvdW5kbWVtb3JpZXM=" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;soundmemories"><i class="fab fa-github fa-fw"></i>GitHub</span>
      </span>
      <span class="links-of-author-item">
        <span class="exturl" data-url="bWFpbHRvOnNvdW5kbWVtb3JpZXNAMTYzLmNvbQ==" title="E-Mail → mailto:soundmemories@163.com"><i class="fa fa-envelope fa-fw"></i>E-Mail</span>
      </span>
  </div>
  <div class="cc-license animated" itemprop="license">
    <span class="exturl cc-opacity" data-url="aHR0cHM6Ly9jcmVhdGl2ZWNvbW1vbnMub3JnL2xpY2Vuc2VzL2J5LW5jLXNhLzQuMC8="><img src="https://cdnjs.cloudflare.com/ajax/libs/creativecommons-vocabulary/2020.11.3/assets/license_badges/small/by_nc_sa.svg" alt="Creative Commons"></span>
  </div>

        </div>
      </div>
    </div>

    
        <div class="pjax">
        </div>
  </aside>


    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://soundmemories.github.io/2021/06/15/NLP/00.%E6%89%8B%E5%8A%A8%E5%AE%9E%E7%8E%B0/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avstar.png">
      <meta itemprop="name" content="SoundMemories">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="SoundMemories">
      <meta itemprop="description" content="今日事，今日毕">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="手动实现+部分源码 | SoundMemories">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          手动实现+部分源码
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2021-06-15 00:00:00" itemprop="dateCreated datePublished" datetime="2021-06-15T00:00:00+08:00">2021-06-15</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/NLP/" itemprop="url" rel="index"><span itemprop="name">NLP</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>3.3k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>12 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody"><h1 id="dropout">dropout</h1>
<div class="tabs" id="one"><ul class="nav-tabs"><li class="tab active"><a href="#one-1">dropout</a></li><li class="tab"><a href="#one-2">R-Drop</a></li></ul><div class="tab-content"><div class="tab-pane active" id="one-1"><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 基础dropout</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">train</span>(<span class="params">rate, x, w1, b1, w2, b2</span>):</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    rate: dropout概率</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    layer1 = np.maximum(<span class="number">0</span>, np.dot(w1, x)+b1) <span class="comment"># maximum作为relu替代</span></span><br><span class="line">    mask1 = np.random.binomial(<span class="number">1</span>, <span class="number">1</span>-rate, layer1.shape) <span class="comment"># 二项分布，试验次数1，成功概率1-rate，形状同layer1</span></span><br><span class="line">    layer1 = layer1 * mask1</span><br><span class="line"></span><br><span class="line">    layer2 = np.maximum(<span class="number">0</span>, np.dot(w2, layer1)+b2) </span><br><span class="line">    mask2 = np.random.binomial(<span class="number">1</span>, <span class="number">1</span>-rate, layer2.shape) </span><br><span class="line">    layer2 = layer2 * mask2</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> layer2</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">test</span>(<span class="params">rate, x, w1, b1, w2, b2</span>):</span><br><span class="line">    layer1 = np.maximum(<span class="number">0</span>, np.dot(w1, x)+b1) </span><br><span class="line">    layer1 = layer1 * (<span class="number">1</span>-rate) <span class="comment"># 保证测试和训练期望一致，需要乘以1-rate</span></span><br><span class="line"></span><br><span class="line">    layer2 = np.maximum(<span class="number">0</span>, np.dot(w2, layer1)+b2) </span><br><span class="line">    layer2 = layer2 * (<span class="number">1</span>-rate)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> layer2</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">another_train</span>(<span class="params">rate, x, w1, b1, w2, b2</span>):</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    rate: dropout概率</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    layer1 = np.maximum(<span class="number">0</span>, np.dot(w1, x)+b1) <span class="comment"># maximum作为relu替代</span></span><br><span class="line">    mask1 = np.random.binomial(<span class="number">1</span>, <span class="number">1</span>-rate, layer1.shape) <span class="comment"># 二项分布，试验次数1，成功概率1-rate，形状同layer1</span></span><br><span class="line">    layer1 = layer1 * mask1</span><br><span class="line">    layer1 = layer1/(<span class="number">1</span>-rate) <span class="comment"># 把测试阶段的计算挪到训练，减少测试计算量</span></span><br><span class="line"></span><br><span class="line">    layer2 = np.maximum(<span class="number">0</span>, np.dot(w2, layer1)+b2) </span><br><span class="line">    mask2 = np.random.binomial(<span class="number">1</span>, <span class="number">1</span>-rate, layer2.shape) </span><br><span class="line">    layer2 = layer2 * mask2</span><br><span class="line">    layer2 = layer2/(<span class="number">1</span>-rate)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> layer2</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">another_test</span>(<span class="params">rate, x, w1, b1, w2, b2</span>):</span><br><span class="line">    layer1 = np.maximum(<span class="number">0</span>, np.dot(w1, x)+b1) </span><br><span class="line">    layer2 = np.maximum(<span class="number">0</span>, np.dot(w2, layer1)+b2) </span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> layer2</span><br></pre></td></tr></table></figure></div><div class="tab-pane" id="one-2"><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># https://github.com/dropreg/R-Drop</span></span><br><span class="line"><span class="comment"># https://spaces.ac.cn/archives/8496</span></span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="comment"># define your task model, which outputs the classifier logits</span></span><br><span class="line">model = TaskModel()</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">compute_kl_loss</span>(<span class="params">p, q pad_mask=<span class="literal">None</span></span>):</span><br><span class="line">    p_loss = F.kl_div(F.log_softmax(p, dim=-<span class="number">1</span>), F.softmax(q, dim=-<span class="number">1</span>), reduction=<span class="string">&#x27;none&#x27;</span>)</span><br><span class="line">    q_loss = F.kl_div(F.log_softmax(q, dim=-<span class="number">1</span>), F.softmax(p, dim=-<span class="number">1</span>), reduction=<span class="string">&#x27;none&#x27;</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># pad_mask is for seq-level tasks</span></span><br><span class="line">    <span class="keyword">if</span> pad_mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        p_loss.masked_fill_(pad_mask, <span class="number">0.</span>)</span><br><span class="line">        q_loss.masked_fill_(pad_mask, <span class="number">0.</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># You can choose whether to use function &quot;sum&quot; and &quot;mean&quot; depending on your task</span></span><br><span class="line">    p_loss = p_loss.<span class="built_in">sum</span>()</span><br><span class="line">    q_loss = q_loss.<span class="built_in">sum</span>()</span><br><span class="line"></span><br><span class="line">    loss = (p_loss + q_loss) / <span class="number">2</span></span><br><span class="line">    <span class="keyword">return</span> loss</span><br><span class="line"></span><br><span class="line"><span class="comment"># keep dropout and forward twice</span></span><br><span class="line">logits = model(x)</span><br><span class="line"></span><br><span class="line">logits2 = model(x)</span><br><span class="line"></span><br><span class="line"><span class="comment"># cross entropy loss for classifier</span></span><br><span class="line">ce_loss = <span class="number">0.5</span> * (cross_entropy_loss(logits, label) + cross_entropy_loss(logits2, label))</span><br><span class="line"></span><br><span class="line">kl_loss = compute_kl_loss(logits, logits2)</span><br><span class="line"></span><br><span class="line"><span class="comment"># carefully choose hyper-parameters</span></span><br><span class="line">loss = ce_loss + α * kl_loss</span><br></pre></td></tr></table></figure></div></div></div>
<h1 id="算子融合">算子融合</h1>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># RepVGG的算子融合</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">point-wise: 通道融合, 抛弃局部关联性。</span></span><br><span class="line"><span class="string">depth-wise: 局部关联性，抛弃通道融合。</span></span><br><span class="line"><span class="string">普通卷积可看成：depth-wise + 1*1 point-wise</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line">in_channels = <span class="number">2</span></span><br><span class="line">ou_channels = <span class="number">2</span></span><br><span class="line">kernel_size = <span class="number">3</span></span><br><span class="line">w = <span class="number">9</span></span><br><span class="line">h = <span class="number">9</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># res_block = 3*3 conv + 1*1 conv + input</span></span><br><span class="line">x = torch.ones(<span class="number">1</span>, in_channels, w, h) <span class="comment"># 输入图片</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 1.原生方法</span></span><br><span class="line">t1 = time.time()</span><br><span class="line">conv_2d = nn.Conv2d(in_channels, ou_channels, kernel_size, padding=<span class="string">&#x27;same&#x27;</span>)</span><br><span class="line">conv_2d_pointwise = nn.Conv2d(in_channels, ou_channels, <span class="number">1</span>)</span><br><span class="line">result1 = conv_2d(x) + conv_2d_pointwise(x) + x</span><br><span class="line">t2 = time.time()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2.算子融合</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">point-wise卷积-&gt;3*3</span></span><br><span class="line"><span class="string">x-&gt;3*3</span></span><br><span class="line"><span class="string">3个卷积-&gt;1个卷积</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="comment"># point-wise卷积-&gt;2*2*3*3</span></span><br><span class="line"><span class="comment"># pad: 左右上下前后内外，这里只pad 1*1 的左右上下</span></span><br><span class="line">pointwise_to_conv_weight = F.pad(conv_2d_pointwise.weight, [<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>]) <span class="comment"># 2*2*1*1-&gt;2*2*3*3</span></span><br><span class="line">conv_2d_for_pointwise = nn.Conv2d(in_channels, ou_channels, kernel_size, padding=<span class="string">&#x27;same&#x27;</span>)</span><br><span class="line">conv_2d_for_pointwise.weight = nn.Parameter(pointwise_to_conv_weight)</span><br><span class="line">conv_2d_for_pointwise.bias = conv_2d_pointwise.bias</span><br><span class="line"></span><br><span class="line"><span class="comment"># x-&gt;2*2*3*3 </span></span><br><span class="line">zeros = torch.unsqueeze(torch.zeros(kernel_size, kernel_size), <span class="number">0</span>)</span><br><span class="line">stars = torch.unsqueeze(F.pad(torch.ones(<span class="number">1</span>, <span class="number">1</span>), [<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>]), <span class="number">0</span>)</span><br><span class="line">stars_zeros = torch.unsqueeze(torch.cat([stars, zeros], <span class="number">0</span>), <span class="number">0</span>)</span><br><span class="line">zeros_stars = torch.unsqueeze(torch.cat([zeros, stars], <span class="number">0</span>), <span class="number">0</span>)</span><br><span class="line">identity_to_conv_weight = torch.cat([stars_zeros, zeros_stars], <span class="number">0</span>)</span><br><span class="line">identity_to_conv_bias = torch.zeros([ou_channels]) </span><br><span class="line">conv_2d_for_identity = nn.Conv2d(in_channels, ou_channels, kernel_size, padding=<span class="string">&#x27;same&#x27;</span>)</span><br><span class="line">conv_2d_for_identity.weight = nn.Parameter(identity_to_conv_weight)</span><br><span class="line">conv_2d_for_identity.bias = nn.Parameter(identity_to_conv_bias)</span><br><span class="line"></span><br><span class="line">result2 = conv_2d(x) + conv_2d_for_pointwise(x) + conv_2d_for_identity(x)</span><br><span class="line"><span class="built_in">print</span>(torch.<span class="built_in">all</span>(torch.isclose(result1, result2)))</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">tensor(True)</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="comment"># 融合</span></span><br><span class="line">t3 = time.time()</span><br><span class="line">conv_2d_for_fusion = nn.Conv2d(in_channels, ou_channels, kernel_size, padding=<span class="string">&#x27;same&#x27;</span>)</span><br><span class="line">conv_2d_for_fusion.weight = nn.Parameter(conv_2d.weight.data +</span><br><span class="line">                                         conv_2d_for_pointwise.weight.data +</span><br><span class="line">                                         conv_2d_for_identity.weight.data)</span><br><span class="line">conv_2d_for_fusion.bias = nn.Parameter(conv_2d.bias.data +</span><br><span class="line">                                         conv_2d_for_pointwise.bias.data +</span><br><span class="line">                                         conv_2d_for_identity.bias.data)</span><br><span class="line">result3 = conv_2d_for_fusion(x)</span><br><span class="line">t4 = time.time()</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(torch.<span class="built_in">all</span>(torch.isclose(result2, result3)))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;原生写法耗时:&#x27;</span>, t2-t1)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;融合算子耗时:&#x27;</span>, t4-t3)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">tensor(True)</span></span><br><span class="line"><span class="string">原生写法耗时: 0.7473533153533936</span></span><br><span class="line"><span class="string">原生写法耗时: 0.0010013580322265625</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>
<p>ConvMixer 使用了算子融合：<br />
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># https://github.com/locuslab/convmixer</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">ConvMixer</span>(<span class="params">h, depth, kernel_size=<span class="number">9</span>, patch_size=<span class="number">7</span>, n_classes=<span class="number">1000</span></span>):</span><br><span class="line">    Seq, ActBn = nn.Sequential, <span class="keyword">lambda</span> x: Seq(x, nn.GELU(), nn.BatchNorm2d(h))</span><br><span class="line">    Residual = <span class="built_in">type</span>(<span class="string">&#x27;Residual&#x27;</span>, (Seq,), &#123;<span class="string">&#x27;forward&#x27;</span>: <span class="keyword">lambda</span> self, x: self[<span class="number">0</span>](x) + x&#125;)</span><br><span class="line">    <span class="keyword">return</span> Seq(ActBn(nn.Conv2d(<span class="number">3</span>, h, patch_size, stride=patch_size)),</span><br><span class="line">           *[Seq(Residual(ActBn(nn.Conv2d(h, h, kernel_size, groups=h, padding=<span class="string">&quot;same&quot;</span>))),</span><br><span class="line">                ActBn(nn.Conv2d(h, h, <span class="number">1</span>))) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(depth)],</span><br><span class="line">            nn.AdaptiveAvgPool2d((<span class="number">1</span>,<span class="number">1</span>)), nn.Flatten(), nn.Linear(h, n_classes))</span><br></pre></td></tr></table></figure></p>
<h1 id="transformer">Transformer</h1>
<p>seq2seq基础模块分类：<br />
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">- CNN</span><br><span class="line"> - 权重共享</span><br><span class="line">    - 平移不变性</span><br><span class="line">    - 可并行计算</span><br><span class="line"> - 滑动窗口，局部关联性建模，依靠多层堆积来进行长程建模</span><br><span class="line"> - 对相对位置敏感，对绝对位置不敏感</span><br><span class="line"></span><br><span class="line">- RNN(依次有序递归建模)</span><br><span class="line"> - 对顺序敏感</span><br><span class="line"> - 串行计算耗时</span><br><span class="line"> - 长程建模能力弱</span><br><span class="line"> - 计算复杂度与序列长度呈线性关系</span><br><span class="line"> - 单步计算复杂度不变</span><br><span class="line"> - 对相对位置、绝对位置都敏感</span><br><span class="line"></span><br><span class="line">- transformer</span><br><span class="line"> - 无局部假设</span><br><span class="line">    - 可并行计算</span><br><span class="line">    - 对相对位置不敏感</span><br><span class="line"> - 无有序假设</span><br><span class="line">    - 需要位置编码来反映位置变化对特征的影响</span><br><span class="line">    - 对绝对位置不敏感</span><br><span class="line">- 任意两字符都可建模</span><br><span class="line">    - 擅长长短程建模</span><br><span class="line">    - 自注意力机制需要序列长度的平方级别复杂度</span><br><span class="line">- 总结特点：</span><br><span class="line">    - 无先验假设(例如: 局部关联性、有序建模性)</span><br><span class="line">    - 核心计算在于自注意力机制，平方复杂度n^2*d</span><br><span class="line">    - 数据量的要求与先验假设的程度成反比(数据量少，需要注入的先验假设越多，比如对loss和多头的改进假设)</span><br><span class="line">- 使用类型：</span><br><span class="line">    - Encoder only：Bert、分类任务、非流式任务</span><br><span class="line">    - Decoder only：GPT系列、语言建模、自回归生成任务、流式任务</span><br><span class="line">    - Encoder-Decoder：机器翻译、语音识别</span><br></pre></td></tr></table></figure><br />
Transformer结构：<br />
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">- Transformer</span><br><span class="line"> - Encoder</span><br><span class="line">    - input word embedding: 由稀疏的one-hot进入一个不带bias的FFN得到一个稠密的连续向量。</span><br><span class="line">    - position encoding: </span><br><span class="line">        - 通过sin/cos来固定表征：每个位置确定性的; 对于不同长度的句子，两个词相对位置的距离一致; 可以推广到更长的测试句子。</span><br><span class="line">        - pe(pos+k)可以写成pe(pos)的线性组合。</span><br><span class="line">        - 通过残差连接来使得位置信息流入深层。</span><br><span class="line">    - multi-head self-attention</span><br><span class="line">        - 使得建模能力更强，表征空间更丰富。</span><br><span class="line">        - 由多个QKV构成，每组单独计算一个attention向量。</span><br><span class="line">        - 把每组的attention向量拼起来，并入一个不带bias的FFN得到最终的向量。</span><br><span class="line">    - feed-forward network</span><br><span class="line">        - 只考虑embedding的每个维度进行建模。</span><br><span class="line">        - 不同位置参数共享。</span><br><span class="line">        - 类似1*1 point-wise convolution。</span><br><span class="line"> - Decoder</span><br><span class="line">  - output word embedding</span><br><span class="line">  - position encoding</span><br><span class="line">  - mask multi-head self-attention</span><br><span class="line">  - multi-head cross-attention</span><br><span class="line">  - feed-forward network</span><br><span class="line">  - softmax</span><br></pre></td></tr></table></figure><br />
Transformer Pytorch源码：<br />
补充阅读：<span class="exturl" data-url="aHR0cHM6Ly9ubHAuc2Vhcy5oYXJ2YXJkLmVkdS8yMDE4LzA0LzAzL2F0dGVudGlvbi5odG1s">The
Annotated Transformer<i class="fa fa-external-link-alt"></i></span><br />
<div class="tabs" id="tr"><ul class="nav-tabs"><li class="tab active"><a href="#tr-1">Transformer</a></li><li class="tab"><a href="#tr-2">Encoder/Decoder</a></li><li class="tab"><a href="#tr-3">EncoderLayer/DecoderLayer</a></li><li class="tab"><a href="#tr-4">MultiheadAttention</a></li><li class="tab"><a href="#tr-5">Solution 3</a></li></ul><div class="tab-content"><div class="tab-pane active" id="tr-1"><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Transformer</span>(<span class="title class_ inherited__">Module</span>):</span><br><span class="line">    <span class="string">r&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">     Examples::</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; transformer_model = nn.Transformer(nhead=16, num_encoder_layers=12)</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; src = torch.rand((10, 32, 512))</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; tgt = torch.rand((20, 32, 512))</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; out = transformer_model(src, tgt)</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, </span></span><br><span class="line"><span class="params">                d_model: <span class="built_in">int</span> = <span class="number">512</span>,  <span class="comment"># 模型维度</span></span></span><br><span class="line"><span class="params">                nhead: <span class="built_in">int</span> = <span class="number">8</span>,  <span class="comment"># 多头注意力层数</span></span></span><br><span class="line"><span class="params">                num_encoder_layers: <span class="built_in">int</span> = <span class="number">6</span>, <span class="comment"># encoder block数</span></span></span><br><span class="line"><span class="params">                num_decoder_layers: <span class="built_in">int</span> = <span class="number">6</span>, <span class="comment"># decoder block数</span></span></span><br><span class="line"><span class="params">                dim_feedforward: <span class="built_in">int</span> = <span class="number">2048</span>, <span class="comment"># FFN中间维度</span></span></span><br><span class="line"><span class="params">                dropout: <span class="built_in">float</span> = <span class="number">0.1</span>, <span class="comment"># 丢弃概率</span></span></span><br><span class="line"><span class="params">                activation: <span class="type">Union</span>[<span class="built_in">str</span>, <span class="type">Callable</span>[[Tensor], Tensor]] = F.relu, <span class="comment"># 激活函数</span></span></span><br><span class="line"><span class="params">                custom_encoder: <span class="type">Optional</span>[<span class="type">Any</span>] = <span class="literal">None</span>, </span></span><br><span class="line"><span class="params">                custom_decoder: <span class="type">Optional</span>[<span class="type">Any</span>] = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params">                layer_norm_eps: <span class="built_in">float</span> = <span class="number">1e-5</span>, </span></span><br><span class="line"><span class="params">                batch_first: <span class="built_in">bool</span> = <span class="literal">False</span>,  <span class="comment"># batch在第0维？</span></span></span><br><span class="line"><span class="params">                norm_first: <span class="built_in">bool</span> = <span class="literal">False</span>, </span></span><br><span class="line"><span class="params">                device=<span class="literal">None</span>, </span></span><br><span class="line"><span class="params">                dtype=<span class="literal">None</span></span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">            factory_kwargs = &#123;<span class="string">&#x27;device&#x27;</span>: device, <span class="string">&#x27;dtype&#x27;</span>: dtype&#125;</span><br><span class="line">            <span class="built_in">super</span>(Transformer, self).__init__()</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> custom_encoder <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                self.encoder = custom_encoder</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="comment"># EncoderLayer</span></span><br><span class="line">                encoder_layer = TransformerEncoderLayer(d_model, nhead, dim_feedforward, dropout,</span><br><span class="line">                                                        activation, layer_norm_eps, batch_first, norm_first,</span><br><span class="line">                                                        **factory_kwargs)</span><br><span class="line">                encoder_norm = LayerNorm(d_model, eps=layer_norm_eps, **factory_kwargs)</span><br><span class="line">                <span class="comment"># Encoder block</span></span><br><span class="line">                self.encoder = TransformerEncoder(encoder_layer, num_encoder_layers, encoder_norm)</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> custom_decoder <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                self.decoder = custom_decoder</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="comment"># DecoderLayer</span></span><br><span class="line">                decoder_layer = TransformerDecoderLayer(d_model, nhead, dim_feedforward, dropout,</span><br><span class="line">                                                        activation, layer_norm_eps, batch_first, norm_first,</span><br><span class="line">                                                        **factory_kwargs)</span><br><span class="line">                decoder_norm = LayerNorm(d_model, eps=layer_norm_eps, **factory_kwargs)</span><br><span class="line">                <span class="comment"># Decoder block</span></span><br><span class="line">                self.decoder = TransformerDecoder(decoder_layer, num_decoder_layers, decoder_norm)</span><br><span class="line">            ...</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, </span></span><br><span class="line"><span class="params">                src: Tensor,  <span class="comment"># 源</span></span></span><br><span class="line"><span class="params">                tgt: Tensor,  <span class="comment"># 目标</span></span></span><br><span class="line"><span class="params">                src_mask: <span class="type">Optional</span>[Tensor] = <span class="literal">None</span>,  <span class="comment"># 源的mask(通过mask能还原原本句子)</span></span></span><br><span class="line"><span class="params">                tgt_mask: <span class="type">Optional</span>[Tensor] = <span class="literal">None</span>,  <span class="comment"># 目标的mask</span></span></span><br><span class="line"><span class="params">                memory_mask: <span class="type">Optional</span>[Tensor] = <span class="literal">None</span>, <span class="comment"># encoder输出mask</span></span></span><br><span class="line"><span class="params">                src_key_padding_mask: <span class="type">Optional</span>[Tensor] = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params">                tgt_key_padding_mask: <span class="type">Optional</span>[Tensor] = <span class="literal">None</span>, </span></span><br><span class="line"><span class="params">                memory_key_padding_mask: <span class="type">Optional</span>[Tensor] = <span class="literal">None</span></span></span><br><span class="line"><span class="params">                </span>) -&gt; Tensor:</span><br><span class="line">        <span class="string">r&quot;&quot;&quot;Take in and process masked source/target sequences.</span></span><br><span class="line"><span class="string">        Shape:</span></span><br><span class="line"><span class="string">            - src: :math:`(S, N, E)`, `(N, S, E)` if batch_first.</span></span><br><span class="line"><span class="string">            - tgt: :math:`(T, N, E)`, `(N, T, E)` if batch_first.</span></span><br><span class="line"><span class="string">            - src_mask: :math:`(S, S)`.</span></span><br><span class="line"><span class="string">            - tgt_mask: :math:`(T, T)`.</span></span><br><span class="line"><span class="string">            - memory_mask: :math:`(T, S)`.</span></span><br><span class="line"><span class="string">            - src_key_padding_mask: :math:`(N, S)`.</span></span><br><span class="line"><span class="string">            - tgt_key_padding_mask: :math:`(N, T)`.</span></span><br><span class="line"><span class="string">            - memory_key_padding_mask: :math:`(N, S)`.</span></span><br><span class="line"><span class="string">        - output: :math:`(T, N, E)`, `(N, T, E)` if batch_first.</span></span><br><span class="line"><span class="string">        Examples:</span></span><br><span class="line"><span class="string">            &gt;&gt;&gt; output = transformer_model(src, tgt, src_mask=src_mask, tgt_mask=tgt_mask)</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment"># 调用encoder和decoder得到结果</span></span><br><span class="line">        memory = self.encoder(src, mask=src_mask, src_key_padding_mask=src_key_padding_mask)</span><br><span class="line">        output = self.decoder(tgt, memory, tgt_mask=tgt_mask, memory_mask=memory_mask,</span><br><span class="line">                              tgt_key_padding_mask=tgt_key_padding_mask,</span><br><span class="line">                              memory_key_padding_mask=memory_key_padding_mask)</span><br><span class="line">        <span class="keyword">return</span> output</span><br></pre></td></tr></table></figure></div><div class="tab-pane" id="tr-2"><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">TransformerEncoder</span>(<span class="title class_ inherited__">Module</span>):</span><br><span class="line">    <span class="string">r&quot;&quot;&quot;TransformerEncoder is a stack of N encoder layers</span></span><br><span class="line"><span class="string">    Examples::</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; encoder_layer = nn.TransformerEncoderLayer(d_model=512, nhead=8)</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=6)</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; src = torch.rand(10, 32, 512)</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; out = transformer_encoder(src)</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    __constants__ = [<span class="string">&#x27;norm&#x27;</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, </span></span><br><span class="line"><span class="params">                 encoder_layer, <span class="comment"># 定义的encoder block</span></span></span><br><span class="line"><span class="params">                 num_layers, <span class="comment"># block数</span></span></span><br><span class="line"><span class="params">                 norm=<span class="literal">None</span> <span class="comment"># normalization方法</span></span></span><br><span class="line"><span class="params">                 </span>):</span><br><span class="line">        <span class="built_in">super</span>(TransformerEncoder, self).__init__()</span><br><span class="line">        self.layers = _get_clones(encoder_layer, num_layers)</span><br><span class="line">        self.num_layers = num_layers</span><br><span class="line">        self.norm = norm</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, </span></span><br><span class="line"><span class="params">                src: Tensor,  <span class="comment"># 源</span></span></span><br><span class="line"><span class="params">                mask: <span class="type">Optional</span>[Tensor] = <span class="literal">None</span>, <span class="comment"># 源的mask(通过mask能还原原本句子)</span></span></span><br><span class="line"><span class="params">                src_key_padding_mask: <span class="type">Optional</span>[Tensor] = <span class="literal">None</span></span></span><br><span class="line"><span class="params">                </span>) -&gt; Tensor:</span><br><span class="line">        <span class="string">r&quot;&quot;&quot;Pass the input through the encoder layers in turn.&quot;&quot;&quot;</span></span><br><span class="line">        output = src</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> mod <span class="keyword">in</span> self.layers:</span><br><span class="line">            output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.norm <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            output = self.norm(output)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> output</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">TransformerDecoder</span>(<span class="title class_ inherited__">Module</span>):</span><br><span class="line">    <span class="string">r&quot;&quot;&quot;TransformerDecoder is a stack of N decoder layers</span></span><br><span class="line"><span class="string">    Examples::</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; decoder_layer = nn.TransformerDecoderLayer(d_model=512, nhead=8)</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; transformer_decoder = nn.TransformerDecoder(decoder_layer, num_layers=6)</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; memory = torch.rand(10, 32, 512)</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; tgt = torch.rand(20, 32, 512)</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; out = transformer_decoder(tgt, memory)</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    __constants__ = [<span class="string">&#x27;norm&#x27;</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, </span></span><br><span class="line"><span class="params">                 decoder_layer, <span class="comment"># # 定义的decoder block</span></span></span><br><span class="line"><span class="params">                 num_layers,  <span class="comment"># block数</span></span></span><br><span class="line"><span class="params">                 norm=<span class="literal">None</span> <span class="comment"># normalization方法</span></span></span><br><span class="line"><span class="params">                 </span>):</span><br><span class="line">        <span class="built_in">super</span>(TransformerDecoder, self).__init__()</span><br><span class="line">        self.layers = _get_clones(decoder_layer, num_layers)</span><br><span class="line">        self.num_layers = num_layers</span><br><span class="line">        self.norm = norm</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, </span></span><br><span class="line"><span class="params">                tgt: Tensor, <span class="comment"># 目标</span></span></span><br><span class="line"><span class="params">                memory: Tensor, <span class="comment"># encoder输出(最后一层)</span></span></span><br><span class="line"><span class="params">                tgt_mask: <span class="type">Optional</span>[Tensor] = <span class="literal">None</span>, <span class="comment"># 目标的mask</span></span></span><br><span class="line"><span class="params">                memory_mask: <span class="type">Optional</span>[Tensor] = <span class="literal">None</span>, <span class="comment"># encoder输出mask</span></span></span><br><span class="line"><span class="params">                tgt_key_padding_mask: <span class="type">Optional</span>[Tensor] = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params">                memory_key_padding_mask: <span class="type">Optional</span>[Tensor] = <span class="literal">None</span></span></span><br><span class="line"><span class="params">                </span>) -&gt; Tensor:</span><br><span class="line">        <span class="string">r&quot;&quot;&quot;Pass the inputs (and mask) through the decoder layer in turn.&quot;&quot;&quot;</span></span><br><span class="line">        output = tgt</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> mod <span class="keyword">in</span> self.layers:</span><br><span class="line">            output = mod(output, memory, tgt_mask=tgt_mask,</span><br><span class="line">                         memory_mask=memory_mask,</span><br><span class="line">                         tgt_key_padding_mask=tgt_key_padding_mask,</span><br><span class="line">                         memory_key_padding_mask=memory_key_padding_mask)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.norm <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            output = self.norm(output)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> output</span><br></pre></td></tr></table></figure></div><div class="tab-pane" id="tr-3"><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">TransformerEncoderLayer</span>(<span class="title class_ inherited__">Module</span>):</span><br><span class="line">    <span class="string">r&quot;&quot;&quot;TransformerEncoderLayer is made up of self-attn and feedforward network.</span></span><br><span class="line"><span class="string">    Examples::</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; encoder_layer = nn.TransformerEncoderLayer(d_model=512, nhead=8)</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; src = torch.rand(10, 32, 512)</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; out = encoder_layer(src)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Alternatively, when ``batch_first`` is ``True``:</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; encoder_layer = nn.TransformerEncoderLayer(d_model=512, nhead=8, batch_first=True)</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; src = torch.rand(32, 10, 512)</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; out = encoder_layer(src)</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    __constants__ = [<span class="string">&#x27;batch_first&#x27;</span>, <span class="string">&#x27;norm_first&#x27;</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, </span></span><br><span class="line"><span class="params">                 d_model, <span class="comment"># 模型维度</span></span></span><br><span class="line"><span class="params">                 nhead, <span class="comment"># 头数</span></span></span><br><span class="line"><span class="params">                 dim_feedforward=<span class="number">2048</span>, <span class="comment"># FFN中间维度</span></span></span><br><span class="line"><span class="params">                 dropout=<span class="number">0.1</span>, </span></span><br><span class="line"><span class="params">                 activation=F.relu,</span></span><br><span class="line"><span class="params">                 layer_norm_eps=<span class="number">1e-5</span>, </span></span><br><span class="line"><span class="params">                 batch_first=<span class="literal">False</span>, </span></span><br><span class="line"><span class="params">                 norm_first=<span class="literal">False</span>,</span></span><br><span class="line"><span class="params">                 device=<span class="literal">None</span>, </span></span><br><span class="line"><span class="params">                 dtype=<span class="literal">None</span></span></span><br><span class="line"><span class="params">                 </span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">        factory_kwargs = &#123;<span class="string">&#x27;device&#x27;</span>: device, <span class="string">&#x27;dtype&#x27;</span>: dtype&#125;</span><br><span class="line">        <span class="built_in">super</span>(TransformerEncoderLayer, self).__init__()</span><br><span class="line">        self.self_attn = MultiheadAttention(d_model, nhead, dropout=dropout, batch_first=batch_first,</span><br><span class="line">                                            **factory_kwargs)</span><br><span class="line">        <span class="comment"># Implementation of Feedforward model</span></span><br><span class="line">        self.linear1 = Linear(d_model, dim_feedforward, **factory_kwargs) <span class="comment"># 先投射到2048维</span></span><br><span class="line">        self.dropout = Dropout(dropout) </span><br><span class="line">        self.linear2 = Linear(dim_feedforward, d_model, **factory_kwargs) <span class="comment"># 再投射到521维</span></span><br><span class="line"></span><br><span class="line">        self.norm_first = norm_first</span><br><span class="line">        self.norm1 = LayerNorm(d_model, eps=layer_norm_eps, **factory_kwargs)</span><br><span class="line">        self.norm2 = LayerNorm(d_model, eps=layer_norm_eps, **factory_kwargs)</span><br><span class="line">        self.dropout1 = Dropout(dropout)</span><br><span class="line">        self.dropout2 = Dropout(dropout)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Legacy string support for activation function.</span></span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">isinstance</span>(activation, <span class="built_in">str</span>):</span><br><span class="line">            self.activation = _get_activation_fn(activation)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.activation = activation</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__setstate__</span>(<span class="params">self, state</span>):</span><br><span class="line">        <span class="keyword">if</span> <span class="string">&#x27;activation&#x27;</span> <span class="keyword">not</span> <span class="keyword">in</span> state:</span><br><span class="line">            state[<span class="string">&#x27;activation&#x27;</span>] = F.relu</span><br><span class="line">        <span class="built_in">super</span>(TransformerEncoderLayer, self).__setstate__(state)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, </span></span><br><span class="line"><span class="params">                src: Tensor, <span class="comment"># 源</span></span></span><br><span class="line"><span class="params">                src_mask: <span class="type">Optional</span>[Tensor] = <span class="literal">None</span>, <span class="comment"># 源mask</span></span></span><br><span class="line"><span class="params">                src_key_padding_mask: <span class="type">Optional</span>[Tensor] = <span class="literal">None</span></span></span><br><span class="line"><span class="params">                </span>) -&gt; Tensor:</span><br><span class="line">        <span class="string">r&quot;&quot;&quot;Pass the input through the encoder layer.&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># see Fig. 1 of https://arxiv.org/pdf/2002.04745v1.pdf</span></span><br><span class="line"></span><br><span class="line">        x = src</span><br><span class="line">        <span class="keyword">if</span> self.norm_first:</span><br><span class="line">            x = x + self._sa_block(self.norm1(x), src_mask, src_key_padding_mask)</span><br><span class="line">            x = x + self._ff_block(self.norm2(x))</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># norm(x + self-attention block) -&gt; norm(x + feed forward block), 注意，此时的x是前面的输出</span></span><br><span class="line">            x = self.norm1(x + self._sa_block(x, src_mask, src_key_padding_mask))</span><br><span class="line">            x = self.norm2(x + self._ff_block(x))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment"># self-attention block</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_sa_block</span>(<span class="params">self, </span></span><br><span class="line"><span class="params">                  x: Tensor,</span></span><br><span class="line"><span class="params">                  attn_mask: <span class="type">Optional</span>[Tensor], </span></span><br><span class="line"><span class="params">                  key_padding_mask: <span class="type">Optional</span>[Tensor]</span></span><br><span class="line"><span class="params">                  </span>) -&gt; Tensor:</span><br><span class="line">        x = self.self_attn(x, x, x,</span><br><span class="line">                           attn_mask=attn_mask,</span><br><span class="line">                           key_padding_mask=key_padding_mask,</span><br><span class="line">                           need_weights=<span class="literal">False</span>)[<span class="number">0</span>]</span><br><span class="line">        <span class="keyword">return</span> self.dropout1(x)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># feed forward block</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_ff_block</span>(<span class="params">self, x: Tensor</span>) -&gt; Tensor:</span><br><span class="line">        x = self.linear2(self.dropout(self.activation(self.linear1(x))))</span><br><span class="line">        <span class="keyword">return</span> self.dropout2(x)</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">TransformerDecoderLayer</span>(<span class="title class_ inherited__">Module</span>):</span><br><span class="line">    <span class="string">r&quot;&quot;&quot;TransformerDecoderLayer is made up of self-attn, multi-head-attn and feedforward network.</span></span><br><span class="line"><span class="string">    Examples::</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; decoder_layer = nn.TransformerDecoderLayer(d_model=512, nhead=8)</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; memory = torch.rand(10, 32, 512)</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; tgt = torch.rand(20, 32, 512)</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; out = decoder_layer(tgt, memory)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Alternatively, when ``batch_first`` is ``True``:</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; decoder_layer = nn.TransformerDecoderLayer(d_model=512, nhead=8, batch_first=True)</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; memory = torch.rand(32, 10, 512)</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; tgt = torch.rand(32, 20, 512)</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; out = decoder_layer(tgt, memory)</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    __constants__ = [<span class="string">&#x27;batch_first&#x27;</span>, <span class="string">&#x27;norm_first&#x27;</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, </span></span><br><span class="line"><span class="params">                 d_model, </span></span><br><span class="line"><span class="params">                 nhead, </span></span><br><span class="line"><span class="params">                 dim_feedforward=<span class="number">2048</span>, </span></span><br><span class="line"><span class="params">                 dropout=<span class="number">0.1</span>, </span></span><br><span class="line"><span class="params">                 activation=F.relu,</span></span><br><span class="line"><span class="params">                 layer_norm_eps=<span class="number">1e-5</span>, </span></span><br><span class="line"><span class="params">                 batch_first=<span class="literal">False</span>, </span></span><br><span class="line"><span class="params">                 norm_first=<span class="literal">False</span>,</span></span><br><span class="line"><span class="params">                 device=<span class="literal">None</span>, </span></span><br><span class="line"><span class="params">                 dtype=<span class="literal">None</span></span></span><br><span class="line"><span class="params">                 </span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">        factory_kwargs = &#123;<span class="string">&#x27;device&#x27;</span>: device, <span class="string">&#x27;dtype&#x27;</span>: dtype&#125;</span><br><span class="line">        <span class="built_in">super</span>(TransformerDecoderLayer, self).__init__()</span><br><span class="line">        <span class="comment"># 2个多头注意力，一个self-attention，一个cross-attention</span></span><br><span class="line">        self.self_attn = MultiheadAttention(d_model, nhead, dropout=dropout, batch_first=batch_first,</span><br><span class="line">                                            **factory_kwargs)</span><br><span class="line">        self.multihead_attn = MultiheadAttention(d_model, nhead, dropout=dropout, batch_first=batch_first,</span><br><span class="line">                                                 **factory_kwargs)</span><br><span class="line">        <span class="comment"># Implementation of Feedforward model</span></span><br><span class="line">        self.linear1 = Linear(d_model, dim_feedforward, **factory_kwargs)</span><br><span class="line">        self.dropout = Dropout(dropout)</span><br><span class="line">        self.linear2 = Linear(dim_feedforward, d_model, **factory_kwargs)</span><br><span class="line"></span><br><span class="line">        self.norm_first = norm_first</span><br><span class="line">        self.norm1 = LayerNorm(d_model, eps=layer_norm_eps, **factory_kwargs)</span><br><span class="line">        self.norm2 = LayerNorm(d_model, eps=layer_norm_eps, **factory_kwargs)</span><br><span class="line">        self.norm3 = LayerNorm(d_model, eps=layer_norm_eps, **factory_kwargs)</span><br><span class="line">        self.dropout1 = Dropout(dropout)</span><br><span class="line">        self.dropout2 = Dropout(dropout)</span><br><span class="line">        self.dropout3 = Dropout(dropout)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Legacy string support for activation function.</span></span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">isinstance</span>(activation, <span class="built_in">str</span>):</span><br><span class="line">            self.activation = _get_activation_fn(activation)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.activation = activation</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__setstate__</span>(<span class="params">self, state</span>):</span><br><span class="line">        <span class="keyword">if</span> <span class="string">&#x27;activation&#x27;</span> <span class="keyword">not</span> <span class="keyword">in</span> state:</span><br><span class="line">            state[<span class="string">&#x27;activation&#x27;</span>] = F.relu</span><br><span class="line">        <span class="built_in">super</span>(TransformerDecoderLayer, self).__setstate__(state)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, </span></span><br><span class="line"><span class="params">                tgt: Tensor, </span></span><br><span class="line"><span class="params">                memory: Tensor, </span></span><br><span class="line"><span class="params">                tgt_mask: <span class="type">Optional</span>[Tensor] = <span class="literal">None</span>, </span></span><br><span class="line"><span class="params">                memory_mask: <span class="type">Optional</span>[Tensor] = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params">                tgt_key_padding_mask: <span class="type">Optional</span>[Tensor] = <span class="literal">None</span>, </span></span><br><span class="line"><span class="params">                memory_key_padding_mask: <span class="type">Optional</span>[Tensor] = <span class="literal">None</span></span></span><br><span class="line"><span class="params">                </span>) -&gt; Tensor:</span><br><span class="line">        <span class="string">r&quot;&quot;&quot;Pass the inputs (and mask) through the decoder layer.</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            tgt: the sequence to the decoder layer (required).</span></span><br><span class="line"><span class="string">            memory: the sequence from the last layer of the encoder (required).</span></span><br><span class="line"><span class="string">            tgt_mask: the mask for the tgt sequence (optional).</span></span><br><span class="line"><span class="string">            memory_mask: the mask for the memory sequence (optional).</span></span><br><span class="line"><span class="string">            tgt_key_padding_mask: the mask for the tgt keys per batch (optional).</span></span><br><span class="line"><span class="string">            memory_key_padding_mask: the mask for the memory keys per batch (optional).</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment"># see Fig. 1 of https://arxiv.org/pdf/2002.04745v1.pdf</span></span><br><span class="line"></span><br><span class="line">        x = tgt</span><br><span class="line">        <span class="keyword">if</span> self.norm_first:</span><br><span class="line">            x = x + self._sa_block(self.norm1(x), tgt_mask, tgt_key_padding_mask)</span><br><span class="line">            x = x + self._mha_block(self.norm2(x), memory, memory_mask, memory_key_padding_mask)</span><br><span class="line">            x = x + self._ff_block(self.norm3(x))</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># 先self-attention</span></span><br><span class="line">            x = self.norm1(x + self._sa_block(x, tgt_mask, tgt_key_padding_mask))</span><br><span class="line">            <span class="comment"># 再cross-attention</span></span><br><span class="line">            x = self.norm2(x + self._mha_block(x, memory, memory_mask, memory_key_padding_mask))</span><br><span class="line">            <span class="comment"># 再FFN</span></span><br><span class="line">            x = self.norm3(x + self._ff_block(x))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment"># self-attention block</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_sa_block</span>(<span class="params">self, x: Tensor,</span></span><br><span class="line"><span class="params">                  attn_mask: <span class="type">Optional</span>[Tensor], key_padding_mask: <span class="type">Optional</span>[Tensor]</span>) -&gt; Tensor:</span><br><span class="line">        x = self.self_attn(x, x, x,</span><br><span class="line">                           attn_mask=attn_mask,</span><br><span class="line">                           key_padding_mask=key_padding_mask,</span><br><span class="line">                           need_weights=<span class="literal">False</span>)[<span class="number">0</span>]</span><br><span class="line">        <span class="keyword">return</span> self.dropout1(x)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># multihead attention block</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_mha_block</span>(<span class="params">self, x: Tensor, mem: Tensor,</span></span><br><span class="line"><span class="params">                   attn_mask: <span class="type">Optional</span>[Tensor], key_padding_mask: <span class="type">Optional</span>[Tensor]</span>) -&gt; Tensor:</span><br><span class="line">        x = self.multihead_attn(x, mem, mem,</span><br><span class="line">                                attn_mask=attn_mask,</span><br><span class="line">                                key_padding_mask=key_padding_mask,</span><br><span class="line">                                need_weights=<span class="literal">False</span>)[<span class="number">0</span>]</span><br><span class="line">        <span class="keyword">return</span> self.dropout2(x)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># feed forward block</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_ff_block</span>(<span class="params">self, x: Tensor</span>) -&gt; Tensor:</span><br><span class="line">        x = self.linear2(self.dropout(self.activation(self.linear1(x))))</span><br><span class="line">        <span class="keyword">return</span> self.dropout3(x)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">_get_clones</span>(<span class="params">module, N</span>):</span><br><span class="line">    <span class="keyword">return</span> ModuleList([copy.deepcopy(module) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(N)])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">_get_activation_fn</span>(<span class="params">activation</span>):</span><br><span class="line">    <span class="keyword">if</span> activation == <span class="string">&quot;relu&quot;</span>:</span><br><span class="line">        <span class="keyword">return</span> F.relu</span><br><span class="line">    <span class="keyword">elif</span> activation == <span class="string">&quot;gelu&quot;</span>:</span><br><span class="line">        <span class="keyword">return</span> F.gelu</span><br><span class="line"></span><br><span class="line">    <span class="keyword">raise</span> RuntimeError(<span class="string">&quot;activation should be relu/gelu, not &#123;&#125;&quot;</span>.<span class="built_in">format</span>(activation))</span><br><span class="line"></span><br></pre></td></tr></table></figure></div><div class="tab-pane" id="tr-4"><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">MultiheadAttention</span>(<span class="title class_ inherited__">Module</span>):</span><br><span class="line">    <span class="string">r&quot;&quot;&quot;Allows the model to jointly attend to information from different representation subspaces.</span></span><br><span class="line"><span class="string">    Examples::</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; multihead_attn = nn.MultiheadAttention(embed_dim, num_heads)</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; attn_output, attn_output_weights = multihead_attn(query, key, value)</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    __constants__ = [<span class="string">&#x27;batch_first&#x27;</span>]</span><br><span class="line">    bias_k: <span class="type">Optional</span>[torch.Tensor]</span><br><span class="line">    bias_v: <span class="type">Optional</span>[torch.Tensor]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, embed_dim, num_heads, dropout=<span class="number">0.</span>, bias=<span class="literal">True</span>, add_bias_kv=<span class="literal">False</span>, add_zero_attn=<span class="literal">False</span>,</span></span><br><span class="line"><span class="params">                 kdim=<span class="literal">None</span>, vdim=<span class="literal">None</span>, batch_first=<span class="literal">False</span>, device=<span class="literal">None</span>, dtype=<span class="literal">None</span></span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">        factory_kwargs = &#123;<span class="string">&#x27;device&#x27;</span>: device, <span class="string">&#x27;dtype&#x27;</span>: dtype&#125;</span><br><span class="line">        <span class="built_in">super</span>(MultiheadAttention, self).__init__()</span><br><span class="line">        self.embed_dim = embed_dim</span><br><span class="line">        self.kdim = kdim <span class="keyword">if</span> kdim <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">else</span> embed_dim</span><br><span class="line">        self.vdim = vdim <span class="keyword">if</span> vdim <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">else</span> embed_dim</span><br><span class="line">        self._qkv_same_embed_dim = self.kdim == embed_dim <span class="keyword">and</span> self.vdim == embed_dim</span><br><span class="line"></span><br><span class="line">        self.num_heads = num_heads</span><br><span class="line">        self.dropout = dropout</span><br><span class="line">        self.batch_first = batch_first</span><br><span class="line">        self.head_dim = embed_dim // num_heads</span><br><span class="line">        <span class="keyword">assert</span> self.head_dim * num_heads == self.embed_dim, <span class="string">&quot;embed_dim must be divisible by num_heads&quot;</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self._qkv_same_embed_dim <span class="keyword">is</span> <span class="literal">False</span>:</span><br><span class="line">            self.q_proj_weight = Parameter(torch.empty((embed_dim, embed_dim), **factory_kwargs))</span><br><span class="line">            self.k_proj_weight = Parameter(torch.empty((embed_dim, self.kdim), **factory_kwargs))</span><br><span class="line">            self.v_proj_weight = Parameter(torch.empty((embed_dim, self.vdim), **factory_kwargs))</span><br><span class="line">            self.register_parameter(<span class="string">&#x27;in_proj_weight&#x27;</span>, <span class="literal">None</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.in_proj_weight = Parameter(torch.empty((<span class="number">3</span> * embed_dim, embed_dim), **factory_kwargs))</span><br><span class="line">            self.register_parameter(<span class="string">&#x27;q_proj_weight&#x27;</span>, <span class="literal">None</span>)</span><br><span class="line">            self.register_parameter(<span class="string">&#x27;k_proj_weight&#x27;</span>, <span class="literal">None</span>)</span><br><span class="line">            self.register_parameter(<span class="string">&#x27;v_proj_weight&#x27;</span>, <span class="literal">None</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> bias:</span><br><span class="line">            self.in_proj_bias = Parameter(torch.empty(<span class="number">3</span> * embed_dim, **factory_kwargs))</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.register_parameter(<span class="string">&#x27;in_proj_bias&#x27;</span>, <span class="literal">None</span>)</span><br><span class="line">        self.out_proj = NonDynamicallyQuantizableLinear(embed_dim, embed_dim, bias=bias, **factory_kwargs)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> add_bias_kv:</span><br><span class="line">            self.bias_k = Parameter(torch.empty((<span class="number">1</span>, <span class="number">1</span>, embed_dim), **factory_kwargs))</span><br><span class="line">            self.bias_v = Parameter(torch.empty((<span class="number">1</span>, <span class="number">1</span>, embed_dim), **factory_kwargs))</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.bias_k = self.bias_v = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">        self.add_zero_attn = add_zero_attn</span><br><span class="line"></span><br><span class="line">        self._reset_parameters()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_reset_parameters</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">if</span> self._qkv_same_embed_dim:</span><br><span class="line">            xavier_uniform_(self.in_proj_weight)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            xavier_uniform_(self.q_proj_weight)</span><br><span class="line">            xavier_uniform_(self.k_proj_weight)</span><br><span class="line">            xavier_uniform_(self.v_proj_weight)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.in_proj_bias <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            constant_(self.in_proj_bias, <span class="number">0.</span>)</span><br><span class="line">            constant_(self.out_proj.bias, <span class="number">0.</span>)</span><br><span class="line">        <span class="keyword">if</span> self.bias_k <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            xavier_normal_(self.bias_k)</span><br><span class="line">        <span class="keyword">if</span> self.bias_v <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            xavier_normal_(self.bias_v)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__setstate__</span>(<span class="params">self, state</span>):</span><br><span class="line">        <span class="comment"># Support loading old MultiheadAttention checkpoints generated by v1.1.0</span></span><br><span class="line">        <span class="keyword">if</span> <span class="string">&#x27;_qkv_same_embed_dim&#x27;</span> <span class="keyword">not</span> <span class="keyword">in</span> state:</span><br><span class="line">            state[<span class="string">&#x27;_qkv_same_embed_dim&#x27;</span>] = <span class="literal">True</span></span><br><span class="line"></span><br><span class="line">        <span class="built_in">super</span>(MultiheadAttention, self).__setstate__(state)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, query: Tensor, key: Tensor, value: Tensor, key_padding_mask: <span class="type">Optional</span>[Tensor] = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params">                need_weights: <span class="built_in">bool</span> = <span class="literal">True</span>, attn_mask: <span class="type">Optional</span>[Tensor] = <span class="literal">None</span></span>) -&gt; <span class="type">Tuple</span>[Tensor, <span class="type">Optional</span>[Tensor]]:</span><br><span class="line">        <span class="keyword">if</span> self.batch_first:</span><br><span class="line">            query, key, value = [x.transpose(<span class="number">1</span>, <span class="number">0</span>) <span class="keyword">for</span> x <span class="keyword">in</span> (query, key, value)]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> self._qkv_same_embed_dim:</span><br><span class="line">            attn_output, attn_output_weights = F.multi_head_attention_forward(</span><br><span class="line">                query, key, value, self.embed_dim, self.num_heads,</span><br><span class="line">                self.in_proj_weight, self.in_proj_bias,</span><br><span class="line">                self.bias_k, self.bias_v, self.add_zero_attn,</span><br><span class="line">                self.dropout, self.out_proj.weight, self.out_proj.bias,</span><br><span class="line">                training=self.training,</span><br><span class="line">                key_padding_mask=key_padding_mask, need_weights=need_weights,</span><br><span class="line">                attn_mask=attn_mask, use_separate_proj_weight=<span class="literal">True</span>,</span><br><span class="line">                q_proj_weight=self.q_proj_weight, k_proj_weight=self.k_proj_weight,</span><br><span class="line">                v_proj_weight=self.v_proj_weight)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            attn_output, attn_output_weights = F.multi_head_attention_forward(</span><br><span class="line">                query, key, value, self.embed_dim, self.num_heads,</span><br><span class="line">                self.in_proj_weight, self.in_proj_bias,</span><br><span class="line">                self.bias_k, self.bias_v, self.add_zero_attn,</span><br><span class="line">                self.dropout, self.out_proj.weight, self.out_proj.bias,</span><br><span class="line">                training=self.training,</span><br><span class="line">                key_padding_mask=key_padding_mask, need_weights=need_weights,</span><br><span class="line">                attn_mask=attn_mask)</span><br><span class="line">        <span class="keyword">if</span> self.batch_first:</span><br><span class="line">            <span class="keyword">return</span> attn_output.transpose(<span class="number">1</span>, <span class="number">0</span>), attn_output_weights</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> attn_output, attn_output_weights</span><br></pre></td></tr></table></figure></div><div class="tab-pane" id="tr-5"><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"></span><br></pre></td></tr></table></figure></div></div></div></p>
<h1 id="参考资料">参考资料</h1>
<p><span class="exturl" data-url="aHR0cHM6Ly93d3cuam1sci5vcmcvcGFwZXJzL3ZvbHVtZTE1L3NyaXZhc3RhdmExNGEvc3JpdmFzdGF2YTE0YS5wZGY/dXRtX2NvbnRlbnQ9YnVmZmVyNzliNDMmdXRtX21lZGl1bT1zb2NpYWwmdXRtX3NvdXJjZT10d2l0dGVyLmNvbSZ1dG1fY2FtcGFpZ249YnVmZmVyLA==">Dropout:
A Simple Way to Prevent Neural Networks from Overfitting<i class="fa fa-external-link-alt"></i></span><br />
<span class="exturl" data-url="aHR0cHM6Ly9wcm9jZWVkaW5ncy5uZXVyaXBzLmNjL3BhcGVyLzIwMjEvZmlsZS81YTY2YjkyMDBmMjlhYzNmYTBhZTI0NGNjMmE1MWIzOS1QYXBlci5wZGY=">R-Drop:
Regularized Dropout for Neural Networks<i class="fa fa-external-link-alt"></i></span><br />
<span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzIxMDEuMDM2OTcucGRm">RepVGG: Making VGG-style
ConvNets Great Again<i class="fa fa-external-link-alt"></i></span><br />
<span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi41M3l1LmNvbS9wZGYvMjIwMS4wOTc5Mi5wZGY=">Patches Are All You
Need?<i class="fa fa-external-link-alt"></i></span><br />
<span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzE3MDYuMDM3NjIucGRm">Attention Is All You
Need<i class="fa fa-external-link-alt"></i></span></p>

    </div>

    
    
    

    <footer class="post-footer">
          

<div class="post-copyright">
<ul>
  <li class="post-copyright-author">
      <strong>本文作者： </strong>SoundMemories
  </li>
  <li class="post-copyright-link">
      <strong>本文链接：</strong>
      <a href="https://soundmemories.github.io/2021/06/15/NLP/00.%E6%89%8B%E5%8A%A8%E5%AE%9E%E7%8E%B0/" title="手动实现+部分源码">https://soundmemories.github.io/2021/06/15/NLP/00.手动实现/</a>
  </li>
  <li class="post-copyright-license">
      <strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <span class="exturl" data-url="aHR0cHM6Ly9jcmVhdGl2ZWNvbW1vbnMub3JnL2xpY2Vuc2VzL2J5LW5jLXNhLzQuMC8="><i class="fab fa-fw fa-creative-commons"></i>BY-NC-SA</span> 许可协议。转载请注明出处！
  </li>
</ul>
</div>

          <div class="post-tags">
              <a href="/tags/NLP/" rel="tag"><i class="fa fa-tag"></i> NLP</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2021/06/13/NLP/00.TensorFlow/" rel="prev" title="TensorFlow2.0">
                  <i class="fa fa-chevron-left"></i> TensorFlow2.0
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2021/06/19/Paper/00.Paper%E9%98%85%E8%AF%BB%E6%8A%80%E5%B7%A7/" rel="next" title="Paper阅读技巧">
                  Paper阅读技巧 <i class="fa fa-chevron-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="copyright">
    &copy; 2019 – 
    <span itemprop="copyrightYear">2024</span>
    <span class="with-love">
      <i class="fa fa-heart"></i>
    </span>
    <span class="author" itemprop="copyrightHolder">SoundMemories</span>
  </div>
  <div class="powered-by">由 <span class="exturl" data-url="aHR0cHM6Ly9oZXhvLmlv">Hexo</span> & <span class="exturl" data-url="aHR0cHM6Ly90aGVtZS1uZXh0LmpzLm9yZy9tdXNlLw==">NexT.Muse</span> 强力驱动
  </div>

    </div>
  </footer>

  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>
  <div class="sidebar-dimmer"></div>
  <div class="back-to-top" role="button" aria-label="返回顶部">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>
  <a role="button" class="book-mark-link book-mark-link-fixed"></a>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/next-theme-pjax/0.6.0/pjax.min.js" integrity="sha256-vxLn1tSKWD4dqbMRyv940UYw4sXgMtYcK6reefzZrao=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.7.0/jquery.min.js" integrity="sha256-2Pmvv0kuTBOenSvLm6bvfBSSHrUJ+3A7x6P5Ebd07/g=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.js" integrity="sha256-yt2kYMy0w8AbtF89WXb2P1rfjcP/HTHLT7097U8Y5b8=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/medium-zoom/1.0.8/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/muse.js"></script><script src="/js/next-boot.js"></script><script src="/js/bookmark.js"></script><script src="/js/pjax.js"></script>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-generator-searchdb/1.4.1/search.js" integrity="sha256-1kfA5uHPf65M5cphT2dvymhkuyHPQp5A53EGZOnOLmc=" crossorigin="anonymous"></script>
<script src="/js/third-party/search/local-search.js"></script>

  <script class="next-config" data-name="pdf" type="application/json">{"object_url":{"url":"https://cdnjs.cloudflare.com/ajax/libs/pdfobject/2.2.12/pdfobject.min.js","integrity":"sha256-g2xji1rlE3KsGVClvuxTbcR0Kn2+wtQADSff2Tbb4zA="},"url":"/lib/pdf/web/viewer.html"}</script>
  <script src="/js/third-party/tags/pdf.js"></script>

  <script class="next-config" data-name="mermaid" type="application/json">{"enable":true,"theme":{"light":"neutral","dark":"neutral"},"js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mermaid/10.2.3/mermaid.min.js","integrity":"sha256-JFptYy4KzJ5OQP+Q9fubNf3cxpPPmZKqUOovyEONKrQ="}}</script>
  <script src="/js/third-party/tags/mermaid.js"></script>


  <script src="/js/third-party/fancybox.js"></script>



  




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"none","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>


  <script src="https://cdnjs.cloudflare.com/ajax/libs/quicklink/2.3.0/quicklink.umd.js" integrity="sha256-yvJQOINiH9fWemHn0vCA5lsHWJaHs6/ZmO+1Ft04SvM=" crossorigin="anonymous"></script>
  <script class="next-config" data-name="quicklink" type="application/json">{"enable":true,"home":true,"archive":true,"delay":true,"timeout":3000,"priority":true,"url":"https://soundmemories.github.io/2021/06/15/NLP/00.%E6%89%8B%E5%8A%A8%E5%AE%9E%E7%8E%B0/"}</script>
  <script src="/js/third-party/quicklink.js"></script>

</body>
</html>
