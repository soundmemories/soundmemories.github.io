<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222" media="(prefers-color-scheme: light)">
<meta name="theme-color" content="#222" media="(prefers-color-scheme: dark)"><meta name="generator" content="Hexo 6.3.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="https://fonts.loli.net/css?family=Monda:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">

<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css" integrity="sha256-HtsXJanqjKTc8vVQjO4YMhiqFoXkfBsjBWcX91T1jr8=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.css" integrity="sha256-Vzbj7sDDS/woiFS3uNKo8eIuni59rjyNGtXfstRzStA=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"soundmemories.github.io","root":"/","images":"/images","scheme":"Muse","darkmode":true,"version":"8.17.1","exturl":true,"sidebar":{"position":"left","display":"always","padding":18,"offset":12},"copycode":{"enable":true,"style":null},"bookmark":{"enable":true,"color":"#222","save":"auto"},"mediumzoom":true,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"},"path":"/search.xml","localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false}}</script><script src="/js/config.js"></script>

    <meta name="description" content="今日事，今日毕">
<meta property="og:type" content="website">
<meta property="og:title" content="SoundMemories">
<meta property="og:url" content="https://soundmemories.github.io/page/2/index.html">
<meta property="og:site_name" content="SoundMemories">
<meta property="og:description" content="今日事，今日毕">
<meta property="og:locale" content="zh_CN">
<meta property="article:author" content="SoundMemories">
<meta name="twitter:card" content="summary">


<link rel="canonical" href="https://soundmemories.github.io/page/2/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":true,"isPost":false,"lang":"zh-CN","comments":"","permalink":"","path":"page/2/index.html","title":""}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>SoundMemories</title>
  








  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <h1 class="site-title">SoundMemories</h1>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="搜索" role="button">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签<span class="badge">8</span></a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类<span class="badge">8</span></a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档<span class="badge">122</span></a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup"><div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" maxlength="80"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close" role="button">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="search-result-container no-result">
  <div class="search-result-icon">
    <i class="fa fa-spinner fa-pulse fa-5x"></i>
  </div>
</div>

    </div>
  </div>

</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-overview-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="SoundMemories"
      src="/images/avstar.png">
  <p class="site-author-name" itemprop="name">SoundMemories</p>
  <div class="site-description" itemprop="description">今日事，今日毕</div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">122</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">8</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">8</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author animated">
      <span class="links-of-author-item">
        <span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL3NvdW5kbWVtb3JpZXM=" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;soundmemories"><i class="fab fa-github fa-fw"></i>GitHub</span>
      </span>
      <span class="links-of-author-item">
        <span class="exturl" data-url="bWFpbHRvOnNvdW5kbWVtb3JpZXNAMTYzLmNvbQ==" title="E-Mail → mailto:soundmemories@163.com"><i class="fa fa-envelope fa-fw"></i>E-Mail</span>
      </span>
  </div>
  <div class="cc-license animated" itemprop="license">
    <span class="exturl cc-opacity" data-url="aHR0cHM6Ly9jcmVhdGl2ZWNvbW1vbnMub3JnL2xpY2Vuc2VzL2J5LW5jLXNhLzQuMC8="><img src="https://cdnjs.cloudflare.com/ajax/libs/creativecommons-vocabulary/2020.11.3/assets/license_badges/small/by_nc_sa.svg" alt="Creative Commons"></span>
  </div>

        </div>
      </div>
    </div>

    
        <div class="pjax">
        </div>
  </aside>


    </div>

    <div class="main-inner index posts-expand">

    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://soundmemories.github.io/2021/07/15/NLP/02.%E5%9F%BA%E4%BA%8ELSTM%E7%9A%84%E6%83%85%E6%84%9F%E5%88%86%E7%B1%BB/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avstar.png">
      <meta itemprop="name" content="SoundMemories">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="SoundMemories">
      <meta itemprop="description" content="今日事，今日毕">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | SoundMemories">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2021/07/15/NLP/02.%E5%9F%BA%E4%BA%8ELSTM%E7%9A%84%E6%83%85%E6%84%9F%E5%88%86%E7%B1%BB/" class="post-title-link" itemprop="url">基于LSTM的情感分类</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2021-07-15 00:00:00" itemprop="dateCreated datePublished" datetime="2021-07-15T00:00:00+08:00">2021-07-15</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/NLP/" itemprop="url" rel="index"><span itemprop="name">NLP</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>2.5k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>9 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p><strong>本项目使用了word2vec的中文预训练向量</strong><br />
<strong>模型分别有BiLSTM-attention和普通的LSTM两种，自行选择</strong></p>
<p><strong>使用说明</strong>：<br />
1、在<strong>Config</strong>中配置相关参数</p>
<p>2、然后运行<strong>DataProcess.py</strong>，生成相应的word2id，word2vec等文件</p>
<p>3、运行主函数<strong>main.py</strong>，得到训练好的模型，并保存模型</p>
<p>4、运行<strong>eval.py</strong>，读取模型，并得到评价</p>
<p>5、模型<strong>准确率平均85%左右</strong></p>
<figure class="highlight python"><figcaption><span>Sentiment_Analysis_Config.py</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Config</span>():</span><br><span class="line">    update_w2v = <span class="literal">True</span>          <span class="comment"># 是否在训练中更新w2v</span></span><br><span class="line">    vocab_size = <span class="number">54848</span>          <span class="comment"># 词汇量，与word2id中的词汇量一致</span></span><br><span class="line">    n_class = <span class="number">2</span>                 <span class="comment"># 分类数：分别为pos和neg</span></span><br><span class="line">    max_sen_len = <span class="number">65</span>           <span class="comment"># 句子最大长度</span></span><br><span class="line">    embedding_dim = <span class="number">50</span>          <span class="comment"># 词向量维度</span></span><br><span class="line">    batch_size =<span class="number">64</span>            <span class="comment"># 批处理尺寸</span></span><br><span class="line">    hidden_dim=<span class="number">100</span>           <span class="comment"># 隐藏层节点数</span></span><br><span class="line">    n_epoch = <span class="number">30</span>            <span class="comment"># 训练迭代周期，即遍历整个训练样本的次数</span></span><br><span class="line">    lr = <span class="number">0.0001</span>               <span class="comment"># 学习率；若opt=‘adadelta&#x27;，则不需要定义学习率</span></span><br><span class="line">    drop_keep_prob = <span class="number">0.2</span>        <span class="comment"># dropout层，参数keep的比例</span></span><br><span class="line">    num_layers = <span class="number">2</span>              <span class="comment"># LSTM层数</span></span><br><span class="line">    bidirectional=<span class="literal">True</span>         <span class="comment">#是否使用双向LSTM</span></span><br><span class="line">    train_path = <span class="string">&#x27;./word2vec_data/train.txt&#x27;</span></span><br><span class="line">    val_path = <span class="string">&#x27;./word2vec_data/validation.txt&#x27;</span></span><br><span class="line">    test_path = <span class="string">&#x27;./word2vec_data/test.txt&#x27;</span></span><br><span class="line">    pre_path =<span class="string">&#x27;./word2vec_data/pre.txt&#x27;</span></span><br><span class="line">    word2id_path = <span class="string">&#x27;./word2vec_data/word2id.txt&#x27;</span></span><br><span class="line">    pre_word2vec_path = <span class="string">&#x27;./word2vec_data/wiki_word2vec_50.bin&#x27;</span></span><br><span class="line">    corpus_word2vec_path = <span class="string">&#x27;./word2vec_data/word_vec.txt&#x27;</span></span><br><span class="line">    model_state_dict_path=<span class="string">&#x27;./word2vec_data/sen_model.pkl&#x27;</span><span class="comment"># 训练模型保存的地址</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><figcaption><span>Sentiment_Analysis_DataProcess.py</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> unicode_literals, print_function, division</span><br><span class="line"><span class="keyword">from</span> io <span class="keyword">import</span> <span class="built_in">open</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">import</span>  numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> gensim</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataset</span><br><span class="line"><span class="keyword">from</span> Sentiment_Analysis_Config <span class="keyword">import</span> Config</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Data_set</span>(<span class="title class_ inherited__">Dataset</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, Data, Label</span>):</span><br><span class="line">        self.Data = Data</span><br><span class="line">        <span class="keyword">if</span> Label <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:<span class="comment">#考虑对测试集的使用</span></span><br><span class="line">            self.Label = Label</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.Data)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, index</span>):</span><br><span class="line">        <span class="keyword">if</span> self.Label <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            data = torch.from_numpy(self.Data[index])</span><br><span class="line">            label = torch.from_numpy(self.Label[index])</span><br><span class="line">            <span class="keyword">return</span> data, label</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            data = torch.from_numpy(self.Data[index])</span><br><span class="line">            <span class="keyword">return</span> data</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">stopwordslist</span>():<span class="comment">#创建停用词表</span></span><br><span class="line">    stopwords = [line.strip() <span class="keyword">for</span> line <span class="keyword">in</span> <span class="built_in">open</span>(<span class="string">&#x27;word2vec_data/stopword.txt&#x27;</span>,encoding=<span class="string">&#x27;UTF-8&#x27;</span>).readlines()]</span><br><span class="line">    <span class="keyword">return</span> stopwords</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">build_word2id</span>(<span class="params">file</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    :param file: word2id保存地址</span></span><br><span class="line"><span class="string">    :return: None</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment">#num_50=0#统计长度大于50的句子数</span></span><br><span class="line">    stopwords = stopwordslist()</span><br><span class="line">    word2id = &#123;<span class="string">&#x27;_PAD_&#x27;</span>: <span class="number">0</span>&#125;</span><br><span class="line">    path = [Config.train_path, Config.val_path]</span><br><span class="line">    <span class="comment">#print(path)</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> _path <span class="keyword">in</span> path:</span><br><span class="line">        <span class="keyword">with</span> <span class="built_in">open</span>(_path, encoding=<span class="string">&#x27;utf-8&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">            <span class="keyword">for</span> line <span class="keyword">in</span> f.readlines():</span><br><span class="line">                out_list = []</span><br><span class="line">                <span class="comment"># 去停用词</span></span><br><span class="line">                sp = line.strip().split()</span><br><span class="line">                <span class="keyword">for</span> word <span class="keyword">in</span> sp[<span class="number">1</span>:]:</span><br><span class="line">                    <span class="keyword">if</span> word <span class="keyword">not</span> <span class="keyword">in</span> stopwords:</span><br><span class="line">                        rt = re.findall(<span class="string">&#x27;[a-zA-Z]+&#x27;</span>, word)</span><br><span class="line">                        <span class="keyword">if</span> word != <span class="string">&#x27;\t&#x27;</span>:</span><br><span class="line">                            <span class="comment"># if is_number(word):</span></span><br><span class="line">                            <span class="comment"># continue</span></span><br><span class="line">                            <span class="keyword">if</span> <span class="built_in">len</span>(rt) == <span class="number">1</span>:</span><br><span class="line">                                <span class="keyword">continue</span></span><br><span class="line">                            <span class="keyword">else</span>:</span><br><span class="line">                                out_list.append(word)</span><br><span class="line">                <span class="keyword">for</span> word <span class="keyword">in</span> out_list:</span><br><span class="line">                    <span class="keyword">if</span> word <span class="keyword">not</span> <span class="keyword">in</span> word2id.keys():</span><br><span class="line">                        word2id[word] = <span class="built_in">len</span>(word2id)</span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(file, <span class="string">&#x27;w&#x27;</span>, encoding=<span class="string">&#x27;utf-8&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">        <span class="keyword">for</span> w <span class="keyword">in</span> word2id:</span><br><span class="line">            f.write(w+<span class="string">&#x27;\t&#x27;</span>)</span><br><span class="line">            f.write(<span class="built_in">str</span>(word2id[w]))</span><br><span class="line">            f.write(<span class="string">&#x27;\n&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">build_word2vec</span>(<span class="params">fname, word2id, save_to_path=<span class="literal">None</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    :param fname: 预训练的word2vec.</span></span><br><span class="line"><span class="string">    :param word2id: 语料文本中包含的词汇集.</span></span><br><span class="line"><span class="string">    :param save_to_path: 保存训练语料库中的词组对应的word2vec到本地</span></span><br><span class="line"><span class="string">    :return: 语料文本中词汇集对应的word2vec向量&#123;id: word2vec&#125;.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    n_words = <span class="built_in">max</span>(word2id.values()) + <span class="number">1</span></span><br><span class="line">    model = gensim.models.KeyedVectors.load_word2vec_format(fname, binary=<span class="literal">True</span>)</span><br><span class="line">    word_vecs = np.array(np.random.uniform(-<span class="number">1.</span>, <span class="number">1.</span>, [n_words, model.vector_size]))</span><br><span class="line">    <span class="keyword">for</span> word <span class="keyword">in</span> word2id.keys():</span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            word_vecs[word2id[word]] = model[word]</span><br><span class="line">        <span class="keyword">except</span> KeyError:</span><br><span class="line">            <span class="keyword">pass</span></span><br><span class="line">    <span class="keyword">if</span> save_to_path:</span><br><span class="line">        <span class="keyword">with</span> <span class="built_in">open</span>(save_to_path, <span class="string">&#x27;w&#x27;</span>, encoding=<span class="string">&#x27;utf-8&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">            <span class="keyword">for</span> vec <span class="keyword">in</span> word_vecs:</span><br><span class="line">                vec = [<span class="built_in">str</span>(w) <span class="keyword">for</span> w <span class="keyword">in</span> vec]</span><br><span class="line">                f.write(<span class="string">&#x27; &#x27;</span>.join(vec))</span><br><span class="line">                f.write(<span class="string">&#x27;\n&#x27;</span>)</span><br><span class="line">    <span class="keyword">return</span> word_vecs</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">text_to_array</span>(<span class="params">word2id,seq_lenth ,path</span>):  <span class="comment"># 文本转为索引数字模式,</span></span><br><span class="line"></span><br><span class="line">    lable_array=[]</span><br><span class="line">    i=<span class="number">0</span></span><br><span class="line">    sa=[]</span><br><span class="line">    <span class="comment">#获取句子个数</span></span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(path, encoding=<span class="string">&#x27;utf-8&#x27;</span>) <span class="keyword">as</span> f1:</span><br><span class="line">        <span class="keyword">for</span> l1 <span class="keyword">in</span> f1.readlines():</span><br><span class="line">            s= l1.strip().split()</span><br><span class="line">            s1=s[<span class="number">1</span>:]</span><br><span class="line">            new_s = [word2id.get(word, <span class="number">0</span>) <span class="keyword">for</span> word <span class="keyword">in</span> s1]  <span class="comment"># 单词转索引数字</span></span><br><span class="line">            sa.append(new_s)</span><br><span class="line">        <span class="comment">#print(len(sa))</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(path, encoding=<span class="string">&#x27;utf-8&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">        sentences_array=np.zeros(shape=(<span class="built_in">len</span>(sa),seq_lenth))<span class="comment">#行：句子个数 列：句子长度</span></span><br><span class="line">        <span class="keyword">for</span> line <span class="keyword">in</span> f.readlines():</span><br><span class="line">            sl1 = line.strip().split()</span><br><span class="line">            sen=sl1[<span class="number">1</span>:]</span><br><span class="line">            new_sen = [word2id.get(word, <span class="number">0</span>) <span class="keyword">for</span> word <span class="keyword">in</span> sen]  <span class="comment"># 单词转索引数字,不存在则为0</span></span><br><span class="line">            new_sen_np=np.array(new_sen).reshape(<span class="number">1</span>,-<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">            <span class="comment">#补齐每个句子长度，多余补零，少了就直接赋值,0填在前面。</span></span><br><span class="line">            <span class="keyword">if</span> np.size(new_sen_np,<span class="number">1</span>)&lt;seq_lenth:</span><br><span class="line">                sentences_array[i,seq_lenth-np.size(new_sen_np,<span class="number">1</span>):]=new_sen_np[<span class="number">0</span>,:]</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                sentences_array[i, <span class="number">0</span>:seq_lenth]=new_sen_np[<span class="number">0</span>,<span class="number">0</span>:seq_lenth]</span><br><span class="line"></span><br><span class="line">            i=i+<span class="number">1</span></span><br><span class="line">            lable=<span class="built_in">int</span>(sl1[<span class="number">0</span>])<span class="comment">#标签</span></span><br><span class="line">            lable_array.append(lable)</span><br><span class="line">    <span class="keyword">return</span> np.array(sentences_array),lable_array</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">text_to_array_nolable</span>(<span class="params">word2id,seq_lenth ,path</span>):  <span class="comment"># 文本转为索引数字模式,</span></span><br><span class="line"></span><br><span class="line">    i=<span class="number">0</span></span><br><span class="line">    sa=[]</span><br><span class="line">    <span class="comment">#获取句子个数</span></span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(path, encoding=<span class="string">&#x27;utf-8&#x27;</span>) <span class="keyword">as</span> f1:</span><br><span class="line">        <span class="keyword">for</span> l1 <span class="keyword">in</span> f1.readlines():</span><br><span class="line">            s= l1.strip().split()</span><br><span class="line">            s1=s[<span class="number">1</span>:]</span><br><span class="line">            new_s = [word2id.get(word, <span class="number">0</span>) <span class="keyword">for</span> word <span class="keyword">in</span> s1]  <span class="comment"># 单词转索引数字</span></span><br><span class="line">            sa.append(new_s)</span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(path, encoding=<span class="string">&#x27;utf-8&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">        sentences_array=np.zeros(shape=(<span class="built_in">len</span>(sa),seq_lenth))<span class="comment">#行：句子个数 列：句子长度</span></span><br><span class="line">        <span class="keyword">for</span> line <span class="keyword">in</span> f.readlines():</span><br><span class="line">            sl1 = line.strip().split()</span><br><span class="line">            sen=sl1[<span class="number">1</span>:]</span><br><span class="line">            new_sen = [word2id.get(word, <span class="number">0</span>) <span class="keyword">for</span> word <span class="keyword">in</span> sen]  <span class="comment"># 单词转索引数字,不存在则为0</span></span><br><span class="line">            new_sen_np=np.array(new_sen).reshape(<span class="number">1</span>,-<span class="number">1</span>)</span><br><span class="line">            <span class="keyword">if</span> np.size(new_sen_np,<span class="number">1</span>)&lt;seq_lenth:</span><br><span class="line">                sentences_array[i,seq_lenth-np.size(new_sen_np,<span class="number">1</span>):]=new_sen_np[<span class="number">0</span>,:]</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                sentences_array[i, <span class="number">0</span>:seq_lenth]=new_sen_np[<span class="number">0</span>,<span class="number">0</span>:seq_lenth]</span><br><span class="line">            i=i+<span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> np.array(sentences_array)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">to_categorical</span>(<span class="params">y, num_classes=<span class="literal">None</span></span>):<span class="comment">#将类别转化为one-hot编码</span></span><br><span class="line">    y = np.array(y, dtype=<span class="string">&#x27;int&#x27;</span>)</span><br><span class="line">    input_shape = y.shape</span><br><span class="line">    <span class="keyword">if</span> input_shape <span class="keyword">and</span> input_shape[-<span class="number">1</span>] == <span class="number">1</span> <span class="keyword">and</span> <span class="built_in">len</span>(input_shape) &gt; <span class="number">1</span>:</span><br><span class="line">        input_shape = <span class="built_in">tuple</span>(input_shape[:-<span class="number">1</span>])</span><br><span class="line">    y = y.ravel()</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> num_classes:</span><br><span class="line">        num_classes = np.<span class="built_in">max</span>(y) + <span class="number">1</span></span><br><span class="line">    n = y.shape[<span class="number">0</span>]</span><br><span class="line">    categorical = np.zeros((n, num_classes))</span><br><span class="line">    categorical[np.arange(n), y] = <span class="number">1</span></span><br><span class="line">    output_shape = input_shape + (num_classes,)</span><br><span class="line">    categorical = np.reshape(categorical, output_shape)</span><br><span class="line">    <span class="keyword">return</span> categorical</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">prepare_data</span>(<span class="params">w2id, train_path,val_path,test_path,seq_lenth</span>):<span class="comment">#得到数字索引表示的句子和标签</span></span><br><span class="line">    train_array,train_lable = text_to_array(w2id,seq_lenth= seq_lenth,path=train_path)</span><br><span class="line">    val_array,val_lable  = text_to_array(w2id,seq_lenth=seq_lenth,path= val_path)</span><br><span class="line">    test_array,test_lable=text_to_array(w2id,seq_lenth=seq_lenth,path=test_path)</span><br><span class="line"></span><br><span class="line">    <span class="comment">#标签为[1, 1, 1, 1, 1, 1, 1, 1, 0, 0...]将标签转为onehot</span></span><br><span class="line">    <span class="comment">#train_lable=to_categorical(train_lable,num_classes=2)</span></span><br><span class="line">    <span class="comment">#val_lable=to_categorical(val_lable,num_classes=2)</span></span><br><span class="line"></span><br><span class="line">    <span class="string">&quot;&quot;&quot;for i in train_lable:</span></span><br><span class="line"><span class="string">        np.array([i])&quot;&quot;&quot;</span></span><br><span class="line">    train_lable=np.array([train_lable]).T</span><br><span class="line">    val_lable=np.array([val_lable]).T</span><br><span class="line">    test_lable=np.array([test_lable]).T</span><br><span class="line">    <span class="string">&quot;&quot;&quot;转换后标签</span></span><br><span class="line"><span class="string">            [[0. 1.]</span></span><br><span class="line"><span class="string">            [0. 1.]</span></span><br><span class="line"><span class="string">            [0. 1.]</span></span><br><span class="line"><span class="string">            ...</span></span><br><span class="line"><span class="string">            [1. 0.]</span></span><br><span class="line"><span class="string">            [1. 0.]</span></span><br><span class="line"><span class="string">            [1. 0.]]&quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment">#print(train_lab,&quot;\nval\n&quot;,val_lab)</span></span><br><span class="line">    <span class="keyword">return</span> train_array ,train_lable,val_array,val_lable,test_array,test_lable</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#建立word2id</span></span><br><span class="line">build_word2id(<span class="string">&#x27;./word2vec_data/word2id.txt&#x27;</span>)<span class="comment">#建立词toid</span></span><br><span class="line">splist=[]</span><br><span class="line">word2id=&#123;&#125;</span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;./word2vec_data/word2id.txt&#x27;</span>, encoding=<span class="string">&#x27;utf-8&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">        <span class="keyword">for</span> line <span class="keyword">in</span> f.readlines():</span><br><span class="line">            sp = line.strip().split()<span class="comment">#去掉\n \t 等</span></span><br><span class="line">            splist.append(sp)</span><br><span class="line">        word2id=<span class="built_in">dict</span>(splist)<span class="comment">#转成字典</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> key <span class="keyword">in</span> word2id:<span class="comment"># 将字典的值，从str转成int</span></span><br><span class="line">    word2id[key]=<span class="built_in">int</span>(word2id[key])</span><br><span class="line"></span><br><span class="line">id2word=&#123;&#125;<span class="comment">#得到id2word</span></span><br><span class="line"><span class="keyword">for</span> key,val <span class="keyword">in</span> word2id.items():</span><br><span class="line">    id2word[val]=key</span><br><span class="line"><span class="comment">#建立word2vec</span></span><br><span class="line">w2vec=build_word2vec(Config.pre_word2vec_path,word2id,Config.corpus_word2vec_path)</span><br><span class="line"></span><br><span class="line"><span class="comment">#得到句子id表示和标签</span></span><br><span class="line">train_array,train_lable,val_array,val_lable,test_array,test_label=prepare_data(word2id,</span><br><span class="line">                                                         train_path=Config.train_path,</span><br><span class="line">                                                         val_path=Config.val_path,</span><br><span class="line">                                                         test_path=Config.test_path,seq_lenth=Config.max_sen_len)</span><br><span class="line"></span><br><span class="line">np.savetxt(<span class="string">&#x27;./word2vec_data/train_data.txt&#x27;</span>, train_array,fmt=<span class="string">&#x27;%d&#x27;</span>)</span><br><span class="line">np.savetxt(<span class="string">&#x27;./word2vec_data/val_data.txt&#x27;</span>, val_array,fmt=<span class="string">&#x27;%d&#x27;</span>)</span><br><span class="line">np.savetxt(<span class="string">&#x27;./word2vec_data/test_data.txt&#x27;</span>, test_array,fmt=<span class="string">&#x27;%d&#x27;</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><figcaption><span>Sentiment_model.py</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> unicode_literals, print_function, division</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">LSTMModel</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, vocab_size, embedding_dim,pretrained_weight, update_w2v,hidden_dim,</span></span><br><span class="line"><span class="params">                 num_layers,drop_keep_prob,n_class,bidirectional, **kwargs</span>):</span><br><span class="line">        <span class="built_in">super</span>(LSTMModel, self).__init__()</span><br><span class="line">        self.hidden_dim = hidden_dim</span><br><span class="line">        self.num_layers = num_layers</span><br><span class="line">        self.n_class = n_class</span><br><span class="line"></span><br><span class="line">        self.bidirectional = bidirectional</span><br><span class="line">        self.embedding = nn.Embedding.from_pretrained(pretrained_weight)</span><br><span class="line">        self.embedding.weight.requires_grad = update_w2v</span><br><span class="line">        self.encoder = nn.LSTM(input_size=embedding_dim, hidden_size=self.hidden_dim,</span><br><span class="line">                               num_layers=num_layers, bidirectional=self.bidirectional,</span><br><span class="line">                               dropout=drop_keep_prob)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.bidirectional:</span><br><span class="line">            self.decoder1 = nn.Linear(hidden_dim * <span class="number">4</span>, hidden_dim)</span><br><span class="line">            self.decoder2 = nn.Linear(hidden_dim,n_class)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.decoder1 = nn.Linear(hidden_dim * <span class="number">2</span>, hidden_dim)</span><br><span class="line">            self.decoder2 = nn.Linear(hidden_dim,n_class)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, inputs</span>):</span><br><span class="line">        embeddings = self.embedding(inputs)<span class="comment"># [batch, seq_len] =&gt; [batch, seq_len, embed_dim][64,75,50]</span></span><br><span class="line">        states, hidden = self.encoder(embeddings.permute([<span class="number">1</span>, <span class="number">0</span>, <span class="number">2</span>]))<span class="comment">#[75,32,50],[seq_len, batch, embed_dim]</span></span><br><span class="line"></span><br><span class="line">        encoding = torch.cat([states[<span class="number">0</span>], states[-<span class="number">1</span>]], dim=<span class="number">1</span>)<span class="comment">#张量拼接[32,512]</span></span><br><span class="line">        outputs = self.decoder1(encoding)</span><br><span class="line">        <span class="comment">#outputs = F.softmax(outputs, dim=1)</span></span><br><span class="line">        outputs=self.decoder2(outputs)</span><br><span class="line">        <span class="keyword">return</span> outputs</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">LSTM_attention</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, vocab_size, embedding_dim,pretrained_weight, update_w2v,hidden_dim,</span></span><br><span class="line"><span class="params">                 num_layers,drop_keep_prob,n_class,bidirectional, **kwargs</span>):</span><br><span class="line">        <span class="built_in">super</span>(LSTM_attention, self).__init__()</span><br><span class="line">        self.hidden_dim = hidden_dim</span><br><span class="line">        self.num_layers = num_layers</span><br><span class="line">        self.n_class = n_class</span><br><span class="line"></span><br><span class="line">        self.bidirectional = bidirectional</span><br><span class="line">        self.embedding = nn.Embedding.from_pretrained(pretrained_weight)</span><br><span class="line">        self.embedding.weight.requires_grad = update_w2v</span><br><span class="line">        self.encoder = nn.LSTM(input_size=embedding_dim, hidden_size=self.hidden_dim,</span><br><span class="line">                               num_layers=num_layers, bidirectional=self.bidirectional,</span><br><span class="line">                               dropout=drop_keep_prob)</span><br><span class="line"></span><br><span class="line">        <span class="comment">#TODO</span></span><br><span class="line">        <span class="comment"># What is nn. Parameter ? Explain</span></span><br><span class="line">        self.weight_W = nn.Parameter(torch.Tensor(<span class="number">2</span>*hidden_dim, <span class="number">2</span>*hidden_dim))</span><br><span class="line">        self.weight_proj = nn.Parameter(torch.Tensor(<span class="number">2</span>*hidden_dim, <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.bidirectional:</span><br><span class="line">            <span class="comment">#self.decoder1 = nn.Linear(hidden_dim * 2, n_class)</span></span><br><span class="line">            self.decoder1 = nn.Linear(hidden_dim * <span class="number">2</span>, hidden_dim)</span><br><span class="line">            self.decoder2 = nn.Linear(hidden_dim,n_class)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.decoder1 = nn.Linear(hidden_dim * <span class="number">2</span>, hidden_dim)</span><br><span class="line">            self.decoder2 = nn.Linear(hidden_dim,n_class)</span><br><span class="line"></span><br><span class="line">        nn.init.uniform_(self.weight_W, -<span class="number">0.1</span>, <span class="number">0.1</span>)</span><br><span class="line">        nn.init.uniform_(self.weight_proj, -<span class="number">0.1</span>, <span class="number">0.1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, inputs</span>):<span class="number">0</span></span><br><span class="line">        embeddings = self.embedding(inputs)<span class="comment"># [batch, seq_len] =&gt; [batch, seq_len, embed_dim][64,75,50]</span></span><br><span class="line"></span><br><span class="line">        states, hidden = self.encoder(embeddings.permute([<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>]))<span class="comment">#[batch, seq_len, embed_dim]</span></span><br><span class="line">        <span class="comment">#attention</span></span><br><span class="line"></span><br><span class="line">        u = torch.tanh(torch.matmul(states, self.weight_W))</span><br><span class="line">        att = torch.matmul(u, self.weight_proj)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        att_score = F.softmax(att, dim=<span class="number">1</span>)</span><br><span class="line">        scored_x = states * att_score</span><br><span class="line">        encoding = torch.<span class="built_in">sum</span>(scored_x, dim=<span class="number">1</span>)</span><br><span class="line">        outputs = self.decoder1(encoding)</span><br><span class="line">        outputs=self.decoder2(outputs)</span><br><span class="line">        <span class="keyword">return</span> outputs</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><figcaption><span>Sentiment_Analysis_main.py</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> unicode_literals, print_function, division</span><br><span class="line"><span class="keyword">from</span> io <span class="keyword">import</span> <span class="built_in">open</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> optim</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader,Dataset</span><br><span class="line"><span class="keyword">import</span> tqdm</span><br><span class="line"><span class="keyword">from</span> Sentiment_Analysis_DataProcess <span class="keyword">import</span> prepare_data,build_word2vec,Data_set</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> confusion_matrix,f1_score,recall_score</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">from</span> Sentiment_model <span class="keyword">import</span> LSTMModel,LSTM_attention</span><br><span class="line"><span class="keyword">from</span> Sentiment_Analysis_Config <span class="keyword">import</span> Config</span><br><span class="line"><span class="keyword">from</span> Sentiment_Analysis_eval <span class="keyword">import</span> val_accuary</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">train</span>(<span class="params">train_dataloader,model, device, epoches, lr</span>):</span><br><span class="line"></span><br><span class="line">        model.train()</span><br><span class="line">        model = model.to(device)</span><br><span class="line">        <span class="built_in">print</span>(model)</span><br><span class="line">        optimizer = optim.Adam(model.parameters(), lr=lr)</span><br><span class="line">        criterion = nn.CrossEntropyLoss()</span><br><span class="line">        <span class="comment"># scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.2)  # 学习率调整</span></span><br><span class="line">        best_acc = <span class="number">0.85</span></span><br><span class="line">        <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(epoches):  <span class="comment"># 一个epoch可以认为是一次训练循环</span></span><br><span class="line">            train_loss = <span class="number">0.0</span></span><br><span class="line">            correct = <span class="number">0</span></span><br><span class="line">            total = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">            train_dataloader = tqdm.tqdm(train_dataloader)</span><br><span class="line">            <span class="comment"># train_dataloader.set_description(&#x27;[%s%04d/%04d %s%f]&#x27; % </span></span><br><span class="line">            <span class="comment">#                                 (&#x27;Epoch:&#x27;, epoch + 1, epoches, &#x27;lr:&#x27;, scheduler.get_last_lr()[0]))</span></span><br><span class="line">            <span class="keyword">for</span> i, data_ <span class="keyword">in</span> (<span class="built_in">enumerate</span>(train_dataloader)):</span><br><span class="line"></span><br><span class="line">                optimizer.zero_grad()</span><br><span class="line">                input_, target = data_[<span class="number">0</span>], data_[<span class="number">1</span>]</span><br><span class="line">                input_=input_.<span class="built_in">type</span>(torch.LongTensor)</span><br><span class="line">                target=target.<span class="built_in">type</span>(torch.LongTensor)</span><br><span class="line">                input_=input_.to(device)</span><br><span class="line">                target=target.to(device)</span><br><span class="line">                output= model(input_)</span><br><span class="line">                <span class="comment"># 经过模型对象就产生了输出</span></span><br><span class="line">                target=target.squeeze(<span class="number">1</span>)</span><br><span class="line">                loss = criterion(output, target)</span><br><span class="line">                loss.backward()</span><br><span class="line">                optimizer.step()</span><br><span class="line">                train_loss+= loss.item()</span><br><span class="line">                _, predicted = torch.<span class="built_in">max</span>(output, <span class="number">1</span>)</span><br><span class="line">                <span class="comment">#print(predicted.shape)</span></span><br><span class="line">                total += target.size(<span class="number">0</span>)  <span class="comment"># 此处的size()类似numpy的shape: np.shape(train_images)[0]</span></span><br><span class="line">                <span class="comment">#print(target.shape)</span></span><br><span class="line">                correct += (predicted == target).<span class="built_in">sum</span>().item()</span><br><span class="line">                F1=f1_score(target.cpu(),predicted.cpu(),average=<span class="string">&#x27;weighted&#x27;</span>)</span><br><span class="line">                Recall=recall_score(target.cpu(),predicted.cpu(),average=<span class="string">&#x27;micro&#x27;</span>)</span><br><span class="line">                <span class="comment">#CM=confusion_matrix(target.cpu(),predicted.cpu())</span></span><br><span class="line">                postfix = &#123;<span class="string">&#x27;train_loss: &#123;:.5f&#125;,train_acc:&#123;:.3f&#125;%&#x27;</span></span><br><span class="line">                           <span class="string">&#x27;,F1: &#123;:.3f&#125;%,Recall:&#123;:.3f&#125;%&#x27;</span> .<span class="built_in">format</span>(train_loss / (i + <span class="number">1</span>),</span><br><span class="line">                                                                        <span class="number">100</span> * correct / total, <span class="number">100</span>*F1 , <span class="number">100</span>* Recall)&#125;</span><br><span class="line">                train_dataloader.set_postfix(log=postfix)</span><br><span class="line"></span><br><span class="line">            acc=val_accuary(model,val_dataloader,device,criterion)</span><br><span class="line">            <span class="keyword">if</span> acc&gt;best_acc:</span><br><span class="line">                best_acc = acc</span><br><span class="line">                <span class="keyword">if</span> os.path.exists(Config.model_state_dict_path) == <span class="literal">False</span>:</span><br><span class="line">                    os.mkdir(Config.model_state_dict_path)</span><br><span class="line">                torch.save(model,<span class="string">&#x27;./word2vec_data/sen_model_best.pkl&#x27;</span> )</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    splist=[]</span><br><span class="line">    word2id=&#123;&#125;</span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(Config.word2id_path, encoding=<span class="string">&#x27;utf-8&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">            <span class="keyword">for</span> line <span class="keyword">in</span> f.readlines():</span><br><span class="line">                sp = line.strip().split()<span class="comment">#去掉\n \t 等</span></span><br><span class="line">                splist.append(sp)</span><br><span class="line">            word2id=<span class="built_in">dict</span>(splist)<span class="comment">#转成字典</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> key <span class="keyword">in</span> word2id:<span class="comment"># 将字典的值，从str转成int</span></span><br><span class="line">        word2id[key]=<span class="built_in">int</span>(word2id[key])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    id2word=&#123;&#125;<span class="comment">#得到id2word</span></span><br><span class="line">    <span class="keyword">for</span> key,val <span class="keyword">in</span> word2id.items():</span><br><span class="line">        id2word[val]=key</span><br><span class="line"></span><br><span class="line">    device = torch.device(<span class="string">&quot;cuda&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    train_array,train_lable,val_array,val_lable,test_array,test_lable=prepare_data(word2id,</span><br><span class="line">                                                             train_path=Config.train_path,</span><br><span class="line">                                                             val_path=Config.val_path,</span><br><span class="line">                                                             test_path=Config.test_path,seq_lenth=Config.max_sen_len)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    train_loader = Data_set(train_array, train_lable)</span><br><span class="line">    train_dataloader = DataLoader(train_loader,</span><br><span class="line">                                 batch_size=Config.batch_size,</span><br><span class="line">                                 shuffle=<span class="literal">True</span>,</span><br><span class="line">                                 num_workers=<span class="number">0</span>)<span class="comment">#用了workers反而变慢了</span></span><br><span class="line"></span><br><span class="line">    val_loader = Data_set(val_array, val_lable)</span><br><span class="line">    val_dataloader = DataLoader(val_loader,</span><br><span class="line">                                 batch_size=Config.batch_size,</span><br><span class="line">                                 shuffle=<span class="literal">True</span>,</span><br><span class="line">                                 num_workers=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    test_loader = Data_set(test_array, test_lable)</span><br><span class="line">    test_dataloader = DataLoader(test_loader,</span><br><span class="line">                                 batch_size=Config.batch_size,</span><br><span class="line">                                 shuffle=<span class="literal">True</span>,</span><br><span class="line">                                 num_workers=<span class="number">0</span>)</span><br><span class="line">    w2vec=build_word2vec(Config.pre_word2vec_path,word2id,<span class="literal">None</span>)<span class="comment">#生成word2vec</span></span><br><span class="line">    w2vec=torch.from_numpy(w2vec)</span><br><span class="line">    w2vec=w2vec.<span class="built_in">float</span>()<span class="comment">#CUDA接受float32，不接受float64</span></span><br><span class="line">    model=LSTM_attention(Config.vocab_size,Config.embedding_dim,w2vec,Config.update_w2v,</span><br><span class="line">                    Config.hidden_dim,Config.num_layers,Config.drop_keep_prob,Config.n_class,Config.bidirectional)</span><br><span class="line"></span><br><span class="line"><span class="comment">#训练</span></span><br><span class="line">    train(train_dataloader,model=model,device=device,epoches=Config.n_epoch,lr=Config.lr)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#保存模型</span></span><br><span class="line">    <span class="keyword">if</span> os.path.exists(Config.model_state_dict_path) == <span class="literal">False</span>:</span><br><span class="line">           os.mkdir(Config.model_state_dict_path)</span><br><span class="line">    torch.save(model, Config.model_state_dict_path)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><figcaption><span>Sentiment_Analysis_eval.py</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> unicode_literals, print_function, division</span><br><span class="line"><span class="keyword">from</span> io <span class="keyword">import</span> <span class="built_in">open</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> confusion_matrix,f1_score,recall_score,precision_score</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">from</span> Sentiment_model <span class="keyword">import</span> LSTMModel,LSTM_attention</span><br><span class="line"><span class="keyword">from</span> Sentiment_Analysis_Config <span class="keyword">import</span> Config</span><br><span class="line"><span class="keyword">from</span> Sentiment_Analysis_DataProcess <span class="keyword">import</span> prepare_data,build_word2vec,text_to_array_nolable,Data_set</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">val_accuary</span>(<span class="params">model,val_dataloader,device,criterion</span>):</span><br><span class="line">    model = model.to(device)</span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        correct1 = <span class="number">0</span></span><br><span class="line">        total1 = <span class="number">0</span></span><br><span class="line">        val_loss=<span class="number">0.0</span></span><br><span class="line">        <span class="keyword">for</span> j, data_1 <span class="keyword">in</span> (<span class="built_in">enumerate</span>(val_dataloader, <span class="number">0</span>)):</span><br><span class="line">            input1, target1 = data_1[<span class="number">0</span>], data_1[<span class="number">1</span>]</span><br><span class="line">            input1= input1.<span class="built_in">type</span>(torch.LongTensor)</span><br><span class="line">            target1 = target1.<span class="built_in">type</span>(torch.LongTensor)</span><br><span class="line">            target1=target1.squeeze(<span class="number">1</span>)<span class="comment">#从[64,1]到[64]</span></span><br><span class="line">            input1 = input1.to(device)</span><br><span class="line">            target1 = target1.to(device)</span><br><span class="line">            output1 = model(input1)</span><br><span class="line">            loss1 = criterion(output1, target1)</span><br><span class="line">            val_loss += loss1.item()</span><br><span class="line">            _, predicted1 = torch.<span class="built_in">max</span>(output1, <span class="number">1</span>)</span><br><span class="line">            total1 += target1.size(<span class="number">0</span>)<span class="comment"># 此处的size()类似numpy的shape: np.shape(train_images)[0]</span></span><br><span class="line">            correct1 += (predicted1 == target1).<span class="built_in">sum</span>().item()</span><br><span class="line">            F1 = f1_score(target1.cpu(), predicted1.cpu(), average=<span class="string">&#x27;weighted&#x27;</span>)</span><br><span class="line">            Recall = recall_score(target1.cpu(), predicted1.cpu(), average=<span class="string">&#x27;micro&#x27;</span>)</span><br><span class="line">            <span class="comment">#CM = confusion_matrix(target1.cpu(), predicted1.cpu())</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;\nVal accuracy : &#123;:.3f&#125;%,val_loss:&#123;:.3f&#125;, F1_score：&#123;:.3f&#125;%, Recall：&#123;:.3f&#125;%&#x27;</span>.<span class="built_in">format</span>(<span class="number">100</span>*correct1/total1,val_loss,<span class="number">100</span>*F1,<span class="number">100</span>*Recall))</span><br><span class="line">        <span class="keyword">return</span> <span class="number">100</span>*correct1/total1</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">test_accuary</span>(<span class="params">model,test_dataloader,device</span>):</span><br><span class="line">    model = model.to(device)</span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        correct = <span class="number">0</span></span><br><span class="line">        total = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> k, data_test <span class="keyword">in</span> (<span class="built_in">enumerate</span>(test_dataloader, <span class="number">0</span>)):</span><br><span class="line">            input_test, target_ = data_test[<span class="number">0</span>], data_test[<span class="number">1</span>]</span><br><span class="line">            input_test= input_test.<span class="built_in">type</span>(torch.LongTensor)</span><br><span class="line">            target_ = target_.<span class="built_in">type</span>(torch.LongTensor)</span><br><span class="line">            target_=target_.squeeze(<span class="number">1</span>)<span class="comment">#从[64,1]到[64]</span></span><br><span class="line">            input_test = input_test.to(device)</span><br><span class="line">            target_ = target_.to(device)</span><br><span class="line">            output2 = model(input_test)</span><br><span class="line">            _, predicted_test = torch.<span class="built_in">max</span>(output2, <span class="number">1</span>)</span><br><span class="line">            total += target_.size(<span class="number">0</span>)<span class="comment"># 此处的size()类似numpy的shape: np.shape(train_images)[0]</span></span><br><span class="line">            correct += (predicted_test == target_).<span class="built_in">sum</span>().item()</span><br><span class="line">            F1 = f1_score(target_.cpu(), predicted_test.cpu(), average=<span class="string">&#x27;weighted&#x27;</span>)</span><br><span class="line">            Recall = recall_score(target_.cpu(), predicted_test.cpu(), average=<span class="string">&#x27;micro&#x27;</span>)</span><br><span class="line">            CM = confusion_matrix(target_.cpu(), predicted_test.cpu())</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;test accuracy : &#123;:.3f&#125;%, F1_score：&#123;:.3f&#125;%, Recall：&#123;:.3f&#125;%,Confusion_matrix：&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(<span class="number">100</span>*correct/total,<span class="number">100</span>*F1,<span class="number">100</span>*Recall,CM))</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">pre</span>(<span class="params">word2id,model,seq_lenth ,path</span>):</span><br><span class="line">    model.cpu()</span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        input_array=text_to_array_nolable(word2id,seq_lenth,path)</span><br><span class="line">        <span class="comment">#sen_p = sen_p.type(torch.LongTensor)</span></span><br><span class="line">        sen_p = torch.from_numpy(input_array)</span><br><span class="line">        sen_p=sen_p.<span class="built_in">type</span>(torch.LongTensor)</span><br><span class="line">        output_p = model(sen_p)</span><br><span class="line">        _, pred = torch.<span class="built_in">max</span>(output_p, <span class="number">1</span>)</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> pred:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&#x27;预测类别为&#x27;</span>,i.item())</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    splist = []</span><br><span class="line">    word2id = &#123;&#125;</span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(Config.word2id_path, encoding=<span class="string">&#x27;utf-8&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">        <span class="keyword">for</span> line <span class="keyword">in</span> f.readlines():</span><br><span class="line">            sp = line.strip().split()  <span class="comment"># 去掉\n \t 等</span></span><br><span class="line">            splist.append(sp)</span><br><span class="line">        word2id = <span class="built_in">dict</span>(splist)  <span class="comment"># 转成字典</span></span><br><span class="line">    <span class="keyword">for</span> key <span class="keyword">in</span> word2id:  <span class="comment"># 将字典的值，从str转成int</span></span><br><span class="line">        word2id[key] = <span class="built_in">int</span>(word2id[key])</span><br><span class="line"></span><br><span class="line">    device = torch.device(<span class="string">&quot;cuda&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span>)</span><br><span class="line">    train_array, train_lable, val_array, val_lable, test_array, test_lable = prepare_data(word2id,</span><br><span class="line">                                                                                          train_path=Config.train_path,</span><br><span class="line">                                                                                          val_path=Config.val_path,</span><br><span class="line">                                                                                          test_path=Config.test_path,seq_lenth=Config.max_sen_len)</span><br><span class="line">    test_loader = Data_set(test_array, test_lable)</span><br><span class="line">    test_dataloader = DataLoader(test_loader,</span><br><span class="line">                                 batch_size=Config.batch_size,</span><br><span class="line">                                 shuffle=<span class="literal">True</span>,</span><br><span class="line">                                 num_workers=<span class="number">0</span>)</span><br><span class="line">    w2vec = build_word2vec(Config.pre_word2vec_path,</span><br><span class="line">                           word2id,</span><br><span class="line">                          <span class="literal">None</span>)  <span class="comment"># 生成word2vec</span></span><br><span class="line">    w2vec = torch.from_numpy(w2vec)</span><br><span class="line">    w2vec = w2vec.<span class="built_in">float</span>()  <span class="comment"># CUDA接受float32，不接受float64</span></span><br><span class="line"></span><br><span class="line">    model=LSTM_attention(Config.vocab_size,Config.embedding_dim,w2vec,Config.update_w2v,</span><br><span class="line">                        Config.hidden_dim,Config.num_layers,Config.drop_keep_prob,Config.n_class,Config.bidirectional)</span><br><span class="line">    <span class="comment"># 读取模型</span></span><br><span class="line">    <span class="comment">#model1 = torch.load(Config.model_state_dict_path)</span></span><br><span class="line">    model = torch.load(<span class="string">&#x27;./word2vec_data/sen_model_best.pkl&#x27;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">#model.load_state_dict(torch.load(Config.model_state_dict_path)) #仅保存参数</span></span><br><span class="line">    <span class="comment">#验证</span></span><br><span class="line">    <span class="comment">#val_accuary(model1, val_dataloader, device)</span></span><br><span class="line">    <span class="comment">#测试</span></span><br><span class="line">    test_accuary(model,test_dataloader,device)</span><br><span class="line">    <span class="comment">#预测</span></span><br><span class="line">    pre(word2id,model,Config.max_sen_len,Config.pre_path)</span><br></pre></td></tr></table></figure>
<h1 id="参考文献">参考文献</h1>
<p><span class="exturl" data-url="aHR0cHM6Ly9naXRlZS5jb20vaHUteWFuZ2dhbmcvU2VudGltZW50LUFuYWx5c2lzLUNoaW5lc2UtcHl0b3JjaCMlRTUlOEUlOUYlRTUlODglOUIlRTQlQjglOEQlRTYlOTglOTMlRTUlQTYlODIlRTYlOUUlOUMlRTUlQTUlQkQlRTclOTQlQTglRTglQUYlQjclRTclQkIlOTklRTQlQjglQUFzdGFyJUU4JUIwJUEyJUU4JUIwJUEyJUU0JUJBJTg2LSVFNCVCRCU5QyVFOCU4MCU4NSVFNiU5RCU4RSVFNyU4QiU5NyVFNSU5NyVBOA==">Sentiment-Analysis-Chinese-pytorch<i class="fa fa-external-link-alt"></i></span></p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://soundmemories.github.io/2021/06/25/Paper/02.XGBoost%20A%20Scalable%20Tree%20Boosting%20System/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avstar.png">
      <meta itemprop="name" content="SoundMemories">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="SoundMemories">
      <meta itemprop="description" content="今日事，今日毕">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | SoundMemories">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2021/06/25/Paper/02.XGBoost%20A%20Scalable%20Tree%20Boosting%20System/" class="post-title-link" itemprop="url">XGBoost A Scalable Tree Boosting System</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2021-06-25 00:00:00" itemprop="dateCreated datePublished" datetime="2021-06-25T00:00:00+08:00">2021-06-25</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Paper/" itemprop="url" rel="index"><span itemprop="name">Paper</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>4.7k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>17 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="摘要">摘要</h1>
<p>提出名为XGBoost的树提升系统。<br />
提出一种新颖的稀疏数据感知算法用于稀疏数据，一种带权值的分位数略图(weighted
quantile sketch) 来近似实现树的学习。<br />
提出有关缓存访问模式，数据压缩和分片的见解，以构建有延展性的提升树系统。</p>
<h1 id="导读">导读</h1>
<p>机器学习方法里，GradientTree
Boosting（GBDT）是一个在很多应用里都很出彩的技术。提升树方法在很多有标准分类基准的情况下表现很出色。本文提出了一个可扩展的提升树机器学习系统（XGBoost）。XGBoost在2015年的29场比赛获胜队伍中，有17个都使用了XGBoost。</p>
<p>主要贡献：<br />
1、设计和构建高度可扩展的端到端提升树系统。（树的个数能灵活的增加或减少）<br />
2、提出了一个理论上合理的加权分位法。（推荐分割点的时候用，能不用遍历所有的点，只用部分点就行）<br />
3、引入了一种新颖的稀疏感知算法用于并行树学习。（令缺失值有默认方向，稀疏数据处理方法和并行计算）<br />
4、提出了一个有效的用于核外树形学习的缓存感知块结构。（有效使用缓存块处理数据）</p>
<h1 id="内容">内容</h1>
<h2 id="regularized-learning-objective">Regularized Learning
Objective</h2>
<p>给定一个数据集<span class="math inline">\(\mathcal{D}\)</span>，$
n<span class="math inline">\(个样本，每个样本有\)</span>
m$个特征：<br />
<span class="math display">\[
\mathcal{D}= \{(x_i,y_i)\}(|\mathcal{D}|= n,x_i\in \Bbb{R}^m, y_i\in
\Bbb{R})
\]</span><br />
第 $ k$ 棵树对于输入 $ x_i$ 的样本预测结果为$ f_k(x_i)$：<br />
<span class="math display">\[
\hat{y}_i=\varnothing(x_i)=\sum\limits_{k=1}^{K}f_k(x_i),\quad f_k\in
\mathcal{F}
\]</span><br />
其中，<span class="math inline">\(\mathcal{F}=
\{f(x)=w_{q(x)}\}(q:\Bbb{R}^m\to T,w\in \Bbb{R}^T)\)</span>
是CART回归树。$ q(x)$ 表示映射样本 $ x$ 到叶子节点的下标。$ T<span
class="math inline">\(是叶子节点数量。\)</span> w$ 是叶子节点最优解（$
w_i<span
class="math inline">\(表示第\)</span>i$个叶子节点的最优解）。每一棵树都有独立的
$ q$ 和 $ w$。</p>
<h2 id="gradient-tree-boosting">Gradient Tree Boosting</h2>
以上定义了树模型的预测函数，那么接下来定义整个损失函数：<br />
<span class="math display">\[
\mathcal{L}(\varnothing)= \sum\limits_i l(\hat{y}_i,y_i)+\sum\limits_k
\Omega(f_k)\\
\]</span><br />
<span class="math display">\[
\text{where} \quad \Omega( f)=\gamma  T + \frac{1}{2}\lambda||w||^2
\]</span><br />
其中，$ l$ 函数是一个可导的凸函数，用来表示预测值 <span
class="math inline">\(\hat{y}_i\)</span> 和真实值 $ y_i$
之间的差异。<span class="math inline">\(\Omega\)</span>
是是惩罚项（正则化），用来防止树的结构过于复杂。<br />
损失函数 <span class="math inline">\(\mathcal{L}\)</span>
参数中包含了函数，所以不能用传统的优化算法来优化。假设 <span
class="math inline">\(\hat{y}_i^{t}\)</span> 是 $ x_i$ 在第 $ t$
次迭代中的预测值。那么则有：<br />
<span class="math display">\[
\mathcal{L}^{( t)}= \sum\limits_{i=
1}^{n}l(y_i,\hat{y}_i^{(t-1)}+f_t(x_i))+\Omega(f_t)
\]</span><br />
把 $ y_i,_i^{(t-1)}$ 看成 <span class="math inline">\(x\)</span> ，把 $
f_t(x_i))$ 看成 <span class="math inline">\(\Delta x\)</span>
，对上式近似为二阶泰勒级展开：<br />
<span class="math display">\[
\mathcal{L}^{( t)}\simeq \sum\limits_{i=
1}^{n}[l(y_i,\hat{y}_i^{(t-1)})+g_if_t(x_i)+\frac{1}{2} h_if_t^{
2}(x_i)]+\Omega(f_t)
\]</span><br />
<span class="math display">\[
\text{where}
\quad  g_i=\partial_{\hat{y}^{(t-1)}}l(y_i,\hat{y}_i^{(t-1)})\quad
\text{and} \quad h_i=\partial_{\hat{y}^{(t-1)}}^{
2}l(y_i,\hat{y}_i^{(t-1)})
\]</span><br />
其中，$ g_i<span class="math display">\[ h_i$是一阶偏导和二阶偏导。由于$
l(y_i,\hat{y}_i^{(t-1)})$为常数项，可以去除。
定义 $ I_j=\{i|q(x_i)=j\}$ 作为样本 $ x_i$ 被分割到第 $ j$
个叶子节点下的样本下标集合（样本集合）。那么上面公式可以由**对每棵树的样本求和**转成**对每棵树的叶子节点求和**：
\]</span><br />

<span class="math display">\[\begin{aligned}
\tilde{\mathcal{L}}^{( t)}&amp;= \sum\limits_{i=
1}^{n}[g_if_t(x_i)+\frac{1}{2} h_if_t^{ 2}(x_i)]+\Omega(f_t)\\
&amp;= \sum\limits_{i= 1}^{n}[g_if_t(x_i)+\frac{1}{2} h_if_t^{
2}(x_i)]+\gamma  T + \frac{1}{2}\lambda \sum\limits_{ j= 1}^{T}w_j^{
2}\\
&amp;= \sum\limits_{i= 1}^{n}[g_iw_{q(x_i)}+\frac{1}{2}
h_i(w_{q(x_i)})^{ 2}]+\gamma  T + \frac{1}{2}\lambda \sum\limits_{ j=
1}^{T}w_j^{ 2}\\
&amp;= \sum\limits_{j= 1}^{T}[\sum\limits_{i\in
I_j}g_iw_{j}+\frac{1}{2}\sum\limits_{i\in  I_j} h_i(w_{j})^{
2}]+\gamma  T + \frac{1}{2}\lambda \sum\limits_{ j= 1}^{T}w_j^{ 2}\\
&amp;= \sum\limits_{j= 1}^{T}[(\sum\limits_{i\in
I_j}g_i)w_{j}+\frac{1}{2}(\sum\limits_{i\in  I_j} h_i+\lambda)w_{j}^{
2}]+\gamma  T
\end{aligned}\]</span>
<p><span class="math display">\[
**1、如何求出每个叶子节点的最优解？**
对上式求极值点（它是凸函数，求的是极小值），即对 $ w_j$
求一阶导数等于零（也可以看成二元一次方程求解），解得：
\]</span><br />
w_j^*=-<br />
<span class="math display">\[
带入 $\tilde{\mathcal{L}}^{( t)}$ 求得最优值（最小值）：
\]</span><br />
^{( t)}=-_{j= 1}^{T}+T<br />
$$<br />
可以用这个公式来来衡量决策树的质量。有点像决策树信息熵一个道理。</p>
<p><strong>2、对当前决策树做子树分裂时，如何选择哪个特征和特征值进行分裂，使损失函数最小？</strong><br />
如果想要划分后损失函数得到最小值，意味这在做每次划分的时候，要尽量保证划分后的score比划分前的score要更小。那么应该找到
(划分前的score - 划分后的score)
这个差最大的切分点作为我们这一次的划分点。假设 $ I_L$和 $ I_R$
为一个节点 $ I$ 划分后的左子集和右子集，节点 $ I=I_L I_R$
，则得到以下公式：<br />
<span class="math display">\[
\begin{aligned}
\mathcal{L}_{split}&amp;=- \dfrac{1}{2}[\frac{(\sum_{i\in I}g_i)^{
2}}{\sum_{i\in I}h_i+\lambda}-\frac{(\sum_{i\in I_L}g_i)^{
2}}{\sum_{i\in I_L}h_i+\lambda}-\frac{(\sum_{i\in I_R}g_i)^{
2}}{\sum_{i\in I_R}h_i+\lambda}]-\gamma\\
&amp;= \dfrac{1}{2}  [\frac{(\sum_{i\in I_L}g_i)^{ 2}}{\sum_{i\in
I_L}h_i+\lambda}+\frac{(\sum_{i\in I_R}g_i)^{ 2}}{\sum_{i\in
I_R}h_i+\lambda}-\frac{(\sum_{i\in I}g_i)^{ 2}}{\sum_{i\in
I}h_i+\lambda}]-\gamma
\end{aligned}
\]</span><br />
可以用这个公式来衡量是否当前节点是否再应该继续划分下去。每次用不同的特征，计算分数，然后用最大的值那个特征，作为当前树节点的划分点。</p>
<p>相对于GBDT，XGBoost一次性求解出<strong>最优解叶子节点区域</strong>和<strong>每个叶子节点区域最优解</strong>。而GBDT是基于残差（一阶泰勒）拟合一颗CART书，得到<strong>最优叶子节点区域</strong>，再求出<strong>每个叶子节点区域最优解</strong>。</p>
<h2 id="shrinkage-and-column-subsampling">Shrinkage and Column
Subsampling</h2>
<p>除了之前提过的添加正则化的项来防止模型的过拟合之外，还可以用两种方式来防止过拟合：<br />
（1）添加类似梯度下降优化问题中的学习率 $ $
，这个可以收缩每棵树的权重，让每棵树的生长更加稳定。<br />
（2）对样本的特征子采样（随机森林用到过）。每次生成树的时候，只用其中一部分抽样的特征。这样子也能降低过拟合的风险。</p>
<p><strong>精准的贪心算法</strong><br />
贪心算法就是每次都希望找到最优的结果，但这样需每次都遍历所有的特征，对每个特征，又遍历所有划分的可能。然后通过
$ _{split}$
的分数，计算每次划分后的score，取最大的score的对应的特征来进行划分。<br />
<img src="/images/XGB/01.png" width="60%"></p>
<p><strong>近似的贪心算法</strong><br />
用上面贪心算法来寻找最佳划分点，准确度非常不错，但是时间复杂度和空间复杂度都太高了，特别是对于连续值的变量来说，简直是一个大灾难。作者提出一种近似法分位法，先对数据进行分桶(Bucket)，然后桶内的数据相加起来，作为一个代表来进行计算。</p>
<p>那我们应该在什么时候对数据进行分桶呢？有两种方式。<br />
（1）全局分桶（Global
Bucket），可以一开始就对全部的数据进行分桶。后面进行划分的时候只需要使用分桶数据就可以了。<br />
（2）局部分桶（Local Bucket），每次需要对当前leaf
node进行划分的时候，对当前节点里面的数据进行分桶。然后再划分。当然这个时间复杂度也会变的比较高。</p>
<p>具体划分的伪代码如下：<br />
（1）先计算1到M个特征，找出每个特征的分桶的候选点。<br />
（2）然后将候选点之间的数据的g和h求和，装入到Bucket里面，代表这些数据。<br />
（3）后面流程就跟精确的弹性分割算法一样。只是将每个Bucket看成一个x。<br />
<img src="/images/XGB/02.png" width="60%"></p>
<p>下面是作者测试对几种不同的切分算法的AUC结果比较图。可以看得出，当eps=0.05，也就是将数据分成20个Bucket的时候，AUC的分数跟精准的贪心算法一样。<br />
<img src="/images/XGB/03.png" width="60%"></p>
<h2 id="split-finding-algorithms">SPLIT FINDING ALGORITHMS</h2>
<p>提出一种新的分桶的方法<strong>Weighted Quantile
Sketch</strong>（加权分位法）。</p>
<p><strong>加权分位法</strong><br />
上面我们讨论了对数据分成Bucket，再来计算他的节点的split分数。来减少我们的计算量。那么我们如何对数据分桶，才能够比较合理呢？</p>
<p>作者按照对loss的影响权重来进行分桶，让数据分桶后，每个桶对loss的影响权重相同。<br />
定义一个数据集 $ <em>k={(x</em>{1k},h_1),(x_{2k},h_2),...,(x_{nk},h_n)}$
，$ k$ 为样本 <span class="math inline">\(x\)</span> 的特征数。<br />
定义一个排序函数 $ r_k:[0,)$，则：<br />
<span class="math display">\[
r_k(z)=\frac{1}{\sum_{(x,h)\in \mathcal{D}_k}h}\sum_{(x,h)\in
\mathcal{D}_k,x&lt;z}h
\]</span><br />
切分点集合 $ {s_{k1}, s_{k2},...,s_{kl}}$，满足：<br />
<span class="math display">\[
|r_k(s_{k,j})-r_k(s_{k,j+1})|&lt;\epsilon, \quad s_{k1}=\min\limits_i
x_{ik},s_{kl}=\max\limits_i x_{ik}
\]</span><br />
这里 <span class="math inline">\(\epsilon\)</span>
用来衡量划分区间的大小（每个Bucket不能太大，以免错过最优切分点）。这个意味这数据集大约被分为
<span class="math inline">\(\frac{1}{\epsilon}\)</span> 个候选点。</p>
<p>那么 <span class="math inline">\(h_i\)</span> 为什么能够代表 <span
class="math inline">\(x_i\)</span> 的权重来进行排序呢？将公式 <span
class="math inline">\({\tilde{\mathcal{L}}}^{(t)}\)</span> 对 $ h_i$
进行提取，可得到：<br />
<span class="math display">\[
\begin{aligned}
\tilde{\mathcal{L}}^{( t)}&amp;= \sum\limits_{i=
1}^{n}[g_if_t(x_i)+\frac{1}{2} h_if_t^{ 2}(x_i)]+\Omega(f_t)\\
&amp;= \sum\limits_{i= 1}^{n}[\frac{1}{2} h_i\frac{ 2*
g_if_t(x_i)}{h_i}+\frac{1}{2} h_if_t^{ 2}(x_i)]+\Omega(f_t)\\
&amp;= \sum\limits_{i= 1}^{n}\frac{1}{2} h_i[ 2\times
\frac{g_i}{h_i}f_t(x_i)+f_t^{ 2}(x_i)]+\Omega(f_t)\\
&amp;= \sum\limits_{i= 1}^{n}\frac{1}{2} h_i[ 2\times
\frac{g_i}{h_i}f_t(x_i)+f_t^{
2}(x_i)+(\frac{g_i}{h_i})^2-(\frac{g_i}{h_i})^2]+\Omega(f_t)\\
&amp;= \sum\limits_{i= 1}^{n}\frac{1}{2} h_i(f_t(x_i)+\frac{g_i}{h_i})^{
2}+\Omega(f_t)\\
&amp;= \sum\limits_{i= 1}^{n}\frac{1}{2}
h_i(f_t(x_i)-(-\frac{g_i}{h_i}))^{ 2}+\Omega(f_t)+\text{constant}
\end{aligned}
\]</span><br />
发现 $ h_i$ 对结果影响最大，所以用它来进行排序。</p>
<h2 id="sparsity-aware-split-finding">Sparsity-aware Split Finding</h2>
<p>对稀疏数据的处理，重点是针对特征为空时的处理，对<strong>精准的贪心算法</strong>做了改进：<br />
<img src="/images/XGB/04.png" width="60%"></p>
<p>简单来讲，通过两轮遍历可以确保稀疏值位于左子树和右子树的情形，就是对该特征划分为左节点还是右节点做了一次比较，哪个效果好就把它放在哪。</p>
<h2 id="system-design">SYSTEM DESIGN</h2>
<p>重点是优化：<br />
（1）<strong>预排序</strong>：这个算法大量时间消耗在排序上。只需在最开始对每个特征排一次序即可。<br />
这里XGB将所有的列数据都预先排了序。以压缩形式分别存到block里，不同的block可以分布式存储，甚至存到硬盘里。在特征选择的时候，可以并行的处理这些列数据，XGB就是在这实现的并行化，用多线程来实现加速。同时这里还用cache加了一个底层优化：当数据排序后，索引值是乱序的，可能指向了不同的内存地址，找的时候数据是不连续的，这里加了个缓存，让以后找的时候能找到小批量的连续地址，以实现加速！这里是在每个线程里申请了一个internal
buffer来实现的！这个优化在小数据下看不出来，数据越多越明显。</p>
<p>（2）<strong>预取</strong>：尽可能的把数据保存在缓存中，这样不用去磁盘进行读取，减少时间开销。并且给出了对比结果：<br />
<img src="/images/XGB/05.png" width="100%"></p>
<p>（3）内存块的大小也会影响缓存速率。<br />
<img src="/images/XGB/06.png" width="50%"></p>
<p>针对磁盘存储优化。磁盘block不大的情况下：<br />
1.把block数据进行压缩，让它没有那么大。<br />
2.把block数据放在多个磁盘，增大磁盘带宽，让读取速度更。</p>
<h1 id="信息论">信息论</h1>
<h2 id="熵">熵</h2>
<p>如果<span
class="math inline">\(\textbf{X}\)</span>是一个离散型随机变量，取空间值为<span
class="math inline">\(\Bbb{R}\)</span>，其概率分布为$ p(x)=P(=x),x<span
class="math inline">\(。那么，\)</span><span
class="math inline">\(的熵\)</span>H()$定义为：<br />
<span class="math display">\[
H(\textbf{X})=-\sum\limits_{x\in\Bbb{R}} p(x) log p(x)
\]</span><br />
其中，<span class="math inline">\(H(\textbf{X})\)</span>可以写成<span
class="math inline">\(H( p)\)</span>。</p>
<p><strong>熵又称为子信息（self-information），可以视为描述一个随机变量的不确定性的数量</strong>。它表示信源<span
class="math inline">\(\textbf{X}\)</span>每发一个符号（不论发什么符号）所提供的平均信息量。<strong>一个随机变量的熵越大，它的不确定性越大，那么，正确估计其值的可能性就越小。越不确定的随机变量越需要大的信息量用以确定其值。</strong></p>
<p><strong>熵定义了随机变量的不确定性，当熵最大时，随机变量最不确定，最难准确地预测其行为。也就是说，在已知部分知识的前提下，关于未知分布最合理的推断应该是符合已知知识最不确定或最大随机的推断。</strong></p>
<h2 id="联合熵和条件熵">联合熵和条件熵</h2>
<p>如果<span
class="math inline">\(\textbf{X},\textbf{Y}\)</span>是一对离散型随机变量<span
class="math inline">\(\textbf{X},\textbf{Y}\sim p(x,y)\)</span>，<span
class="math inline">\(\textbf{X},\textbf{Y}\)</span>的联合熵（joint
entropy）<span
class="math inline">\(H(\textbf{X},\textbf{Y})\)</span>定义为：<br />
<span class="math display">\[
H(\textbf{X},\textbf{Y})=-\sum\limits_{ x\in\textbf{X}}\sum\limits_{
y\in\textbf{Y}} p(x,y) log p(x,y)
\]</span><br />
联合熵实际上就是描述一对随机变量平均所需要的信息量。</p>
<p>给定随机变量<span class="math inline">\(\textbf{X}\)</span>的情况下，
随机变量<span
class="math inline">\(\textbf{Y}\)</span>的条件熵（conditionalentropy）：<br />
<span class="math display">\[
\begin{aligned}
H(\textbf{Y}|\textbf{X})&amp;=\sum\limits_{ x\in\textbf{X}} p(x)
H(\textbf{Y}|\textbf{X}= x)\\
&amp;=\sum\limits_{ x\in\textbf{X}} p(x)[-\sum\limits_{ y\in\textbf{Y}}
p(y|x) log p(y|x)]\\
&amp;=-\sum\limits_{ x\in\textbf{X}}\sum\limits_{ y\in\textbf{Y}} p(x,y)
log p(y|x)
\end{aligned}
\]</span><br />
将其中的联合概率<span class="math inline">\(log
p(x,y)\)</span>展开，可得：<br />
<span class="math display">\[
\begin{aligned}
H(\textbf{X},\textbf{Y})&amp;=-\sum\limits_{
x\in\textbf{X}}\sum\limits_{ y\in\textbf{Y}} p(x,y) log[ p(x)p(y|x)]\\
&amp;=-\sum\limits_{ x\in\textbf{X}}\sum\limits_{ y\in\textbf{Y}} p(x,y)
[log p(x) + log p(y|x)]\\
&amp;=-\sum\limits_{ x\in\textbf{X}}\sum\limits_{ y\in\textbf{Y}} p(x,y)
log p(x)-\sum\limits_{ x\in\textbf{X}}\sum\limits_{ y\in\textbf{Y}}
p(x,y) log p(y|x)\\
&amp;=-\sum\limits_{ x\in\textbf{X}} p(x) log p(x)-\sum\limits_{
x\in\textbf{X}}\sum\limits_{ y\in\textbf{Y}} p(x,y) log p(y|x)\\
&amp;=H(\textbf{X})+H(\textbf{Y}|\textbf{X})
\end{aligned}
\]</span><br />
我们称上式为熵的连锁规则。推广到一般情况，有：<br />
<span class="math display">\[
H(\textbf{X}_1,\textbf{X}_2,...,\textbf{X}_n)=H(\textbf{X}_1)+H(\textbf{X}_2|\textbf{X}_1)+...+H(\textbf{X}_n|\textbf{X}_1,\textbf{X}_2,...,\textbf{X}_{n-1})
\]</span></p>
<h2 id="互信息">互信息</h2>
<p>根据熵的连锁规则， 有：<br />
<span class="math display">\[
H(\textbf{X},\textbf{Y})=H(\textbf{X})+H(\textbf{Y}|\textbf{X})=H(\textbf{Y})+H(\textbf{X}|\textbf{Y})
\]</span><br />
因此：<br />
<span class="math display">\[
H(\textbf{X})-H(\textbf{X}|\textbf{Y})=H(\textbf{Y})-H(\textbf{Y}|\textbf{X})
\]</span><br />
这个差叫做<span class="math inline">\(\textbf{X}\)</span>和<span
class="math inline">\(\textbf{Y}\)</span>的互信息（mutual information,
MI），记作<span
class="math inline">\(I(\textbf{X};\textbf{Y})\)</span>。<br />
或者定义为：如果<span class="math inline">\((\textbf{X},\textbf{Y})\sim
p(x,y)\)</span>，则<span
class="math inline">\(\textbf{X},\textbf{Y}\)</span>之间的互信息<span
class="math inline">\(I(\textbf{X};\textbf{Y})=H(\textbf{X})-H(\textbf{X}|\textbf{Y})\)</span>。</p>
<p><strong><span
class="math inline">\(I(\textbf{X};\textbf{Y})\)</span>反映的是在知道了<span
class="math inline">\(\textbf{Y}\)</span>的值以后<span
class="math inline">\(\textbf{X}\)</span>的不确定性的减少量。可以理解为<span
class="math inline">\(\textbf{Y}\)</span>的值透露了多少关于<span
class="math inline">\(\textbf{X}\)</span>的信息量。</strong><br />
互信息和熵之间的关系：<br />
<img src="/images/XGB/互信息.png" width="40%"></p>
<p>如果将定义中的<span
class="math inline">\(H(\textbf{X})\)</span>和<span
class="math inline">\(H(\textbf{X}|\textbf{Y})\)</span>展开，可得：<br />
<span class="math display">\[
\begin{aligned}
I(\textbf{X};\textbf{Y})&amp;=H(\textbf{X})-H(\textbf{X}|\textbf{Y})\\
&amp;=H(\textbf{X})+H(\textbf{Y})-H(\textbf{X},\textbf{Y})\\
&amp;=\sum\limits_{ x} p(x) log \frac{1}{p(x)}  + \sum\limits_{ y} p(y)
log \frac{1}{p(y)}  + \sum\limits_{ x,y} p(x,y) log p(x,y) \\
&amp;=\sum\limits_{ x,y} p(x,y) log \frac{p(x,y)}{p(x)p(y)} \\
\end{aligned}
\]</span><br />
由于<span class="math inline">\(H(\textbf{X}|\textbf{X})=0\)</span>，
因此，<br />
<span class="math display">\[
H(\textbf{X})=H(\textbf{X})-H(\textbf{X}|\textbf{X})=I(\textbf{X};\textbf{X})
\]</span><br />
这一方面说明了为什么熵又称为自信息，另一方面说明了两个完全相互依赖的变量之间的互信息并不是一个常量，而是取决于它们的熵。</p>
<p>实际上，互信息体现了两变量之间的依赖程度：<br />
如果<span
class="math inline">\(I(\textbf{X};\textbf{Y})≫0\)</span>，表明<span
class="math inline">\(\textbf{X}\)</span>和<span
class="math inline">\(\textbf{Y}\)</span>是高度相关的；<br />
如果<span
class="math inline">\(I(\textbf{X};\textbf{Y})=0\)</span>，表明<span
class="math inline">\(\textbf{X}\)</span>和<span
class="math inline">\(\textbf{Y}\)</span>是相互独立的；<br />
如果<span
class="math inline">\(I(\textbf{X};\textbf{Y})≪0\)</span>，表明<span
class="math inline">\(\textbf{Y}\)</span>的出现不但未使<span
class="math inline">\(\textbf{X}\)</span>的不确定性减小，反而增大了<span
class="math inline">\(\textbf{X}\)</span>的不确定性，是非常是不利的。</p>
<h2 id="相对熵">相对熵</h2>
<p>相对熵（relative entropy）又称Kullback-Leibler差异（KullbackLeibler
divergence），或简称KL距离，是<strong>衡量相同事件空间里两个概率分布相对差距的测度</strong>。两个概率分布$
p(x)<span class="math inline">\(和\)</span> q(x)$的相对熵定义为：<br />
<span class="math display">\[
\begin{aligned}
D( p||q)&amp;=\sum\limits_{ x\in\textbf{X}} p(x) log\frac{ p(x)}{
q(x)}\\
&amp;=\sum\limits_{ x\in\textbf{X}} p(x) log  p(x)  - \sum\limits_{
x\in\textbf{X}} p(x) log q(x)\\
&amp;=-H( p)+H( p,q)
\end{aligned}
\]</span><br />
其中，<span class="math inline">\(H(
p)\)</span>恒不变，只需考虑交叉熵<span class="math inline">\(H(
p,q)\)</span>即可。$ q(x)$ 分布越接近 $ p(x)$，那么散度值越小。<br />
有时会将KL散度称为KL距离，但它并不满足距离的性质：KL散度不是对称的；KL散度不满足三角不等式。</p>
<h2 id="交叉熵">交叉熵</h2>
<p>根据前面熵的定义，知道熵是一个不确定性的测度，也就是说，我们对于某件事情知道得越多，那么，熵就越小，因而对于试验的结果我们越不感到意外。<strong>交叉熵的概念就是用来衡量估计模型与真实概率分布之间差异情况的。</strong><br />
如果一个随机变量<span class="math inline">\(\textbf{X}\sim
p(x)\)</span>，$ q(x)<span class="math inline">\(为用于近似\)</span>
p(x)<span class="math inline">\(的概率分布，那么，随机变量\)</span><span
class="math inline">\(和模型\)</span> p(x)$之间的交叉熵（cross
entropy）定义为：<br />
<span class="math display">\[
\begin{aligned}
H( p,q)&amp;=H( p)+D( p||q)\\
&amp;=-\sum\limits_{ x\in\textbf{X}} p(x) log  q(x)\\
\end{aligned}
\]</span></p>
<h1 id="参考文献">参考文献</h1>
<p><span class="exturl" data-url="aHR0cHM6Ly9kbC5hY20ub3JnL2RvaS9wZGYvMTAuMTE0NS8yOTM5NjcyLjI5Mzk3ODU=">XGBoost:
A Scalable Tree Boosting System<i class="fa fa-external-link-alt"></i></span><br />
<span class="exturl" data-url="aHR0cHM6Ly93d3cuY25ibG9ncy5jb20vcGluYXJkL3AvMTA5Nzk4MDguaHRtbA==">XGBoost算法原理小结<i class="fa fa-external-link-alt"></i></span><br />
<span class="exturl" data-url="aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FkYnN6c2ovYXJ0aWNsZS9kZXRhaWxzLzc5NjE1NzEy">XGBoost
论文翻译+个人注释<i class="fa fa-external-link-alt"></i></span><br />
<span class="exturl" data-url="aHR0cHM6Ly96aHVhbmxhbi56aGlodS5jb20vcC84OTU4OTIyMg==">XGBoost论文详解<i class="fa fa-external-link-alt"></i></span><br />
[统计自然语言处理（第二版），宗成庆]<br />
# 扩展<br />
<span class="exturl" data-url="aHR0cHM6Ly9tcC53ZWl4aW4ucXEuY29tL3MvRHk0cnRiX0JqMmI3Q0FCVEVWNFRuUQ==">Xgboost ·
十三问十三答<i class="fa fa-external-link-alt"></i></span></p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://soundmemories.github.io/2021/06/20/NLP/01.%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%92%8C%E8%AF%8D%E5%90%91%E9%87%8F/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avstar.png">
      <meta itemprop="name" content="SoundMemories">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="SoundMemories">
      <meta itemprop="description" content="今日事，今日毕">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | SoundMemories">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2021/06/20/NLP/01.%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%92%8C%E8%AF%8D%E5%90%91%E9%87%8F/" class="post-title-link" itemprop="url">语言模型和词向量</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2021-06-20 00:00:00" itemprop="dateCreated datePublished" datetime="2021-06-20T00:00:00+08:00">2021-06-20</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/NLP/" itemprop="url" rel="index"><span itemprop="name">NLP</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>18k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>1:04</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="语言模型">语言模型</h1>
<p><strong>模型</strong>指的是对事物的数学抽象，那么<strong>语言模型</strong>指的就是对语言现象的数学抽象。准确的讲，给定一个句子
<span class="math inline">\(w\)</span> ，语言模型就是计算句子的出现概率
<span class="math inline">\(p(w)\)</span>
的模型，而统计的对象就是人工标注而成的语料库。</p>
<p>假设构建如下的小型语料库：<br />
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">商品 和 服务</span><br><span class="line">商品 和服 物美价廉</span><br><span class="line">服务 和 货币</span><br></pre></td></tr></table></figure><br />
每个句子出现的概率都是 <span
class="math inline">\(\dfrac{1}{3}\)</span>，因为样本空间为 3 ，这 3
次基数平均分给了 3 个句子，所以它们的概率都为<span
class="math inline">\(\dfrac{1}{3}\)</span>，既然它们的概率之和为 1
，那么其他句子的概率自然为 0 了，这就是语言模型。然而 <span
class="math inline">\(p(w)\)</span>
的计算非常难：句子数量无穷无尽，无法枚举。即便是大型语料库，也只能“枚举”有限的数百万个句子。实际遇到的句子大部分都在语料库之外，意味着它们的概率都被当作
0，这种现象被称为<strong>数据稀疏</strong>。枚举不可行，我们需要一种可计算的、更合理的概率估计方法。</p>
<p>考虑到句子由单词构成，句子无限，单词有限。于是我们从单词构成句子的角度出发去建模句子，把句子表示为单词列表
<span class="math inline">\(\textbf{w}=w_1w_2...w_k\)</span>，每个 <span
class="math inline">\(w_t,
t\in[1,k]\)</span>都是一个单词，然后定义<strong>语言模型</strong>：<br />
<span class="math display">\[
\begin{aligned}
p(\textbf{w})&amp;=p(w_1w_2...w_k)\\&amp;=p(w_1|w_0)\times
p(w_2|w_0w_1)\times...\times p(w_{k+1}|w_0w_1...w_k)\\&amp;=
\prod\limits _{t=1}^{k+1}p(w_{t}|w_0w_1...w_{t-1})
\end{aligned}
\]</span><br />
其中，<span class="math inline">\(w_0\)</span>=BOS（begin of
sentence，或&lt;s&gt;），<span
class="math inline">\(w_{k+1}\)</span>=EOS（end of
sentence，或&lt;/s&gt;），用来标记句子首尾两个特殊“单词”。<br />
也就是说，语言模型模拟说话顺序：给定已经说出口的词语序列，预测下一个词语的后验概率。一个单词一个单词地乘上后验概率，我们就能估计任意一句话的概率。以极大似然估计来计算每个后验概率，即：<br />
<span class="math display">\[
\begin{aligned}
p(w_{t}|w_0w_1...w_{t-1})=p_{MLE}(w_{t}|w_0w_1...w_{t-1})=\dfrac{c(w_0...w_t)}{c(w_0...w_{t-1})}
\end{aligned}
\]</span><br />
其中，<span class="math inline">\(c(w_0...w_t)\)</span>表示<span
class="math inline">\(w_0...w_t\)</span>的计数。</p>
<p>以上面小型语料库为例，计算<span class="math inline">\(p(商品 和
服务)\)</span>出现的概率？<br />
（1）<span class="math inline">\(p(商品|BOS)
=\frac{2}{3}\)</span>（“商品”作为第一个词出现的次数为2，所有单词作为第一个词出现的次数为3）；<br />
（2）<span class="math inline">\(p(和|BOS 商品)
=\frac{1}{2}\)</span>（“BOS 商品 和”出现的次数为1，“BOS
商品”出现的次数为2）；<br />
（3）<span class="math inline">\(p(服务|BOS 商品 和)
=\frac{1}{1}\)</span>（“BOS 商品 和 服务”出现的次数为1，“BOS 商品
和”出现的次数为1）；<br />
（4）<span class="math inline">\(p(EOS|BOS 商品 和 服务)
=\frac{1}{1}\)</span>（“BOS 商品 和 服务 EOS”出现的次数为1，“BOS 商品 和
服务”出现的次数为1）；<br />
整个句子的概率是4者乘积：<span class="math inline">\(p(商品 和
服务)=\frac{2}{3}\times \frac{1}{2}\times \frac{1}{1}\times
\frac{1}{1}=\frac{1}{3}\)</span>。</p>
<p>但是随着句子长度增大，语言模型会遇到如下问题：<br />
（1）<strong>数据稀疏</strong>。指长度越大的句子越难出现，语料库中极有可能统计不到长句子的频次，导致<span
class="math inline">\(p(w_{t}|w_0w_1...w_{t-1})\)</span>为0。<br />
（2）<strong>计算代价大</strong>。t越大，需要存储的<span
class="math inline">\(p(w_{t}|w_0w_1...w_{t-1})\)</span>就越多。</p>
<h2 id="n元语法">n元语法</h2>
<p>为了解决上面两个问题，使用<strong>马尔可夫假设</strong>（Markov
Assumption）来简化语言模型：给定时间线上有一串事件顺序发生，假设每个事件的发生概率只取决于前一个事件，那么这串事件构成的因果链被称作<strong>马尔可夫链</strong>。</p>
<p>在语言模型中，第t个事件指的是<span
class="math inline">\(w_t\)</span>作为第t个单词出现。也就是说，马尔科夫链假设每个单词出现的概率只取决于前一个单词：<br />
<span class="math display">\[
\begin{aligned}
p(w_{t}|w_0w_1...w_{t-1})=p(w_t|w_{t-1})
\end{aligned}
\]</span><br />
基于此假设，需要计算的量一下子减少了不少，由于每次计算只涉及连续两个单词的二元接续，所以此语言模型称为<strong>二元语法</strong>（bi-gram），也叫<strong>一阶马尔科夫链</strong>：<br />
<span class="math display">\[
\begin{aligned}
p(\textbf{w})&amp;=p(w_1w_2...w_k)\\&amp;=p(w_1|w_0)\times
p(w_2|w_1)\times...\times p(w_{k+1}|w_k)\\&amp;=
\prod\limits _{t=1}^{k+1}p(w_{t}|w_{t-1})
\end{aligned}
\]</span></p>
<p>那么根据这个思路推广下，可以得到<strong>n元语法</strong>（n-gram）的定义：每个单词出现的概率，仅取决于该单词之前n个单词，即：<br />
<span class="math display">\[
\begin{aligned}
p(\textbf{w})=\prod\limits_{t=1}^{k+n-1}p(w_{t}|w_{t-(n-1)}...w_{t-1})
\end{aligned}
\]</span><br />
当<span
class="math inline">\(\text{n=1}\)</span>时（即出现在第t位的词<span
class="math inline">\(w_t\)</span>独立于历史时），称为<strong>一元语法</strong>（uni-gram）；当<span
class="math inline">\(\text{n=2}\)</span>时（即出现在第t位的词<span
class="math inline">\(w_t\)</span>仅与它前面的一个历史词<span
class="math inline">\(w_{t-1}\)</span>有关），称为<strong>二元语法</strong>（bi-gram），也叫<strong>一阶马尔科夫链</strong>；当<span
class="math inline">\(\text{n=3}\)</span>时（即出现在第t位的词<span
class="math inline">\(w_t\)</span>仅与它前面的两个历史词<span
class="math inline">\(w_{t-1}w_{t-2}\)</span>有关），称为<strong>三元语法</strong>（tri-gram），也叫<strong>二阶马尔科夫链</strong>。当<span
class="math inline">\(n\geqslant
4\)</span>时数据稀疏和计算代价又变的显著了，实际工程中几乎不使用。另外，深度学习带了一种递归神经网络语言模型（RNN
Language
Model），理论上可以记忆无限个单词，可以看作“无穷元语法”（∞-gram）。</p>
<p>以<strong>二元语法</strong>（bi-gram）为例，计算<span
class="math inline">\(p(商品 和 服务)\)</span>出现的概率？<br />
（1）<span class="math inline">\(p(商品|BOS)
=\frac{2}{3}\)</span>；<br />
（2）<span class="math inline">\(p(和|商品)
=\frac{1}{2}\)</span>；<br />
（3）<span class="math inline">\(p(服务|和)
=\frac{1}{2}\)</span>；<br />
（4）<span class="math inline">\(p(EOS|服务)
=\frac{1}{1}\)</span>；<br />
整个句子的概率是4者乘积：<span class="math inline">\(p(商品 和
服务)=\frac{2}{3}\times \frac{1}{2}\times \frac{1}{2}\times
\frac{1}{1}=\frac{1}{6}\)</span>。</p>
<p>这次的概率比上次的<span
class="math inline">\(\dfrac{1}{3}\)</span>要小一半，剩下的概率到哪里去了呢？来算算语料库之外的新句子<span
class="math inline">\(p(商品 和 货币)\)</span>就知道了：<br />
（1）<span class="math inline">\(p(商品|BOS)
=\frac{2}{3}\)</span>；<br />
（2）<span class="math inline">\(p(和|商品)
=\frac{1}{2}\)</span>；<br />
（3）<span class="math inline">\(p(货币|和)
=\frac{1}{2}\)</span>；<br />
（4）<span class="math inline">\(p(EOS|货币)
=\frac{1}{1}\)</span>；<br />
整个句子的概率是4者乘积：<span class="math inline">\(p(商品 和
货币)=\frac{2}{3}\times \frac{1}{2}\times \frac{1}{2}\times
\frac{1}{1}=\frac{1}{6}\)</span>。<br />
原来剩下的<span
class="math inline">\(\dfrac{1}{6}\)</span>分配给了语料库之外的句子，它们的概率终于不是0了，这样就缓解了一部分数据稀疏的问题。</p>
<h2 id="模型评估困惑度">模型评估：困惑度</h2>
<p>在理想情况下，对两个语言模型A，B进行评估，选定一个特定的任务比如拼写纠错系统，把两个模型A，B都应用在此任务中，最后比较准确率，从而判断A，B的表现。这种评估方法是以应用为中心的度量方法，通过在下游任务中的性能来进行评估。那有没有更简单的评估方法？不需要放在特定的任务中验证？——————<strong>困惑度</strong>（Perplexity）。</p>
<p><strong>困惑度</strong>（Perplexity）是一种信息论测度，用来测量一个概率模型预测样本的好坏，困惑度越低越好。给定一个包含k个词的文本预料<span
class="math inline">\(\textbf{w}=w_1w_2...w_k\)</span>，和一个基于历史行为的语言模型，其预测结果为<span
class="math inline">\(p(\textbf{w})\)</span>，则这个语言模型在这个语料的困惑度是：<br />
<span class="math display">\[
\begin{aligned}
pp(\textbf{w})=2^{-\dfrac{1}{k}\log p(\textbf{w})}
\end{aligned}
\]</span><br />
以二元语法（bi-gram）为例，使用平均交叉熵，此时困惑度可以表示为：<br />
<span class="math display">\[
\begin{aligned}
pp(\textbf{w})&amp;=2^{-\dfrac{1}{k}\log
p(\textbf{w})}\\&amp;=2^{-\dfrac{1}{k}\log \prod\limits
_{t=1}^{k+1}p(w_{t}|w_{t-1})}\\&amp;=
2^{-\dfrac{1}{k}\sum\limits_{t=1}^{k+1}\log p(w_{t}|w_{t-1})}
\end{aligned}
\]</span><br />
模型的困惑度越小越好。</p>
<p>计算"商品 和 服务"的困惑度？<br />
（1）<span class="math inline">\(p(商品|BOS)
=\frac{2}{3}\)</span>；<br />
（2）<span class="math inline">\(p(和|商品)
=\frac{1}{2}\)</span>；<br />
（3）<span class="math inline">\(p(服务|和)
=\frac{1}{2}\)</span>；<br />
（4）<span class="math inline">\(p(EOS|服务)
=\frac{1}{1}\)</span>；<br />
整个句子的困惑度：<span class="math inline">\(pp(商品 和
服务)=2^{-\dfrac{1}{3}(log\frac{2}{3}+log\frac{1}{2}+log\frac{1}{2}+log\frac{1}{1})}\)</span>。</p>
<h2 id="数据平滑">数据平滑</h2>
<p>n元语法虽然有效，但它有一大不足，以二元语法为例，如果<span
class="math inline">\(c(w_tw_{t-1})\text{=0}\)</span>，因为计算句子概率时的乘法计算，导致整个语料的0-概率分配。0概率会造成非常大的困惑度，这是一种很糟糕的情况。一种避免0-概率事件的方法是使用平滑技术，确保为每个可能的情况都分配一个概率（可能非常小）。</p>
<h3 id="加法平滑">加法平滑</h3>
<p>最简单的一类方法是<strong>加法平滑</strong>（Additive
Smoothing），以下公式都以二元语法（bi-gram）为例。</p>
<p>不加平滑时：<br />
<span class="math display">\[
\begin{aligned}
p_{ MLE}(w_tw_{t-1})=\dfrac{c(w_{t-1}w_t)}{c(w_t)}
\end{aligned}
\]</span></p>
<p><strong>加一平滑</strong>（Add-one Smoothing/Laplace
Smoothing）：<br />
<span class="math display">\[
\begin{aligned}
p_{
add-one}(w_t|w_{t-1})=\dfrac{c(w_{t-1}w_t)+\textcolor{red}{1}}{c(w_t)+\textcolor{red}{V}},
\quad \textcolor{red}{V}是词表大小，即语料库中所有单词去重的总数
\end{aligned}
\]</span><br />
<strong>加K平滑</strong>（Add-K Smoothing/Laplace Smoothing）：<br />
<span class="math display">\[
\begin{aligned}
p_{
add-k}(w_t|w_{t-1})=\dfrac{c(w_{t-1}w_t)+\textcolor{red}{k}}{c(w_t)+\textcolor{red}{kV}},
\quad \textcolor{red}{V}是词表大小，即语料库中所有单词去重的总数
\end{aligned}
\]</span></p>
<h3 id="插值法">插值法</h3>
<p>另外一类方法使用back-off策略，即如果没有观测到n元语法，那么就基于n-1元语法计算，利用低阶n元语法平滑高阶n元语法，这就产生了很多方案，最简单的一种是<strong>线性插值法</strong>（Linear
Interpolation）。</p>
<p>三元语法（tri-gram）的线性插值法：<br />
<span class="math display">\[
\begin{aligned}
p_{tiny Int}(w_t|w_{t-2}w_{t-1})=\lambda_1
p(w_t|w_{t-2}w_{t-1})+\lambda_2p(w_t|w_{t-1})+\lambda_3p(w_t),\quad
\lambda_1+\lambda_2+\lambda_3=1
\end{aligned}
\]</span><br />
二元语法（bi-gram）的线性插值法：<br />
<span class="math display">\[
\begin{aligned}
p_{ Int}(w_t|w_{t-1})=\lambda_1 p(w_t|w_{t-1})+\lambda_2 p(w_t),\quad
\lambda_1+\lambda_2=1
\end{aligned}
\]</span><br />
一元语法（uni-gram）的线性插值法：<br />
<span class="math display">\[
\begin{aligned}
p_{ Int}(w_t)=\lambda_1 p(w_t)+\lambda_2\frac{1}{V},\quad
\lambda_1+\lambda_2=1
\end{aligned}
\]</span><br />
其中，V是词表大小，即语料库中所有单词去重的总数。</p>
<h3 id="古德-图灵平滑">古德-图灵平滑</h3>
<p><strong>古德-图灵平滑</strong>（Good-Turing
Smoothing）：对于任何一个出现 <span class="math inline">\(r\)</span>
次n元语法，都假设它出现了<span
class="math inline">\(r^*\)</span>次：<br />
<span class="math display">\[
\begin{aligned}
r^*=\frac{(r+1)N_{r+1}}{N_r} ,\quad  N_r表示训练预料中出现
r次的n元语法的数目
\end{aligned}
\]</span><br />
要把整个统计数转化为概率，只需要进行归一化处理：对于统计数为<span
class="math inline">\(r\)</span>的n元语法，其概率为：<br />
<span class="math display">\[
\begin{aligned}
p_r=\frac{r^*}{N}=\frac{(r+1)N_{r+1}}{N_r*N},\quad  N=\sum\limits_{r=1}^{\infty}r^*
N_r
\end{aligned}
\]</span><br />
注意到：<br />
<span class="math display">\[
\begin{aligned}
N=\sum\limits_{r=1}^{\infty}r^*
N_r=\sum\limits_{r=1}^{\infty}(r+1)N_{r+1}=\sum\limits_{r=1}^{\infty}rN_r
\end{aligned}
\]</span><br />
也就是说，N等于整个分布中的最初的计数。这样，样本中所有事件的概率之和为：<br />
<span class="math display">\[
\begin{aligned}
\sum\limits_{r&gt;0}N_rp_r=1-\frac{N_1}{N}&lt;1
\end{aligned}
\]</span><br />
因此，有$ N_1/N<span
class="math inline">\(的概率剩余量可以分配给所有未见事件（\)</span>r=0$的事件）。</p>
<p>但古德-图灵平滑也有其缺陷，比如某一个 $ N_{r+1}$ 为0，此时就无法计算
<span class="math inline">\(p_{r}\)</span>
了，一般这种情况，我们使用机器学习算法去拟合 $
N_{r}$，这样就可以把缺失的部分补上。</p>
<h1 id="词向量">词向量</h1>
<p>主要考虑单词或句子，甚至是文章的表示，一般把它们进行向量化处理。</p>
<h2 id="相似度">相似度</h2>
<h3 id="距离">距离</h3>
<p>单词/句子用向量表示后，可以计算它们的<strong>距离</strong>来判断相似度（距离越大，相似度越低），<span
class="math inline">\(i\)</span>为向量下标：<br />
（1）<strong>欧氏距离</strong>：<span
class="math inline">\(d=||A-B||_2=\sqrt{\sum \limits_{i=1}^n(A_{i} -
B_{i})^2}\)</span>，两个点的直线距离。<br />
（2）<strong>曼哈顿距离</strong>：<span
class="math inline">\(d=||A-B||_1=\sum
\limits_{i=1}^{n}|A_{i}-B_{i}|\)</span>，各个维度的长度差进行累加。常用计算城市间到达距离计算。<br />
（3）<strong>闵科夫斯基距离</strong>：<span
class="math inline">\(d=||A-B||_P=\sqrt[p]{\sum
\limits_{i=1}^n|A_{i}-B_{i}|^p}\)</span>，可以根据p来决定距离，如果p=1就是曼哈顿距离；p=2就是欧氏距离；当p趋近无穷时，就会变为长度差最大那个距离。</p>
<h3 id="方向">方向</h3>
<p>但是向量不光有大小还有<strong>方向</strong>的，两个向量之间是有夹角的，从这个角度发现了<strong>余弦相似度</strong>（值越大，相似度越高），<span
class="math inline">\(i\)</span>为向量下标：<br />
<span class="math display">\[
\begin{aligned}
cos(A,B)=\frac{A \cdot B}{|A||B|}=\frac{\sum
\limits_{i=1}^{n}A_iB_i}{\sqrt{\sum \limits_{i=1}^{n}A_i^2}\sqrt{\sum
\limits_{i=1}^{n}B_i^2}}
\end{aligned}
\]</span><br />
余弦相似度的<strong>取值范围是[-1,
1]，相同的两个向量之间的相似度为1</strong>。</p>
<p>当一对文本相似度的长度差距很大、但内容相近时，如果使用词频/词向量作为特征：<br />
（1）如果使用欧氏距离的话，它们在特定空间中的欧氏距离通常很大，因而相似度低；<br />
（2）而如果使用余弦相似度的话，它们之间的夹角可能很小，因而相似度高。<br />
在高维情况下：<br />
（1）余弦相似度依然保持“<strong>相同时为1，正交时为0，相反时为-1</strong>”的性质；<br />
（2）而欧式距离的数值则受维度的影响，范围不固定，并且含义也比较模糊。</p>
<p>如果希望得到类似于距离的表示，使用<span
class="math inline">\(\textbf{1-cos(A,B)}\)</span>即为<strong>余弦距离</strong>，<strong>其取值范围是[0,
2]，相同的两个向量余弦距离为0</strong>。<br />
在一些场景，比如word2vec中，其向量的模长是经过归一化的，此时欧式距离与余弦距离有着单调的关系：<br />
<span class="math display">\[
\begin{aligned}
||A-B||_2=\sqrt{2(1-cos(A,B))}
\end{aligned}
\]</span><br />
其中，<span class="math inline">\(||A-B||_2\)</span>表示欧氏距离，<span
class="math inline">\(cos(A,B)\)</span>表示余弦相似度，<span
class="math inline">\(1-cos(A,B)\)</span>表示余弦距离。此时，如果选择距离小的（相似度最大）的近邻，那么使用余弦相似度和欧氏距离的结果是相同的。<br />
总的来说：<strong>欧氏距离体现数值上的绝对差异，余弦距离体现方向上的相对差异。</strong></p>
<h2 id="one-hot">One-Hot</h2>
<p><strong>词袋模型</strong>（Bag-of-words
model）：将所有词语装进一个袋子里，不考虑其词法和语序的问题，即每个词语都是独立的。最简单的一种就是<strong>独热表示</strong>（One-Hot
Representation）。<br />
假设，词典：[是，天空，蓝色，的]。<strong>每个单词的表示</strong>：<br />
“是”　——&gt;[1, 0, 0, 0]<br />
“天空”——&gt;[0, 1, 0, 0]<br />
“蓝色”——&gt;[0, 0, 1, 0]<br />
“的”　——&gt;[0, 0, 0, 1]<br />
向量的维度等于词典的的大小。<br />
<div class="note info"><p>利用One-Hot表示法无法表达<strong>单词</strong>之间的相似度！不管用欧氏距离（任意两个词的相似度计算结果都相同）还是余弦相似度（任意两个词的相似度计算结果都是0）。</p>
<p>One-Hot表示单词/句子的缺点：<br />
（1）<strong>稀疏性</strong>（Sparsity）：如果词典非常大，维度就会很大，而一个句子可能只有很少的词，导致出现很多0，造成稀疏问题。核心问题是维度太大。<br />
（2）<strong>弱语义</strong>（Semantically
Weak）：无法表达词与词之间（语义）的相似度，因为One-Hot表示的单词向量是正交的。核心问题是每个单词的向量只能有一个有效值（local
representation），且取值只能是{0,1}。</p>
</div></p>
<h3 id="boolean">boolean</h3>
<p><strong>每个句子的表示（boolean）</strong>，向量对应的下标与字典的下标相匹配，其值为该词语出现为1，没出现为0：<br />
“天空 是 蓝色”——&gt;[1, 1, 1, 0]<br />
“蓝色 是 蓝色”——&gt;[1, 0, 1, 0]</p>
<h3 id="count">count</h3>
<p><strong>每个句子的表示（count）</strong>，向量对应的下标与字典的下标相匹配，其值为该词语出现的次数：<br />
“天空 是 蓝色”——&gt;[1, 1, 1, 0]<br />
“蓝色 是 蓝色”——&gt;[1, 0, 2, 0]</p>
<h3 id="tf-idf">TF-IDF</h3>
<p>一句话中每个词的重要程度是不同的，但boolean（每个单词权重相同）和count（出现次数越多不一定越重要）都不合理。由此考虑到新的计算方式————<strong>TF-IDF</strong>。<br />
<strong>TF-IDF</strong>（term frequency–inverse document
frequency）是一种用于信息检索与文本挖掘的常用加权技术。用以评估一个词，对于一个文件集或一个语料库中的其中一份文件的重要程度。词的重要性随着它在文件中出现的次数成正比增加，但同时会随着它在语料库中出现的频率成反比下降。其公式为：<br />
<span class="math display">\[
\begin{aligned}
\text{TF-IDF(t,d)}=\text{TF(t,d)}\times \text{IDF(t)}
\end{aligned}
\]</span></p>
<p>在一份给定的文件里，<strong>词频</strong>（term
frequency，TF）指的是某一个给定的词语在该文件中出现的频率。这个数字是对<strong>词数</strong>（term
count）的归一化，以防止它偏向长的文件。对于某一特定文件 <span
class="math inline">\(d_j\)</span> 里的词语 <span
class="math inline">\(t_i\)</span>
来说，它的词频（在本文件的重要程度）可表示为：<br />
<span class="math display">\[
\begin{aligned}
\text{TF}(t_i,d_j)=\frac{n_{i,j}}{\sum_k n_{k,j}}
\end{aligned}
\]</span><br />
其中，<span class="math inline">\(n_{i,j}\)</span> 是该词在文件 <span
class="math inline">\(d_j\)</span> 中的出现次数，而分母则是在文件 <span
class="math inline">\(d_j\)</span> 中所有字词的出现次数之和。<br />
有时 <span class="math inline">\(\text{TF}(t_i,d_j)\)</span>
也可以直接采用词频 <span class="math inline">\(n_{i,j}\)</span>
计算，不进行归一化处理。</p>
<p><strong>逆向文件频率</strong>（inverse document
frequency，IDF）是一个词语普遍重要性的度量（在整体文件的重要程度，和文件频率反比关系）。某一特定词语的IDF———总文件数目除以包含该词语的文件数目，再取对数（防止它的值过大）：<br />
<span class="math display">\[
\begin{aligned}
\text{IDF}(t_i)=log\frac{|D|}{|1+\{j:t_i\in d_j\}|}
\end{aligned}
\]</span><br />
其中，<span class="math inline">\(|D|\)</span> 是语料库中文件总数，<span
class="math inline">\(\{j:t_i\in d_j\}\)</span> 是包含词语 <span
class="math inline">\(t_i\)</span>
的文件数目（如果词语不存在资料库中，按 1 处理）。</p>
<p>假设，词典：[是，天空，蓝色，的]，语料库：[“天空 是 蓝色”, “蓝色 是
蓝色”]。<br />
<strong>每个句子的表示（TF-IDF）</strong>，向量对应的下标与字典的下标相匹配，其值为该词语的TF-IDF（TF按词频计算）：<br />
“天空 是 蓝色”——&gt;<span class="math inline">\([1·\log\frac{2}{1},
1·\log\frac{2}{2}, 1·\log\frac{2}{2}, 0]\)</span><br />
“蓝色 是 蓝色”——&gt;<span class="math inline">\([1\log\frac{2}{1}, 0,
2·\log\frac{2}{2}, 0]\)</span></p>
<h2 id="word2vec">word2vec</h2>
<p>之前我们说了One-Hot表示方法有<strong>稀疏性</strong>、<strong>弱语义</strong>缺点，那么如何解决这些问题？————分布式表示。</p>
<p><strong>分布式表示</strong>（Distributed
Representation）的思路是：通过训练，将每个词用<strong>低维度</strong>的向量表示（解决稀疏性/高维度问题，维度不再依赖字典长度），并且每个单词的向量<strong>有多个有效值</strong>（global
representation），每个维度上的有效值不再是{0,1}，而是介于[0,1]的值（解决弱语义问题，可计算相似度）。这种把词映射到低维的向量表示，也叫做<strong>词嵌入</strong>（word
embedding）。对于句子表示，可以使用平均策略，即句子中所有词的向量求和，再取均值。</p>
<p><strong><span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL3RtaWtvbG92L3dvcmQydmVj">word2vec<i class="fa fa-external-link-alt"></i></span></strong>
就是分布式表示方法的一种，它将词的语义表示为训练语料库中上下文的向量。它根据输入和输出的不同分为<strong>CBOW</strong>（Continuous
Bag-Of-Words）和<strong>Skip-Gram</strong>两种模型。而且word2vec对这两种方法进行了优化，从而得到<strong>Hierarchical
Softmax</strong>模型和<strong>Negative Sampling</strong>模型。<br />
<img src="/images/语言模型和词向量/CBOW和Skip-gram.png" width="80%" height="80%"></p>
<p><strong>CBOW</strong>：基于上下文词（输入）预测中心词（输出）。<br />
<strong>Skip-Gram</strong>：基于中心词（输入）预测上下文词（输出）。</p>
<h3 id="skip-gram">Skip-Gram</h3>
<p><strong>Skip-Gram</strong>核心思想是：用中心词（输入）预测上下文词（输出）。输入的中心词使用One-Hot向量表示，引入一个大小为
<span class="math inline">\(c\)</span>
的窗口，那么上下文词就是由中心词左 <span
class="math inline">\(c\)</span> 个词和右 <span
class="math inline">\(c\)</span> 个词构成（一般用<span
class="math inline">\(2c\)</span>表示上下文）我们希望模型输出的就是这些上下文词，而通过<strong>神经网络输出+softmax</strong>计算得到的是所有词的概率，那么只需要优化上下文词的概率最大即可达到我们的目的。</p>
<p>把这个核心思想转化成数学表示，假设有如下一句话：<br />
<span class="math display">\[
[w_1...w_{t-1}w_tw_{t+1}...w_V]
\]</span><br />
其中 <span class="math inline">\(w_t\)</span> 代表第 <span
class="math inline">\(t\)</span> 个词，总共有$
V$个词。我们要计算的就是：<br />
<span class="math display">\[
\prod\limits_{t=1}^Vp(\text{context}(w_t)|w_t)
\]</span><br />
每一个 <span class="math inline">\(p(\text{context}(w_t)|w_t)\)</span>
是相互独立的。<br />
此时引入窗口参数 <span class="math inline">\(i\in
\text{[-c,c]}\)</span>，<span class="math inline">\(c\)</span>
为窗口大小。中心词 <span class="math inline">\(w_t\)</span> 的上下文词
<span class="math inline">\(\text{context}(w_t)\)</span> 就是其左 <span
class="math inline">\(c\)</span> 个词和右 <span
class="math inline">\(c\)</span> 个词构成，上式变为：<br />
<span class="math display">\[
\begin{aligned}
\prod\limits_{t=1}^V \prod\limits_{i=-c}^c p(w_{t+i}|w_t)
\end{aligned}
\]</span><br />
每一个 <span class="math inline">\(p(\text{context}(w_i)|w_t)\)</span>
是相互独立且同分布的。<br />
为了方便计算，转成<span
class="math inline">\(log\)</span>（其实就是对数损失函数），并且取均值：<br />
<span class="math display">\[
\begin{aligned}
\dfrac{1}{\text{V}} \sum\limits_{t=1}^V \sum\limits_{i=-c}^c \log
p(w_{t+i}|w_t)
\end{aligned}
\]</span><br />
我们的目标函数就是引入参数 <span
class="math inline">\(\theta\)</span>，使整个式子最大化：<br />
<span class="math display">\[
\begin{aligned}
\mathop{argmax}\limits_{\theta}\dfrac{1}{\text{V}} \sum\limits_{t=1}^V
\sum\limits_{i=-c}^c \log p(w_{t+i}|w_t;\theta)
=\mathop{argmin}\limits_{\theta}-\dfrac{1}{\text{V}} \sum\limits_{t=1}^V
\sum\limits_{i=-c}^c \log p(w_{t+i}|w_t;\theta)
\end{aligned}
\]</span><br />
上式其实就是我们的优化函数<span
class="math inline">\(J(\theta)\)</span>，这里为了方便优化计算，最大化转成了最小化，而引入的参数
<span class="math inline">\(\theta\)</span>
其实就是我们要找的<strong>词向量</strong>。</p>
<hr />
<p>那么如何计算呢？一般采用的方法是一个三层的神经网络结构，分为输入层，隐藏层和输出层（softmax层）。<br />
这个神经网络计算的是<strong>一个词</strong>（输入）预测<strong>所有词</strong>（输出）的情况：<br />
<img src="/images/语言模型和词向量/one-word.png" width="60%" height="60%"></p>
<p><span class="math inline">\(\text{V}\)</span>：词汇表的长度;<br />
<span
class="math inline">\(\text{N}\)</span>：隐层神经元个数（词向量维度，需要我们自己指定）;<br />
<span
class="math inline">\(\text{W}\)</span>：输入层到隐层的权重矩阵（词向量矩阵，每一行代表一个词的词向量），维度是<span
class="math inline">\(\small [V,N]\)</span>;<br />
<span
class="math inline">\(\text{W}^\prime\)</span>：隐层到输出层的权重矩阵（词向量矩阵，每一列代表一个词的词向量），维度是<span
class="math inline">\(\small [N,V]\)</span>;</p>
<p>我们需要做的是用输入的词去预测输出的词（方便书写这里用行向量表示一个词）：<br />
（1）输入层的一个单词 <span class="math inline">\(w_t\)</span>
使用One-Hot表示：<br />
<span class="math display">\[
w_t=[x_1...x_t...x_V]
\]</span><br />
其中，只有 <span class="math inline">\(x_t\)</span>
为1，其余为0，其中t是输入单词在词汇表中的索引下标，它的维度是<span
class="math inline">\(\small [1,V]\)</span>。<br />
（2）输入的词 <span class="math inline">\(w_t\)</span> 和词向量矩阵 $ W$
相乘，得到一个维度为<span class="math inline">\(\small
[1,N]\)</span>的隐层向量 <span
class="math inline">\(h\)</span>。此过程可看作从词向量矩阵 $
W$取对应的词向量。<br />
<span class="math display">\[
\begin{aligned}
h=w_t\cdot\text{W}
\end{aligned}
\]</span><br />
（3）隐层向量 <span class="math inline">\(h\)</span> 和 词向量矩阵 <span
class="math inline">\(W^\prime\)</span> 相乘，得到一个维度为<span
class="math inline">\(\small [1,V]\)</span>的输出向量 <span
class="math inline">\(y\)</span>。此过程可看作计算当前词向量和所有词向量的相似度。从这个过程可看出<strong>word2vec中隐藏层没有用激活函数</strong>。<br />
<span class="math display">\[
\begin{aligned}
y=h\cdot\text{W}^\prime
\end{aligned}
\]</span><br />
（4）输出向量 <span class="math inline">\(y\)</span>
再通过softmax计算，从而得到概率。此过程可看作把相似度转成概率，即向量
<span class="math inline">\(y\)</span>
的每个值是当前词向量和另一个词向量相似的概率。下面公式是预测一个词的概率：<br />
<span class="math display">\[
\normalsize p(w_{i,i\ne k}|w_t)=p(w_{i,i\ne
k}|y)=\dfrac{e^{y_i}}{\sum\limits_{j=1}^V e^{y_j}}
\]</span><br />
其中，<span class="math inline">\(w_i\)</span> 代表词表中第 <span
class="math inline">\(i\)</span> 个词且<span
class="math inline">\(i\mathbb{\ne} t\)</span>（非中心词）。<span
class="math inline">\(y_i\)</span>为在原始输出向量<span
class="math inline">\(y\)</span>中，与单词<span
class="math inline">\(w_i\)</span>所对应的维度取值。<span
class="math inline">\(y\)</span>向量通过softamx计算出来就是当前词<span
class="math inline">\(w_t\)</span>和所有词的相似概率。<br />
为什么是softmax？因为其值域是<span
class="math inline">\([0,1]\)</span>，且所有结果的和为<span
class="math inline">\(1\)</span>。符合我们想要得到概率的目的。</p>
<hr />
<p><img src="/images/语言模型和词向量/skip-gram.png" width="40%"></p>
<p>那么<strong>Skip-Gram</strong>是怎么计算的呢？回到核心思想：用<strong>中心词</strong>（输入）预测<strong>上下文词</strong>（输出）。<br />
它引入了窗口 <span class="math inline">\(c\)</span> ，上下文词就是<span
class="math inline">\(2c\)</span>（左 <span
class="math inline">\(c\)</span> 个词和右 <span
class="math inline">\(c\)</span> 个词构成），目标是给定一个词<span
class="math inline">\(w_t\)</span>预测上下文词的概率最大化。以下是一个词<span
class="math inline">\(w_t\)</span>的优化公式：<br />
<span class="math display">\[
\normalsize
p(w_{t+i,i\in[-c,c]}|w_t)=\dfrac{e^{y_i}}{\sum\limits_{j=1}^V e^{y_j}}
\]</span><br />
<span class="math display">\[
\normalsize \mathop{argmax}\limits_{W,W^{\prime}}
\prod\limits_{i=-c}^{c}
p(w_{t+i}|w_t;\scriptsize{W,W^{\prime}})\normalsize
=\mathop{min}\limits_{W,W^{\prime}} \sum\limits_{i=-c}^{c}
-logp(w_{t+i}|w_t;\scriptsize{W,W^{\prime}})
\]</span><br />
把所有中心词都训练一遍，就是以下公式：<br />
<span class="math display">\[
\begin{aligned}
\mathop{argmax}\limits_{W,W^{\prime}}\dfrac{1}{\text{V}}
\sum\limits_{t=1}^V \sum\limits_{i=-c}^c \log
p(w_{t+i}|w_t;\scriptsize{W,W^{\prime}})\normalsize
=\mathop{argmin}\limits_{W,W^{\prime}}-\dfrac{1}{\text{V}}
\sum\limits_{t=1}^V \sum\limits_{i=-c}^c \log
p(w_{t+i}|w_t;\scriptsize{W,W^{\prime}})
\end{aligned}
\]</span><br />
其中，损失函数选择对数损失函数（<span
class="math inline">\(log\)</span>），全局损失定义为所有训练样本上的平均损失。<br />
上式损失函数优化过程就可以通过反向传播方法优化（基于梯度的优化），这里不再赘述。所有的中心词都训练一遍后，得到的<span
class="math inline">\(\text{W}\)</span>和<span
class="math inline">\(\text{W}^\prime\)</span>（论文中是<span
class="math inline">\(v\)</span>和<span
class="math inline">\(u\)</span>，代表中心词向量和上下文词向量）就是我们需要的词向量，可选其中一个作为V个词的N维向量表示（word2vec中一般选择<span
class="math inline">\(\text{W}\)</span>作为词向量）。</p>
<h3 id="cbow">CBOW</h3>
<p><img src="/images/语言模型和词向量/cbow.png" width="40%"></p>
<p><strong>CBOW</strong>核心思想：用上下文词（输入）预测中心词（输出）。<br />
（1）输入是多个词（上下文词）的One-Hot表示。在计算隐层的向量前，对输入向量和取均值即可。<br />
<span class="math display">\[
\begin{aligned}
h=\frac{\sum\limits_{i=-c}^c w_{t+i}\cdot
W}{2c}=\frac{\sum\limits_{i=-c}^c h_{t+i}}{2c}
\end{aligned}
\]</span><br />
（2）输出是上下文词向量和某一个词向量相似的概率，使中心词概率最大即可。以下是一个词<span
class="math inline">\(w_t\)</span>的优化公式：<br />
<span class="math display">\[
\normalsize \mathop{argmax}\limits_{W,W^{\prime}}
p(w_t|w_{t+i,i\in[-c,c]};\scriptsize{W,W^{\prime}})\normalsize
=\mathop{argmin}\limits_{W,W^{\prime}}
-logp(w_t|w_{t+i,i\in[-c,c]};\scriptsize{W,W^{\prime}})
\]</span><br />
把所有中心词的上下文词都训练一遍，就是以下公式：<br />
<span class="math display">\[
\begin{aligned}
\mathop{argmax}\limits_{W,W^{\prime}}\dfrac{1}{\text{V}}
\sum\limits_{t=1}^V \sum\limits_{i=-c}^c \log
p(w_t|w_{t+i};\scriptsize{W,W^{\prime}})\normalsize
=\mathop{argmin}\limits_{W,W^{\prime}}-\dfrac{1}{\text{V}}
\sum\limits_{t=1}^V \sum\limits_{i=-c}^c \log
p(w_t|w_{t+i};\scriptsize{W,W^{\prime}})
\end{aligned}
\]</span></p>
<h3 id="hierarchical-softmax">Hierarchical Softmax</h3>
<p>word2vec也是用了CBOW与Skip-Gram来训练模型与得到词向量，但没有使用神经网络结构，而是使用<strong>霍夫曼树</strong>（Huffman）来替代隐藏层到输出层的过程。</p>
<hr />
<p>我们先来复习下<strong>霍夫曼树</strong>，其特点是<strong>带权路径最短</strong>。首先明确一些概念：<br />
（1）<strong>路径</strong>：指从树种一个结点到另一个结点的分支所构成的路线。<br />
（2）<strong>路径长度</strong>：指路径上的分支数目。<br />
（3）<strong>树的路径长度</strong>：指从根到每个结点的路径长度之和。<br />
（4）<strong>带权路径长度</strong>：结点具有权值，从该结点到根之间的路径长度乘以结点的权值，就是该结点的带权路径长度。<br />
（5）<strong>树的带权路径长度</strong>（WPL）：指树中所有叶子结点的带权路径长度之和。</p>
<p><strong>霍夫曼树的构造方法</strong>（霍夫曼树可以是n叉树，我们主要以二叉树为例）<br />
给定<span class="math inline">\(n\)</span>个权值，用这<span
class="math inline">\(n\)</span>个权值构造霍夫曼树的算法如下：<br />
（1）将这个<span
class="math inline">\(n\)</span>个权值分别看作只有根节点的n棵二叉树，这些二叉树构成的集合记为<span
class="math inline">\(\small F\)</span>。<br />
（2）从<span class="math inline">\(\small
F\)</span>中选出两棵根节点的权值最小的数（假设为<span
class="math inline">\(a\)</span>、<span
class="math inline">\(b\)</span>），作为左、右子树，构造一棵新的二叉树（假设为<span
class="math inline">\(c\)</span>），新的二叉树的根节点权值为左、右子树根节点权值之和。<br />
（3）从F中删除<span class="math inline">\(a\)</span>、<span
class="math inline">\(b\)</span>，加入新构造的树<span
class="math inline">\(c\)</span>。<br />
（4）重复（2）（3）两步，直到<span class="math inline">\(\small
F\)</span>中只剩下一棵树为止，这棵树就是霍夫曼树。</p>
<p>一个简单的例子：“this is an example of a huffman tree”
中得到的字母频率（权重）来建构霍夫曼树。<br />
<img src="/images/语言模型和词向量/huffman.png" width="60%"></p>
<p><strong>霍夫曼树特点</strong>：<br />
（1）权重（频率）越大的结点，距离根结点越近。<br />
（2）树的带权路径长度最短。</p>
<p><strong>霍夫曼编码</strong>：<br />
一般得到霍夫曼树后我们会对叶子节点进行霍夫曼编码，由于权重高的叶子节点越靠近根节点，而权重低的叶子节点会远离根节点，这样我们的高权重节点编码值较短，而低权重值编码值较长。这保证的树的带权路径最短，也符合我们的信息论，即我们希望越常用的词拥有更短的编码。如何编码呢？一般对于一个霍夫曼树的节点（根节点除外），可以约定<strong>左子树编码为0</strong>，<strong>右子树编码为1</strong>。如下图所示：<br />
<img src="/images/语言模型和词向量/huffman1.png" width="60%"></p>
<hr />
<p><strong>word2vec中，霍夫曼编码方式和正常的相反，即约定沿着左子树走编码为1（负类），沿着右子树走编码为0（正类），同时约定左子树的权重不小于右子树的权重。</strong></p>
<p><strong>Hierarchical
Softmax</strong>的<strong>隐藏层</strong>到<strong>输出概率</strong>的计算过程（CBOW）：<br />
首先按照<strong>词频</strong>建立一棵霍夫曼树，叶子结点就是词典中的每个单词，但顺序和词典中不一定相同。假设预测的词（叶子节点）为<span
class="math inline">\(w\)</span>，定义一些符号：<br />
（1）<span class="math inline">\(p^w\)</span>：从根节点到<span
class="math inline">\(w\)</span>对应叶子节点的路径。<br />
（2）<span class="math inline">\(n^w\)</span>：路径<span
class="math inline">\(p^w\)</span>中包含结点个数。<br />
（3）<span
class="math inline">\(p_1^w,p_2^w,...,p_{n^w}^w\)</span>：路径<span
class="math inline">\(p^w\)</span>中的<span
class="math inline">\(n^w\)</span>个结点，<span
class="math inline">\(p_1^w\)</span>表示根节点，<span
class="math inline">\(p_{n^w}^w\)</span>表示词<span
class="math inline">\(w\)</span>对应的叶子结点。<br />
（4）<span
class="math inline">\(d_2^w,d_3^w,...,d_{n^w}^w\)</span>：路径<span
class="math inline">\(p^w\)</span>中的<span
class="math inline">\(n^w\)</span>个结点的编码，总共有<span
class="math inline">\(n^w\text{-1}\)</span>个，根结点不对应编码，每个编码值为<span
class="math inline">\(\{0,1\}\)</span>。<br />
（5）<span
class="math inline">\(\theta_1^w,\theta_2^w,...,\theta_{n^w-1}^w\)</span>：路径<span
class="math inline">\(p^w\)</span>中的<span
class="math inline">\(n^w\text{-1}\)</span>个结点的向量（维度为<span
class="math inline">\(N,1\)</span>），不包括叶子结点。</p>
<p>对于词典中任意的词<span
class="math inline">\(w\)</span>，霍夫曼树中必存在一条从根结点到词<span
class="math inline">\(w\)</span>叶子节点的路径<span
class="math inline">\(p^w\)</span>（路径唯一）。路径<span
class="math inline">\(p^w\)</span>上存在<span
class="math inline">\(n^w\text{-1}\)</span>个分支(边)，每个分支看做一次二分类，每一次分类就产生一个概率，将这些概率乘起来就是预测词<span
class="math inline">\(w\)</span>的概率，即<span
class="math inline">\(p(w|context(w))\)</span>。<br />
由此可以给出<span class="math inline">\(w\)</span>的条件概率：<br />
<span class="math display">\[
p(w|context(w))=\prod\limits_{j=2}^{n^w}p(d_j^w|h_w;\theta_{j-1}^w)
\]</span><br />
从根节点到叶节点经过了<span
class="math inline">\(n^w\text{-1}\)</span>个节点，编码从下标2开始（根节点无编码），对应的参数向量下标从1开始（根节点为1）。其中<span
class="math inline">\(h_w\)</span>是隐藏层向量(context向量加权和)。<br />
其中<strong>每个</strong> <span class="math inline">\(\small
p(d_j^w|h_w;\theta_{j-1}^w)\)</span> 都是一个逻辑回归二分类：<br />
<span class="math display">\[
p(d_j^w|h_w;\theta_{j-1}^w)=
\begin{cases}
   \sigma(h_w\cdot\theta_{j-1}^w), &amp;d_j^w=0\quad\text{(正类)}\\
   1-\sigma(h_w\cdot\theta_{j-1}^w), &amp;d_j^w=1\quad\text{(负类)}
\end{cases}
\]</span><br />
其中<span
class="math inline">\(h_w\)</span>是隐藏层向量(context向量加权和)，<span
class="math inline">\(\sigma\)</span>是sigmoid函数。<br />
考虑到<span
class="math inline">\(d\)</span>只有0和1两种取值，我们可以用指数形式方便地将其写到一起：<br />
<span class="math display">\[
p(d_j^w|h_w;\theta_{j-1}^w)=[\sigma(h_w\cdot\theta_{j-1}^w)]^{1-d_j^w}\cdot[1-\sigma(h_w\cdot\theta_{j-1}^w)]^{d_j^w}
\]</span><br />
所以对于<span
class="math inline">\(p(w|context(w))\)</span>，目标函数取对数似然，引入窗口<span
class="math inline">\(C\)</span>代表<span
class="math inline">\(context(w)\)</span>：<br />
<span class="math display">\[
\begin{aligned}
p(w|context(w))&amp;=\sum\limits_{w\in
C}log\prod\limits_{j=2}^{n^w}\{[\sigma(h_w\cdot\theta_{j-1}^w)]^{1-d_j^w}\cdot[1-\sigma(h_w\cdot\theta_{j-1}^w)]^{d_j^w}\}\\
&amp;=\sum\limits_{w\in C}\sum\limits_{j=2}^{n^w}\{(1-d_j^w)\cdot
log[\sigma(h_w\cdot\theta_{j-1}^w)]+d_j^w\cdot
log[1-\sigma(h_w\cdot\theta_{j-1}^w)]\}
\end{aligned}
\]</span><br />
其中<span
class="math inline">\(C\)</span>是上下文单词。接下来只需要对<span
class="math inline">\(h_w\)</span>和<span
class="math inline">\(\theta_{j-1}^w\)</span>求梯度，然后用随机梯度上升法优化即可，这个过程和逻辑回归梯度优化类似。</p>
<p>以 <strong>CBOW：上下文词（输入）预测中心词（输出）</strong>
为例，从输入层到隐藏层计算方式不变，最后得到维度为<span
class="math inline">\([1,N]\)</span>的隐藏层向量<span
class="math inline">\(h\)</span>：<br />
<img src="/images/语言模型和词向量/hs.png"></p>
<p>使用<span class="math inline">\(w\)</span>表示图中<span
class="math inline">\(w_2\)</span>，其计算过程如下：<br />
第1次：<span
class="math inline">\(p(d_2^w|h_w;\theta_1^w)=1-\sigma(h_w\cdot\theta_1^w)\)</span><br />
第2次：<span
class="math inline">\(p(d_3^w|h_w;\theta_2^w)=1-\sigma(h_w\cdot\theta_2^w)\)</span><br />
第3次：<span
class="math inline">\(p(d_4^w|h_w;\theta_3^w)=\sigma(h_w\cdot\theta_3^w)\)</span></p>
<p><strong>基于Hierarchical
Softmax的CBOW</strong>模型算法流程，梯度迭代使用了随机梯度上升法：<br />
输入：基于CBOW的语料训练样本，词典大小<span
class="math inline">\(V\)</span>，中心词<span
class="math inline">\(w\)</span>，词向量的维度大小<span
class="math inline">\(N\)</span>，上下文<span
class="math inline">\(C\)</span>大小<span
class="math inline">\(2c\)</span>，步长<span
class="math inline">\(\eta\)</span>。<br />
输出：霍夫曼树的内部节点模型参数<span
class="math inline">\(\theta\)</span>，所有的词向量<span
class="math inline">\(W=w_1,w_2,...,w_V\)</span>。<br />
（1）基于语料训练样本建立霍夫曼树。<br />
（2）随机初始化所有的模型参数<span
class="math inline">\(\theta\)</span>和所有的词向量<span
class="math inline">\(W\)</span>。<br />
（3）进行梯度上升迭代过程，对于训练集中的每一个样本<span
class="math inline">\((context(w),w)\)</span>做如下处理：</p>
<ul>
<li>e=0，计算<span
class="math inline">\(h_w=\frac{1}{2c}\sum\limits_{i=-c}^cw_{i}\)</span></li>
<li>for <span class="math inline">\(\text{j=2}\)</span> to <span
class="math inline">\(n^w\)</span>，计算：
<ul>
<li><span
class="math inline">\(f=\sigma(h_w\theta_{j-1}^w)\)</span></li>
<li><span class="math inline">\(g=\eta(1-d_j^w-f)\)</span></li>
<li><span class="math inline">\(e=e+g\cdot\theta_{j-1}^w\)</span></li>
<li><span class="math inline">\(\theta_{j-1}^w=\theta_{j-1}^w+g\cdot
h_w\)</span></li>
</ul></li>
<li>对于<span
class="math inline">\((context(w),w)\)</span>中的每一个词向量<span
class="math inline">\(w_i\)</span>（共<span
class="math inline">\(2c\)</span>个）进行更新：
<ul>
<li><span class="math inline">\(w_i=w_i+e\)</span></li>
</ul></li>
<li>如果梯度收敛，则结束梯度迭代，否则回到（3）继续迭代。</li>
</ul>
<p><strong>基于Hierarchical
Softmax的Skip-Gram</strong>模型算法流程，梯度迭代使用了随机梯度上升法：<br />
输入：基于Skip-Gram的语料训练样本，词典大小<span
class="math inline">\(V\)</span>，中心词<span
class="math inline">\(w\)</span>，词向量的维度大小<span
class="math inline">\(N\)</span>，上下文<span
class="math inline">\(C\)</span>大小<span
class="math inline">\(2c\)</span>，步长<span
class="math inline">\(\eta\)</span>。<br />
输出：霍夫曼树的内部节点模型参数<span
class="math inline">\(\theta\)</span>，所有的词向量<span
class="math inline">\(W=w_1,w_2,...,w_V\)</span>。<br />
（1）基于语料训练样本建立霍夫曼树。<br />
（2）随机初始化所有的模型参数<span
class="math inline">\(\theta\)</span>和所有的词向量<span
class="math inline">\(W\)</span>。<br />
（3）进行梯度上升迭代过程，对于训练集中的每一个样本<span
class="math inline">\((context(w),w)\)</span>做如下处理：</p>
<ul>
<li>for <span class="math inline">\(s=context(w)\)</span>，计算：
<ul>
<li>e=0，<span class="math inline">\(h_w=w_i\)</span></li>
<li>for <span class="math inline">\(\text{j=2}\)</span> to <span
class="math inline">\(n^w\)</span>，计算：
<ul>
<li><span
class="math inline">\(f=\sigma(h_w\theta_{j-1}^s)\)</span></li>
<li><span class="math inline">\(g=\eta(1-d_j^s-f)\)</span></li>
<li><span class="math inline">\(e=e+g\cdot\theta_{j-1}^s\)</span></li>
<li><span class="math inline">\(\theta_{j-1}^s=\theta_{j-1}^s+g\cdot
h_w\)</span></li>
</ul></li>
</ul></li>
<li>对于<span
class="math inline">\((context(w),w)\)</span>中的每一个词向量<span
class="math inline">\(w_i\)</span>（共<span
class="math inline">\(2c\)</span>个）进行更新：
<ul>
<li><span class="math inline">\(w_i=w_i+e\)</span></li>
</ul></li>
<li>如果梯度收敛，则结束梯度迭代，否则回到（3）继续迭代。</li>
</ul>
<p><strong>优点</strong>：在前面的<strong>CBOW</strong>和<strong>Skip-gram</strong>模型中，softmax计算时分母时需要对所有词的值进行计算求和，word2vec的<strong>Hierarchical
Softmax</strong>采用了霍夫曼二叉树来替代从隐藏层到输出softmax层的过程，<strong>之前softmax计算量为<span
class="math inline">\(V\)</span>，现在为<span
class="math inline">\(log_2V\)</span></strong>。</p>
<p><strong>为什么word2vec中用<span
class="math inline">\(\text{W}\)</span>作为词向量？</strong><br />
在之前讲的三层网络中我们可以选择词向量是<span
class="math inline">\(\{\text{W},\text{W}^\prime\normalsize\}\)</span>，word2vec这中<span
class="math inline">\(\text{W}^\prime\)</span>替换成<span
class="math inline">\(\large\theta\)</span>了，所以word2vec一般采用<span
class="math inline">\(\text{W}\)</span>作为词向量而不用<span
class="math inline">\(\text{W}^\prime\)</span>作为词向量。除此原因外，输入矩阵<span
class="math inline">\(\text{W}\)</span>和输出矩阵<span
class="math inline">\(\text{W}^\prime\)</span>可以看作<strong>所有词作为中心词</strong>或<strong>所有词作为上下文词</strong>而产生的词向量，它们侧重点不同，在不同算法作用也不同，比如在Skip-gram中<span
class="math inline">\(\text{W}\)</span>可看作所有词作为中心词而产生的词向量，在CBOW中<span
class="math inline">\(\text{W}\)</span>可看作所有词作为上下文词产生的词向量。对于<strong>于Hierarchical
Softmax</strong>和后面的<strong>Negative Sampling</strong>都代替了<span
class="math inline">\(\text{W}^\prime\)</span>，从而都是选择<span
class="math inline">\(\text{W}\)</span>作为词向量，而<span
class="math inline">\(\text{W}\)</span>作为中心词而得到词向量是Skip-gram中实现，所以word2vec中选择Skip-gram效果会更好。</p>
<h3 id="negative-sampling">Negative Sampling</h3>
<p>在讲基于<strong>Negative
Sampling</strong>的word2vec模型前，我们先看看<strong>Hierarchical
Softmax</strong>的的缺点。HS使用了霍夫曼树，不难发现对于词频高的词计算很快，但对于词频低的词计算很慢。如果我们的训练样本里的中心词<span
class="math inline">\(w\)</span>是一个很生僻的词，那么就得在霍夫曼树中辛苦的向下走很久了。能不能不用搞这么复杂的一颗霍夫曼树，将模型变的更加简单呢？这就是<strong>Negative
Sampling</strong>（负采样）。</p>
<p><strong>Negative
Sampling</strong>就是这么一种求解word2vec模型的方法，它<strong>摒弃了霍夫曼树</strong>，采用了Negative
Sampling（负采样）的方法来求解，下面我们就来讲述NS中下预测一个词过程：<br />
（1）已知词<span class="math inline">\(w\)</span>的上下文<span
class="math inline">\(context(w)\)</span>，需要预测<span
class="math inline">\(w\)</span>，那么认为词<span
class="math inline">\(w\)</span>作为中心词就是一个正样本<span
class="math inline">\((context(w),w)\)</span>，其他词作为中心词就是负样本<span
class="math inline">\((context(w),w_i)\)</span>，其中<span
class="math inline">\(i\in[1,\text{neg}]\)</span>。通过<strong>负采样</strong>得到
<span class="math inline">\(\text{neg}\)</span> （自己指定）个负样本 +
一个正样本，用<span
class="math inline">\(u\)</span>表示它们的集合。<br />
（2）利用这一个正例<span class="math inline">\((context(w),w)\)</span>和
<span class="math inline">\(\text{neg}\)</span> 个负例<span
class="math inline">\((context(w),w_i)\)</span>进行逻辑回归二分类，我们希望正样本分类概率最大化，可以通过梯度优化完成。这个就是预测一个词的过程。</p>
<p>整个过程要明白两个核心问题：1）如何利用<span
class="math inline">\(u\)</span>来做逻辑回归二分类？
2）如何进行负采样？</p>
<hr />
<p><strong>我们通过基于Negative
Sampling的CBOW来解答第一个问题。</strong><br />
预测一个词时优化的目标函数<span
class="math inline">\(g(w)\)</span>表示为：<br />
<span class="math display">\[
g(w)=\prod\limits_{u\in\{w\}\cup\{w_1...w_{neg}\}}p(u|context(w))
\]</span><br />
其中：<br />
<span class="math display">\[
\begin{aligned}
p(u|context(w))&amp;=
\begin{cases}
   \sigma(h_w\cdot\theta^{u}), &amp;y_u=1\quad\text{(正类)}\\
   1-\sigma(h_w\cdot\theta^{u}), &amp;y_u=0\quad\text{(负类)}
\end{cases}
~\\
\\&amp;=[\sigma(h_w\cdot\theta^u)]^{y_u}\cdot[1-\sigma(h_w\cdot\theta^u)]^{1-y_u}
\end{aligned}
\]</span><br />
上式，代入<span class="math inline">\(g(w)\)</span>中：<br />
<span class="math display">\[
g(w)=\prod\limits_{u\in\{w\}\cup\{w_1...w_{neg}\}}[\sigma(h_w\cdot\theta^u)]^{y_u}\cdot[1-\sigma(h_w\cdot\theta^u)]^{1-y_u}
\]</span><br />
这就是预测一次的优化函数了。此时引入窗口<span
class="math inline">\(C=2c\)</span>，整体的优化函数就是：<br />
<span class="math display">\[
\begin{aligned}
L=log\prod\limits_{C}g(w)=\sum\limits_{C}log(g(w))&amp;=\sum\limits_{C}log\prod\limits_{u\in\{w\}\cup\{w_1...w_{neg}\}}\{[\sigma(h_w\cdot\theta^u)]^{y_u}\cdot[1-\sigma(h_w\cdot\theta^u)]^{1-y_u}\}\\
&amp;=\sum\limits_{C}\sum\limits_{u\in\{w\}\cup\{w_1...w_{neg}\}}\{y_u\cdot
log[\sigma(h_w\cdot\theta^u)]+(1-y_u)\cdot
log[1-\sigma(h_w\cdot\theta^u)]\}
\end{aligned}
\]</span><br />
之后使用随机梯度上升优化即可。</p>
<p><strong>基于Negative Sampling的CBOW</strong>：<br />
输入：基于CBOW的语料训练样本，词典大小<span
class="math inline">\(V\)</span>，中心词<span
class="math inline">\(w\)</span>，词向量的维度大小<span
class="math inline">\(N\)</span>，上下文<span
class="math inline">\(C\)</span>大小<span
class="math inline">\(2c\)</span>，步长<span
class="math inline">\(\eta\)</span>。<br />
输出：词汇表每个词对应的模型参数<span
class="math inline">\(\theta\)</span>，所有的词向量<span
class="math inline">\(W=x_1,x_2,...,x_V\)</span>（避免和下面负样本混淆）。<br />
（1）随机初始化所有的模型参数<span
class="math inline">\(\theta\)</span>和所有的词向量<span
class="math inline">\(W\)</span>。<br />
（2）对于每个训练样本<span
class="math inline">\((context(w),w)\)</span>，负采样出<span
class="math inline">\(neg\)</span>个负样本<span
class="math inline">\((context(w),w_i)\)</span>，其中<span
class="math inline">\(i\in[1,\text{neg}]\)</span>。<br />
（3）进行梯度上升迭代过程，对于训练集中的每一个样本<span
class="math inline">\((context(w),w,w_1...w_{neg})\)</span>做如下处理：</p>
<ul>
<li>e=0，计算<span
class="math inline">\(h_w=\frac{1}{2c}\sum\limits_{i=-c}^cx_{i}\)</span></li>
<li>for <span
class="math inline">\(u=\{w\}\cup\{w_1...w_{neg}\}\)</span>，计算：
<ul>
<li><span class="math inline">\(f=\sigma(h_w\theta^u)\)</span></li>
<li><span class="math inline">\(g=\eta(y^u-f)\)</span></li>
<li><span class="math inline">\(e=e+g\cdot\theta^u\)</span></li>
<li><span class="math inline">\(\theta^u=\theta^u+g\cdot
h_w\)</span></li>
</ul></li>
<li>对于<span
class="math inline">\(context(w)\)</span>中的每一个词向量<span
class="math inline">\(x_i\)</span>（共<span
class="math inline">\(2c\)</span>个）进行更新：
<ul>
<li><span class="math inline">\(x_i=x_i+e\)</span></li>
</ul></li>
<li>如果梯度收敛，则结束梯度迭代，否则回到（3）继续迭代。</li>
</ul>
<p><strong>基于Negative Sampling的Skip-gram</strong>：<br />
输入：基于CBOW的语料训练样本，词典大小<span
class="math inline">\(V\)</span>，中心词<span
class="math inline">\(w\)</span>，词向量的维度大小<span
class="math inline">\(N\)</span>，上下文<span
class="math inline">\(C\)</span>大小<span
class="math inline">\(2c\)</span>，步长<span
class="math inline">\(\eta\)</span>。<br />
输出：词汇表每个词对应的模型参数<span
class="math inline">\(\theta\)</span>，所有的词向量<span
class="math inline">\(W=x_1,x_2,...,x_V\)</span>（避免和下面负样本混淆）。<br />
（1）随机初始化所有的模型参数<span
class="math inline">\(\theta\)</span>和所有的词向量<span
class="math inline">\(W\)</span>。<br />
（2）对于每个训练样本<span
class="math inline">\((context(w),w)\)</span>，负采样出<span
class="math inline">\(neg\)</span>个负样本<span
class="math inline">\((context(w),w_i)\)</span>，其中<span
class="math inline">\(i\in[1,\text{neg}]\)</span>。<br />
（3）进行梯度上升迭代过程，对于训练集中的每一个样本<span
class="math inline">\((context(w),w,w_1...w_{neg})\)</span>做如下处理：</p>
<ul>
<li>for <span class="math inline">\(s=context(w)\)</span>，计算：
<ul>
<li>e=0，<span class="math inline">\(h_w=x_i\)</span></li>
<li>for <span
class="math inline">\(u=\{w\}\cup\{w_1...w_{neg}\}\)</span>，计算：
<ul>
<li><span class="math inline">\(f=\sigma(h_w^s\theta^u)\)</span></li>
<li><span class="math inline">\(g=\eta(y^u-f)\)</span></li>
<li><span class="math inline">\(e=e+g\cdot\theta^u\)</span></li>
<li><span class="math inline">\(\theta^u=\theta^u+g\cdot
h_w^s\)</span></li>
</ul></li>
</ul></li>
<li>对于<span
class="math inline">\(context(w)\)</span>中的每一个词向量<span
class="math inline">\(x_i\)</span>（共<span
class="math inline">\(2c\)</span>个）进行更新：
<ul>
<li><span class="math inline">\(x_i^s=x_i^s+e\)</span></li>
</ul></li>
<li>如果梯度收敛，则结束梯度迭代，否则回到（3）继续迭代。</li>
</ul>
<hr />
<p><strong>第二个问题：负采样如何做？</strong><br />
对于<strong>Negative
Sampling</strong>模型，负采样是一个很重要的环节，对于一个中心词<span
class="math inline">\(w\)</span>，如何生成<span
class="math inline">\(neg\)</span>个负样本呢？由于词典中的词在预料中出现的频次不同，我们希望那些<strong>高频词被选为负样本的概率大</strong>，<strong>低频词被选为负样本的概率低</strong>，这就是负采样的本质要求————<strong>带权采样问题</strong>。</p>
<p>通过一段通俗的描述来帮助理解带权采样的机理：<br />
设大小为<span class="math inline">\(V\)</span>的词典<span
class="math inline">\(D\)</span>中每一个词对应一个线段<span
class="math inline">\(l(w)\)</span>，长度为：<br />
<span class="math display">\[
len(w)=\frac{count(w)}{\sum\limits_{u\in D}count(u)}
\]</span><br />
其中，分子表示一个词在语料中出现的次数（分母中的求和项用来做归一化）。将这些线段连接起来（共<span
class="math inline">\(V\)</span>个线段），形成一个长度为 1
的单位线段。如果随机的往这个线段上打点，则其中长度越长的线段（对应高频词）被打中的概率越大。</p>
<p>word2vec中词典<span
class="math inline">\(D\)</span>的词设置权值时，不是直接使用<span
class="math inline">\(count(w)\)</span>，而是对其做了<span
class="math inline">\(\alpha\)</span>次幂，其中<span
class="math inline">\(\alpha=\large\frac{3}{4}\)</span>，即上式变为：<br />
<span class="math display">\[
len(w)=\frac{[count(w)]^{\large\frac{3}{4}}}{\normalsize\sum\limits_{u\in
D}[count(u)]^{\large\frac{3}{4}}}
\]</span></p>
<p>word2vec中具体做法是：把上面长度为 1
的单位线段分成<strong>等距离</strong>的M份（<span
class="math inline">\(\text{M&gt;&gt;V}\)</span>），把这M份映射到前面讲的<strong>非等距离</strong>的V份中去。然后每次生成一个<span
class="math inline">\([1,M]\)</span>间的随机整数，代表选择<span
class="math inline">\(M\)</span>份中的一份，然后按映射找到对应<span
class="math inline">\(V\)</span>份中的一份，此时它对应的单词就是我们选择的负样本了。这样重复取<span
class="math inline">\(neg\)</span>次，就得到所有负样本了。word2vec中<span
class="math inline">\(M=10^8\)</span>（对应源码中变量table_size）。</p>
<h3 id="a-good-word-embedding">a good word embedding</h3>
<p><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzE1MDcuMDU1MjMucGRmJTIwaHR0cDovL2llZWV4cGxvcmUuaWVlZS5vcmcvZG9jdW1lbnQvNzQ3ODQxNy8=">How
to Generate a Good Word
Embedding?<i class="fa fa-external-link-alt"></i></span>论文中对比了不同模型，总结了选择word embedding经验。<br />
不同模型之间主要区别有两点：<br />
1、目标词和上下文关系。上下文来预测目标词（这类模型更能够捕获单词之间的可替代关系）、目标词来预测上下文<br />
2、上下文表示方法。<br />
<img src="/images/语言模型和词向量/good_embedding1.png"></p>
<p>不同模型之间上下文表示：<br />
<img src="/images/语言模型和词向量/good_embedding2.png"></p>
<p>据研究估计，<strong>文本含义信息的20%来自于词序，剩下的来自于词的选择</strong>。所以忽略词序信息的模型，将会损失约20%的信息。</p>
<p>本文做了三类对比实验：<br />
1、<strong>研究词向量的语义特性</strong>。该类实验是为了对比词向量的语义特性，包括：WordSim353，TOEFL，analogy
task：semantic和syntactic。<br />
2、<strong>将词向量作为特征</strong>。该类实验是为了对比词向量作为处理其他任务的特征时，对该任务性能的提升。包括：文本分类和命名实体识别。前者将词向量加权平均得到文本向量来分类，权值是词频，数据集用的是IMDB；后者用CoNLL03数据集做NER任务。<br />
3、<strong>用词向量来初始化神经网络模型</strong>。该类实验是为了研究词向量作为神经网络的初始值，对NN模型的提升。包括：CNN文本分类和词性标注。前者用了我们之前提到过的Kim的CNN模型，将句子表示成矩阵作为CNN的输入得到句子的表示，进行情感分类，数据集是Stanford
Sentiment Treebank；后者用Wall Street Journal数据集进行了POS
tagging任务。</p>
<p>该文对比了6种模型，并得到如下结论：<br />
Q：<strong>哪个模型最好？如何选择c和w的关系以及c的表示方法？</strong><br />
A：对于一个小型数据集来说，类似skip-gram这样越简单的模型效果越好，对于一个大型数据集来说，稍微复杂一点的模型，比如cbow和order就可以取得非常好的效果。真实数据中，skip-gram、cbow和order这样的模型足够了。在语义任务中，通过c来预测w的模型要更优于C&amp;W这种将c和w都放在输入层的模型。</p>
<p>Q：<strong>数据集的规模和所属领域对词向量的效果有哪些影响？</strong><br />
A：数据集的<strong>领域远比规模重要</strong>，给定一个任务之后，选择任务相关的领域数据将会提升性能，相反，如果数据并不相关，将会导致更差的性能。当然，如果数据都属于同一领域，规模越大性能越好。</p>
<p>Q：<strong>在训练模型时迭代多少次可以有效地避免过拟合？</strong><br />
A：通常的做法是在测试数据集上观察误差，当误差开始上升时即可停止训练，但经过本文的研究，这种方法并不能得到最好的task结果：因为训练词向量的目标是尽可能精确地预测目标词，这个优化目标和实际任务并不一致。因此最好的做法是：直接用实际任务的验证集来挑选迭代次数，即用task
data作为early
stopping的数据。如果实际任务非常耗时，则可以随机挑选某个简单任务（如：情感分类）及其验证集来挑选迭代次数。</p>
<p>Q：<strong>词向量的维度与效果之间的关系？</strong><br />
A：做词向量语义分析任务时，一般维度越大，效果越好。做具体NLP任务时（用作输入特征、或者网络初始化），50维之后效果提升就比较少了。</p>
<p>本文作者做了大量的工作，针对当前词向量模型的方方面面问题进行了研究，并且给出了许多有意义的结果。对此想了解更多，可以看一下作者的<span class="exturl" data-url="aHR0cDovL2xpY3N0YXIubmV0L2FyY2hpdmVzLzYyMA==">《How to Generate a Good Word
Embedding?》导读<i class="fa fa-external-link-alt"></i></span>。</p>
<hr />
<p><strong>word2vec结果评估</strong>：<br />
1、通过kmeans聚类，查看聚类的簇分布。<br />
2、通过词向量计算单词之间的相似度，查看相似词。<br />
3、通过类比：a之于b等价于c之于d。<br />
4、使用tsne降维可视化查看词的分布。</p>
<p><strong>word2vec输入向量和输出向量</strong>（<span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzE2MDguMDU4NTkucGRm">Using the Output Embedding
to Improve Language Models<i class="fa fa-external-link-alt"></i></span>）：<br />
1、在skip-gram模型中，在常见的衡量词向量的指标上，输出向量略微弱于输入向量。<br />
2、<strong>在基于RNN的语言模型中，输出向量反而强于输入向量</strong>。<br />
3、强制输入向量的转置作为输出向量，这可以使得输入向量等于输出向量。这种方式得到的词向量能够提升语言模型的困惑度perplexity。</p>
<p><strong>word2vec计算句子相似度</strong>（<span class="exturl" data-url="aHR0cDovL25scC50b3duL2Jsb2cvc2VudGVuY2Utc2ltaWxhcml0eS8=">Comparing Sentence
Similarity Methods<i class="fa fa-external-link-alt"></i></span>）：<br />
1、无监督方法：<br />
（1）对句子中所有的词的词向量求平均，获得句子embedding。<br />
（2）对句子中所有的词的词向量加权平均，每个词的权重为tf-idf，获得句子embedding。<br />
（3）对句子中所有的词的词向量加权平均，每个词的权重为smooth inverse
frequency:SIF（<span
class="math inline">\(\frac{a}{a+p(w)}\)</span>，<span
class="math inline">\(a\)</span>为超参数通常取0.001，<span
class="math inline">\(p(w)\)</span>为数据集中单词<span
class="math inline">\(w\)</span>的词频）；然后考虑所有的句子，并执行主成分分析；最后对每个句子的词向量加权平均减去first
principal componet，获得句子embedding。<br />
（4）通过 Word Mover's Distance:WMD
，直接度量句子之间的相似度。WMD：使用两个句子中单词的词向量来衡量一个句子中的单词需要在语义空间中移动到另一个句子中的单词的最小距离。<br />
2、有监督方法：<br />
（5）通过分类任务来训练一个文本分类器，取最后一个hidden
layer的输出作为句子embedding。就是使用文本分类器的前几层作为encoder。<br />
（6）直接训练一对句子的相似性，其优点是可以直接得到句子embeding。<br />
<strong>最终结论是：简单加权的词向量平均已经可以作为一个较好的baseline。</strong></p>
<h3 id="常见问题">常见问题</h3>
<p>问：噪声词在实际中被建议设为中心词的单字概率的3/4次幂，为什么？<br />
答：在保证高频词容易被抽到的大方向下，通过权重3/4次幂的方式，适当提升低频词、罕见词被抽到的概率。如果不这么做，低频词，罕见词很难被抽到，以至于不被更新到对应的Embedding。</p>
<p>问：一些“the”和“a”之类的英文高频词会对结果产生什么影响？如何处理？（可参考<span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzEzMTAuNDU0Ni5wZGY=">word2vec论文<i class="fa fa-external-link-alt"></i></span>第2.3节）<br />
答：噪声词为高频词对词向量训练没有什么效果，因为高频词太普遍了，为了抵消罕见词和高频词之间的不平衡，使用简单的二次抽样：训练集中的每个单词
<span class="math inline">\(w_i\)</span>
将有一定概率被丢弃，丢弃概率为：<br />
<span class="math display">\[
P(w_i)=1-\sqrt{\frac{t}{f(w_i)}}
\]</span><br />
其中 <span class="math inline">\(f(w_i)\)</span> 是单词 <span
class="math inline">\(w_i\)</span> 的频率，<span
class="math inline">\(t\)</span> 是选择的阈值，通常在 <span
class="math inline">\(10^{-5}\)</span>
左右，选择这个二次抽样公式是因为它主动地对频率大于 <span
class="math inline">\(t\)</span>
的词进行二次抽样，同时保持了频率ranking，即随着单词在语料库中出现的词频越来越大，该单词保留的概率越来越低。。虽然这个二次抽样公式是启发式选择的，但我们发现它在实践中运作良好。它加快了训练速度，甚至显着提高了罕见词所学向量的准确性。</p>
<p>问：如何训练包括“new york”在内的词组向量？（可参考<span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzEzMTAuNDU0Ni5wZGY=">word2vec论文<i class="fa fa-external-link-alt"></i></span>第4节）<br />
答：首先要找到经常一同出现但在其他语境中并不常见的单词。 例如，"New York
Times"和"Toronto Maple
Leafs"在训练集中将被独一无二的token所取代，而"this
is"将保持不变。这样，我们可以形成许多合理的短语，而不会大大增加词汇量的大小。理论上，我们可以使用所有的n元文法训练Skip-gram模型，但是这太消耗内存。
许多识别文本中短语的技术之前已经被开发出来了，
然而，比较它们超过了我们的工作范围。
我们决定使用一种简单的数据驱动方法，基于unigram和bigram的计数来形成短语：<br />
<span class="math display">\[
score(w_i,w_j) = \frac{count(w_iw_j)-\sigma}{count(w_i)\times
count(w_j)}
\]</span><br />
<span class="math inline">\(\sigma\)</span>
被用作折扣系数，防止形成太多由非常罕见的单词组成的短语。
得分高于所选阈值的bigram将被用作短语。
通常，我们逐渐减少阈值对训练数据进行2-4次传递，从而允许形成更长的短语(由数个单词组成)。</p>
<h2 id="glove">Glove</h2>
<p>学习词向量的所有无监督方法最终都是基于语料库的单词共现统计，因此这些模型之间存在共性。词向量学习算法有两个主要的模型族：</p>
<ul>
<li>基于全局矩阵分解的方法，如：LSA：latent semantic analysis。
<ul>
<li>优点：能够有效的利用全局的统计信息。</li>
<li>缺点：在单词类比任务（如：国王 vs 王后 类比于男人 vs
女人）中表现相对较差。</li>
</ul></li>
<li>基于局部上下文窗口的方法，如：word2vec。
<ul>
<li>优点：在单词类比任务中表现较好。</li>
<li>缺点：因为word2vec
在独立的局部上下文窗口上训练，因此难以利用单词的全局统计信息。</li>
<li>GloVe:Global Vectors for Word Representation，结合了 LSA 算法和
Word2Vec 算法的优点，既考虑了全局统计信息，又利用了局部上下文。</li>
</ul></li>
</ul>
<hr />
<p>设单词-单词共现矩阵为 <span class="math inline">\(X\)</span>
，其中元素 <span class="math inline">\(x_{ij}\)</span> 为词 <span
class="math inline">\(j\)</span> 出现在词 <span
class="math inline">\(i\)</span>
环境(context)的次数。这里"环境"有多种可能的定义。举个例子，在一段文本序列中，如果词
<span class="math inline">\(j\)</span> 出现在词 <span
class="math inline">\(i\)</span>
左边或者右边不超过10个词的距离，我们就可以认为词 <span
class="math inline">\(j\)</span> 出现在词 <span
class="math inline">\(i\)</span> 的环境一次，令 <span
class="math inline">\(x_i=\sum_k x_{ik}\)</span> 为任意词出现在词 <span
class="math inline">\(i\)</span> 的环境的次数，那么：<br />
<span class="math display">\[
P_{ij}=P(j|i)=\frac{x_{ij}}{x_i}
\]</span><br />
为词 <span class="math inline">\(j\)</span> 出现在词 <span
class="math inline">\(i\)</span> 的环境的概率。这一概率也称词 <span
class="math inline">\(i\)</span> 和词 <span
class="math inline">\(j\)</span> 的共现概率。</p>
<p>Glove论文展示了以下一组词对的共现概率与比值：</p>
<p><img src="/images/word2vec/1.png" width="60%"></p>
<p>我们通过商标可以观察以下现象：</p>
<ul>
<li>对于与“ice”相关但与“gas”无关的单词 <span
class="math inline">\(k\)</span> ，例如 <span
class="math inline">\(k=solid\)</span>
，我们预计会有更大的共现概率比值，例如8.9。</li>
<li>对于与“steam”相关但与“ice”无关的单词 <span
class="math inline">\(k\)</span> ，例如 <span
class="math inline">\(k=gas\)</span>
，我们预计较小的共现概率比值，例如0.085。</li>
<li>对于同时与“ice”和“steam”相关的单词 <span
class="math inline">\(k\)</span> ，例如 <span
class="math inline">\(k=water\)</span>
，我们预计其共现概率的比值接近1，例如1.36.</li>
<li>对于与“ice”和“steam”都不相关的单词 <span
class="math inline">\(k\)</span> ，例如 <span
class="math inline">\(k=fashion\)</span>
，我们预计共现概率的比值接近1，例如0.96.</li>
</ul>
<p>由此可见，共现概率的比值能够直观地表达词与词之间的关系。因此，我们可以设计三个词向量的函数来拟合这个比值。<br />
<span class="math display">\[
f(v_i,v_j,\tilde{v}_k)=\frac{P_{ik}}{P_{jk}}
\]</span><br />
其中，<span class="math inline">\(i\)</span> 和 <span
class="math inline">\(j\)</span>是中心词，<span
class="math inline">\(k\)</span> 是上下文词。<br />
<span class="math inline">\(f\)</span> 可以有多种设计，由于 <span
class="math inline">\(f\)</span>
映射的是向量空间，而向量空间是一个线性空间。因此从右侧的除法可以联想到减法：<br />
<span class="math display">\[
f(v_i,v_j,\tilde{v}_k)=f(v_i-v_j,\tilde{v}_k)
\]</span><br />
又因为共现概率的比值是标量，所以我们要求 <span
class="math inline">\(f\)</span> 是标量函数，即 <span
class="math inline">\(v_i-v_j\)</span> 和 <span
class="math inline">\(\tilde{v}_k\)</span>
均为向量，结果要求是标量，因此可以联想到向量的内积：<br />
<span class="math display">\[
f((v_i-v_j)^T\tilde{v}_k)=f(v_i^T\tilde{v}_k-v_j^T\tilde{v}_k)
\]</span><br />
我们希望左边为差的形式转为右边为商的方式，定义：<br />
<span class="math display">\[
f(v_i^T\tilde{v}_k-v_j^T\tilde{v}_k)=\frac{f(v_i^T\tilde{v}_k)}{f(v_j^T\tilde{v}_k)}
\]</span><br />
其中 <span
class="math inline">\(f(v_i^T\tilde{v}_k)=P_{ik}\)</span>，<span
class="math inline">\(f(v_j^T\tilde{v}_k)=P_{jk}\)</span>。<br />
上式左边为差的形式，右边为商的形式。因此联想到 <span
class="math inline">\(exp\)</span> 函数，即 <span
class="math inline">\(f\)</span> 为 <span
class="math inline">\(exp\)</span> 函数：<br />
<span class="math display">\[
exp(v_i^T\tilde{v}_k)=P_{ik}=\frac{x_{ik}}{x_i}
\]</span><br />
等式两边应用 <span class="math inline">\(log\)</span> 函数：<br />
<span class="math display">\[
\begin{aligned}
v_i^T\tilde{v}_k=log(x_{ik})-log(x_i)
\end{aligned}
\]</span><br />
即：<br />
<span class="math display">\[
log(x_{ik})=v_i^T\tilde{v}_k+log(x_i)
\]</span><br />
由于向量的内积具有对称性，即 <span class="math inline">\(X\)</span>
为对称矩阵，需要满足整个条件，但上面式子不满足，因为 <span
class="math inline">\(log(x_i)\)</span> 和 <span
class="math inline">\(log(x_k)\)</span>
不一定相等的，为了解决这个问题，模型引入两个偏置项：<br />
<span class="math display">\[
log(x_{ik})=v_i^T\tilde{v}_k+b_i+b_k
\]</span><br />
将索引 <span class="math inline">\(i\)</span> 和 <span
class="math inline">\(k\)</span>
互换，我们可以验证对称性得两个性质可以同时被上式满足。</p>
<p>上面的公式仅仅是理想状态，实际上只能要求左右两边尽可能相等。于是设计代价函数为：<br />
<span class="math display">\[
J=\sum\limits_{i,j=1}^Vf(x_{ij})(v_i^T\tilde{v}_j+b_i+b_j-log(x_{ij}))^2
\]</span><br />
其中，<span class="math inline">\(f(x_{ij})\)</span>
代表不同词频的词的重要性（权重）。使用优化算法最小化它即可。</p>
<p>对于权重函数 <span class="math inline">\(f(x_{ij})\)</span>
，一个建议的选择是：当 <span class="math inline">\(x&lt;c\)</span>
（例如 <span class="math inline">\(c=100\)</span>），令 <span
class="math inline">\(f(x)=(x/c)^\alpha\)</span> （例如 <span
class="math inline">\(\alpha=0.75\)</span>），反之令 <span
class="math inline">\(f(x)=1\)</span>
。需要注意的是，损失函数的计算复杂度与共现词频矩阵 <span
class="math inline">\(X\)</span> 中非零元素的数目呈线性关系。我们可以从
<span class="math inline">\(X\)</span>
中随机采样小批量非零元素，使用随机梯度下降迭代词向量和偏移项。当所有词向量学习得到后，Glove使用一个词得中心词向量与上下文词向量之和作为该词最终词向量。</p>
<p>GloVe 模型性能与语料库大小的关系：<br />
-
在语法任务中，模型性能随着语料库大小的增长而单调增长。这是因为语料库越大，则语法的统计结果越可靠。<br />
-
在语义任务中，模型性能与语料库绝对大小无关，而与语料库的有效大小有关。有效大小指的是语料库中，与目标语义相关的内容的大小。</p>
<p>GloVe 模型超参数选择：</p>
<ul>
<li>词向量大小：词向量大小越大，则模型性能越好。但是词向量超过 200
维时，维度增加的收益是递减的。</li>
<li>窗口对称性：计算一个单词的上下文时，上下文窗口可以是对称的，也可以是非对称的。
<ul>
<li>对称窗口：既考虑单词左侧的上下文，又考虑单词右侧的上下文。</li>
<li>非对称窗口：只考虑单词左侧的上下文。因为语言的阅读习惯是从左到右，所以只考虑左侧的上下文，不考虑右侧的上下文。</li>
</ul></li>
<li>窗口大小：
<ul>
<li>语法任务：选择小的、非对称的窗口时，模型性能更好。因为语法是局部的，所以小窗口即可；因为语法是依赖于单词顺序的，所以需要非对称窗口。</li>
<li>语义任务：则需要选择更大的窗口。因为语义是非局部的。</li>
</ul></li>
</ul>
<h2 id="fasttext">fastText</h2>
<p>fastText 是 Facebook AI Research 在 2016
年开源的文本分类器，其提出是在论文 《Bag of Tricks for Efficient Text
Classification》 中。目前 fastText 作为文本分类的基准模型。<br />
fastText 的优点是：在保持分类效果的同时，大大缩短了训练时间。</p>
<p>fastText在使用负采样的skip-gram模型基础上，将每个中心词视为子词(subword)的集合，并学习子词的词向量。<br />
以where这个词为例，设子词为3个字符，它的子词包括“&lt;wh”、“whe”、“her”、“ere”、“re&gt;”
和特殊子词（整词）“&lt;where&gt;”。其中的“&lt;”和“&gt;”是为了将作为前后缀的子词区分出来。而且，这里的子词“her”与整词“&lt;her&gt;”也可被分。给定一个词
<span
class="math inline">\(w\)</span>，我们通常可以把字符长度在3-6之间的所有子词和特殊子词的并集
<span class="math inline">\(\mathcal{G}_w\)</span>
取出。假设词典中任意子词 <span class="math inline">\(g\)</span>
的子词向量为 <span
class="math inline">\(z_g\)</span>，我们可以把使用负采样的skip-gram模型的损失函数（中心词
<span class="math inline">\(w_c\)</span>，上下文词 <span
class="math inline">\(w_o\)</span>，噪声词 <span
class="math inline">\(w_k\)</span> 在词典中的索引为 <span
class="math inline">\(i_k\)</span>）：<br />
<span class="math display">\[
-logP(w_o|w_c)=-log\frac{1}{1+exp(-u_o^Tv_c)}-\sum\limits_{k=1,w_k\sim
P(w)}^K log\frac{1}{1+exp(u_{i_k}^Tv_c)}
\]</span><br />
直接替换成：<br />
<span class="math display">\[
-logP(w_o|w_c)=-log\frac{1}{1+exp(-u_o^T\sum_{g\in \mathcal{G}_w }z_g
)}-\sum\limits_{k=1,w_k\sim P(w)}^K
log\frac{1}{1+exp(u_{i_k}^T\sum_{g\in \mathcal{G}_w }z_g)}
\]</span><br />
我们可以看到，原中心词向量被替换成了中心词的子词向量的和。与整词学习（word2vec和Glove）不同，词典以外的新词的词向量可以使用fastText中相应的子词向量之和。</p>
<p>fastText对于一些语言较重要(一定程度避免OOV)，例如阿拉伯语、德语和俄语。例如，德语中有很多复合词，例如兵乓球（table
tennis）在德语中叫“Tischtennis”。fastText可以通过子词可以表达两个词的相关性，例如“Tischtennis”和“Tennis”。</p>
<p><strong>总结：Glove用词向量表达共现词频的对数。fastText用子词向量之和表达整词。</strong></p>
<p>Fasttext用作文本分类时，预测的是标签值。可看作 字符or单词 +
N-gram方式输入，比如2-gram时<code>[w1,w2,w3]+[w12,w23]</code>。由于N-gram过多，使用hash分桶的方式确定N-gram的向量表示，缺点是桶少的话不同的N-gram会分到同一个桶(同一个向量表示)。Fasttext在做负采样时借点了Word2Vec方式，高词频被选为负样本概率大，同时上一个根号(降低高词频采样概率，提高低频词采样概率)。</p>
<p>Fasttext可以做文本分类：<span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzE2MDcuMDE3NTl2Mi5wZGY=">Bag of Tricks for
Efficient Text Classification<i class="fa fa-external-link-alt"></i></span>。<br />
Fasttext可以训练词向量：<span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzE2MDcuMDQ2MDYucGRm">Enriching word vectors with
subword information<i class="fa fa-external-link-alt"></i></span>。<br />
Fasttext官方：https://fasttext.cc/docs/en/supervised-tutorial.html</p>
<p><strong>补充问答</strong>：<br />
问：如果一个词出现在另一个词的背景窗口中，如何利用它们之间在文本序列的距离重新设计条件概率
<span class="math inline">\(P_{ij}\)</span> 的计算方式？（提示：可参考<span class="exturl" data-url="aHR0cHM6Ly9ubHAuc3RhbmZvcmQuZWR1L3B1YnMvZ2xvdmUucGRm">GloVe论文<i class="fa fa-external-link-alt"></i></span>4.2节）<br />
问：如果丢弃Glove中的偏移项，是否也可以满足任意一对词共现的对称性？<br />
问：在fastText中，子词过多怎么办？（例如，6字英文组合数为<span
class="math inline">\(26^6\)</span>）？（提示：可参考<span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzE2MDcuMDE3NTl2Mi5wZGY=">fastText论文<i class="fa fa-external-link-alt"></i></span>3.2节）</p>
<h1 id="补充">补充</h1>
<h2 id="softmax">softmax</h2>
<p><span class="math display">\[
\begin{aligned}
S_i=\dfrac{e^{V_i}}{\sum_j e^{V_j}}
\end{aligned}
\]</span><br />
其中，一个向量$ V<span class="math inline">\(共有\)</span>j<span
class="math inline">\(个值，\)</span>V_i<span
class="math inline">\(表示第\)</span>i<span class="math inline">\(个值。
首先对所有值进行\)</span>e^x$计算，保证所有值都是大于0的。其次进行归一化，保证所有值的和为1。这些特点非常符合概率的要求，所以经常把softmax处理后的值当成概率。</p>
<h2 id="sigmoid">sigmoid</h2>
<p><span class="math display">\[
\sigma(x)=\dfrac{1}{1+e^{-x}}
\]</span><br />
定义域为<span
class="math inline">\((-\infty,+\infty)\)</span>，值域为<span
class="math inline">\((0,1)\)</span>，下图给出了sigmoid的图像：<br />
<img src="/images/语言模型和词向量/sigmoid.png" width="40%"></p>
<p>sigmoid函数<strong>导函数</strong>具有性质：<br />
<span class="math display">\[
\sigma^\prime(x)=\sigma(x)[1-\sigma(x)]
\]</span><br />
由此可知：<br />
<span class="math display">\[
[log\sigma(x)]^\prime=1-\sigma(x)
\]</span><br />
<span class="math display">\[
[log(1-\sigma(x))]^\prime=-\sigma(x)
\]</span></p>
<p>sigmoid的每一次计算是相互独立的，是对当前事件的一次独立判断。我们把它用作二分类，是因为每一次判断的结果都可以根据阈值划分为两类，比如阈值为t，那么计算结果大于t的为一类，低于t的为另一类。也可以把计算结果看作二分类中一类的概率，比如计算结果为p，那么事件是一类的概率就是p，另一个类概率就是1-p。</p>
<h2 id="gensim">gensim</h2>
<p>Word2Vec：<br />
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 4.0</span></span><br><span class="line">gensim.models.word2vec.Word2Vec(sentences=<span class="literal">None</span>, corpus_file=<span class="literal">None</span>, vector_size=<span class="number">100</span>, </span><br><span class="line">alpha=<span class="number">0.025</span>, window=<span class="number">5</span>, min_count=<span class="number">5</span>, max_vocab_size=<span class="literal">None</span>, sample=<span class="number">0.001</span>, seed=<span class="number">1</span>, workers=<span class="number">3</span>, </span><br><span class="line">min_alpha=<span class="number">0.0001</span>, sg=<span class="number">0</span>, hs=<span class="number">0</span>, negative=<span class="number">5</span>, ns_exponent=<span class="number">0.75</span>, cbow_mean=<span class="number">1</span>, hashfxn=&lt;built-<span class="keyword">in</span> function <span class="built_in">hash</span>&gt;, </span><br><span class="line">epochs=<span class="number">5</span>, null_word=<span class="number">0</span>, trim_rule=<span class="literal">None</span>, sorted_vocab=<span class="number">1</span>, batch_words=<span class="number">10000</span>, </span><br><span class="line">compute_loss=<span class="literal">False</span>, callbacks=(), comment=<span class="literal">None</span>, max_final_vocab=<span class="literal">None</span>) </span><br><span class="line"><span class="comment"># 3.8</span></span><br><span class="line">gensim.models.word2vec.Word2Vec(sentences=<span class="literal">None</span>, corpus_file=<span class="literal">None</span>, size=<span class="number">100</span>, </span><br><span class="line">alpha=<span class="number">0.025</span>, window=<span class="number">5</span>, min_count=<span class="number">5</span>, max_vocab_size=<span class="literal">None</span>, sample=<span class="number">0.001</span>, seed=<span class="number">1</span>, workers=<span class="number">3</span>, </span><br><span class="line">min_alpha=<span class="number">0.0001</span>, sg=<span class="number">0</span>, hs=<span class="number">0</span>, negative=<span class="number">5</span>, ns_exponent=<span class="number">0.75</span>, cbow_mean=<span class="number">1</span>, hashfxn=&lt;built-<span class="keyword">in</span> function <span class="built_in">hash</span>&gt;, </span><br><span class="line"><span class="built_in">iter</span>=<span class="number">5</span>, null_word=<span class="number">0</span>, trim_rule=<span class="literal">None</span>, sorted_vocab=<span class="number">1</span>, batch_words=<span class="number">10000</span>, </span><br><span class="line">compute_loss=<span class="literal">False</span>, callbacks=(), max_final_vocab=<span class="literal">None</span>)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">sentences：(iterable of iterables, optional) 要分析的语料，可以是一个列表，或者从文件中遍历读出。</span></span><br><span class="line"><span class="string">          大语料库，建议直接从磁盘/网络流迭代传输句子。参阅word2vec模块中的BrownCorpus，Text8Corpus或LineSentence。</span></span><br><span class="line"><span class="string">corpus_file：(str, optional。LineSentence) 格式的语料库文件路径。</span></span><br><span class="line"><span class="string">size/vector_size：(int, optional) 词向量的维度，默认值是100。</span></span><br><span class="line"><span class="string">      这个维度的取值一般与我们的语料的大小相关，如果是不大的语料，比如小于100M的文本语料，则使用默认值一般就可以了。</span></span><br><span class="line"><span class="string">      如果是超大的语料，建议增大维度。</span></span><br><span class="line"><span class="string">window：(int, optional) 即词向量上下文最大距离。</span></span><br><span class="line"><span class="string">        这个参数在讲解中标记为c，值越大，则和某一词较远的词也会产生上下文关系。默认值为5。</span></span><br><span class="line"><span class="string">        如果是小语料则这个值可以设的更小。对于一般的语料这个值推荐在[5,10]之间。</span></span><br><span class="line"><span class="string">min_count：(int, optional) 忽略词频小于此值的单词。这个值可以去掉一些很生僻的低频词，默认是5。</span></span><br><span class="line"><span class="string">           如果是小语料，可以调低这个值。</span></span><br><span class="line"><span class="string">workers：(int, optional) 训练模型时使用的线程数。</span></span><br><span class="line"><span class="string">sg：(&#123;0, 1&#125;, optional) word2vec两个模型选择。0：CBOW模型。1：Skip-Gram模。默认是0即CBOW模型。</span></span><br><span class="line"><span class="string">hs：(&#123;0, 1&#125;, optional) word2vec两个解法选择。0：Negative Sampling。1：Hierarchical Softmax。默认是0即Negative Sampling。</span></span><br><span class="line"><span class="string">negative：(int, optional) 即使用Negative Sampling时负采样的个数，默认是5。推荐在[3,10]之间。这个参数在讲解中标记为neg。</span></span><br><span class="line"><span class="string">ns_exponent：(float, optional) 负采样分布指数。1.0样本值与频率成正比，0.0样本所有单词均等，负值更多地采样低频词。</span></span><br><span class="line"><span class="string">cbow_mean：(&#123;0, 1&#125;, optional) 仅用于CBOW在做投影的时候。</span></span><br><span class="line"><span class="string">           为0，则算法中的h为上下文的词向量之和，为1则为上下文的词向量的平均值。</span></span><br><span class="line"><span class="string">           在讲解中是按照词向量的平均值来描述的。</span></span><br><span class="line"><span class="string">alpha：(float, optional) 在随机梯度下降法中迭代的初始步长。讲解中标记为η，默认是0.025。</span></span><br><span class="line"><span class="string">min_alpha：(float, optional) 随着训练的进行，学习率线性下降到min_alpha。</span></span><br><span class="line"><span class="string">           由于算法支持在迭代的过程中逐渐减小步长，min_alpha给出了最小的迭代步长值。</span></span><br><span class="line"><span class="string">           随机梯度下降中每轮的迭代步长可以由iter，alpha， min_alpha一起得出。</span></span><br><span class="line"><span class="string">           对于大语料，需要对alpha, min_alpha,iter一起调参，来选择合适的三个值。</span></span><br><span class="line"><span class="string">seed：(int, optional) 随机数发生器种子。</span></span><br><span class="line"><span class="string">max_vocab_size：(int, optional) 词汇构建期间RAM的限制。</span></span><br><span class="line"><span class="string">                如果有更多的独特单词，则修剪不常见的单词。每1000万个类型的字需要大约1GB的RAM。</span></span><br><span class="line"><span class="string">max_final_vocab：(int, optional) 自动选择匹配的min_count将词汇限制为目标词汇大小。</span></span><br><span class="line"><span class="string">sample：(float, optional) 高频词随机下采样的配置阈值，范围是(0,1e-5)。</span></span><br><span class="line"><span class="string">hashfxn：(function, optional) 哈希函数用于随机初始化权重，以提高训练的可重复性。</span></span><br><span class="line"><span class="string">iter/epochs：随机梯度下降法中迭代的最大次数，默认是5。对于大语料，可以增大这个值。</span></span><br><span class="line"><span class="string">trim_rule：(function, optional) 词汇修剪规则，指定某些词语是否应保留在词汇表中，修剪掉或使用默认值处理。</span></span><br><span class="line"><span class="string">sorted_vocab：(&#123;0, 1&#125;, optional) 如果为1，则在分配单词索引前按降序对词汇表进行排序。</span></span><br><span class="line"><span class="string">batch_words：(int, optional) 每一个batch传递给线程单词的数量。</span></span><br><span class="line"><span class="string">compute_loss：(bool, optional) 如果为True，则计算并存储可使用get_latest_training_loss()检索的损失值。</span></span><br><span class="line"><span class="string">callbacks：(iterable of CallbackAny2Vec, optional) 在训练中特定阶段执行回调序列。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="comment"># 建立模型后</span></span><br><span class="line">build_vocab(sentences)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">遍历一次语料库建立词典</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="comment"># 3.8</span></span><br><span class="line">train(sentences=<span class="literal">None</span>, corpus_file=<span class="literal">None</span>, total_examples=<span class="literal">None</span>, total_words=<span class="literal">None</span>, </span><br><span class="line">epochs=<span class="literal">None</span>, start_alpha=<span class="literal">None</span>, end_alpha=<span class="literal">None</span>, word_count=<span class="number">0</span>, queue_factor=<span class="number">2</span>, </span><br><span class="line">report_delay=<span class="number">1.0</span>, compute_loss=<span class="literal">False</span>, callbacks=())</span><br><span class="line"><span class="comment"># 4.0</span></span><br><span class="line">train(corpus_iterable=<span class="literal">None</span>, corpus_file=<span class="literal">None</span>, total_examples=<span class="literal">None</span>, total_words=<span class="literal">None</span>, </span><br><span class="line">epochs=<span class="literal">None</span>, start_alpha=<span class="literal">None</span>, end_alpha=<span class="literal">None</span>, word_count=<span class="number">0</span>, queue_factor=<span class="number">2</span>, </span><br><span class="line">report_delay=<span class="number">1.0</span>, compute_loss=<span class="literal">False</span>, callbacks=(), **kwargs)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">第二次遍历语料库建立神经网络模型</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="comment"># 方法</span></span><br><span class="line">similar_by_word()</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">某一个词向量最相近的词集合</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">similarity()</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">两个词向量的相近程度</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">doesnt_match()</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">找出不同类的词</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">save_word2vec_format(<span class="string">&#x27;./model.txt&#x27;</span>,binary = <span class="literal">False</span>)</span><br><span class="line">save_word2vec_format(<span class="string">&#x27;./model.bin&#x27;</span>,binary = <span class="literal">True</span>)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">第一种，保存了训练的全部信息，可以在读取后追加训练</span></span><br><span class="line"><span class="string">第二种，保存为word2vec文本格式但是保存时丢失了词汇树等部分信息，不能追加训练</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">KeyedVectors.load_word2vec_format(<span class="string">&#x27;./model.bin&#x27;</span>, binary=<span class="literal">False</span>)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">加载模型</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="comment"># 使用Word2Vec得到model后：</span></span><br><span class="line"><span class="comment"># 3.8</span></span><br><span class="line">model.wv.vocab</span><br><span class="line">model.wv.vocab.keys()</span><br><span class="line"><span class="comment"># 4.0</span></span><br><span class="line">model.wv.key_to_index <span class="comment"># 字典</span></span><br><span class="line">model.wv.index_to_key <span class="comment"># 列表</span></span><br><span class="line">model.wv.vectors</span><br><span class="line"><span class="comment"># 共有</span></span><br><span class="line">model.vector_size</span><br><span class="line">model.wv[<span class="string">&#x27;key&#x27;</span>] <span class="comment"># 取单个词向量</span></span><br></pre></td></tr></table></figure><br />
LdaMulticore：<br />
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 4.0/3.8</span></span><br><span class="line">gensim.models.ldamulticore.LdaMulticore(corpus=<span class="literal">None</span>, num_topics=<span class="number">100</span>, </span><br><span class="line">id2word=<span class="literal">None</span>, workers=<span class="literal">None</span>, chunksize=<span class="number">2000</span>, passes=<span class="number">1</span>, batch=<span class="literal">False</span>, alpha=<span class="string">&#x27;symmetric&#x27;</span>, </span><br><span class="line">eta=<span class="literal">None</span>, decay=<span class="number">0.5</span>, offset=<span class="number">1.0</span>, eval_every=<span class="number">10</span>, iterations=<span class="number">50</span>, gamma_threshold=<span class="number">0.001</span>, </span><br><span class="line">random_state=<span class="literal">None</span>, minimum_probability=<span class="number">0.01</span>, minimum_phi_value=<span class="number">0.01</span>, </span><br><span class="line">per_word_topics=<span class="literal">False</span>)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">corpus: &#123;iterable of list of (int, float), scipy.sparse.csc&#125;，语料库。</span></span><br><span class="line"><span class="string">num_topics: int，主题数量。</span></span><br><span class="line"><span class="string">id2word: &#123;dict of (int, str), gensim.corpora.dictionary.Dictionary&#125;，单词id-&gt;单词 词典。</span></span><br><span class="line"><span class="string">workers: int，线程数。</span></span><br><span class="line"><span class="string">chunksize: int，每个训练模块使用文档数量。</span></span><br><span class="line"><span class="string">passes: int，训练期间通过语料库的次数。</span></span><br><span class="line"><span class="string">alpha: float，文档-主题分布的先验信念。</span></span><br><span class="line"><span class="string">      &#x27;symmetric&#x27;: 使用 1.0 / num_topics 的固定对称先验。</span></span><br><span class="line"><span class="string">      &#x27;asymmetric&#x27;: 使用 1.0 / (topic_index + sqrt(num_topics)) 的固定归一化非对称先验。</span></span><br><span class="line"><span class="string">eta: float，主题-词分布的先验信念。</span></span><br><span class="line"><span class="string">      &#x27;symmetric&#x27;: 使用 1.0 / num_topics 的固定对称先验。</span></span><br><span class="line"><span class="string">      &#x27;auto&#x27;: 从语料库中学习非对称先验。</span></span><br><span class="line"><span class="string">per_word_topics: 如果为 True，该模型还会计算一个主题列表，按每个单词最可能的主题的降序排序，</span></span><br><span class="line"><span class="string">                  以及它们的 phi 值乘以特征长度（即字数）。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><br />
词典：<br />
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line">gensim.corpora.dictionary.Dictionary(documents=<span class="literal">None</span>, prune_at=<span class="number">2000000</span>)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">根据文档生成词典</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="comment"># 属性</span></span><br><span class="line">token2id</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">词典：&#123;token:tokenId&#125;</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">dfs</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">单词出现的频率</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="comment"># 方法</span></span><br><span class="line">doc2bow(document, allow_update=<span class="literal">False</span>, return_missing=<span class="literal">False</span>)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">将文档转成词袋格式：一个列表，列表中每个元素为(token_id, token_count)</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">filter_n_most_frequent(remove_n)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">过滤掉出现频率最高的remove_n个单词</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">filter_extremes(no_below=<span class="number">5</span>, no_above=<span class="number">0.5</span>, keep_n=<span class="number">100000</span>) </span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">去掉出现次数低于no_below的。</span></span><br><span class="line"><span class="string">去掉出现次数高于no_above的（百分数）。</span></span><br><span class="line"><span class="string">在上面基础上，保留出现频率前keep_n的单词。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">filter_tokens(bad_ids=<span class="literal">None</span>, good_ids=<span class="literal">None</span>)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">两种用法:</span></span><br><span class="line"><span class="string">(1)去掉bad_id对应的词。</span></span><br><span class="line"><span class="string">(2)保留good_id对应的词而去掉其他词。</span></span><br><span class="line"><span class="string">注意，这里bad_ids和good_ids都是列表形式。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">compacity() </span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">执行完过滤操作以后，可能会造成单词的序号之间有空隙。</span></span><br><span class="line"><span class="string">可以使用该函数来对词典来进行重新排序，去掉这些空隙。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure></p>
<h1 id="参考文献">参考文献</h1>
<p><span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL2hhbmtjcy9IYW5MUA==">HanLP: Han Language
Processing<i class="fa fa-external-link-alt"></i></span><br />
统计自然语言处理 第二版 (宗成庆著)<br />
<span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzEzMDEuMzc4MS5wZGY=">Efficient Estimation of
Word Representations in Vector Space<i class="fa fa-external-link-alt"></i></span><br />
<span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzEzMTAuNDU0Ni5wZGY=">Distributed
Representations of Words and Phrases and their
Compositionality<i class="fa fa-external-link-alt"></i></span><br />
<span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzE0MTEuMjczOC5wZGY=">word2vec Parameter
Learning Explained<i class="fa fa-external-link-alt"></i></span><br />
<span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzE1MDcuMDU1MjMucGRmJTIwaHR0cDovL2llZWV4cGxvcmUuaWVlZS5vcmcvZG9jdW1lbnQvNzQ3ODQxNy8=">How
to Generate a Good Word Embedding?<i class="fa fa-external-link-alt"></i></span><br />
<span class="exturl" data-url="aHR0cDovL25scC50b3duL2Jsb2cvc2VudGVuY2Utc2ltaWxhcml0eS8=">Comparing Sentence
Similarity Methods<i class="fa fa-external-link-alt"></i></span><br />
<span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzE2MDguMDU4NTkucGRm">Using the Output
Embedding to Improve Language Models<i class="fa fa-external-link-alt"></i></span><br />
<span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzE0MDIuMzcyMi5wZGY=">word2vec Explained:
Deriving Mikolov et al.’sNegative-Sampling Word-Embedding
Method<i class="fa fa-external-link-alt"></i></span><br />
<span class="exturl" data-url="aHR0cHM6Ly9ubHAuc3RhbmZvcmQuZWR1L3B1YnMvZ2xvdmUucGRm">GloVe: Global Vectors
for Word Representation<i class="fa fa-external-link-alt"></i></span><br />
<span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzE2MDcuMDE3NTl2Mi5wZGY=">Bag of Tricks for
Efficient Text Classification<i class="fa fa-external-link-alt"></i></span><br />
<span class="exturl" data-url="aHR0cHM6Ly96aC53aWtpcGVkaWEub3JnL3dpa2kvJUU5JTlDJThEJUU1JUE0JUFCJUU2JTlCJUJDJUU3JUJDJTk2JUU3JUEwJTgx">维基百科：霍夫曼编码<i class="fa fa-external-link-alt"></i></span><br />
<span class="exturl" data-url="aHR0cHM6Ly93d3cuY25ibG9ncy5jb20vcGluYXJkL3AvNzI0MzUxMy5odG1s">word2vec原理(二)
基于Hierarchical Softmax的模型<i class="fa fa-external-link-alt"></i></span><br />
<span class="exturl" data-url="aHR0cHM6Ly93d3cuY25ibG9ncy5jb20vcGluYXJkL3AvNzI0OTkwMy5odG1s">word2vec原理(三)
基于Negative Sampling的模型<i class="fa fa-external-link-alt"></i></span><br />
<span class="exturl" data-url="aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2l0cGx1cy9hcnRpY2xlL2RldGFpbHMvMzc5Njk5Nzk=">word2vec
中的数学原理详解（四）基于 Hierarchical Softmax 的模型<i class="fa fa-external-link-alt"></i></span><br />
<span class="exturl" data-url="aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2l0cGx1cy9hcnRpY2xlL2RldGFpbHMvMzc5OTg3OTc=">word2vec
中的数学原理详解（五）基于 Negative Sampling 的模型<i class="fa fa-external-link-alt"></i></span><br />
<span class="exturl" data-url="aHR0cHM6Ly96aC12Mi5kMmwuYWkvY2hhcHRlcl9uYXR1cmFsLWxhbmd1YWdlLXByb2Nlc3NpbmctcHJldHJhaW5pbmcvd29yZDJ2ZWMuaHRtbA==">词嵌入（Word2vec）<i class="fa fa-external-link-alt"></i></span><br />
<span class="exturl" data-url="aHR0cHM6Ly93d3cuYmlsaWJpbGkuY29tL3ZpZGVvL0JWMTJXNDExdjdHYT9mcm9tPXNlYXJjaCZzZWlkPTE0MTM1NDUwODQ5MzA3Mzg5Njc5JnNwbV9pZF9mcm9tPTMzMy4zMzcuMC4w">[MXNet/Gluon]
动手学深度学习第十六课：词向量（word2vec）<i class="fa fa-external-link-alt"></i></span><br />
<span class="exturl" data-url="aHR0cHM6Ly93d3cuYmlsaWJpbGkuY29tL3ZpZGVvL0JWMWFXNDExZTcyOD9mcm9tPXNlYXJjaCZzZWlkPTU3NTg3NDE1MjA2MTQ0OTEzMzEmc3BtX2lkX2Zyb209MzMzLjMzNy4wLjA=">[MXNet/Gluon]
动手学深度学习第十七课：GloVe、fastText和使用预训练的词向量<i class="fa fa-external-link-alt"></i></span></p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://soundmemories.github.io/2021/06/20/Paper/01.Unsupervised%20Data%20Augmentation%20for%20Consistency%20Training/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avstar.png">
      <meta itemprop="name" content="SoundMemories">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="SoundMemories">
      <meta itemprop="description" content="今日事，今日毕">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | SoundMemories">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2021/06/20/Paper/01.Unsupervised%20Data%20Augmentation%20for%20Consistency%20Training/" class="post-title-link" itemprop="url">Unsupervised Data Augmentation for Consistency Training</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2021-06-20 00:00:00" itemprop="dateCreated datePublished" datetime="2021-06-20T00:00:00+08:00">2021-06-20</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Paper/" itemprop="url" rel="index"><span itemprop="name">Paper</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>4k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>15 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="摘要">摘要</h1>
<p>背景：深度学习的模型训练通常依赖大量的标签数据（如Bert、XLNet），在只有少量数据上通常表现不好。由此产生了数据增强，但以前的研究都是基于监督学习的，并且效果不是特别理想。</p>
<p>贡献：本文提出了一种对无监督（无标签）数据增强方式（半监督学习中无标签数据的增强），简称UDA。UDA方法生成的无监督数据与原始无监督数据具备<strong>分布的一致性</strong>，而以前的方法通常只是应用高斯噪声和dropout噪声（无法保证一致性）。</p>
<p>效果：使用这种数据增强方法，在极少量数据集上，六种语言任务和三种视觉任务都得到了明显的提升。IMDb数据分类任务上，仅仅使用20个带标签数据加UDA方法，就超过了25000个带标签数据的训练模型，错误率达到了4.2%。在CIFAR-10上仅用4000张标签图片就达到了2.7%的错误率。在SVHN任务上，仅仅用250个标签数据就达到了2.85%的错误率，这相当于用全数据集才能达到的正确率，而它们的数量级差别达到了1或2（差10倍或100倍）。在大量标签数据集上，UDA同样表现优秀，在ImageNet任务上，使用10%带标签数据，UDA方法就将Top1和Top5的准确率分别由55.1%提高到77.3%，68.7提高到88.5%。在全数据集上，则分别由78.3%提高到94.4%，79%提高到94.5%。</p>
<h1 id="导读">导读</h1>
<p>深度学习需要大量带标签数据，但是实际工程中很难满足，这就需要数据标注，但数据标注是一项耗时耗力的工作。所以，充分利用未标注数据是一个很有意义的研究方向。而半监督方法，是最有前景的方法之一，当前半监督方法可归结为三类：<br />
（1）基于图卷积和图嵌入的图标签传播方法。<br />
（2）将目标数据作为潜变量进行预测。<br />
（3）强制一致/平滑。这种方法在许多任务中被证明具有较好的效果。</p>
<p>强制平滑方法只是使得模型对应较小的噪声不那么敏感。常用方法就是：对于一个样本，添加一些噪声（例如高斯噪声）然后强制让模型对于加噪和不加噪的数据的输出尽量的相似。直观而言就是一个好的模型，应该能够适应各种小的、不改变样本性质的扰动。通常由于扰动函数的不同会有各种不同的方案。</p>
<p>本文在Sajjadi、
Laine等人的研究的基础上，从有监督数据中学习扰动函数，从而得到最优的数据增强方法。良好的数据增强方法能够大大提高模型的结果，并且数据增强方法能应用于各领域。<strong>本文使用的优化方法是最小化增强数据与真实数据之间的KL散度</strong>。虽然有监督数据的数据增强取得了很多成功，但是大量的无监督数据使得UDA这种无监督数据增强方法拥有更广阔前景。</p>
<p>主要贡献：<br />
（1）提出一种TSA方法，该方法能够在无标签数据大于标签数据的时候防止过拟合。<br />
（2）证明<strong>针对性的数据增强</strong>（如AutoAugment）效果明显优于无针对性的数据增强。<br />
（3）验证了本文方法在NLP任务上（如Bert）上的有效性。<br />
（4）在CV和NLP任务中，本文方法都表现优异。<br />
（5）研究一种能应用于分类数据中有标签数据和无标签数据不匹配情况的方法（数据不平衡处理方法）。</p>
<h1 id="内容">内容</h1>
<h2 id="有监督数据增强">有监督数据增强</h2>
<p>在保持标签相同（同一类别）的情况下，通过某种转换方法扩充出类似于真实数据的训练数据。简单而言就是，有一个样本<span
class="math inline">\(x\)</span>，通过转换函数<span
class="math inline">\(q(x)\)</span>生成新数据<span
class="math inline">\(\hat{x}\)</span>，新旧数据有相同的数据标签<span
class="math inline">\(y(\hat{x})=y(x)\)</span>。通常为了得到的增强数据与原始数据相似，使用的是最大似然估计方法。</p>
<p>数据增强方法可以看成是从有标签数据中扩充出更多的有标签数据，然后用扩充数据进行模型训练。因此，扩充数据相对于原始数据必须是有效的变换（例如图片缩放对图片识别可能有效，图片旋转可能无效）。也因此，如何设计转换函数至关重要。</p>
<p>目前，针对NLP任务的有监督数据增强方法已经取得了很大进展。虽然有成果，但是它通常被比喻成“蛋糕上的樱桃”，只是提高有限的性能，这是由于监督数据通常都是少量的。因此，本文研究了一种基于大量数据的无监督数据增强方法。</p>
<h2 id="无监督数据增强">无监督数据增强</h2>
<p>本文研究了一种利用无监督数据的强制平滑方法（类似VAT）。工作流程如下：<br />
<img src="/images/UDA/UDA.png" width="90%"></p>
<p>（1）监督学习部分，使用交叉熵损失函数，模型是<span
class="math inline">\(p_\theta(y|x)\)</span>。<br />
（2）无监督学习部分，使用强制平滑损失函数，对无标签数据进行数据增强，使增强前和增强后的数据分布越相近越好。增强前模型<span
class="math inline">\(p_{\tilde{\theta}}(y|x)\)</span>，增强后模型<span
class="math inline">\(p_\theta(y|\hat{x})\)</span>。<br />
（3）最后，同时使用有标签和无标签数据，把二者模型结合起来，得到Final
Loss。</p>
<p>本文使用最小化<strong>增强后的无标签数据</strong>和<strong>增强前无标签数据</strong>的KL散度。公式如下：<br />
<img src="/images/UDA/UDA_loss1.png" width="60%"></p>
<p>为了同时使用带标签数据和无标签数据，作者在计算带标签数据时上加上交叉熵损失和权重<span
class="math inline">\(\lambda\)</span>。<br />
<img src="/images/UDA/UDA_loss2.png" width="40%"></p>
<p>其中<span class="math inline">\(\it
q(\hat{x}|x)\)</span>是数据增强变换，<span
class="math inline">\(\tilde{\theta}\)</span>是当前参数<span
class="math inline">\(\theta\)</span>的固定副本，表明梯度像Miyato等人所建议的那样，不是通过<span
class="math inline">\(\tilde{\theta}\)</span>传播的。这里使用的数据增强与监督数据增强中使用的增强方法相同。由于数据增强耗时比较大，所以数据增强是离线生成的，单个原始样本会生成多个增强样本。</p>
<p>在无监督学习时，使用了针对性的数据增强：<br />
（1）<strong>Back-translation</strong>：回译能够在保证语义不变的情况下，生成多样的句式。实验证明，在QANet上，这种策略取得了良好的效果。因此作者在情感分类问题等数据集，如IMDb，Yelp-2，Yelp-5，Amazon-2，Amazon-5上采用了这种策略，同时，他们发现，句式的多样性比较有效性更重要。所以使用了<strong>RandAugument</strong>。<br />
（2）<strong>RandAugument</strong>：随机抽样增强，加入噪声。采用随机抽样代替集束搜索策略（一种贪心策略）。具体而言，作者使用WMT14语料库来训练英语到法语和法语到英语的翻译模型，并对每个句子执行回译，而不是整个段落，因为WMT14中的并行数据是用于句子级翻译，而情感分类语料库中的输入类型是段落。<br />
（3）<strong>TF-IDF word
replacement</strong>：虽然回译能够很好的进行数据扩充，但是它并不能保证扩充的句子包含关键词。而对于某些任务，如DBPedia任务，它的目标是预测某些句子属于维基百科的哪个词条。因此关键字非常重要，本文研究了一种在保留TF-IDF高的关键字，用其他非关键字替代TF-IDF分数低的非关键字扩充方案，详细见论文附录B。<br />
增强结果如图所示：<br />
<img src="/images/UDA/trans.png" width="80%"></p>
<p>当然，对CV任务用了<strong>AutoAugument</strong>：用强化学习来搜索图像增强的“最优”组合，其性能明显优于任何人工设计的优化方法。作者使用已发现的增强策略，在CIFAR-10，
SVHN和ImageNet上进行了实验，并在CIFAR-10，SVHN上组合应用了Cutout技术。<br />
增强结果如图所示：<br />
<img src="/images/UDA/trans2.png" width="80%"></p>
<h2
id="数据增强在多样性和有效性上的平衡">数据增强在多样性和有效性上的平衡</h2>
<p>虽然在一些非常优秀的数据增强方法中，能够得到很好的多样性和有效性。但是，由于多样性是通过改变原始数据得到的，所以，它存在改变数据类别的风险，所以，多样性和有效性是存在一定矛盾的。</p>
<p>对于图像分类，AutoAugment算法在有监督的环境下，根据验证集的性能进行优化，从而自动找到多样性和有效性之间的最佳点。</p>
<p>对于文本分类，作者调整随机抽样的强度。一方面，当强度为0时，随机抽样解码退化为贪婪方法，产生完全有效但完全相同的样本。另一方面，当作者使用1的强度时，随机抽样会产生非常不同但几乎不可读的样本。作者发现，设置Softmax强度为0.7、0.8或0.9的表现最好。</p>
<h2 id="训练技巧">训练技巧</h2>
<p>要介绍一些针对不同问题，不同场景下的训练技巧。</p>
<p><strong>Training Signal
Annealing（TSA）</strong>：针对标签数据与未标签数据不平衡时的场景。由于有大量的未标签数据需要UDA处理，所以需要一个较大模型，但是由于较大模型很容易在少量标签数据下过拟合，所以，提出了本方法用于解决该问题。<br />
TSA原理就是在训练过程中，随着未标签数据的增加，逐步去除带标签数据，从而避免模型过拟合到带标签的训练数据。具体而言，就是在训练的<span
class="math inline">\(t\)</span>时刻，设置一个阈值<span
class="math inline">\(\eta_t\)</span>，当<span
class="math inline">\(\frac{1}{k}\leqslant\eta_t\leqslant
1\)</span>，其中<span
class="math inline">\(k\)</span>是类别数。当某个标签计算的<span
class="math inline">\(p_\theta(y^*|x)\)</span>大于阈值<span
class="math inline">\(\eta_t\)</span>，就将该标签数据移除出计算损失的过程，而只计算miniBatch里面的其余数据。假定miniBatch样本记作B，那么该策略计算损失如下：<br />
<img src="/images/UDA/TSA.png" width="40%"><br />
过滤后的样本集合：<br />
<img src="/images/UDA/TSA2.png" width="40%"></p>
<p>阈值<span
class="math inline">\(\eta_t\)</span>用于防止模型过拟合到标签数据。随着<span
class="math inline">\(\eta_t\)</span>向1靠近，模型只能缓慢地从标注的实例中得到监督，大大缓解了过拟合问题。假设T是总训练步数，t是当前的训练步数。为了考虑未标记数据和标记数据的不同比率，有以下三种<span
class="math inline">\(\eta_t\)</span>更新计算方式：<br />
<img src="/images/UDA/TSA3.png" width="90%"></p>
<p>对于数据量少，容易过拟合的情况，使用指数形式比较好。对于标签数据不容易过拟合的情况，比如标签数据比较多或者使用了有效的正则化手段时，使用对数形式会比较好。使用不同更新方式的效果：<br />
<img src="/images/UDA/TSA4.png" width="50%"></p>
<p><strong>Sharpening Predictions</strong><br />
当标签数据很少时，未标签数据和预测的未标签数据分布会很平坦。因此，在计算KL散度时，主要贡献的部分来自于标签数据。例如在Imagenet任务中，使用10%标签数据下，未标签数据的分布明显比标签数据的分布更加平坦。而比较丰富的数据分布是比较有利于模型训练的，因此，提出以下三种锐化方案：<br />
（1）基于置信度的mask：对模型预测效果不好的，预测的概率小于一定阈值的标签，不计算一致性损失。<br />
（2）最小化熵：最小化熵就是使得预测的增广数据能够拥有一个较低的熵，因此，需要在计算损失时，加上熵的计算。<br />
（3）Softmax控制：通过调整Softmax控制输出， <span
class="math inline">\(p_{\tilde{\theta}}(y|x)\)</span>通过<span
class="math inline">\(Softmax(l(x)/\tau)\)</span>计算，其中<span
class="math inline">\(l(x)\)</span>表示结果逻辑分布概率，<span
class="math inline">\(\tau\)</span>表示强度。<span
class="math inline">\(\tau\)</span>越小，分布越锐化。</p>
<p><strong>Domain-relevance Data Filtering</strong><br />
通常，作者希望能够运用领域外的数据，因为它比较容易获取。但是，一般领域外的数据和领域内的数据不匹配。由于数据分布的不匹配，使用领域外的数据往往对模型是有负面影响的。为了获取与当前任务相关的域数据，本文采用一种通用的检测领域外数据的技术。作者用领域内的数据训练了一个模型，让后用它去评估领域外的数据，然后过滤掉置信度低的数据。具体说就是，对于分类任务，对所有领域外数据进行概率计算，只使用其中分类正确且概率高的数据。</p>
<h2 id="实验结果">实验结果</h2>
<p>本文对文本分类和视觉相关任务，运用UDA进行了实验。包括六项文本分类任务和三项图片分类任务。<br />
### 文本分类<br />
实验是基于Bert进行的，因为它在许多NLP任务中表现都很好。具体实验设置请看原始论文，实验结果如下：<br />
<img src="/images/UDA/01.png" width="80%"></p>
<p>实验结果表明，运用UDA后，基本都取得了较大的提高。同时，作者还实验了<strong>不同数量的标签</strong>对结果的影响，结果如下：<br />
<img src="/images/UDA/02.png" width="80%"></p>
<p>作者实验对比了UDA与半监督方法，结果显示，UDA结果明显更优。<br />
<img src="/images/UDA/03.png" width="90%"></p>
<p>同时，作者还对比实验了不同模型的情况：<br />
<img src="/images/UDA/04.png" width="80%"></p>
<h3 id="图像任务">图像任务</h3>
<p>ImageNet之所以要单独拿出来，是因为它是一个很有挑战性的任务，而且数据量很大。作者使用10%标签数据和全数据分别做了对比（图片尺寸224）。<strong>10%标签数据</strong>，ImageNet对比实验结果：<br />
<img src="/images/UDA/05.png" width="50%"></p>
<p><strong>全数据</strong>，ImageNet对比实验结果：<br />
<img src="/images/UDA/06.png" width="50%"></p>
<p>作者做了<strong>使用不同训练策略</strong>下的情况，TSA对比实验结果：<br />
<img src="/images/UDA/07.png" width="50%"></p>
<p>最后，作者做了<strong>消融实验，对比不同策略的重要性</strong>。不同模块的消融实验结果：<br />
<img src="/images/UDA/08.png" width="50%"></p>
<h1 id="总结">总结</h1>
<p>本文提供了一种无监督数据（无标签）数据增强方式，通过<strong>Back-translation</strong>、<strong>RandAugument</strong>、<strong>TF-IDF
word
replacement</strong>方法对无监督文本数据增强，使用<strong>AutoAugument</strong>对图像数据进行增强，最后使用KL散度使新生成的样本数据和原样本数据分布一致，最后结合有监督数据（有标签）形成最终的损失函数，通过<strong>TSA</strong>处理了无标签数据大于有标签数据的过拟合问题。</p>
<p>本文重要的是使用了针对性的数据增强，并且效果很好，不同于传统的高斯噪声、dropout噪声、或者简单的仿射变换，这种针对性的增强能生成更有效的噪声。并且对扰动的有效性和多样性进行了平衡。</p>
<p>这种针对性的思想值得学习，并且考虑分布影响，结合可以利用的增强方式，比如EDA中提到的同义词替换（synonym
replacement）和随机插入（random Insertion，RI）。</p>
<h1 id="参考文献">参考文献</h1>
<p><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzE5MDQuMTI4NDh2Mi5wZGY=">Unsupervised Data
Augmentation for Consistency Training<i class="fa fa-external-link-alt"></i></span><br />
<span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzE5MDQuMTI4NDgucGRmP3JlZj1oYWNrZXJub29uLmNvbQ==">Unsupervised
Data Augmentation for Consistency Training<i class="fa fa-external-link-alt"></i></span><br />
<span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzE5MDEuMTExOTYucGRm">EDA: Easy Data
Augmentation Techniques for Boosting Performance on Text Classification
Tasks<i class="fa fa-external-link-alt"></i></span><br />
<span class="exturl" data-url="aHR0cHM6Ly9zZXZlbm9sZC5naXRodWIuaW8vMjAyMC8wNi90ZXh0X0VEQS8=">自然语言处理之文本数据增强<i class="fa fa-external-link-alt"></i></span><br />
Github：<span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL2dvb2dsZS1yZXNlYXJjaC91ZGE=">uda<i class="fa fa-external-link-alt"></i></span><br />
Github：<span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL3poYW5sYW9iYW4vRURBX05MUF9mb3JfQ2hpbmVzZQ==">EDA_NLP_for_Chinese<i class="fa fa-external-link-alt"></i></span><br />
<span class="exturl" data-url="aHR0cHM6Ly93d3cuamlhbnNodS5jb20vcC81ZDRlMThiOGRlMDQ=">谷歌惊艳的无监督数据增强方法--Unsupervised
Data Augmentation for Consistency Training<i class="fa fa-external-link-alt"></i></span></p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://soundmemories.github.io/2021/06/19/NLP/00.NLP%E7%AE%80%E4%BB%8B/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avstar.png">
      <meta itemprop="name" content="SoundMemories">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="SoundMemories">
      <meta itemprop="description" content="今日事，今日毕">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | SoundMemories">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2021/06/19/NLP/00.NLP%E7%AE%80%E4%BB%8B/" class="post-title-link" itemprop="url">NLP简介</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2021-06-19 00:00:00" itemprop="dateCreated datePublished" datetime="2021-06-19T00:00:00+08:00">2021-06-19</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/NLP/" itemprop="url" rel="index"><span itemprop="name">NLP</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>5k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>18 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="什么是nlp">什么是NLP？</h1>
<p><strong>NLP（Natural Language
Processing，自然语言处理）</strong>，是融合了计算机科学、人工智能、语言学的交叉学科。其主要目的是让计算机学会处理人类的语言，甚至实现终极目标——————理解人类语言。</p>
<p>NLP可以概括为<strong>NLP=NLU+NLG</strong>：<br />
<strong>NLU（Natural Language
Understand，自然语言理解）</strong>：语音或文本 ——&gt;
结构化的语义。<br />
<strong>NLG（Natural Language
Generation，自然语言生成）</strong>：把结构化的语义 ——&gt;
文本或语音。<br />
          <!--noindex-->
            <div class="post-button">
              <a class="btn" href="/2021/06/19/NLP/00.NLP%E7%AE%80%E4%BB%8B/#more" rel="contents">
                阅读全文 &raquo;
              </a>
            </div>
          <!--/noindex-->
        
      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://soundmemories.github.io/2021/06/19/Paper/00.Paper%E9%98%85%E8%AF%BB%E6%8A%80%E5%B7%A7/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avstar.png">
      <meta itemprop="name" content="SoundMemories">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="SoundMemories">
      <meta itemprop="description" content="今日事，今日毕">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | SoundMemories">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2021/06/19/Paper/00.Paper%E9%98%85%E8%AF%BB%E6%8A%80%E5%B7%A7/" class="post-title-link" itemprop="url">Paper阅读技巧</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2021-06-19 00:00:00" itemprop="dateCreated datePublished" datetime="2021-06-19T00:00:00+08:00">2021-06-19</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Paper/" itemprop="url" rel="index"><span itemprop="name">Paper</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>3.3k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>12 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="paper">Paper</h1>
<p>Paper一般会发表在期刊和顶会中，在期刊上发表Paper需要花费大量时间和精力不断修改完善论文，所以周期很长，尤其是顶级期刊。而在顶会中发表论文周期会短些，所以很多重要的成果都先在顶会中出现。一般工作后，重点关注顶会即可。<br />
          <!--noindex-->
            <div class="post-button">
              <a class="btn" href="/2021/06/19/Paper/00.Paper%E9%98%85%E8%AF%BB%E6%8A%80%E5%B7%A7/#more" rel="contents">
                阅读全文 &raquo;
              </a>
            </div>
          <!--/noindex-->
        
      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://soundmemories.github.io/2021/06/15/NLP/00.%E6%89%8B%E5%8A%A8%E5%AE%9E%E7%8E%B0/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avstar.png">
      <meta itemprop="name" content="SoundMemories">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="SoundMemories">
      <meta itemprop="description" content="今日事，今日毕">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | SoundMemories">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2021/06/15/NLP/00.%E6%89%8B%E5%8A%A8%E5%AE%9E%E7%8E%B0/" class="post-title-link" itemprop="url">手动实现+部分源码</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2021-06-15 00:00:00" itemprop="dateCreated datePublished" datetime="2021-06-15T00:00:00+08:00">2021-06-15</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/NLP/" itemprop="url" rel="index"><span itemprop="name">NLP</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>3.3k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>12 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="dropout">dropout</h1>
<div class="tabs" id="one"><ul class="nav-tabs"><li class="tab active"><a href="#one-1">dropout</a></li><li class="tab"><a href="#one-2">R-Drop</a></li></ul><div class="tab-content"><div class="tab-pane active" id="one-1"><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 基础dropout</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">train</span>(<span class="params">rate, x, w1, b1, w2, b2</span>):</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    rate: dropout概率</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    layer1 = np.maximum(<span class="number">0</span>, np.dot(w1, x)+b1) <span class="comment"># maximum作为relu替代</span></span><br><span class="line">    mask1 = np.random.binomial(<span class="number">1</span>, <span class="number">1</span>-rate, layer1.shape) <span class="comment"># 二项分布，试验次数1，成功概率1-rate，形状同layer1</span></span><br><span class="line">    layer1 = layer1 * mask1</span><br><span class="line"></span><br><span class="line">    layer2 = np.maximum(<span class="number">0</span>, np.dot(w2, layer1)+b2) </span><br><span class="line">    mask2 = np.random.binomial(<span class="number">1</span>, <span class="number">1</span>-rate, layer2.shape) </span><br><span class="line">    layer2 = layer2 * mask2</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> layer2</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">test</span>(<span class="params">rate, x, w1, b1, w2, b2</span>):</span><br><span class="line">    layer1 = np.maximum(<span class="number">0</span>, np.dot(w1, x)+b1) </span><br><span class="line">    layer1 = layer1 * (<span class="number">1</span>-rate) <span class="comment"># 保证测试和训练期望一致，需要乘以1-rate</span></span><br><span class="line"></span><br><span class="line">    layer2 = np.maximum(<span class="number">0</span>, np.dot(w2, layer1)+b2) </span><br><span class="line">    layer2 = layer2 * (<span class="number">1</span>-rate)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> layer2</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">another_train</span>(<span class="params">rate, x, w1, b1, w2, b2</span>):</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    rate: dropout概率</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    layer1 = np.maximum(<span class="number">0</span>, np.dot(w1, x)+b1) <span class="comment"># maximum作为relu替代</span></span><br><span class="line">    mask1 = np.random.binomial(<span class="number">1</span>, <span class="number">1</span>-rate, layer1.shape) <span class="comment"># 二项分布，试验次数1，成功概率1-rate，形状同layer1</span></span><br><span class="line">    layer1 = layer1 * mask1</span><br><span class="line">    layer1 = layer1/(<span class="number">1</span>-rate) <span class="comment"># 把测试阶段的计算挪到训练，减少测试计算量</span></span><br><span class="line"></span><br><span class="line">    layer2 = np.maximum(<span class="number">0</span>, np.dot(w2, layer1)+b2) </span><br><span class="line">    mask2 = np.random.binomial(<span class="number">1</span>, <span class="number">1</span>-rate, layer2.shape) </span><br><span class="line">    layer2 = layer2 * mask2</span><br><span class="line">    layer2 = layer2/(<span class="number">1</span>-rate)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> layer2</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">another_test</span>(<span class="params">rate, x, w1, b1, w2, b2</span>):</span><br><span class="line">    layer1 = np.maximum(<span class="number">0</span>, np.dot(w1, x)+b1) </span><br><span class="line">    layer2 = np.maximum(<span class="number">0</span>, np.dot(w2, layer1)+b2) </span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> layer2</span><br></pre></td></tr></table></figure></div><div class="tab-pane" id="one-2"><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># https://github.com/dropreg/R-Drop</span></span><br><span class="line"><span class="comment"># https://spaces.ac.cn/archives/8496</span></span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="comment"># define your task model, which outputs the classifier logits</span></span><br><span class="line">model = TaskModel()</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">compute_kl_loss</span>(<span class="params">p, q pad_mask=<span class="literal">None</span></span>):</span><br><span class="line">    p_loss = F.kl_div(F.log_softmax(p, dim=-<span class="number">1</span>), F.softmax(q, dim=-<span class="number">1</span>), reduction=<span class="string">&#x27;none&#x27;</span>)</span><br><span class="line">    q_loss = F.kl_div(F.log_softmax(q, dim=-<span class="number">1</span>), F.softmax(p, dim=-<span class="number">1</span>), reduction=<span class="string">&#x27;none&#x27;</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># pad_mask is for seq-level tasks</span></span><br><span class="line">    <span class="keyword">if</span> pad_mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        p_loss.masked_fill_(pad_mask, <span class="number">0.</span>)</span><br><span class="line">        q_loss.masked_fill_(pad_mask, <span class="number">0.</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># You can choose whether to use function &quot;sum&quot; and &quot;mean&quot; depending on your task</span></span><br><span class="line">    p_loss = p_loss.<span class="built_in">sum</span>()</span><br><span class="line">    q_loss = q_loss.<span class="built_in">sum</span>()</span><br><span class="line"></span><br><span class="line">    loss = (p_loss + q_loss) / <span class="number">2</span></span><br><span class="line">    <span class="keyword">return</span> loss</span><br><span class="line"></span><br><span class="line"><span class="comment"># keep dropout and forward twice</span></span><br><span class="line">logits = model(x)</span><br><span class="line"></span><br><span class="line">logits2 = model(x)</span><br><span class="line"></span><br><span class="line"><span class="comment"># cross entropy loss for classifier</span></span><br><span class="line">ce_loss = <span class="number">0.5</span> * (cross_entropy_loss(logits, label) + cross_entropy_loss(logits2, label))</span><br><span class="line"></span><br><span class="line">kl_loss = compute_kl_loss(logits, logits2)</span><br><span class="line"></span><br><span class="line"><span class="comment"># carefully choose hyper-parameters</span></span><br><span class="line">loss = ce_loss + α * kl_loss</span><br></pre></td></tr></table></figure></div></div></div>
<h1 id="算子融合">算子融合</h1>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># RepVGG的算子融合</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">point-wise: 通道融合, 抛弃局部关联性。</span></span><br><span class="line"><span class="string">depth-wise: 局部关联性，抛弃通道融合。</span></span><br><span class="line"><span class="string">普通卷积可看成：depth-wise + 1*1 point-wise</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line">in_channels = <span class="number">2</span></span><br><span class="line">ou_channels = <span class="number">2</span></span><br><span class="line">kernel_size = <span class="number">3</span></span><br><span class="line">w = <span class="number">9</span></span><br><span class="line">h = <span class="number">9</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># res_block = 3*3 conv + 1*1 conv + input</span></span><br><span class="line">x = torch.ones(<span class="number">1</span>, in_channels, w, h) <span class="comment"># 输入图片</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 1.原生方法</span></span><br><span class="line">t1 = time.time()</span><br><span class="line">conv_2d = nn.Conv2d(in_channels, ou_channels, kernel_size, padding=<span class="string">&#x27;same&#x27;</span>)</span><br><span class="line">conv_2d_pointwise = nn.Conv2d(in_channels, ou_channels, <span class="number">1</span>)</span><br><span class="line">result1 = conv_2d(x) + conv_2d_pointwise(x) + x</span><br><span class="line">t2 = time.time()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2.算子融合</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">point-wise卷积-&gt;3*3</span></span><br><span class="line"><span class="string">x-&gt;3*3</span></span><br><span class="line"><span class="string">3个卷积-&gt;1个卷积</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="comment"># point-wise卷积-&gt;2*2*3*3</span></span><br><span class="line"><span class="comment"># pad: 左右上下前后内外，这里只pad 1*1 的左右上下</span></span><br><span class="line">pointwise_to_conv_weight = F.pad(conv_2d_pointwise.weight, [<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>]) <span class="comment"># 2*2*1*1-&gt;2*2*3*3</span></span><br><span class="line">conv_2d_for_pointwise = nn.Conv2d(in_channels, ou_channels, kernel_size, padding=<span class="string">&#x27;same&#x27;</span>)</span><br><span class="line">conv_2d_for_pointwise.weight = nn.Parameter(pointwise_to_conv_weight)</span><br><span class="line">conv_2d_for_pointwise.bias = conv_2d_pointwise.bias</span><br><span class="line"></span><br><span class="line"><span class="comment"># x-&gt;2*2*3*3 </span></span><br><span class="line">zeros = torch.unsqueeze(torch.zeros(kernel_size, kernel_size), <span class="number">0</span>)</span><br><span class="line">stars = torch.unsqueeze(F.pad(torch.ones(<span class="number">1</span>, <span class="number">1</span>), [<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>]), <span class="number">0</span>)</span><br><span class="line">stars_zeros = torch.unsqueeze(torch.cat([stars, zeros], <span class="number">0</span>), <span class="number">0</span>)</span><br><span class="line">zeros_stars = torch.unsqueeze(torch.cat([zeros, stars], <span class="number">0</span>), <span class="number">0</span>)</span><br><span class="line">identity_to_conv_weight = torch.cat([stars_zeros, zeros_stars], <span class="number">0</span>)</span><br><span class="line">identity_to_conv_bias = torch.zeros([ou_channels]) </span><br><span class="line">conv_2d_for_identity = nn.Conv2d(in_channels, ou_channels, kernel_size, padding=<span class="string">&#x27;same&#x27;</span>)</span><br><span class="line">conv_2d_for_identity.weight = nn.Parameter(identity_to_conv_weight)</span><br><span class="line">conv_2d_for_identity.bias = nn.Parameter(identity_to_conv_bias)</span><br><span class="line"></span><br><span class="line">result2 = conv_2d(x) + conv_2d_for_pointwise(x) + conv_2d_for_identity(x)</span><br><span class="line"><span class="built_in">print</span>(torch.<span class="built_in">all</span>(torch.isclose(result1, result2)))</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">tensor(True)</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="comment"># 融合</span></span><br><span class="line">t3 = time.time()</span><br><span class="line">conv_2d_for_fusion = nn.Conv2d(in_channels, ou_channels, kernel_size, padding=<span class="string">&#x27;same&#x27;</span>)</span><br><span class="line">conv_2d_for_fusion.weight = nn.Parameter(conv_2d.weight.data +</span><br><span class="line">                                         conv_2d_for_pointwise.weight.data +</span><br><span class="line">                                         conv_2d_for_identity.weight.data)</span><br><span class="line">conv_2d_for_fusion.bias = nn.Parameter(conv_2d.bias.data +</span><br><span class="line">                                         conv_2d_for_pointwise.bias.data +</span><br><span class="line">                                         conv_2d_for_identity.bias.data)</span><br><span class="line">result3 = conv_2d_for_fusion(x)</span><br><span class="line">t4 = time.time()</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(torch.<span class="built_in">all</span>(torch.isclose(result2, result3)))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;原生写法耗时:&#x27;</span>, t2-t1)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;融合算子耗时:&#x27;</span>, t4-t3)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">tensor(True)</span></span><br><span class="line"><span class="string">原生写法耗时: 0.7473533153533936</span></span><br><span class="line"><span class="string">原生写法耗时: 0.0010013580322265625</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>
<p>ConvMixer 使用了算子融合：<br />
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># https://github.com/locuslab/convmixer</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">ConvMixer</span>(<span class="params">h, depth, kernel_size=<span class="number">9</span>, patch_size=<span class="number">7</span>, n_classes=<span class="number">1000</span></span>):</span><br><span class="line">    Seq, ActBn = nn.Sequential, <span class="keyword">lambda</span> x: Seq(x, nn.GELU(), nn.BatchNorm2d(h))</span><br><span class="line">    Residual = <span class="built_in">type</span>(<span class="string">&#x27;Residual&#x27;</span>, (Seq,), &#123;<span class="string">&#x27;forward&#x27;</span>: <span class="keyword">lambda</span> self, x: self[<span class="number">0</span>](x) + x&#125;)</span><br><span class="line">    <span class="keyword">return</span> Seq(ActBn(nn.Conv2d(<span class="number">3</span>, h, patch_size, stride=patch_size)),</span><br><span class="line">           *[Seq(Residual(ActBn(nn.Conv2d(h, h, kernel_size, groups=h, padding=<span class="string">&quot;same&quot;</span>))),</span><br><span class="line">                ActBn(nn.Conv2d(h, h, <span class="number">1</span>))) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(depth)],</span><br><span class="line">            nn.AdaptiveAvgPool2d((<span class="number">1</span>,<span class="number">1</span>)), nn.Flatten(), nn.Linear(h, n_classes))</span><br></pre></td></tr></table></figure></p>
<h1 id="transformer">Transformer</h1>
<p>seq2seq基础模块分类：<br />
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">- CNN</span><br><span class="line"> - 权重共享</span><br><span class="line">    - 平移不变性</span><br><span class="line">    - 可并行计算</span><br><span class="line"> - 滑动窗口，局部关联性建模，依靠多层堆积来进行长程建模</span><br><span class="line"> - 对相对位置敏感，对绝对位置不敏感</span><br><span class="line"></span><br><span class="line">- RNN(依次有序递归建模)</span><br><span class="line"> - 对顺序敏感</span><br><span class="line"> - 串行计算耗时</span><br><span class="line"> - 长程建模能力弱</span><br><span class="line"> - 计算复杂度与序列长度呈线性关系</span><br><span class="line"> - 单步计算复杂度不变</span><br><span class="line"> - 对相对位置、绝对位置都敏感</span><br><span class="line"></span><br><span class="line">- transformer</span><br><span class="line"> - 无局部假设</span><br><span class="line">    - 可并行计算</span><br><span class="line">    - 对相对位置不敏感</span><br><span class="line"> - 无有序假设</span><br><span class="line">    - 需要位置编码来反映位置变化对特征的影响</span><br><span class="line">    - 对绝对位置不敏感</span><br><span class="line">- 任意两字符都可建模</span><br><span class="line">    - 擅长长短程建模</span><br><span class="line">    - 自注意力机制需要序列长度的平方级别复杂度</span><br><span class="line">- 总结特点：</span><br><span class="line">    - 无先验假设(例如: 局部关联性、有序建模性)</span><br><span class="line">    - 核心计算在于自注意力机制，平方复杂度n^2*d</span><br><span class="line">    - 数据量的要求与先验假设的程度成反比(数据量少，需要注入的先验假设越多，比如对loss和多头的改进假设)</span><br><span class="line">- 使用类型：</span><br><span class="line">    - Encoder only：Bert、分类任务、非流式任务</span><br><span class="line">    - Decoder only：GPT系列、语言建模、自回归生成任务、流式任务</span><br><span class="line">    - Encoder-Decoder：机器翻译、语音识别</span><br></pre></td></tr></table></figure><br />
Transformer结构：<br />
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">- Transformer</span><br><span class="line"> - Encoder</span><br><span class="line">    - input word embedding: 由稀疏的one-hot进入一个不带bias的FFN得到一个稠密的连续向量。</span><br><span class="line">    - position encoding: </span><br><span class="line">        - 通过sin/cos来固定表征：每个位置确定性的; 对于不同长度的句子，两个词相对位置的距离一致; 可以推广到更长的测试句子。</span><br><span class="line">        - pe(pos+k)可以写成pe(pos)的线性组合。</span><br><span class="line">        - 通过残差连接来使得位置信息流入深层。</span><br><span class="line">    - multi-head self-attention</span><br><span class="line">        - 使得建模能力更强，表征空间更丰富。</span><br><span class="line">        - 由多个QKV构成，每组单独计算一个attention向量。</span><br><span class="line">        - 把每组的attention向量拼起来，并入一个不带bias的FFN得到最终的向量。</span><br><span class="line">    - feed-forward network</span><br><span class="line">        - 只考虑embedding的每个维度进行建模。</span><br><span class="line">        - 不同位置参数共享。</span><br><span class="line">        - 类似1*1 point-wise convolution。</span><br><span class="line"> - Decoder</span><br><span class="line">  - output word embedding</span><br><span class="line">  - position encoding</span><br><span class="line">  - mask multi-head self-attention</span><br><span class="line">  - multi-head cross-attention</span><br><span class="line">  - feed-forward network</span><br><span class="line">  - softmax</span><br></pre></td></tr></table></figure><br />
Transformer Pytorch源码：<br />
补充阅读：<span class="exturl" data-url="aHR0cHM6Ly9ubHAuc2Vhcy5oYXJ2YXJkLmVkdS8yMDE4LzA0LzAzL2F0dGVudGlvbi5odG1s">The
Annotated Transformer<i class="fa fa-external-link-alt"></i></span><br />
<div class="tabs" id="tr"><ul class="nav-tabs"><li class="tab active"><a href="#tr-1">Transformer</a></li><li class="tab"><a href="#tr-2">Encoder/Decoder</a></li><li class="tab"><a href="#tr-3">EncoderLayer/DecoderLayer</a></li><li class="tab"><a href="#tr-4">MultiheadAttention</a></li><li class="tab"><a href="#tr-5">Solution 3</a></li></ul><div class="tab-content"><div class="tab-pane active" id="tr-1"><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Transformer</span>(<span class="title class_ inherited__">Module</span>):</span><br><span class="line">    <span class="string">r&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">     Examples::</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; transformer_model = nn.Transformer(nhead=16, num_encoder_layers=12)</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; src = torch.rand((10, 32, 512))</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; tgt = torch.rand((20, 32, 512))</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; out = transformer_model(src, tgt)</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, </span></span><br><span class="line"><span class="params">                d_model: <span class="built_in">int</span> = <span class="number">512</span>,  <span class="comment"># 模型维度</span></span></span><br><span class="line"><span class="params">                nhead: <span class="built_in">int</span> = <span class="number">8</span>,  <span class="comment"># 多头注意力层数</span></span></span><br><span class="line"><span class="params">                num_encoder_layers: <span class="built_in">int</span> = <span class="number">6</span>, <span class="comment"># encoder block数</span></span></span><br><span class="line"><span class="params">                num_decoder_layers: <span class="built_in">int</span> = <span class="number">6</span>, <span class="comment"># decoder block数</span></span></span><br><span class="line"><span class="params">                dim_feedforward: <span class="built_in">int</span> = <span class="number">2048</span>, <span class="comment"># FFN中间维度</span></span></span><br><span class="line"><span class="params">                dropout: <span class="built_in">float</span> = <span class="number">0.1</span>, <span class="comment"># 丢弃概率</span></span></span><br><span class="line"><span class="params">                activation: <span class="type">Union</span>[<span class="built_in">str</span>, <span class="type">Callable</span>[[Tensor], Tensor]] = F.relu, <span class="comment"># 激活函数</span></span></span><br><span class="line"><span class="params">                custom_encoder: <span class="type">Optional</span>[<span class="type">Any</span>] = <span class="literal">None</span>, </span></span><br><span class="line"><span class="params">                custom_decoder: <span class="type">Optional</span>[<span class="type">Any</span>] = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params">                layer_norm_eps: <span class="built_in">float</span> = <span class="number">1e-5</span>, </span></span><br><span class="line"><span class="params">                batch_first: <span class="built_in">bool</span> = <span class="literal">False</span>,  <span class="comment"># batch在第0维？</span></span></span><br><span class="line"><span class="params">                norm_first: <span class="built_in">bool</span> = <span class="literal">False</span>, </span></span><br><span class="line"><span class="params">                device=<span class="literal">None</span>, </span></span><br><span class="line"><span class="params">                dtype=<span class="literal">None</span></span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">            factory_kwargs = &#123;<span class="string">&#x27;device&#x27;</span>: device, <span class="string">&#x27;dtype&#x27;</span>: dtype&#125;</span><br><span class="line">            <span class="built_in">super</span>(Transformer, self).__init__()</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> custom_encoder <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                self.encoder = custom_encoder</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="comment"># EncoderLayer</span></span><br><span class="line">                encoder_layer = TransformerEncoderLayer(d_model, nhead, dim_feedforward, dropout,</span><br><span class="line">                                                        activation, layer_norm_eps, batch_first, norm_first,</span><br><span class="line">                                                        **factory_kwargs)</span><br><span class="line">                encoder_norm = LayerNorm(d_model, eps=layer_norm_eps, **factory_kwargs)</span><br><span class="line">                <span class="comment"># Encoder block</span></span><br><span class="line">                self.encoder = TransformerEncoder(encoder_layer, num_encoder_layers, encoder_norm)</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> custom_decoder <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                self.decoder = custom_decoder</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="comment"># DecoderLayer</span></span><br><span class="line">                decoder_layer = TransformerDecoderLayer(d_model, nhead, dim_feedforward, dropout,</span><br><span class="line">                                                        activation, layer_norm_eps, batch_first, norm_first,</span><br><span class="line">                                                        **factory_kwargs)</span><br><span class="line">                decoder_norm = LayerNorm(d_model, eps=layer_norm_eps, **factory_kwargs)</span><br><span class="line">                <span class="comment"># Decoder block</span></span><br><span class="line">                self.decoder = TransformerDecoder(decoder_layer, num_decoder_layers, decoder_norm)</span><br><span class="line">            ...</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, </span></span><br><span class="line"><span class="params">                src: Tensor,  <span class="comment"># 源</span></span></span><br><span class="line"><span class="params">                tgt: Tensor,  <span class="comment"># 目标</span></span></span><br><span class="line"><span class="params">                src_mask: <span class="type">Optional</span>[Tensor] = <span class="literal">None</span>,  <span class="comment"># 源的mask(通过mask能还原原本句子)</span></span></span><br><span class="line"><span class="params">                tgt_mask: <span class="type">Optional</span>[Tensor] = <span class="literal">None</span>,  <span class="comment"># 目标的mask</span></span></span><br><span class="line"><span class="params">                memory_mask: <span class="type">Optional</span>[Tensor] = <span class="literal">None</span>, <span class="comment"># encoder输出mask</span></span></span><br><span class="line"><span class="params">                src_key_padding_mask: <span class="type">Optional</span>[Tensor] = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params">                tgt_key_padding_mask: <span class="type">Optional</span>[Tensor] = <span class="literal">None</span>, </span></span><br><span class="line"><span class="params">                memory_key_padding_mask: <span class="type">Optional</span>[Tensor] = <span class="literal">None</span></span></span><br><span class="line"><span class="params">                </span>) -&gt; Tensor:</span><br><span class="line">        <span class="string">r&quot;&quot;&quot;Take in and process masked source/target sequences.</span></span><br><span class="line"><span class="string">        Shape:</span></span><br><span class="line"><span class="string">            - src: :math:`(S, N, E)`, `(N, S, E)` if batch_first.</span></span><br><span class="line"><span class="string">            - tgt: :math:`(T, N, E)`, `(N, T, E)` if batch_first.</span></span><br><span class="line"><span class="string">            - src_mask: :math:`(S, S)`.</span></span><br><span class="line"><span class="string">            - tgt_mask: :math:`(T, T)`.</span></span><br><span class="line"><span class="string">            - memory_mask: :math:`(T, S)`.</span></span><br><span class="line"><span class="string">            - src_key_padding_mask: :math:`(N, S)`.</span></span><br><span class="line"><span class="string">            - tgt_key_padding_mask: :math:`(N, T)`.</span></span><br><span class="line"><span class="string">            - memory_key_padding_mask: :math:`(N, S)`.</span></span><br><span class="line"><span class="string">        - output: :math:`(T, N, E)`, `(N, T, E)` if batch_first.</span></span><br><span class="line"><span class="string">        Examples:</span></span><br><span class="line"><span class="string">            &gt;&gt;&gt; output = transformer_model(src, tgt, src_mask=src_mask, tgt_mask=tgt_mask)</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment"># 调用encoder和decoder得到结果</span></span><br><span class="line">        memory = self.encoder(src, mask=src_mask, src_key_padding_mask=src_key_padding_mask)</span><br><span class="line">        output = self.decoder(tgt, memory, tgt_mask=tgt_mask, memory_mask=memory_mask,</span><br><span class="line">                              tgt_key_padding_mask=tgt_key_padding_mask,</span><br><span class="line">                              memory_key_padding_mask=memory_key_padding_mask)</span><br><span class="line">        <span class="keyword">return</span> output</span><br></pre></td></tr></table></figure></div><div class="tab-pane" id="tr-2"><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">TransformerEncoder</span>(<span class="title class_ inherited__">Module</span>):</span><br><span class="line">    <span class="string">r&quot;&quot;&quot;TransformerEncoder is a stack of N encoder layers</span></span><br><span class="line"><span class="string">    Examples::</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; encoder_layer = nn.TransformerEncoderLayer(d_model=512, nhead=8)</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=6)</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; src = torch.rand(10, 32, 512)</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; out = transformer_encoder(src)</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    __constants__ = [<span class="string">&#x27;norm&#x27;</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, </span></span><br><span class="line"><span class="params">                 encoder_layer, <span class="comment"># 定义的encoder block</span></span></span><br><span class="line"><span class="params">                 num_layers, <span class="comment"># block数</span></span></span><br><span class="line"><span class="params">                 norm=<span class="literal">None</span> <span class="comment"># normalization方法</span></span></span><br><span class="line"><span class="params">                 </span>):</span><br><span class="line">        <span class="built_in">super</span>(TransformerEncoder, self).__init__()</span><br><span class="line">        self.layers = _get_clones(encoder_layer, num_layers)</span><br><span class="line">        self.num_layers = num_layers</span><br><span class="line">        self.norm = norm</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, </span></span><br><span class="line"><span class="params">                src: Tensor,  <span class="comment"># 源</span></span></span><br><span class="line"><span class="params">                mask: <span class="type">Optional</span>[Tensor] = <span class="literal">None</span>, <span class="comment"># 源的mask(通过mask能还原原本句子)</span></span></span><br><span class="line"><span class="params">                src_key_padding_mask: <span class="type">Optional</span>[Tensor] = <span class="literal">None</span></span></span><br><span class="line"><span class="params">                </span>) -&gt; Tensor:</span><br><span class="line">        <span class="string">r&quot;&quot;&quot;Pass the input through the encoder layers in turn.&quot;&quot;&quot;</span></span><br><span class="line">        output = src</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> mod <span class="keyword">in</span> self.layers:</span><br><span class="line">            output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.norm <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            output = self.norm(output)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> output</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">TransformerDecoder</span>(<span class="title class_ inherited__">Module</span>):</span><br><span class="line">    <span class="string">r&quot;&quot;&quot;TransformerDecoder is a stack of N decoder layers</span></span><br><span class="line"><span class="string">    Examples::</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; decoder_layer = nn.TransformerDecoderLayer(d_model=512, nhead=8)</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; transformer_decoder = nn.TransformerDecoder(decoder_layer, num_layers=6)</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; memory = torch.rand(10, 32, 512)</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; tgt = torch.rand(20, 32, 512)</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; out = transformer_decoder(tgt, memory)</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    __constants__ = [<span class="string">&#x27;norm&#x27;</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, </span></span><br><span class="line"><span class="params">                 decoder_layer, <span class="comment"># # 定义的decoder block</span></span></span><br><span class="line"><span class="params">                 num_layers,  <span class="comment"># block数</span></span></span><br><span class="line"><span class="params">                 norm=<span class="literal">None</span> <span class="comment"># normalization方法</span></span></span><br><span class="line"><span class="params">                 </span>):</span><br><span class="line">        <span class="built_in">super</span>(TransformerDecoder, self).__init__()</span><br><span class="line">        self.layers = _get_clones(decoder_layer, num_layers)</span><br><span class="line">        self.num_layers = num_layers</span><br><span class="line">        self.norm = norm</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, </span></span><br><span class="line"><span class="params">                tgt: Tensor, <span class="comment"># 目标</span></span></span><br><span class="line"><span class="params">                memory: Tensor, <span class="comment"># encoder输出(最后一层)</span></span></span><br><span class="line"><span class="params">                tgt_mask: <span class="type">Optional</span>[Tensor] = <span class="literal">None</span>, <span class="comment"># 目标的mask</span></span></span><br><span class="line"><span class="params">                memory_mask: <span class="type">Optional</span>[Tensor] = <span class="literal">None</span>, <span class="comment"># encoder输出mask</span></span></span><br><span class="line"><span class="params">                tgt_key_padding_mask: <span class="type">Optional</span>[Tensor] = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params">                memory_key_padding_mask: <span class="type">Optional</span>[Tensor] = <span class="literal">None</span></span></span><br><span class="line"><span class="params">                </span>) -&gt; Tensor:</span><br><span class="line">        <span class="string">r&quot;&quot;&quot;Pass the inputs (and mask) through the decoder layer in turn.&quot;&quot;&quot;</span></span><br><span class="line">        output = tgt</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> mod <span class="keyword">in</span> self.layers:</span><br><span class="line">            output = mod(output, memory, tgt_mask=tgt_mask,</span><br><span class="line">                         memory_mask=memory_mask,</span><br><span class="line">                         tgt_key_padding_mask=tgt_key_padding_mask,</span><br><span class="line">                         memory_key_padding_mask=memory_key_padding_mask)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.norm <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            output = self.norm(output)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> output</span><br></pre></td></tr></table></figure></div><div class="tab-pane" id="tr-3"><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">TransformerEncoderLayer</span>(<span class="title class_ inherited__">Module</span>):</span><br><span class="line">    <span class="string">r&quot;&quot;&quot;TransformerEncoderLayer is made up of self-attn and feedforward network.</span></span><br><span class="line"><span class="string">    Examples::</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; encoder_layer = nn.TransformerEncoderLayer(d_model=512, nhead=8)</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; src = torch.rand(10, 32, 512)</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; out = encoder_layer(src)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Alternatively, when ``batch_first`` is ``True``:</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; encoder_layer = nn.TransformerEncoderLayer(d_model=512, nhead=8, batch_first=True)</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; src = torch.rand(32, 10, 512)</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; out = encoder_layer(src)</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    __constants__ = [<span class="string">&#x27;batch_first&#x27;</span>, <span class="string">&#x27;norm_first&#x27;</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, </span></span><br><span class="line"><span class="params">                 d_model, <span class="comment"># 模型维度</span></span></span><br><span class="line"><span class="params">                 nhead, <span class="comment"># 头数</span></span></span><br><span class="line"><span class="params">                 dim_feedforward=<span class="number">2048</span>, <span class="comment"># FFN中间维度</span></span></span><br><span class="line"><span class="params">                 dropout=<span class="number">0.1</span>, </span></span><br><span class="line"><span class="params">                 activation=F.relu,</span></span><br><span class="line"><span class="params">                 layer_norm_eps=<span class="number">1e-5</span>, </span></span><br><span class="line"><span class="params">                 batch_first=<span class="literal">False</span>, </span></span><br><span class="line"><span class="params">                 norm_first=<span class="literal">False</span>,</span></span><br><span class="line"><span class="params">                 device=<span class="literal">None</span>, </span></span><br><span class="line"><span class="params">                 dtype=<span class="literal">None</span></span></span><br><span class="line"><span class="params">                 </span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">        factory_kwargs = &#123;<span class="string">&#x27;device&#x27;</span>: device, <span class="string">&#x27;dtype&#x27;</span>: dtype&#125;</span><br><span class="line">        <span class="built_in">super</span>(TransformerEncoderLayer, self).__init__()</span><br><span class="line">        self.self_attn = MultiheadAttention(d_model, nhead, dropout=dropout, batch_first=batch_first,</span><br><span class="line">                                            **factory_kwargs)</span><br><span class="line">        <span class="comment"># Implementation of Feedforward model</span></span><br><span class="line">        self.linear1 = Linear(d_model, dim_feedforward, **factory_kwargs) <span class="comment"># 先投射到2048维</span></span><br><span class="line">        self.dropout = Dropout(dropout) </span><br><span class="line">        self.linear2 = Linear(dim_feedforward, d_model, **factory_kwargs) <span class="comment"># 再投射到521维</span></span><br><span class="line"></span><br><span class="line">        self.norm_first = norm_first</span><br><span class="line">        self.norm1 = LayerNorm(d_model, eps=layer_norm_eps, **factory_kwargs)</span><br><span class="line">        self.norm2 = LayerNorm(d_model, eps=layer_norm_eps, **factory_kwargs)</span><br><span class="line">        self.dropout1 = Dropout(dropout)</span><br><span class="line">        self.dropout2 = Dropout(dropout)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Legacy string support for activation function.</span></span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">isinstance</span>(activation, <span class="built_in">str</span>):</span><br><span class="line">            self.activation = _get_activation_fn(activation)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.activation = activation</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__setstate__</span>(<span class="params">self, state</span>):</span><br><span class="line">        <span class="keyword">if</span> <span class="string">&#x27;activation&#x27;</span> <span class="keyword">not</span> <span class="keyword">in</span> state:</span><br><span class="line">            state[<span class="string">&#x27;activation&#x27;</span>] = F.relu</span><br><span class="line">        <span class="built_in">super</span>(TransformerEncoderLayer, self).__setstate__(state)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, </span></span><br><span class="line"><span class="params">                src: Tensor, <span class="comment"># 源</span></span></span><br><span class="line"><span class="params">                src_mask: <span class="type">Optional</span>[Tensor] = <span class="literal">None</span>, <span class="comment"># 源mask</span></span></span><br><span class="line"><span class="params">                src_key_padding_mask: <span class="type">Optional</span>[Tensor] = <span class="literal">None</span></span></span><br><span class="line"><span class="params">                </span>) -&gt; Tensor:</span><br><span class="line">        <span class="string">r&quot;&quot;&quot;Pass the input through the encoder layer.&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># see Fig. 1 of https://arxiv.org/pdf/2002.04745v1.pdf</span></span><br><span class="line"></span><br><span class="line">        x = src</span><br><span class="line">        <span class="keyword">if</span> self.norm_first:</span><br><span class="line">            x = x + self._sa_block(self.norm1(x), src_mask, src_key_padding_mask)</span><br><span class="line">            x = x + self._ff_block(self.norm2(x))</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># norm(x + self-attention block) -&gt; norm(x + feed forward block), 注意，此时的x是前面的输出</span></span><br><span class="line">            x = self.norm1(x + self._sa_block(x, src_mask, src_key_padding_mask))</span><br><span class="line">            x = self.norm2(x + self._ff_block(x))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment"># self-attention block</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_sa_block</span>(<span class="params">self, </span></span><br><span class="line"><span class="params">                  x: Tensor,</span></span><br><span class="line"><span class="params">                  attn_mask: <span class="type">Optional</span>[Tensor], </span></span><br><span class="line"><span class="params">                  key_padding_mask: <span class="type">Optional</span>[Tensor]</span></span><br><span class="line"><span class="params">                  </span>) -&gt; Tensor:</span><br><span class="line">        x = self.self_attn(x, x, x,</span><br><span class="line">                           attn_mask=attn_mask,</span><br><span class="line">                           key_padding_mask=key_padding_mask,</span><br><span class="line">                           need_weights=<span class="literal">False</span>)[<span class="number">0</span>]</span><br><span class="line">        <span class="keyword">return</span> self.dropout1(x)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># feed forward block</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_ff_block</span>(<span class="params">self, x: Tensor</span>) -&gt; Tensor:</span><br><span class="line">        x = self.linear2(self.dropout(self.activation(self.linear1(x))))</span><br><span class="line">        <span class="keyword">return</span> self.dropout2(x)</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">TransformerDecoderLayer</span>(<span class="title class_ inherited__">Module</span>):</span><br><span class="line">    <span class="string">r&quot;&quot;&quot;TransformerDecoderLayer is made up of self-attn, multi-head-attn and feedforward network.</span></span><br><span class="line"><span class="string">    Examples::</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; decoder_layer = nn.TransformerDecoderLayer(d_model=512, nhead=8)</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; memory = torch.rand(10, 32, 512)</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; tgt = torch.rand(20, 32, 512)</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; out = decoder_layer(tgt, memory)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Alternatively, when ``batch_first`` is ``True``:</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; decoder_layer = nn.TransformerDecoderLayer(d_model=512, nhead=8, batch_first=True)</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; memory = torch.rand(32, 10, 512)</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; tgt = torch.rand(32, 20, 512)</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; out = decoder_layer(tgt, memory)</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    __constants__ = [<span class="string">&#x27;batch_first&#x27;</span>, <span class="string">&#x27;norm_first&#x27;</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, </span></span><br><span class="line"><span class="params">                 d_model, </span></span><br><span class="line"><span class="params">                 nhead, </span></span><br><span class="line"><span class="params">                 dim_feedforward=<span class="number">2048</span>, </span></span><br><span class="line"><span class="params">                 dropout=<span class="number">0.1</span>, </span></span><br><span class="line"><span class="params">                 activation=F.relu,</span></span><br><span class="line"><span class="params">                 layer_norm_eps=<span class="number">1e-5</span>, </span></span><br><span class="line"><span class="params">                 batch_first=<span class="literal">False</span>, </span></span><br><span class="line"><span class="params">                 norm_first=<span class="literal">False</span>,</span></span><br><span class="line"><span class="params">                 device=<span class="literal">None</span>, </span></span><br><span class="line"><span class="params">                 dtype=<span class="literal">None</span></span></span><br><span class="line"><span class="params">                 </span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">        factory_kwargs = &#123;<span class="string">&#x27;device&#x27;</span>: device, <span class="string">&#x27;dtype&#x27;</span>: dtype&#125;</span><br><span class="line">        <span class="built_in">super</span>(TransformerDecoderLayer, self).__init__()</span><br><span class="line">        <span class="comment"># 2个多头注意力，一个self-attention，一个cross-attention</span></span><br><span class="line">        self.self_attn = MultiheadAttention(d_model, nhead, dropout=dropout, batch_first=batch_first,</span><br><span class="line">                                            **factory_kwargs)</span><br><span class="line">        self.multihead_attn = MultiheadAttention(d_model, nhead, dropout=dropout, batch_first=batch_first,</span><br><span class="line">                                                 **factory_kwargs)</span><br><span class="line">        <span class="comment"># Implementation of Feedforward model</span></span><br><span class="line">        self.linear1 = Linear(d_model, dim_feedforward, **factory_kwargs)</span><br><span class="line">        self.dropout = Dropout(dropout)</span><br><span class="line">        self.linear2 = Linear(dim_feedforward, d_model, **factory_kwargs)</span><br><span class="line"></span><br><span class="line">        self.norm_first = norm_first</span><br><span class="line">        self.norm1 = LayerNorm(d_model, eps=layer_norm_eps, **factory_kwargs)</span><br><span class="line">        self.norm2 = LayerNorm(d_model, eps=layer_norm_eps, **factory_kwargs)</span><br><span class="line">        self.norm3 = LayerNorm(d_model, eps=layer_norm_eps, **factory_kwargs)</span><br><span class="line">        self.dropout1 = Dropout(dropout)</span><br><span class="line">        self.dropout2 = Dropout(dropout)</span><br><span class="line">        self.dropout3 = Dropout(dropout)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Legacy string support for activation function.</span></span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">isinstance</span>(activation, <span class="built_in">str</span>):</span><br><span class="line">            self.activation = _get_activation_fn(activation)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.activation = activation</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__setstate__</span>(<span class="params">self, state</span>):</span><br><span class="line">        <span class="keyword">if</span> <span class="string">&#x27;activation&#x27;</span> <span class="keyword">not</span> <span class="keyword">in</span> state:</span><br><span class="line">            state[<span class="string">&#x27;activation&#x27;</span>] = F.relu</span><br><span class="line">        <span class="built_in">super</span>(TransformerDecoderLayer, self).__setstate__(state)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, </span></span><br><span class="line"><span class="params">                tgt: Tensor, </span></span><br><span class="line"><span class="params">                memory: Tensor, </span></span><br><span class="line"><span class="params">                tgt_mask: <span class="type">Optional</span>[Tensor] = <span class="literal">None</span>, </span></span><br><span class="line"><span class="params">                memory_mask: <span class="type">Optional</span>[Tensor] = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params">                tgt_key_padding_mask: <span class="type">Optional</span>[Tensor] = <span class="literal">None</span>, </span></span><br><span class="line"><span class="params">                memory_key_padding_mask: <span class="type">Optional</span>[Tensor] = <span class="literal">None</span></span></span><br><span class="line"><span class="params">                </span>) -&gt; Tensor:</span><br><span class="line">        <span class="string">r&quot;&quot;&quot;Pass the inputs (and mask) through the decoder layer.</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            tgt: the sequence to the decoder layer (required).</span></span><br><span class="line"><span class="string">            memory: the sequence from the last layer of the encoder (required).</span></span><br><span class="line"><span class="string">            tgt_mask: the mask for the tgt sequence (optional).</span></span><br><span class="line"><span class="string">            memory_mask: the mask for the memory sequence (optional).</span></span><br><span class="line"><span class="string">            tgt_key_padding_mask: the mask for the tgt keys per batch (optional).</span></span><br><span class="line"><span class="string">            memory_key_padding_mask: the mask for the memory keys per batch (optional).</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment"># see Fig. 1 of https://arxiv.org/pdf/2002.04745v1.pdf</span></span><br><span class="line"></span><br><span class="line">        x = tgt</span><br><span class="line">        <span class="keyword">if</span> self.norm_first:</span><br><span class="line">            x = x + self._sa_block(self.norm1(x), tgt_mask, tgt_key_padding_mask)</span><br><span class="line">            x = x + self._mha_block(self.norm2(x), memory, memory_mask, memory_key_padding_mask)</span><br><span class="line">            x = x + self._ff_block(self.norm3(x))</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># 先self-attention</span></span><br><span class="line">            x = self.norm1(x + self._sa_block(x, tgt_mask, tgt_key_padding_mask))</span><br><span class="line">            <span class="comment"># 再cross-attention</span></span><br><span class="line">            x = self.norm2(x + self._mha_block(x, memory, memory_mask, memory_key_padding_mask))</span><br><span class="line">            <span class="comment"># 再FFN</span></span><br><span class="line">            x = self.norm3(x + self._ff_block(x))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment"># self-attention block</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_sa_block</span>(<span class="params">self, x: Tensor,</span></span><br><span class="line"><span class="params">                  attn_mask: <span class="type">Optional</span>[Tensor], key_padding_mask: <span class="type">Optional</span>[Tensor]</span>) -&gt; Tensor:</span><br><span class="line">        x = self.self_attn(x, x, x,</span><br><span class="line">                           attn_mask=attn_mask,</span><br><span class="line">                           key_padding_mask=key_padding_mask,</span><br><span class="line">                           need_weights=<span class="literal">False</span>)[<span class="number">0</span>]</span><br><span class="line">        <span class="keyword">return</span> self.dropout1(x)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># multihead attention block</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_mha_block</span>(<span class="params">self, x: Tensor, mem: Tensor,</span></span><br><span class="line"><span class="params">                   attn_mask: <span class="type">Optional</span>[Tensor], key_padding_mask: <span class="type">Optional</span>[Tensor]</span>) -&gt; Tensor:</span><br><span class="line">        x = self.multihead_attn(x, mem, mem,</span><br><span class="line">                                attn_mask=attn_mask,</span><br><span class="line">                                key_padding_mask=key_padding_mask,</span><br><span class="line">                                need_weights=<span class="literal">False</span>)[<span class="number">0</span>]</span><br><span class="line">        <span class="keyword">return</span> self.dropout2(x)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># feed forward block</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_ff_block</span>(<span class="params">self, x: Tensor</span>) -&gt; Tensor:</span><br><span class="line">        x = self.linear2(self.dropout(self.activation(self.linear1(x))))</span><br><span class="line">        <span class="keyword">return</span> self.dropout3(x)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">_get_clones</span>(<span class="params">module, N</span>):</span><br><span class="line">    <span class="keyword">return</span> ModuleList([copy.deepcopy(module) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(N)])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">_get_activation_fn</span>(<span class="params">activation</span>):</span><br><span class="line">    <span class="keyword">if</span> activation == <span class="string">&quot;relu&quot;</span>:</span><br><span class="line">        <span class="keyword">return</span> F.relu</span><br><span class="line">    <span class="keyword">elif</span> activation == <span class="string">&quot;gelu&quot;</span>:</span><br><span class="line">        <span class="keyword">return</span> F.gelu</span><br><span class="line"></span><br><span class="line">    <span class="keyword">raise</span> RuntimeError(<span class="string">&quot;activation should be relu/gelu, not &#123;&#125;&quot;</span>.<span class="built_in">format</span>(activation))</span><br><span class="line"></span><br></pre></td></tr></table></figure></div><div class="tab-pane" id="tr-4"><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">MultiheadAttention</span>(<span class="title class_ inherited__">Module</span>):</span><br><span class="line">    <span class="string">r&quot;&quot;&quot;Allows the model to jointly attend to information from different representation subspaces.</span></span><br><span class="line"><span class="string">    Examples::</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; multihead_attn = nn.MultiheadAttention(embed_dim, num_heads)</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; attn_output, attn_output_weights = multihead_attn(query, key, value)</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    __constants__ = [<span class="string">&#x27;batch_first&#x27;</span>]</span><br><span class="line">    bias_k: <span class="type">Optional</span>[torch.Tensor]</span><br><span class="line">    bias_v: <span class="type">Optional</span>[torch.Tensor]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, embed_dim, num_heads, dropout=<span class="number">0.</span>, bias=<span class="literal">True</span>, add_bias_kv=<span class="literal">False</span>, add_zero_attn=<span class="literal">False</span>,</span></span><br><span class="line"><span class="params">                 kdim=<span class="literal">None</span>, vdim=<span class="literal">None</span>, batch_first=<span class="literal">False</span>, device=<span class="literal">None</span>, dtype=<span class="literal">None</span></span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">        factory_kwargs = &#123;<span class="string">&#x27;device&#x27;</span>: device, <span class="string">&#x27;dtype&#x27;</span>: dtype&#125;</span><br><span class="line">        <span class="built_in">super</span>(MultiheadAttention, self).__init__()</span><br><span class="line">        self.embed_dim = embed_dim</span><br><span class="line">        self.kdim = kdim <span class="keyword">if</span> kdim <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">else</span> embed_dim</span><br><span class="line">        self.vdim = vdim <span class="keyword">if</span> vdim <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">else</span> embed_dim</span><br><span class="line">        self._qkv_same_embed_dim = self.kdim == embed_dim <span class="keyword">and</span> self.vdim == embed_dim</span><br><span class="line"></span><br><span class="line">        self.num_heads = num_heads</span><br><span class="line">        self.dropout = dropout</span><br><span class="line">        self.batch_first = batch_first</span><br><span class="line">        self.head_dim = embed_dim // num_heads</span><br><span class="line">        <span class="keyword">assert</span> self.head_dim * num_heads == self.embed_dim, <span class="string">&quot;embed_dim must be divisible by num_heads&quot;</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self._qkv_same_embed_dim <span class="keyword">is</span> <span class="literal">False</span>:</span><br><span class="line">            self.q_proj_weight = Parameter(torch.empty((embed_dim, embed_dim), **factory_kwargs))</span><br><span class="line">            self.k_proj_weight = Parameter(torch.empty((embed_dim, self.kdim), **factory_kwargs))</span><br><span class="line">            self.v_proj_weight = Parameter(torch.empty((embed_dim, self.vdim), **factory_kwargs))</span><br><span class="line">            self.register_parameter(<span class="string">&#x27;in_proj_weight&#x27;</span>, <span class="literal">None</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.in_proj_weight = Parameter(torch.empty((<span class="number">3</span> * embed_dim, embed_dim), **factory_kwargs))</span><br><span class="line">            self.register_parameter(<span class="string">&#x27;q_proj_weight&#x27;</span>, <span class="literal">None</span>)</span><br><span class="line">            self.register_parameter(<span class="string">&#x27;k_proj_weight&#x27;</span>, <span class="literal">None</span>)</span><br><span class="line">            self.register_parameter(<span class="string">&#x27;v_proj_weight&#x27;</span>, <span class="literal">None</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> bias:</span><br><span class="line">            self.in_proj_bias = Parameter(torch.empty(<span class="number">3</span> * embed_dim, **factory_kwargs))</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.register_parameter(<span class="string">&#x27;in_proj_bias&#x27;</span>, <span class="literal">None</span>)</span><br><span class="line">        self.out_proj = NonDynamicallyQuantizableLinear(embed_dim, embed_dim, bias=bias, **factory_kwargs)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> add_bias_kv:</span><br><span class="line">            self.bias_k = Parameter(torch.empty((<span class="number">1</span>, <span class="number">1</span>, embed_dim), **factory_kwargs))</span><br><span class="line">            self.bias_v = Parameter(torch.empty((<span class="number">1</span>, <span class="number">1</span>, embed_dim), **factory_kwargs))</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.bias_k = self.bias_v = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">        self.add_zero_attn = add_zero_attn</span><br><span class="line"></span><br><span class="line">        self._reset_parameters()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_reset_parameters</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">if</span> self._qkv_same_embed_dim:</span><br><span class="line">            xavier_uniform_(self.in_proj_weight)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            xavier_uniform_(self.q_proj_weight)</span><br><span class="line">            xavier_uniform_(self.k_proj_weight)</span><br><span class="line">            xavier_uniform_(self.v_proj_weight)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.in_proj_bias <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            constant_(self.in_proj_bias, <span class="number">0.</span>)</span><br><span class="line">            constant_(self.out_proj.bias, <span class="number">0.</span>)</span><br><span class="line">        <span class="keyword">if</span> self.bias_k <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            xavier_normal_(self.bias_k)</span><br><span class="line">        <span class="keyword">if</span> self.bias_v <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            xavier_normal_(self.bias_v)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__setstate__</span>(<span class="params">self, state</span>):</span><br><span class="line">        <span class="comment"># Support loading old MultiheadAttention checkpoints generated by v1.1.0</span></span><br><span class="line">        <span class="keyword">if</span> <span class="string">&#x27;_qkv_same_embed_dim&#x27;</span> <span class="keyword">not</span> <span class="keyword">in</span> state:</span><br><span class="line">            state[<span class="string">&#x27;_qkv_same_embed_dim&#x27;</span>] = <span class="literal">True</span></span><br><span class="line"></span><br><span class="line">        <span class="built_in">super</span>(MultiheadAttention, self).__setstate__(state)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, query: Tensor, key: Tensor, value: Tensor, key_padding_mask: <span class="type">Optional</span>[Tensor] = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params">                need_weights: <span class="built_in">bool</span> = <span class="literal">True</span>, attn_mask: <span class="type">Optional</span>[Tensor] = <span class="literal">None</span></span>) -&gt; <span class="type">Tuple</span>[Tensor, <span class="type">Optional</span>[Tensor]]:</span><br><span class="line">        <span class="keyword">if</span> self.batch_first:</span><br><span class="line">            query, key, value = [x.transpose(<span class="number">1</span>, <span class="number">0</span>) <span class="keyword">for</span> x <span class="keyword">in</span> (query, key, value)]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> self._qkv_same_embed_dim:</span><br><span class="line">            attn_output, attn_output_weights = F.multi_head_attention_forward(</span><br><span class="line">                query, key, value, self.embed_dim, self.num_heads,</span><br><span class="line">                self.in_proj_weight, self.in_proj_bias,</span><br><span class="line">                self.bias_k, self.bias_v, self.add_zero_attn,</span><br><span class="line">                self.dropout, self.out_proj.weight, self.out_proj.bias,</span><br><span class="line">                training=self.training,</span><br><span class="line">                key_padding_mask=key_padding_mask, need_weights=need_weights,</span><br><span class="line">                attn_mask=attn_mask, use_separate_proj_weight=<span class="literal">True</span>,</span><br><span class="line">                q_proj_weight=self.q_proj_weight, k_proj_weight=self.k_proj_weight,</span><br><span class="line">                v_proj_weight=self.v_proj_weight)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            attn_output, attn_output_weights = F.multi_head_attention_forward(</span><br><span class="line">                query, key, value, self.embed_dim, self.num_heads,</span><br><span class="line">                self.in_proj_weight, self.in_proj_bias,</span><br><span class="line">                self.bias_k, self.bias_v, self.add_zero_attn,</span><br><span class="line">                self.dropout, self.out_proj.weight, self.out_proj.bias,</span><br><span class="line">                training=self.training,</span><br><span class="line">                key_padding_mask=key_padding_mask, need_weights=need_weights,</span><br><span class="line">                attn_mask=attn_mask)</span><br><span class="line">        <span class="keyword">if</span> self.batch_first:</span><br><span class="line">            <span class="keyword">return</span> attn_output.transpose(<span class="number">1</span>, <span class="number">0</span>), attn_output_weights</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> attn_output, attn_output_weights</span><br></pre></td></tr></table></figure></div><div class="tab-pane" id="tr-5"><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"></span><br></pre></td></tr></table></figure></div></div></div></p>
<h1 id="参考资料">参考资料</h1>
<p><span class="exturl" data-url="aHR0cHM6Ly93d3cuam1sci5vcmcvcGFwZXJzL3ZvbHVtZTE1L3NyaXZhc3RhdmExNGEvc3JpdmFzdGF2YTE0YS5wZGY/dXRtX2NvbnRlbnQ9YnVmZmVyNzliNDMmdXRtX21lZGl1bT1zb2NpYWwmdXRtX3NvdXJjZT10d2l0dGVyLmNvbSZ1dG1fY2FtcGFpZ249YnVmZmVyLA==">Dropout:
A Simple Way to Prevent Neural Networks from Overfitting<i class="fa fa-external-link-alt"></i></span><br />
<span class="exturl" data-url="aHR0cHM6Ly9wcm9jZWVkaW5ncy5uZXVyaXBzLmNjL3BhcGVyLzIwMjEvZmlsZS81YTY2YjkyMDBmMjlhYzNmYTBhZTI0NGNjMmE1MWIzOS1QYXBlci5wZGY=">R-Drop:
Regularized Dropout for Neural Networks<i class="fa fa-external-link-alt"></i></span><br />
<span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzIxMDEuMDM2OTcucGRm">RepVGG: Making VGG-style
ConvNets Great Again<i class="fa fa-external-link-alt"></i></span><br />
<span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi41M3l1LmNvbS9wZGYvMjIwMS4wOTc5Mi5wZGY=">Patches Are All You
Need?<i class="fa fa-external-link-alt"></i></span><br />
<span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzE3MDYuMDM3NjIucGRm">Attention Is All You
Need<i class="fa fa-external-link-alt"></i></span></p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://soundmemories.github.io/2021/06/13/NLP/00.TensorFlow/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avstar.png">
      <meta itemprop="name" content="SoundMemories">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="SoundMemories">
      <meta itemprop="description" content="今日事，今日毕">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | SoundMemories">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2021/06/13/NLP/00.TensorFlow/" class="post-title-link" itemprop="url">TensorFlow2.0</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2021-06-13 00:00:00" itemprop="dateCreated datePublished" datetime="2021-06-13T00:00:00+08:00">2021-06-13</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/NLP/" itemprop="url" rel="index"><span itemprop="name">NLP</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>2.3k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>8 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>TensorFlow程序 = 张量数据结构 +
计算图算法语言，<strong>张量</strong>和<strong>计算图</strong>是TensorFlow的核心概念。<br />
Tensorflow的基本数据结构是张量Tensor，张量即多维数组，这和numpy中的array很类似。从行为特性来看，有两种类型的张量，常量constant和变量Variable。常量的值在计算图中不可以被重新赋值，变量可以在计算图中用assign等算子重新赋值。</p>
<h1 id="常量张量">常量张量</h1>
<p>张量的数据类型和numpy.array基本一一对应。<br />
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">i = tf.constant(<span class="number">1</span>) <span class="comment"># tf.int32 </span></span><br><span class="line">l = tf.constant(<span class="number">1</span>, dtype=tf.int64) <span class="comment"># tf.int64 </span></span><br><span class="line">f = tf.constant(<span class="number">1.23</span>) <span class="comment">#tf.float32 </span></span><br><span class="line">d = tf.constant(<span class="number">3.14</span>, dtype=tf.double) <span class="comment"># tf.double </span></span><br><span class="line">s = tf.constant(<span class="string">&quot;hello world&quot;</span>) <span class="comment"># tf.string</span></span><br><span class="line">b = tf.constant(<span class="literal">True</span>) <span class="comment">#tf.bool</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(tf.int64 == np.int64) </span><br><span class="line"><span class="built_in">print</span>(tf.<span class="built_in">bool</span> == np.<span class="built_in">bool</span>)</span><br><span class="line"><span class="built_in">print</span>(tf.double == np.float64)</span><br><span class="line"><span class="built_in">print</span>(tf.string == np.unicode) <span class="comment"># tf.string类型和np.unicode类型不等价</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">True</span></span><br><span class="line"><span class="string">True</span></span><br><span class="line"><span class="string">True</span></span><br><span class="line"><span class="string">False</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="comment"># 可以用tf.cast改变张量的数据类型。</span></span><br><span class="line">h = tf.constant([<span class="number">123</span>,<span class="number">456</span>], dtype=tf.int32)</span><br><span class="line">f = tf.cast(h, tf.float32)</span><br><span class="line"><span class="built_in">print</span>(h.dtype, f.dtype)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">&lt;dtype: &#x27;int32&#x27;&gt; &lt;dtype: &#x27;float32&#x27;&gt;</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="comment"># 可以用numpy方法将tensorflow中的张量转化成numpy中的张量。</span></span><br><span class="line"><span class="comment"># 可以用shape方法查看张量的尺寸。</span></span><br><span class="line">y = tf.constant([[<span class="number">1.0</span>,<span class="number">2.0</span>],[<span class="number">3.0</span>,<span class="number">4.0</span>]])</span><br><span class="line"><span class="built_in">print</span>(y.numpy()) <span class="comment">#转换成np.array</span></span><br><span class="line"><span class="built_in">print</span>(y.shape)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">[[1. 2.]</span></span><br><span class="line"><span class="string"> [3. 4.]]</span></span><br><span class="line"><span class="string">(2, 2)</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="comment"># 不同类型的数据可以用不同维度(rank)的张量来表示。</span></span><br><span class="line">vector = tf.constant([<span class="number">1.0</span>,<span class="number">2.0</span>,<span class="number">3.0</span>,<span class="number">4.0</span>]) <span class="comment">#向量，1维张量</span></span><br><span class="line"><span class="built_in">print</span>(tf.rank(vector))</span><br><span class="line"><span class="built_in">print</span>(vector.numpy().ndim) <span class="comment"># tf.rank的作用和numpy的ndim方法相同</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">tf.Tensor(1, shape=(), dtype=int32)</span></span><br><span class="line"><span class="string">1</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="comment"># 字符串解码</span></span><br><span class="line">u = tf.constant(<span class="string">u&quot;你好 世界&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(u.numpy())  </span><br><span class="line"><span class="built_in">print</span>(u.numpy().decode(<span class="string">&quot;utf-8&quot;</span>))</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">b&#x27;\xe4\xbd\xa0\xe5\xa5\xbd \xe4\xb8\x96\xe7\x95\x8c&#x27;</span></span><br><span class="line"><span class="string">你好 世界</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure></p>
<h1 id="变量张量">变量张量</h1>
<p>模型中需要被训练的参数一般被设置成变量。<br />
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 常量值不可以改变，常量的重新赋值相当于创造新的内存空间</span></span><br><span class="line">c = tf.constant([<span class="number">1.0</span>,<span class="number">2.0</span>])</span><br><span class="line"><span class="built_in">print</span>(c)</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">id</span>(c))</span><br><span class="line">c = c + tf.constant([<span class="number">1.0</span>,<span class="number">1.0</span>])</span><br><span class="line"><span class="built_in">print</span>(c)</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">id</span>(c))</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">tf.Tensor([1. 2.], shape=(2,), dtype=float32)</span></span><br><span class="line"><span class="string">5276289568</span></span><br><span class="line"><span class="string">tf.Tensor([2. 3.], shape=(2,), dtype=float32)</span></span><br><span class="line"><span class="string">5276290240</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="comment"># 变量的值可以改变，可以通过assign, assign_add等方法给变量重新赋值</span></span><br><span class="line">v = tf.Variable([<span class="number">1.0</span>,<span class="number">2.0</span>],name = <span class="string">&quot;v&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(v)</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">id</span>(v))</span><br><span class="line">v.assign_add([<span class="number">1.0</span>,<span class="number">1.0</span>])</span><br><span class="line"><span class="built_in">print</span>(v)</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">id</span>(v))</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">&lt;tf.Variable &#x27;v:0&#x27; shape=(2,) dtype=float32, numpy=array([1., 2.], dtype=float32)&gt;</span></span><br><span class="line"><span class="string">5276259888</span></span><br><span class="line"><span class="string">&lt;tf.Variable &#x27;v:0&#x27; shape=(2,) dtype=float32, numpy=array([2., 3.], dtype=float32)&gt;</span></span><br><span class="line"><span class="string">5276259888</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure></p>
<h1 id="计算图">计算图</h1>
<p>TensorFlow有静态计算图，动态计算图，以及Autograph。<br />
在TensorFlow1.0时代，采用的是静态计算图，需要先使用TensorFlow的各种算子创建计算图，然后再开启一个会话Session，显式执行计算图。<br />
在TensorFlow2.0时代，采用的是动态计算图，即每使用一个算子后，该算子会被动态加入到隐含的默认计算图中立即执行得到结果，而无需开启Session。<br />
使用动态计算图即Eager
Excution的好处是方便调试程序，它会让TensorFlow代码的表现和Python原生代码的表现一样，写起来就像写numpy一样，各种日志打印，控制流全部都是可以使用的。缺点是运行效率相对会低一些。因为使用动态图会有许多次Python进程和TensorFlow的C++进程之间的通信。而静态计算图构建完成之后几乎全部在TensorFlow内核上使用C++代码执行，效率更高。此外静态图会对计算步骤进行一定的优化，剪去和结果无关的计算步骤。</p>
<p>如果需要在TensorFlow2.0中使用静态图，可以使用@tf.function装饰器将普通Python函数转换成对应的TensorFlow计算图构建代码。运行该函数就相当于在TensorFlow1.0中用Session执行代码。使用tf.function构建静态图的方式叫做
Autograph.</p>
<p><strong>计算图构成</strong>：<br />
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">计算图由节点(nodes)和线(edges)组成。</span><br><span class="line">节点：表示操作符Operator，或者称为算子，线表示计算间的依赖。</span><br><span class="line">实线表示有数据传递依赖，传递的数据即张量。虚线通常可以表示控制依赖，即执行先后顺序。</span><br></pre></td></tr></table></figure><br />
<img
src="https://lyhue1991.github.io/eat_tensorflow2_in_30_days/data/strjoin_graph.png" /></p>
<p><strong>静态计算图</strong><br />
在TensorFlow1.0中，使用静态计算图分两步，第一步定义计算图，第二步在会话中执行计算图。<br />
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="comment">#定义计算图</span></span><br><span class="line">g = tf.Graph()</span><br><span class="line"><span class="keyword">with</span> g.as_default():</span><br><span class="line">    <span class="comment">#placeholder为占位符，执行会话时候指定填充对象</span></span><br><span class="line">    x = tf.placeholder(name=<span class="string">&#x27;x&#x27;</span>, shape=[], dtype=tf.string)  </span><br><span class="line">    y = tf.placeholder(name=<span class="string">&#x27;y&#x27;</span>, shape=[], dtype=tf.string)</span><br><span class="line">    z = tf.string_join([x,y],name = <span class="string">&#x27;join&#x27;</span>,separator=<span class="string">&#x27; &#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#执行计算图</span></span><br><span class="line"><span class="keyword">with</span> tf.Session(graph = g) <span class="keyword">as</span> sess:</span><br><span class="line">    <span class="built_in">print</span>(sess.run(fetches = z,feed_dict = &#123;x:<span class="string">&quot;hello&quot;</span>,y:<span class="string">&quot;world&quot;</span>&#125;))</span><br></pre></td></tr></table></figure><br />
TensorFlow2.0为了确保对老版本tensorflow项目的兼容性，在tf.compat.v1子模块中保留了对TensorFlow1.0那种静态计算图构建风格的支持。<br />
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">g = tf.compat.v1.Graph()</span><br><span class="line"><span class="keyword">with</span> g.as_default():</span><br><span class="line">    x = tf.compat.v1.placeholder(name=<span class="string">&#x27;x&#x27;</span>, shape=[], dtype=tf.string)</span><br><span class="line">    y = tf.compat.v1.placeholder(name=<span class="string">&#x27;y&#x27;</span>, shape=[], dtype=tf.string)</span><br><span class="line">    z = tf.strings.join([x,y],name = <span class="string">&quot;join&quot;</span>,separator = <span class="string">&quot; &quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.compat.v1.Session(graph = g) <span class="keyword">as</span> sess:</span><br><span class="line">    <span class="comment"># fetches的结果非常像一个函数的返回值，而feed_dict中的占位符相当于函数的参数序列。</span></span><br><span class="line">    result = sess.run(fetches = z,feed_dict = &#123;x:<span class="string">&quot;hello&quot;</span>,y:<span class="string">&quot;world&quot;</span>&#125;)</span><br><span class="line">    <span class="built_in">print</span>(result)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">b&#x27;hello world&#x27;</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><br />
<strong>动态计算图</strong><br />
TensorFlow2.0中，使用的是<strong>动态计算图</strong>和<strong>Autograph</strong>。<br />
动态计算图已经不区分计算图的定义和执行了，而是定义后立即执行。因此称之为
Eager
Excution。Eager这个英文单词的原意是"迫不及待的"，也就是立即执行的意思。<br />
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 动态计算图在每个算子处都进行构建，构建后立即执行</span></span><br><span class="line">x = tf.constant(<span class="string">&quot;hello&quot;</span>)</span><br><span class="line">y = tf.constant(<span class="string">&quot;world&quot;</span>)</span><br><span class="line">z = tf.strings.join([x,y],separator=<span class="string">&quot; &quot;</span>)</span><br><span class="line"></span><br><span class="line">tf.<span class="built_in">print</span>(z)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">hello world</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="comment"># 可以将动态计算图代码的输入和输出关系封装成函数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">strjoin</span>(<span class="params">x,y</span>):</span><br><span class="line">    z =  tf.strings.join([x,y],separator = <span class="string">&quot; &quot;</span>)</span><br><span class="line">    tf.<span class="built_in">print</span>(z)</span><br><span class="line">    <span class="keyword">return</span> z</span><br><span class="line"></span><br><span class="line">result = strjoin(tf.constant(<span class="string">&quot;hello&quot;</span>),tf.constant(<span class="string">&quot;world&quot;</span>))</span><br><span class="line"><span class="built_in">print</span>(result)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">hello world</span></span><br><span class="line"><span class="string">tf.Tensor(b&#x27;hello world&#x27;, shape=(), dtype=string)</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><br />
<strong>Autograph</strong><br />
在TensorFlow2.0中，动态计算图运行效率相对较低，可以用@tf.function装饰器将普通Python函数转换成和TensorFlow1.0对应的静态计算图构建代码。如果采用Autograph的方式使用计算图，第一步定义计算图变成了定义函数，第二步执行计算图变成了调用函数。不需要使用会话了，一些都像原始的Python语法一样自然。<br />
实践中，一般会先用动态计算图调试代码，然后在需要提高性能的的地方利用@tf.function切换成Autograph获得更高的效率。<br />
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用autograph构建静态图</span></span><br><span class="line"></span><br><span class="line"><span class="meta">@tf.function</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">strjoin</span>(<span class="params">x,y</span>):</span><br><span class="line">    z =  tf.strings.join([x,y],separator = <span class="string">&quot; &quot;</span>)</span><br><span class="line">    tf.<span class="built_in">print</span>(z)</span><br><span class="line">    <span class="keyword">return</span> z</span><br><span class="line"></span><br><span class="line">result = strjoin(tf.constant(<span class="string">&quot;hello&quot;</span>),tf.constant(<span class="string">&quot;world&quot;</span>))</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(result)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">hello world</span></span><br><span class="line"><span class="string">tf.Tensor(b&#x27;hello world&#x27;, shape=(), dtype=string)</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="keyword">import</span> datetime</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建日志</span></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line">stamp = datetime.datetime.now().strftime(<span class="string">&quot;%Y%m%d-%H%M%S&quot;</span>)</span><br><span class="line">logdir = os.path.join(<span class="string">&#x27;data&#x27;</span>, <span class="string">&#x27;autograph&#x27;</span>, stamp)</span><br><span class="line"></span><br><span class="line"><span class="comment">## 在 Python3 下建议使用 pathlib 修正各操作系统的路径</span></span><br><span class="line"><span class="comment"># from pathlib import Path</span></span><br><span class="line"><span class="comment"># stamp = datetime.datetime.now().strftime(&quot;%Y%m%d-%H%M%S&quot;)</span></span><br><span class="line"><span class="comment"># logdir = str(Path(&#x27;../../data/autograph/&#x27; + stamp))</span></span><br><span class="line"></span><br><span class="line">writer = tf.summary.create_file_writer(logdir)</span><br><span class="line"></span><br><span class="line"><span class="comment">#开启autograph跟踪</span></span><br><span class="line">tf.summary.trace_on(graph=<span class="literal">True</span>, profiler=<span class="literal">True</span>) </span><br><span class="line"></span><br><span class="line"><span class="comment">#执行autograph</span></span><br><span class="line">result = strjoin(<span class="string">&quot;hello&quot;</span>,<span class="string">&quot;world&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#将计算图信息写入日志</span></span><br><span class="line"><span class="keyword">with</span> writer.as_default():</span><br><span class="line">    tf.summary.trace_export(</span><br><span class="line">        name=<span class="string">&quot;autograph&quot;</span>,</span><br><span class="line">        step=<span class="number">0</span>,</span><br><span class="line">        profiler_outdir=logdir)</span><br><span class="line"></span><br><span class="line"><span class="comment">#启动 tensorboard在jupyter中的魔法命令</span></span><br><span class="line">%load_ext tensorboard</span><br><span class="line"></span><br><span class="line"><span class="comment">#启动tensorboard</span></span><br><span class="line">%tensorboard --logdir ../../data/autograph/</span><br></pre></td></tr></table></figure></p>
<h1 id="自动微分">自动微分</h1>
<p>Tensorflow一般使用梯度磁带tf.GradientTape来记录正向运算过程，然后反播磁带自动得到梯度值。这种利用tf.GradientTape求微分的方法叫做Tensorflow的自动微分机制。<br />
<div class="tabs" id="one"><ul class="nav-tabs"><li class="tab active"><a href="#one-1">利用梯度磁带求导数</a></li><li class="tab"><a href="#one-2">利用梯度磁带和优化器求最小值</a></li></ul><div class="tab-content"><div class="tab-pane active" id="one-1"><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np </span><br><span class="line"></span><br><span class="line"><span class="comment"># f(x) = a*x**2 + b*x + c的导数</span></span><br><span class="line">x = tf.Variable(<span class="number">0.0</span>,name=<span class="string">&quot;x&quot;</span>,dtype=tf.float32)</span><br><span class="line">a = tf.constant(<span class="number">1.0</span>)</span><br><span class="line">b = tf.constant(-<span class="number">2.0</span>)</span><br><span class="line">c = tf.constant(<span class="number">1.0</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.GradientTape() <span class="keyword">as</span> tape:</span><br><span class="line">    y = a*tf.<span class="built_in">pow</span>(x,<span class="number">2</span>) + b*x + c</span><br><span class="line"></span><br><span class="line">dy_dx = tape.gradient(y,x)</span><br><span class="line"><span class="built_in">print</span>(dy_dx)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">tf.Tensor(-2.0, shape=(), dtype=float32)</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="comment"># 对常量张量也可以求导，需要增加watch</span></span><br><span class="line"><span class="keyword">with</span> tf.GradientTape() <span class="keyword">as</span> tape:</span><br><span class="line">    tape.watch([a,b,c])</span><br><span class="line">    y = a*tf.<span class="built_in">pow</span>(x,<span class="number">2</span>) + b*x + c</span><br><span class="line"></span><br><span class="line">dy_dx,dy_da,dy_db,dy_dc = tape.gradient(y,[x,a,b,c])</span><br><span class="line"><span class="built_in">print</span>(dy_da)</span><br><span class="line"><span class="built_in">print</span>(dy_dc)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">tf.Tensor(0.0, shape=(), dtype=float32)</span></span><br><span class="line"><span class="string">tf.Tensor(1.0, shape=(), dtype=float32)</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="comment"># 可以求二阶导数</span></span><br><span class="line"><span class="keyword">with</span> tf.GradientTape() <span class="keyword">as</span> tape2:</span><br><span class="line">    <span class="keyword">with</span> tf.GradientTape() <span class="keyword">as</span> tape1:   </span><br><span class="line">        y = a*tf.<span class="built_in">pow</span>(x,<span class="number">2</span>) + b*x + c</span><br><span class="line">    dy_dx = tape1.gradient(y,x)   </span><br><span class="line">dy2_dx2 = tape2.gradient(dy_dx,x)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(dy2_dx2)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">tf.Tensor(2.0, shape=(), dtype=float32)</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="comment"># 可以在autograph中使用</span></span><br><span class="line"><span class="meta">@tf.function</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">f</span>(<span class="params">x</span>):   </span><br><span class="line">    a = tf.constant(<span class="number">1.0</span>)</span><br><span class="line">    b = tf.constant(-<span class="number">2.0</span>)</span><br><span class="line">    c = tf.constant(<span class="number">1.0</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 自变量x转换成tf.float32</span></span><br><span class="line">    x = tf.cast(x,tf.float32)</span><br><span class="line">    <span class="keyword">with</span> tf.GradientTape() <span class="keyword">as</span> tape:</span><br><span class="line">        tape.watch(x)</span><br><span class="line">        y = a*tf.<span class="built_in">pow</span>(x,<span class="number">2</span>)+b*x+c</span><br><span class="line">    dy_dx = tape.gradient(y,x) </span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span>((dy_dx,y))</span><br><span class="line"></span><br><span class="line">tf.<span class="built_in">print</span>(f(tf.constant(<span class="number">0.0</span>)))</span><br><span class="line">tf.<span class="built_in">print</span>(f(tf.constant(<span class="number">1.0</span>)))</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">(-2, 1)</span></span><br><span class="line"><span class="string">(0, 0)</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure></div><div class="tab-pane" id="one-2"><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 求f(x) = a*x**2 + b*x + c的最小值</span></span><br><span class="line"><span class="comment"># 使用optimizer.apply_gradients</span></span><br><span class="line">x = tf.Variable(<span class="number">0.0</span>,name = <span class="string">&quot;x&quot;</span>,dtype = tf.float32)</span><br><span class="line">a = tf.constant(<span class="number">1.0</span>)</span><br><span class="line">b = tf.constant(-<span class="number">2.0</span>)</span><br><span class="line">c = tf.constant(<span class="number">1.0</span>)</span><br><span class="line"></span><br><span class="line">optimizer = tf.keras.optimizers.SGD(learning_rate=<span class="number">0.01</span>)</span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1000</span>):</span><br><span class="line">    <span class="keyword">with</span> tf.GradientTape() <span class="keyword">as</span> tape:</span><br><span class="line">        y = a*tf.<span class="built_in">pow</span>(x,<span class="number">2</span>) + b*x + c</span><br><span class="line">    dy_dx = tape.gradient(y,x)</span><br><span class="line">    optimizer.apply_gradients(grads_and_vars=[(dy_dx,x)])</span><br><span class="line"></span><br><span class="line">tf.<span class="built_in">print</span>(<span class="string">&quot;y =&quot;</span>,y,<span class="string">&quot;; x =&quot;</span>,x)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">y = 0 ; x = 0.999998569</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="comment"># 求f(x) = a*x**2 + b*x + c的最小值</span></span><br><span class="line"><span class="comment"># 使用optimizer.minimize</span></span><br><span class="line"><span class="comment"># optimizer.minimize相当于先用tape求gradient,再apply_gradient</span></span><br><span class="line">x = tf.Variable(<span class="number">0.0</span>,name = <span class="string">&quot;x&quot;</span>,dtype = tf.float32)</span><br><span class="line"><span class="comment">#注意f()无参数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">f</span>():   </span><br><span class="line">    a = tf.constant(<span class="number">1.0</span>)</span><br><span class="line">    b = tf.constant(-<span class="number">2.0</span>)</span><br><span class="line">    c = tf.constant(<span class="number">1.0</span>)</span><br><span class="line">    y = a*tf.<span class="built_in">pow</span>(x,<span class="number">2</span>)+b*x+c</span><br><span class="line">    <span class="keyword">return</span>(y)</span><br><span class="line"></span><br><span class="line">optimizer = tf.keras.optimizers.SGD(learning_rate=<span class="number">0.01</span>)   </span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1000</span>):</span><br><span class="line">    optimizer.minimize(f,[x])   </span><br><span class="line"></span><br><span class="line">tf.<span class="built_in">print</span>(<span class="string">&quot;y =&quot;</span>,f(),<span class="string">&quot;; x =&quot;</span>,x)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">y = 0 ; x = 0.999998569</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="comment"># 在autograph中完成最小值求解</span></span><br><span class="line"><span class="comment"># 使用optimizer.apply_gradients</span></span><br><span class="line">x = tf.Variable(<span class="number">0.0</span>,name = <span class="string">&quot;x&quot;</span>,dtype = tf.float32)</span><br><span class="line">optimizer = tf.keras.optimizers.SGD(learning_rate=<span class="number">0.01</span>)</span><br><span class="line"></span><br><span class="line"><span class="meta">@tf.function</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">minimizef</span>():</span><br><span class="line">    a = tf.constant(<span class="number">1.0</span>)</span><br><span class="line">    b = tf.constant(-<span class="number">2.0</span>)</span><br><span class="line">    c = tf.constant(<span class="number">1.0</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> tf.<span class="built_in">range</span>(<span class="number">1000</span>): <span class="comment">#注意autograph时使用tf.range(1000)而不是range(1000)</span></span><br><span class="line">        <span class="keyword">with</span> tf.GradientTape() <span class="keyword">as</span> tape:</span><br><span class="line">            y = a*tf.<span class="built_in">pow</span>(x,<span class="number">2</span>) + b*x + c</span><br><span class="line">        dy_dx = tape.gradient(y,x)</span><br><span class="line">        optimizer.apply_gradients(grads_and_vars=[(dy_dx,x)])</span><br><span class="line"></span><br><span class="line">    y = a*tf.<span class="built_in">pow</span>(x,<span class="number">2</span>) + b*x + c</span><br><span class="line">    <span class="keyword">return</span> y</span><br><span class="line"></span><br><span class="line">tf.<span class="built_in">print</span>(minimizef())</span><br><span class="line">tf.<span class="built_in">print</span>(x)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">0</span></span><br><span class="line"><span class="string">0.999998569</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="comment"># 在autograph中完成最小值求解</span></span><br><span class="line"><span class="comment"># 使用optimizer.minimize</span></span><br><span class="line">x = tf.Variable(<span class="number">0.0</span>,name = <span class="string">&quot;x&quot;</span>,dtype = tf.float32)</span><br><span class="line">optimizer = tf.keras.optimizers.SGD(learning_rate=<span class="number">0.01</span>)   </span><br><span class="line"></span><br><span class="line"><span class="meta">@tf.function</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">f</span>():   </span><br><span class="line">    a = tf.constant(<span class="number">1.0</span>)</span><br><span class="line">    b = tf.constant(-<span class="number">2.0</span>)</span><br><span class="line">    c = tf.constant(<span class="number">1.0</span>)</span><br><span class="line">    y = a*tf.<span class="built_in">pow</span>(x,<span class="number">2</span>)+b*x+c</span><br><span class="line">    <span class="keyword">return</span>(y)</span><br><span class="line"></span><br><span class="line"><span class="meta">@tf.function</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">train</span>(<span class="params">epoch</span>):  </span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> tf.<span class="built_in">range</span>(epoch):  </span><br><span class="line">        optimizer.minimize(f,[x])</span><br><span class="line">    <span class="keyword">return</span>(f())</span><br><span class="line"></span><br><span class="line">tf.<span class="built_in">print</span>(train(<span class="number">1000</span>))</span><br><span class="line">tf.<span class="built_in">print</span>(x)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">0</span></span><br><span class="line"><span class="string">0.999998569</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure></div></div></div></p>
<h1 id="参考资料">参考资料</h1>
<p><span class="exturl" data-url="aHR0cHM6Ly93d3cudGVuc29yZmxvdy5vcmcvdmVyc2lvbnMvcjIuMC9hcGlfZG9jcy9weXRob24vdGY=">TensorFlow官网<i class="fa fa-external-link-alt"></i></span><br />
<span class="exturl" data-url="aHR0cHM6Ly96aC12Mi5kMmwuYWkvY2hhcHRlcl9wcmVsaW1pbmFyaWVzL25kYXJyYXkuaHRtbA==">动手学深度学习<i class="fa fa-external-link-alt"></i></span><br />
<span class="exturl" data-url="aHR0cHM6Ly9seWh1ZTE5OTEuZ2l0aHViLmlvL2VhdF90ZW5zb3JmbG93Ml9pbl8zMF9kYXlzL2NoaW5lc2UvMi4lRTYlQTAlQjglRTUlQkYlODMlRTYlQTYlODIlRTUlQkYlQjUvMi0xJTJDJUU1JUJDJUEwJUU5JTg3JThGJUU2JTk1JUIwJUU2JThEJUFFJUU3JUJCJTkzJUU2JTlFJTg0Lw==">30天吃掉那只Tensorflow2<i class="fa fa-external-link-alt"></i></span><br />
<span class="exturl" data-url="aHR0cHM6Ly93d3cuYmlsaWJpbGkuY29tL3ZpZGVvL0JWMTI1NHkxQzdWTS8/c3BtX2lkX2Zyb209MzMzLjc4OA==">TensorFlow视频精讲<i class="fa fa-external-link-alt"></i></span></p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://soundmemories.github.io/2021/06/10/NLP/00.Pytorch/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avstar.png">
      <meta itemprop="name" content="SoundMemories">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="SoundMemories">
      <meta itemprop="description" content="今日事，今日毕">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | SoundMemories">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2021/06/10/NLP/00.Pytorch/" class="post-title-link" itemprop="url">Pytorch</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2021-06-10 00:00:00" itemprop="dateCreated datePublished" datetime="2021-06-10T00:00:00+08:00">2021-06-10</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/NLP/" itemprop="url" rel="index"><span itemprop="name">NLP</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>16k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>1:01</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="张量类型">张量类型</h1>
<p>张量的数据类型和numpy.array基本一一对应，但是不支持str类型。<br />
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">torch.float64/torch.double</span><br><span class="line">torch.float32/torch.<span class="built_in">float</span>  <span class="comment"># 一般神经网络建模使用的都是torch.float32类型。</span></span><br><span class="line">torch.float16</span><br><span class="line">torch.int64/torch.long</span><br><span class="line">torch.int32/torch.<span class="built_in">int</span></span><br><span class="line">torch.int16</span><br><span class="line">torch.int8</span><br><span class="line">torch.uint8</span><br><span class="line">torch.<span class="built_in">bool</span></span><br></pre></td></tr></table></figure></p>
<p>张量常用属性：<br />
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">a = torch.rand([<span class="number">2</span>,<span class="number">2</span>])</span><br><span class="line">a.dtype <span class="comment"># 元素类型</span></span><br><span class="line">a.shape/size() <span class="comment"># 形状</span></span><br><span class="line">a.device <span class="comment"># 设备</span></span><br><span class="line"><span class="comment"># 不同类型的数据可以用不同维度(dimension)的张量来表示，一般有几层中括号，就是多少维的张量。</span></span><br><span class="line">a.dim()</span><br></pre></td></tr></table></figure></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch </span><br><span class="line"></span><br><span class="line"><span class="comment"># 自动推断数据类型</span></span><br><span class="line">i = torch.tensor(<span class="number">1</span>); <span class="built_in">print</span>(i, i.dtype) <span class="comment"># type(i)是&lt;class &#x27;torch.Tensor&#x27;&gt;</span></span><br><span class="line">x = torch.tensor(<span class="number">2.0</span>); <span class="built_in">print</span>(x, x.dtype)</span><br><span class="line">b = torch.tensor(<span class="literal">True</span>); <span class="built_in">print</span>(b, b.dtype)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">tensor(1) torch.int64</span></span><br><span class="line"><span class="string">tensor(2.) torch.float32</span></span><br><span class="line"><span class="string">tensor(True) torch.bool</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 指定数据类型</span></span><br><span class="line">i = torch.tensor(<span class="number">1</span>, dtype=torch.int32); <span class="built_in">print</span>(i, i.dtype)</span><br><span class="line">x = torch.tensor(<span class="number">2.0</span>, dtype=torch.double); <span class="built_in">print</span>(x, x.dtype)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">tensor(1, dtype=torch.int32) torch.int32</span></span><br><span class="line"><span class="string">tensor(2., dtype=torch.float64) torch.float64</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用特定类型构造函数</span></span><br><span class="line">i = torch.IntTensor(<span class="number">1</span>); <span class="built_in">print</span>(i,i.dtype) <span class="comment"># 这里1是容量，不是数据</span></span><br><span class="line">x = torch.Tensor(np.array(<span class="number">2.0</span>)); <span class="built_in">print</span>(x,x.dtype) <span class="comment"># 等价于torch.FloatTensor</span></span><br><span class="line">b = torch.BoolTensor(np.array([<span class="number">1</span>,<span class="number">0</span>,<span class="number">2</span>,<span class="number">0</span>])); <span class="built_in">print</span>(b,b.dtype)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">tensor([5], dtype=torch.int32) torch.int32</span></span><br><span class="line"><span class="string">tensor(2.) torch.float32</span></span><br><span class="line"><span class="string">tensor([ True, False,  True, False]) torch.bool</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 不同类型进行转换</span></span><br><span class="line">i = torch.tensor(<span class="number">1</span>); <span class="built_in">print</span>(i,i.dtype)</span><br><span class="line">x = i.<span class="built_in">float</span>(); <span class="built_in">print</span>(x,x.dtype) <span class="comment"># 调用 float方法转换成浮点类型</span></span><br><span class="line">y = i.<span class="built_in">type</span>(torch.<span class="built_in">float</span>); <span class="built_in">print</span>(y,y.dtype) <span class="comment"># 使用type函数转换成浮点类型</span></span><br><span class="line">z = i.type_as(x);<span class="built_in">print</span>(z,z.dtype) <span class="comment"># 使用type_as方法转换成某个Tensor相同类型</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">tensor(1) torch.int64</span></span><br><span class="line"><span class="string">tensor(1.) torch.float32</span></span><br><span class="line"><span class="string">tensor(1.) torch.float32</span></span><br><span class="line"><span class="string">tensor(1.) torch.float32</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>
<h1 id="张量尺寸">张量尺寸</h1>
<p>可以使用shape属性或者size()方法查看张量在每一维的长度。<br />
可以使用view/reshape方法改变张量的尺寸，view需张量连续，reshape会重新复制一份数据，所以不需要数据连续。<br />
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># shape/size()查看维度长度</span></span><br><span class="line">vector = torch.tensor([<span class="number">1.0</span>,<span class="number">2.0</span>,<span class="number">3.0</span>,<span class="number">4.0</span>])</span><br><span class="line"><span class="built_in">print</span>(vector.size())</span><br><span class="line"><span class="built_in">print</span>(vector.shape)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">torch.Size([4])</span></span><br><span class="line"><span class="string">torch.Size([4])</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用view可以改变张量尺寸</span></span><br><span class="line">vector = torch.arange(<span class="number">0</span>,<span class="number">12</span>)</span><br><span class="line"><span class="built_in">print</span>(vector)</span><br><span class="line"><span class="built_in">print</span>(vector.shape)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11])</span></span><br><span class="line"><span class="string">torch.Size([12])</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">matrix34 = vector.view(<span class="number">3</span>,<span class="number">4</span>)</span><br><span class="line"><span class="built_in">print</span>(matrix34)</span><br><span class="line"><span class="built_in">print</span>(matrix34.shape)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">tensor([[ 0,  1,  2,  3],</span></span><br><span class="line"><span class="string">        [ 4,  5,  6,  7],</span></span><br><span class="line"><span class="string">        [ 8,  9, 10, 11]])</span></span><br><span class="line"><span class="string">torch.Size([3, 4])</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 有些操作会让张量存储结构扭曲(如：转置操作)，直接使用view会失败，可以用reshape方法</span></span><br><span class="line">matrix26 = torch.arange(<span class="number">0</span>,<span class="number">12</span>).view(<span class="number">2</span>,<span class="number">6</span>)</span><br><span class="line"><span class="built_in">print</span>(matrix26)</span><br><span class="line"><span class="built_in">print</span>(matrix26.shape)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">tensor([[ 0,  1,  2,  3,  4,  5],</span></span><br><span class="line"><span class="string">        [ 6,  7,  8,  9, 10, 11]])</span></span><br><span class="line"><span class="string">torch.Size([2, 6])</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span> </span><br><span class="line">matrix62 = matrix26.t() <span class="comment"># 转置操作让张量存储结构扭曲</span></span><br><span class="line"><span class="built_in">print</span>(matrix62.is_contiguous())</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">False</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="comment"># 直接使用view方法会失败，可以使用reshape方法</span></span><br><span class="line"><span class="comment">#matrix34 = matrix62.view(3,4) #error!</span></span><br><span class="line">matrix34 = matrix62.reshape(<span class="number">3</span>,<span class="number">4</span>) <span class="comment">#等价于matrix34 = matrix62.contiguous().view(3,4)</span></span><br><span class="line"><span class="built_in">print</span>(matrix34)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">tensor([[ 0,  6,  1,  7],</span></span><br><span class="line"><span class="string">        [ 2,  8,  3,  9],</span></span><br><span class="line"><span class="string">        [ 4, 10,  5, 11]])</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure></p>
<h1 id="张量和numpy数组">张量和numpy数组</h1>
<p>可以用numpy方法从Tensor得到numpy数组，也可以用torch.from_numpy从numpy数组得到Tensor。<strong>这两种方法关联的Tensor和numpy数组是共享数据内存的，如果改变其中一个，另外一个的值也会发生改变</strong>。如果有需要，可以用张量的clone方法拷贝张量，中断这种关联。<br />
此外，还可以使用item方法从标量张量得到对应的Python数值。使用tolist方法从张量得到对应的Python数值列表。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 从numpy数组得到Tensor</span></span><br><span class="line">np_array = np.array([[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">4</span>]])</span><br><span class="line">x_np = torch.from_numpy(np_array) </span><br><span class="line"><span class="built_in">print</span>(x_np)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">tensor([[1, 2],</span></span><br><span class="line"><span class="string">        [3, 4]], dtype=torch.int32)</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="comment"># 改变np_array，x_np也随之改变。</span></span><br><span class="line">np.add(np_array,<span class="number">1</span>,out= np_array)</span><br><span class="line"><span class="built_in">print</span>(x_np)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">tensor([[2, 3],</span></span><br><span class="line"><span class="string">        [4, 5]], dtype=torch.int32)</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="comment"># 同理x_np改变，np_array也会随之改变。</span></span><br><span class="line">x_np.add_(<span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(np_array)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">[[3 4]</span></span><br><span class="line"><span class="string"> [5 6]]</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="comment"># 可以用 clone() 方法拷贝张量，独立中断这种关联，拷贝后的张量和原始张量内存。</span></span><br><span class="line">x_np_c = x_np.clone().numpy() <span class="comment"># 中断了和x_np关联</span></span><br><span class="line">x_np.add_(<span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(x_np_c)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">[[3 4]</span></span><br><span class="line"><span class="string"> [5 6]]</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="comment"># item方法和tolist方法可以将张量转换成Python数值和数值列表。</span></span><br><span class="line">i = torch.tensor(<span class="number">1.0</span>)</span><br><span class="line"><span class="built_in">print</span>(i.item())</span><br><span class="line">x = torch.tensor([<span class="number">1.0</span>,<span class="number">2.0</span>])</span><br><span class="line"><span class="built_in">print</span>(x.tolist())</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">1.0</span></span><br><span class="line"><span class="string">[1.0, 2.0]</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>
<p>张量创建参考了很多numpy方法，比如ones_like，rand_like，rand，ones，zeros等等，可参考<a
href="https://soundmemories.github.io/2020/04/22/Machine%20Learning/02.ML-NumPy/">numpy创建方法</a>。</p>
<h1 id="张量操作">张量操作</h1>
<p>张量具有100多种操作，包含了算数、线性代数、矩阵运算（转置/索引/切片）、采样等，详情见<span class="exturl" data-url="aHR0cHM6Ly9weXRvcmNoLm9yZy9kb2NzL3N0YWJsZS90b3JjaC5odG1s">官网介绍<i class="fa fa-external-link-alt"></i></span>。<br />
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 数据量大需要用GPU提高速度，把数据移动到GPU计算</span></span><br><span class="line">tensor = torch.rand(<span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line"><span class="keyword">if</span> torch.cuda.is_available():</span><br><span class="line">  tensor = tensor.to(<span class="string">&#x27;cuda&#x27;</span>)</span><br><span class="line">  <span class="built_in">print</span>(<span class="string">f&quot;Device tensor is stored on: <span class="subst">&#123;tensor.device&#125;</span>&quot;</span>)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">Device tensor is stored on: cuda:0</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><br />
常用方法：<br />
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br></pre></td><td class="code"><pre><span class="line">torch.is_tensor(obj) <span class="comment"># 是否为tensor？</span></span><br><span class="line">torch.is_complex(<span class="built_in">input</span>) <span class="comment"># 是否为复数类型？只支持torch.complex64和torch.complex128。</span></span><br><span class="line">torch.is_floating_point(<span class="built_in">input</span>) <span class="comment"># 是否为float类型？</span></span><br><span class="line">torch.is_nonzero(<span class="built_in">input</span>) <span class="comment"># 是否为&gt;0的值？只能是一个元素。</span></span><br><span class="line">torch.numel(<span class="built_in">input</span>) <span class="comment"># 返回张量的元素数量。</span></span><br><span class="line">torch.set_default_tensor_type(t) <span class="comment"># 设置全局默认张量类型，为t类型。</span></span><br><span class="line"></span><br><span class="line">torch.tensor、as_tensor、from_numpy <span class="comment"># 创建张量，建议用tensor创建。</span></span><br><span class="line">torch.zeros(*size,...,requires_grad=<span class="literal">False</span>) <span class="comment"># 创建全0张量，可选择梯度是否计算。</span></span><br><span class="line">torch.zeros_like(<span class="built_in">input</span>,...)  <span class="comment"># 创建类似input形状的全0张量。</span></span><br><span class="line">torch.ones(*size,...) <span class="comment"># 创建全1张量，可选择梯度是否计算。</span></span><br><span class="line">torch.ones_like(<span class="built_in">input</span>,...)  <span class="comment"># 创建类似input形状的全1张量。</span></span><br><span class="line">torch.arange(start=<span class="number">0</span>, end, step=<span class="number">1</span>,...) <span class="comment"># 创建张量列表，[start, end)，默认int32。建议用此方法。</span></span><br><span class="line">torch.<span class="built_in">range</span>(start=<span class="number">0</span>, end, step=<span class="number">1</span>,...) <span class="comment"># 创建张量列表，[start, end]，默认float32。</span></span><br><span class="line">torch.linspace(start, end, steps...) <span class="comment"># 创建线性张量，steps个元素，自动线性增长。</span></span><br><span class="line">torch.logspace(start, end, steps, base=<span class="number">10.0</span>,,,) <span class="comment"># 创建对数张量，steps个元素，自动对数(10为底)增长。</span></span><br><span class="line">torch.eye(n, m=<span class="literal">None</span>,...) <span class="comment"># 创建2维张量，主对角线全为1，其余为0，只传入一个值为方阵。</span></span><br><span class="line">torch.full(size, fill_value,...) <span class="comment"># 创建size形状张量，用fill_value(列表/元组等)数据填充。</span></span><br><span class="line"></span><br><span class="line">torch.cat(tensors, dim=<span class="number">0</span>,...) <span class="comment"># 按dim维度拼接张量，tensors为多个张量序列(每个张量dim一定要相同)。</span></span><br><span class="line">torch.concat() <span class="comment"># torch.cat的别名。</span></span><br><span class="line">torch.chunk(<span class="built_in">input</span>, chunks, dim=<span class="number">0</span>) <span class="comment"># 将张量按dim维度拆分成chunks块(均分)。</span></span><br><span class="line">torch.gather(<span class="built_in">input</span>, dim, index...) <span class="comment"># 按dim和index从input取值。</span></span><br><span class="line">torch.hstack(tensors,...) <span class="comment"># 沿着列堆叠。</span></span><br><span class="line">torch.reshape(<span class="built_in">input</span>, shape) <span class="comment"># 改变张量形状，返回新张量。view就地改变，需张量contiguous。</span></span><br><span class="line">torch.scatter(<span class="built_in">input</span>, dim, index, src) <span class="comment"># 将src中数据根据index中的索引按照dim的方向填进input中，根据src下标找index值，</span></span><br><span class="line">                                        <span class="comment"># 和scatter_()作用相同，但不会改变原来张量(带_为就地修改)。</span></span><br><span class="line">torch.split(tensor, split_size, dim=<span class="number">0</span>) <span class="comment"># 按split_size切分张量，split_size整数为每块元素个数，列表时为指定每块元素个数。</span></span><br><span class="line">torch.squeeze(<span class="built_in">input</span>, dim=<span class="literal">None</span>,...) <span class="comment"># 对dim维度压缩，未指定时压缩所有为1的维度。</span></span><br><span class="line">torch.stack(tensors, dim=<span class="number">0.</span>..) <span class="comment"># 按dim维度堆叠张量，会增加维度，cat不会增加维度。</span></span><br><span class="line">torch.take(<span class="built_in">input</span>, index) <span class="comment"># 把张量铺平看成一维数组，按照index索引取值。</span></span><br><span class="line">torch.tile(<span class="built_in">input</span>, dims) <span class="comment"># 复制dims-1次数据，dims形状小于input形状时，dims前面补1。</span></span><br><span class="line">                        <span class="comment">#  (8, 6, 4, 2) and dims is (2, 2), then dims is treated as (1, 1, 2, 2)</span></span><br><span class="line">torch.transpose(<span class="built_in">input</span>, dim0, dim1) <span class="comment"># dim0和dim1交换维度。</span></span><br><span class="line">torch.unbind(<span class="built_in">input</span>, dim=<span class="number">0</span>) <span class="comment"># 沿着dim维度切片得到多个张量，返回所有张量元组。</span></span><br><span class="line">torch.unsqueeze(<span class="built_in">input</span>, dim) <span class="comment"># 在dim维度增加一维，和squeeze相反。</span></span><br><span class="line">torch.where(condition, x, y) <span class="comment"># 根据condition条件判断，成立为x，否则为y。</span></span><br><span class="line"></span><br><span class="line">torch.manual_seed(seed) <span class="comment"># 设定随机数种子，包括cpu和GPU，不包括numpy。</span></span><br><span class="line">torch.bernoulli(<span class="built_in">input</span>,...) <span class="comment"># 伯努利次采样，input为每个位置的概率，返回每个位置伯努利采样结果(0/1)。</span></span><br><span class="line">torch.normal(mean, std, size,...) <span class="comment"># 返回正态分布(高斯分布)，mean和std可以为整数/序列。</span></span><br><span class="line">torch.rand(*size, ...) <span class="comment"># 生成随机数，[0,1)区间均匀分布采样。</span></span><br><span class="line">torch.randint(low=<span class="number">0</span>, high, size,...) <span class="comment"># 生成随机整数，[low,high)区间均匀分布采样。</span></span><br><span class="line">torch.randn(*size,...) <span class="comment"># 生成随机数，正态分布采样(均值0，标准差1)。</span></span><br><span class="line">torch.randperm(n, ...) <span class="comment"># 随机组合，[0,n)区间随机打乱组合。</span></span><br><span class="line"></span><br><span class="line">torch.<span class="built_in">abs</span>(<span class="built_in">input</span>,...) <span class="comment"># 绝对值。</span></span><br><span class="line">torch.add(<span class="built_in">input</span>, other, *, alpha=<span class="number">1</span>,...)  <span class="comment"># 相加，input + alpha*other。</span></span><br><span class="line">torch.ceil(<span class="built_in">input</span>,...) <span class="comment"># 取上界。</span></span><br><span class="line">torch.div(<span class="built_in">input</span>, other,...) <span class="comment"># 相除，input/other。</span></span><br><span class="line">torch.exp(<span class="built_in">input</span>,...) <span class="comment"># 指数，e^input。</span></span><br><span class="line">torch.floor(<span class="built_in">input</span>,...) <span class="comment"># 取下界。</span></span><br><span class="line">torch.log(<span class="built_in">input</span>,...) <span class="comment"># 对数,log_e(input)。</span></span><br><span class="line">torch.mul(<span class="built_in">input</span>, other,...) <span class="comment"># 相乘，input*other。</span></span><br><span class="line">torch.neg(<span class="built_in">input</span>,...) <span class="comment"># 取负，-1*input。</span></span><br><span class="line">torch.<span class="built_in">pow</span>(<span class="built_in">input</span>, exponent,...) <span class="comment"># 幂指数，input^exponent。</span></span><br><span class="line">torch.sqrt(<span class="built_in">input</span>,...) <span class="comment"># 开方。</span></span><br><span class="line">torch.<span class="built_in">round</span>(<span class="built_in">input</span>,...) <span class="comment"># 保留整数部分，四舍五入。</span></span><br><span class="line">torch.trunc(<span class="built_in">input</span>,...) <span class="comment"># 保留整数部分，向0归整。fix别名。</span></span><br><span class="line">torch.sub(<span class="built_in">input</span>, other, *, alpha=<span class="number">1</span>,...) <span class="comment"># 相减，input - alpha*other。</span></span><br><span class="line">torch.clamp(<span class="built_in">input</span>, <span class="built_in">min</span>=<span class="literal">None</span>, <span class="built_in">max</span>=<span class="literal">None</span>,...) <span class="comment"># 限制范围[min,max]。</span></span><br><span class="line"></span><br><span class="line">torch.argmax(<span class="built_in">input</span>, dim, keepdim=<span class="literal">False</span>) <span class="comment"># input最大值索引，不写dim将按input展平比较。</span></span><br><span class="line">torch.argmin(<span class="built_in">input</span>, dim=<span class="literal">None</span>, keepdim=<span class="literal">False</span>) <span class="comment"># input最小值索引，不写dim将按input展平比较。</span></span><br><span class="line">torch.<span class="built_in">all</span>(<span class="built_in">input</span>, dim, keepdim=<span class="literal">False</span>,...) <span class="comment"># input元素全为True？不写dim将按input展平比较。</span></span><br><span class="line">torch.<span class="built_in">any</span>(<span class="built_in">input</span>, dim, keepdim=<span class="literal">False</span>,...) <span class="comment"># input元素不全为True？不写dim将按input展平比较。</span></span><br><span class="line">torch.<span class="built_in">max</span>(<span class="built_in">input</span>, dim, keepdim=<span class="literal">False</span>,...) <span class="comment"># input最大值, 不写dim将按input展平比较。</span></span><br><span class="line">torch.<span class="built_in">min</span>(<span class="built_in">input</span>, dim, keepdim=<span class="literal">False</span>,...) <span class="comment"># input最小值, 不写dim将按input展平比较。</span></span><br><span class="line">torch.mean(<span class="built_in">input</span>, dim, keepdim=<span class="literal">False</span>,...) <span class="comment"># input均值, 不写dim将按input展平比较。</span></span><br><span class="line">torch.median(<span class="built_in">input</span>, dim, keepdim=<span class="literal">False</span>,...) <span class="comment"># input中位数, 不写dim将按input展平比较。</span></span><br><span class="line">torch.<span class="built_in">sum</span>(<span class="built_in">input</span>, dim, keepdim=<span class="literal">False</span>,...) <span class="comment"># input求和, 不写dim将按input展平比较。</span></span><br><span class="line"></span><br><span class="line">torch.argsort(<span class="built_in">input</span>, dim=-<span class="number">1</span>, descending=<span class="literal">False</span>) <span class="comment"># 给定维度升序排序后的索引。</span></span><br><span class="line">torch.equal(<span class="built_in">input</span>, other) <span class="comment"># input和other形状和元素相同为True。</span></span><br><span class="line">torch.ge(<span class="built_in">input</span>, other,...) <span class="comment"># 大于等于。</span></span><br><span class="line">torch.gt(<span class="built_in">input</span>, other,...) <span class="comment"># 大于。</span></span><br><span class="line">torch.isin(elements, test_elements,...) <span class="comment"># elements元素在test_elements中出现为True，形状同elements。</span></span><br><span class="line">torch.isnan(<span class="built_in">input</span>) <span class="comment"># nan？</span></span><br><span class="line">torch.le(<span class="built_in">input</span>, other,...) <span class="comment"># 小于等于。</span></span><br><span class="line">torch.lt(<span class="built_in">input</span>, other,...) <span class="comment"># 小于。</span></span><br><span class="line">torch.maximum(<span class="built_in">input</span>, other,...) <span class="comment"># 留下input和other相同位置最大值。</span></span><br><span class="line">torch.minimum(<span class="built_in">input</span>, other,...) <span class="comment"># 留下input和other相同位置最小值。</span></span><br><span class="line">torch.ne(<span class="built_in">input</span>, other,...) <span class="comment"># 不等于。</span></span><br><span class="line">torch.sort(<span class="built_in">input</span>, dim=-<span class="number">1</span>, descending=<span class="literal">False</span>,...) <span class="comment"># 按dim排序，默认升序。</span></span><br><span class="line">torch.topk(<span class="built_in">input</span>, k, dim=<span class="literal">None</span>, largest=<span class="literal">True</span>, <span class="built_in">sorted</span>=<span class="literal">True</span>,...) <span class="comment"># topk个元素，默认最大k个。</span></span><br><span class="line"></span><br><span class="line">torch.mm(mat1, mat2, out=<span class="literal">None</span>) <span class="comment"># 两个二维矩阵的矩阵乘法，并且不支持broadcast操作。</span></span><br><span class="line"><span class="comment"># mat1 x mat2 -&gt; n×m × m×d = n×d</span></span><br><span class="line">torch.bmm(bmat1, bmat2, out=<span class="literal">None</span>) <span class="comment"># 三维带batch的矩阵乘法。</span></span><br><span class="line"><span class="comment"># bmat1 x bmat2 -&gt; b×n×m × b×m×d = b×n×d</span></span><br><span class="line">torch.matmul(<span class="built_in">input</span>, other, out=<span class="literal">None</span>) <span class="comment"># 支持broadcast操作，使用起来比较复杂。</span></span><br><span class="line"><span class="comment"># 针对多维数据 matmul() 乘法，可以认为该乘法使用使用两个参数的后两个维度来计算，其他的维度都可以认为是batch维度。</span></span><br><span class="line">torch.mul(mat1, other, out=<span class="literal">None</span>) <span class="comment"># 矩阵逐元素乘法。</span></span><br><span class="line"><span class="comment"># 其中 other 乘数可以是标量，也可以是任意维度的矩阵， 只要满足最终相乘是可以broadcast的即可。</span></span><br></pre></td></tr></table></figure><br />
不规则取值示例：<br />
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 班级成绩册的例子，有4个班级，每个班级10个学生，每个学生7门科目成绩。可以用一个4×10×7的张量来表示。</span></span><br><span class="line">minval=<span class="number">0</span></span><br><span class="line">maxval=<span class="number">100</span></span><br><span class="line">scores = torch.floor(minval + (maxval-minval)*torch.rand([<span class="number">4</span>,<span class="number">10</span>,<span class="number">7</span>])).<span class="built_in">int</span>()</span><br><span class="line"><span class="comment"># 抽取每个班级第0个学生，第5个学生，第9个学生的全部成绩</span></span><br><span class="line">torch.index_select(scores,dim = <span class="number">1</span>,index = torch.tensor([<span class="number">0</span>,<span class="number">5</span>,<span class="number">9</span>]))</span><br><span class="line"><span class="comment"># 抽取每个班级第0个学生，第5个学生，第9个学生的第1门课程，第3门课程，第6门课程成绩</span></span><br><span class="line">torch.index_select(torch.index_select(scores, dim=<span class="number">1</span>, index=torch.tensor([<span class="number">0</span>,<span class="number">5</span>,<span class="number">9</span>]))</span><br><span class="line">                   ,dim=<span class="number">2</span>,index=torch.tensor([<span class="number">1</span>,<span class="number">3</span>,<span class="number">6</span>]))</span><br><span class="line"><span class="comment"># 抽取第0个班级第0个学生的第0门课程，第2个班级的第4个学生的第1门课程，第3个班级的第9个学生第6门课程成绩</span></span><br><span class="line"><span class="comment"># take将输入看成一维数组，输出和index同形状</span></span><br><span class="line">torch.take(scores,torch.tensor([<span class="number">0</span>*<span class="number">10</span>*<span class="number">7</span>+<span class="number">0</span>,<span class="number">2</span>*<span class="number">10</span>*<span class="number">7</span>+<span class="number">4</span>*<span class="number">7</span>+<span class="number">1</span>,<span class="number">3</span>*<span class="number">10</span>*<span class="number">7</span>+<span class="number">9</span>*<span class="number">7</span>+<span class="number">6</span>]))</span><br><span class="line"><span class="comment"># 抽取分数大于等于80分的分数（布尔索引）</span></span><br><span class="line"><span class="comment"># 结果是1维张量</span></span><br><span class="line">torch.masked_select(scores, scores&gt;=<span class="number">80</span>)</span><br></pre></td></tr></table></figure><br />
如果要通过修改张量的部分元素值得到新的张量，可以使用torch.where，torch.index_fill
和 torch.masked_fill。<br />
torch.where可以理解为if的张量版本。<br />
torch.index_fill的选取元素逻辑和torch.index_select相同。<br />
torch.masked_fill的选取元素逻辑和torch.masked_select相同。<br />
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 如果分数大于60分，赋值成1，否则赋值成0</span></span><br><span class="line">ifpass = torch.where(scores&gt;<span class="number">60</span>,torch.tensor(<span class="number">1</span>),torch.tensor(<span class="number">0</span>))</span><br><span class="line"><span class="comment"># 将每个班级第0个学生，第5个学生，第9个学生的全部成绩赋值成满分</span></span><br><span class="line"><span class="comment"># 等价于 scores.index_fill(dim = 1,index = torch.tensor([0,5,9]),value = 100)</span></span><br><span class="line">torch.index_fill(scores,dim = <span class="number">1</span>,index = torch.tensor([<span class="number">0</span>,<span class="number">5</span>,<span class="number">9</span>]),value = <span class="number">100</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将分数小于60分的分数赋值成60分</span></span><br><span class="line"><span class="comment"># 等价于scores.masked_fill(scores&lt;60,60)</span></span><br><span class="line">torch.masked_fill(scores,scores&lt;<span class="number">60</span>,<span class="number">60</span>)</span><br></pre></td></tr></table></figure></p>
<h1 id="自动微分">自动微分</h1>
<p>关于自动微分的论文：<span class="exturl" data-url="aHR0cHM6Ly93d3cuam1sci5vcmcvcGFwZXJzL3ZvbHVtZTE4LzE3LTQ2OC8xNy00NjgucGRm">Automatic
Differentiation in Machine Learning: a Survey<i class="fa fa-external-link-alt"></i></span>。<br />
为什么深度学习框架都用反向传播计算梯度？<br />
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">假设 a=f(x), b=g(a), y=h(b)</span><br><span class="line"></span><br><span class="line">p_y/p_x = p_h/p_b * p_g/p_a * p_f/p_x (链式法则+雅可比)</span><br><span class="line">雅可比矩阵大小：|y|*|b|, |b|*|a|, |a|*|x|</span><br><span class="line"></span><br><span class="line">统计计算量：</span><br><span class="line">forward mode <span class="keyword">for</span> AD：</span><br><span class="line">|y|*|b| (|b|*|a|, |a|*|x|) = bax+ybx</span><br><span class="line"></span><br><span class="line">reverse mode <span class="keyword">for</span> AD:</span><br><span class="line">(|y|*|b|, |b|*|a|) |a|*|x| = yba+yax</span><br><span class="line"></span><br><span class="line">假设a=b，变成x跟y的比较：</span><br><span class="line">当x&gt;y, 输入特征大于输出特征, reverse mode计算量比较小(深度网络一般输入维度x&gt;y的，所以用方向传播省计算量)</span><br><span class="line">当x&lt;y, 输入特征小于输出特征, forward mode计算量比较小</span><br></pre></td></tr></table></figure><br />
Pytorch一般通过反向传播<code>backward()</code>方法实现这种求梯度计算。该方法求得的梯度将存在对应自变量张量的grad属性下。<br />
除此之外，也能够调用<code>torch.autograd.grad()</code>函数来实现求梯度计算。这就是Pytorch的自动微分机制。<br />
注意，Pytorch梯度会自动累加，不想累加使用<code>.grad_zero_()</code>清除当前梯度。<br />
利用backward方法求导数：<br />
<div class="tabs" id="one"><ul class="nav-tabs"><li class="tab active"><a href="#one-1">标量反向传播</a></li><li class="tab"><a href="#one-2">非标量反向传播</a></li><li class="tab"><a href="#one-3">非标量用标量反向传播</a></li></ul><div class="tab-content"><div class="tab-pane active" id="one-1"><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np </span><br><span class="line"><span class="keyword">import</span> torch </span><br><span class="line"></span><br><span class="line"><span class="comment"># f(x) = a*x^2 + b*x + c的导数</span></span><br><span class="line">x = torch.tensor(<span class="number">0.0</span>, requires_grad=<span class="literal">True</span>) <span class="comment"># x需要被求导</span></span><br><span class="line">a = torch.tensor(<span class="number">1.0</span>)</span><br><span class="line">b = torch.tensor(-<span class="number">2.0</span>)</span><br><span class="line">c = torch.tensor(<span class="number">1.0</span>)</span><br><span class="line">y = a*torch.<span class="built_in">pow</span>(x,<span class="number">2</span>) + b*x + c </span><br><span class="line"></span><br><span class="line">y.backward()</span><br><span class="line">dy_dx = x.grad</span><br><span class="line"><span class="built_in">print</span>(dy_dx)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">tensor(-2.)</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure></div><div class="tab-pane" id="one-2"><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np </span><br><span class="line"><span class="keyword">import</span> torch </span><br><span class="line"></span><br><span class="line"><span class="comment"># f(x) = a*x^2 + b*x + c</span></span><br><span class="line">x = torch.tensor([[<span class="number">0.0</span>,<span class="number">0.0</span>],[<span class="number">1.0</span>,<span class="number">2.0</span>]], requires_grad=<span class="literal">True</span>) <span class="comment"># x需要被求导</span></span><br><span class="line">a = torch.tensor(<span class="number">1.0</span>)</span><br><span class="line">b = torch.tensor(-<span class="number">2.0</span>)</span><br><span class="line">c = torch.tensor(<span class="number">1.0</span>)</span><br><span class="line">y = a*torch.<span class="built_in">pow</span>(x,<span class="number">2</span>) + b*x + c </span><br><span class="line"></span><br><span class="line">gradient = torch.tensor([[<span class="number">1.0</span>,<span class="number">1.0</span>],[<span class="number">1.0</span>,<span class="number">1.0</span>]])</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;x:\n&quot;</span>,x)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;y:\n&quot;</span>,y)</span><br><span class="line"><span class="comment"># 当loss非标量时，backward需要传入和结果形状一致的张量。loss对参数求完梯度后乘以这个张量作为结果。</span></span><br><span class="line"><span class="comment"># 官网自动微分：https://pytorch.org/tutorials/beginner/basics/autogradqs_tutorial.html</span></span><br><span class="line">y.backward(gradient=gradient)</span><br><span class="line">x_grad = x.grad</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;x_grad:\n&quot;</span>,x_grad)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">x:</span></span><br><span class="line"><span class="string"> tensor([[0., 0.],</span></span><br><span class="line"><span class="string">        [1., 2.]], requires_grad=True)</span></span><br><span class="line"><span class="string">y:</span></span><br><span class="line"><span class="string"> tensor([[1., 1.],</span></span><br><span class="line"><span class="string">        [0., 1.]], grad_fn=&lt;AddBackward0&gt;)</span></span><br><span class="line"><span class="string">x_grad:</span></span><br><span class="line"><span class="string"> tensor([[-2., -2.],</span></span><br><span class="line"><span class="string">        [ 0.,  2.]])</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="comment"># 手动计算</span></span><br><span class="line"><span class="keyword">from</span> torch.autograd.functional <span class="keyword">import</span> jacobian</span><br><span class="line"><span class="comment"># jacobian需要输入一个函数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">func</span>(<span class="params">x</span>):</span><br><span class="line">    <span class="keyword">return</span> a*torch.<span class="built_in">pow</span>(x,<span class="number">2</span>) + b*x + c</span><br><span class="line"><span class="comment"># 把矩阵运算转成向量运算</span></span><br><span class="line">x_1 = torch.ones_like(func(x[<span class="number">0</span>])) @ jacobian(func,x[<span class="number">0</span>])</span><br><span class="line"><span class="built_in">print</span>(x_1)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">tensor([-2., -2.])</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">x_2 = torch.ones_like(func(x[<span class="number">1</span>])) @ jacobian(func,x[<span class="number">1</span>])</span><br><span class="line"><span class="built_in">print</span>(x_2)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">tensor([0., 2.])</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure></div><div class="tab-pane" id="one-3"><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np </span><br><span class="line"><span class="keyword">import</span> torch </span><br><span class="line"></span><br><span class="line"><span class="comment"># f(x) = a*x^2 + b*x + c</span></span><br><span class="line">x = torch.tensor([[<span class="number">0.0</span>,<span class="number">0.0</span>],[<span class="number">1.0</span>,<span class="number">2.0</span>]], requires_grad=<span class="literal">True</span>) <span class="comment"># x需要被求导</span></span><br><span class="line">a = torch.tensor(<span class="number">1.0</span>)</span><br><span class="line">b = torch.tensor(-<span class="number">2.0</span>)</span><br><span class="line">c = torch.tensor(<span class="number">1.0</span>)</span><br><span class="line">y = a*torch.<span class="built_in">pow</span>(x,<span class="number">2</span>) + b*x + c </span><br><span class="line"></span><br><span class="line">gradient = torch.tensor([[<span class="number">1.0</span>,<span class="number">1.0</span>],[<span class="number">1.0</span>,<span class="number">1.0</span>]])</span><br><span class="line">z = torch.<span class="built_in">sum</span>(y*gradient)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;x:&quot;</span>,x)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;y:&quot;</span>,y)</span><br><span class="line">z.backward()</span><br><span class="line">x_grad = x.grad</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;x_grad:\n&quot;</span>,x_grad)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">x: tensor([[0., 0.],</span></span><br><span class="line"><span class="string">        [1., 2.]], requires_grad=True)</span></span><br><span class="line"><span class="string">y: tensor([[1., 1.],</span></span><br><span class="line"><span class="string">        [0., 1.]], grad_fn=&lt;AddBackward0&gt;)</span></span><br><span class="line"><span class="string">x_grad:</span></span><br><span class="line"><span class="string"> tensor([[-2., -2.],</span></span><br><span class="line"><span class="string">        [ 0.,  2.]])</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure></div></div></div></p>
<p>利用autograd.grad方法求导数：<br />
<div class="tabs" id="two"><ul class="nav-tabs"><li class="tab active"><a href="#two-1">单个标量</a></li><li class="tab"><a href="#two-2">多个标量</a></li></ul><div class="tab-content"><div class="tab-pane active" id="two-1"><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np </span><br><span class="line"><span class="keyword">import</span> torch </span><br><span class="line"></span><br><span class="line"><span class="comment"># f(x) = a*x^2 + b*x + c的导数</span></span><br><span class="line">x = torch.tensor(<span class="number">0.0</span>, requires_grad=<span class="literal">True</span>) <span class="comment"># x需要被求导</span></span><br><span class="line">a = torch.tensor(<span class="number">1.0</span>)</span><br><span class="line">b = torch.tensor(-<span class="number">2.0</span>)</span><br><span class="line">c = torch.tensor(<span class="number">1.0</span>)</span><br><span class="line">y = a*torch.<span class="built_in">pow</span>(x,<span class="number">2</span>) + b*x + c</span><br><span class="line"></span><br><span class="line"><span class="comment"># create_graph 设置为 True 将允许创建更高阶的导数 </span></span><br><span class="line">dy_dx = torch.autograd.grad(y,x, create_graph=<span class="literal">True</span>)[<span class="number">0</span>]</span><br><span class="line"><span class="built_in">print</span>(dy_dx.data)</span><br><span class="line"><span class="comment"># 求二阶导数</span></span><br><span class="line">dy2_dx2 = torch.autograd.grad(dy_dx,x)[<span class="number">0</span>] </span><br><span class="line"><span class="built_in">print</span>(dy2_dx2.data)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">tensor(-2.)</span></span><br><span class="line"><span class="string">tensor(2.)</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure></div><div class="tab-pane" id="two-2"><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np </span><br><span class="line"><span class="keyword">import</span> torch </span><br><span class="line"></span><br><span class="line">x1 = torch.tensor(<span class="number">1.0</span>, requires_grad=<span class="literal">True</span>) <span class="comment"># x需要被求导</span></span><br><span class="line">x2 = torch.tensor(<span class="number">2.0</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">y1 = x1*x2</span><br><span class="line">y2 = x1+x2</span><br><span class="line"></span><br><span class="line"><span class="comment"># 允许同时对多个自变量求导数</span></span><br><span class="line">(dy1_dx1,dy1_dx2) = torch.autograd.grad(outputs=y1, inputs=[x1,x2], retain_graph=<span class="literal">True</span>)</span><br><span class="line"><span class="built_in">print</span>(dy1_dx1,dy1_dx2)</span><br><span class="line"><span class="comment"># 如果有多个因变量，相当于把多个因变量的梯度结果求和</span></span><br><span class="line">(dy12_dx1,dy12_dx2) = torch.autograd.grad(outputs=[y1,y2], inputs=[x1,x2])</span><br><span class="line"><span class="built_in">print</span>(dy12_dx1,dy12_dx2)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">tensor(2.) tensor(1.)</span></span><br><span class="line"><span class="string">tensor(3.) tensor(2.)</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure></div></div></div></p>
<p>利用自动微分和优化器求最小值：<br />
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np </span><br><span class="line"><span class="keyword">import</span> torch </span><br><span class="line"></span><br><span class="line"><span class="comment"># f(x) = a*x^2 + b*x + c的最小值</span></span><br><span class="line">x = torch.tensor(<span class="number">0.0</span>, requires_grad=<span class="literal">True</span>) <span class="comment"># x需要被求导</span></span><br><span class="line">a = torch.tensor(<span class="number">1.0</span>)</span><br><span class="line">b = torch.tensor(-<span class="number">2.0</span>)</span><br><span class="line">c = torch.tensor(<span class="number">1.0</span>)</span><br><span class="line"></span><br><span class="line">optimizer = torch.optim.SGD(params=[x], lr=<span class="number">0.01</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">f</span>(<span class="params">x</span>):</span><br><span class="line">    result = a*torch.<span class="built_in">pow</span>(x,<span class="number">2</span>) + b*x + c </span><br><span class="line">    <span class="keyword">return</span>(result)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">500</span>):</span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    y = f(x)</span><br><span class="line">    y.backward()</span><br><span class="line">    optimizer.step()</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;y=&quot;</span>,f(x).data,<span class="string">&quot;;&quot;</span>,<span class="string">&quot;x=&quot;</span>,x.data)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">y= tensor(0.) ; x= tensor(1.0000)</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure></p>
<h1 id="动态计算图">动态计算图</h1>
<p>Pytorch的计算图由<strong>节点</strong>和<strong>边</strong>组成，节点表示张量或者Function，边表示张量和Function之间的依赖关系。<br />
Pytorch中的计算图是动态图。这里的动态主要有两重含义。<strong>第一层含义</strong>：计算图的<strong>正向传播是立即执行</strong>。无需等待完整的计算图创建完毕，每条语句都会在计算图中动态添加节点和边，并立即执行正向传播得到计算结果。<strong>第二层含义</strong>：计算图在<strong>反向传播后立即销毁</strong>。下次调用需要重新构建计算图。如果在程序中使用了backward方法执行了反向传播，或者利用torch.autograd.grad方法计算了梯度，那么创建的计算图会被立即销毁，释放存储空间，下次调用需要重新创建。<br />
<div class="tabs" id="ll"><ul class="nav-tabs"><li class="tab active"><a href="#ll-1">正向传播是立即执行</a></li><li class="tab"><a href="#ll-2">反向传播后立即销毁</a></li></ul><div class="tab-content"><div class="tab-pane active" id="ll-1"><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch </span><br><span class="line">w = torch.tensor([[<span class="number">3.0</span>,<span class="number">1.0</span>]],requires_grad=<span class="literal">True</span>)</span><br><span class="line">b = torch.tensor([[<span class="number">3.0</span>]],requires_grad=<span class="literal">True</span>)</span><br><span class="line">X = torch.tensor([[<span class="number">2.0</span>,<span class="number">4.0</span>]])</span><br><span class="line">Y = torch.tensor(<span class="number">1</span>)</span><br><span class="line">Y_hat = X@w.t() + b  <span class="comment"># Y_hat定义后其正向传播被立即执行，与其后面的loss创建语句无关</span></span><br><span class="line">loss = torch.mean(torch.<span class="built_in">pow</span>(Y_hat-Y,<span class="number">2</span>))</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(loss.data)</span><br><span class="line"><span class="built_in">print</span>(Y_hat.data)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">tensor(144.)</span></span><br><span class="line"><span class="string">tensor([[13.]])</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure></div><div class="tab-pane" id="ll-2"><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch </span><br><span class="line">w = torch.tensor([[<span class="number">3.0</span>,<span class="number">1.0</span>]],requires_grad=<span class="literal">True</span>)</span><br><span class="line">b = torch.tensor([[<span class="number">3.0</span>]],requires_grad=<span class="literal">True</span>)</span><br><span class="line">X = torch.tensor([[<span class="number">2.0</span>,<span class="number">4.0</span>]],requires_grad=<span class="literal">False</span>)</span><br><span class="line">Y = torch.tensor(<span class="number">1</span>,requires_grad=<span class="literal">False</span>)</span><br><span class="line">Y_hat = X@w.t() + b  <span class="comment"># Y_hat定义后其正向传播被立即执行，与其后面的loss创建语句无关</span></span><br><span class="line">loss = torch.mean(torch.<span class="built_in">pow</span>(Y_hat-Y,<span class="number">2</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment">#计算图在反向传播后立即销毁，如果需要保留计算图, 需要backward中设置retain_graph=True</span></span><br><span class="line">loss.backward(retain_graph=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#loss.backward() #若不加retain_graph，如果再次执行反向传播将报错</span></span><br></pre></td></tr></table></figure></div></div></div></p>
<p>计算图中的 张量我们已经比较熟悉了, 计算图中的另外一种节点是Function,
实际上就是Pytorch中各种对张量操作的函数。这些Function和我们Python中的函数有一个较大的区别，那就是它同时包括正向计算逻辑和反向传播的逻辑。我们可以通过继承torch.autograd.Function来创建这种支持反向传播的Function。<br />
<div class="tabs" id="s"><ul class="nav-tabs"><li class="tab active"><a href="#s-1">自定义Function</a></li><li class="tab"><a href="#s-2">调用自定义Function</a></li><li class="tab"><a href="#s-3">反向传播计算过程</a></li></ul><div class="tab-content"><div class="tab-pane active" id="s-1"><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">MyReLU</span>(torch.autograd.Function):</span><br><span class="line">    <span class="comment">#正向传播逻辑，可以用ctx存储一些值，供反向传播使用。</span></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">ctx, <span class="built_in">input</span></span>):</span><br><span class="line">        ctx.save_for_backward(<span class="built_in">input</span>)</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">input</span>.clamp(<span class="built_in">min</span>=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">#反向传播逻辑</span></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">backward</span>(<span class="params">ctx, grad_output</span>):</span><br><span class="line">        <span class="built_in">input</span>, = ctx.saved_tensors</span><br><span class="line">        grad_input = grad_output.clone()</span><br><span class="line">        grad_input[<span class="built_in">input</span> &lt; <span class="number">0</span>] = <span class="number">0</span></span><br><span class="line">        <span class="keyword">return</span> grad_input</span><br></pre></td></tr></table></figure></div><div class="tab-pane" id="s-2"><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch </span><br><span class="line">w = torch.tensor([[<span class="number">3.0</span>,<span class="number">1.0</span>]],requires_grad=<span class="literal">True</span>)</span><br><span class="line">b = torch.tensor([[<span class="number">3.0</span>]],requires_grad=<span class="literal">True</span>)</span><br><span class="line">X = torch.tensor([[-<span class="number">1.0</span>,-<span class="number">1.0</span>],[<span class="number">1.0</span>,<span class="number">1.0</span>]])</span><br><span class="line">Y = torch.tensor([[<span class="number">2.0</span>,<span class="number">3.0</span>]])</span><br><span class="line"></span><br><span class="line">relu = MyReLU.apply <span class="comment"># relu现在也可以具有正向传播和反向传播功能</span></span><br><span class="line">Y_hat = relu(X@w.t() + b)</span><br><span class="line">loss = torch.mean(torch.<span class="built_in">pow</span>(Y_hat-Y,<span class="number">2</span>))</span><br><span class="line"></span><br><span class="line">loss.backward()</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(w.grad)</span><br><span class="line"><span class="built_in">print</span>(b.grad)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">tensor([[4.5000, 4.5000]])</span></span><br><span class="line"><span class="string">tensor([[4.5000]])</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="comment"># Y_hat的梯度函数即是我们自己所定义的 MyReLU.backward</span></span><br><span class="line"><span class="built_in">print</span>(Y_hat.grad_fn)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">&lt;torch.autograd.function.MyReLUBackward object at 0x1205a46c8&gt;</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure></div><div class="tab-pane" id="s-3"><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch </span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">loss.backward()语句调用后，依次发生以下计算过程：</span></span><br><span class="line"><span class="string">(1) loss自己的grad梯度赋值为1，即对自身的梯度为1。</span></span><br><span class="line"><span class="string">(2) loss根据其自身梯度以及关联的backward方法，计算出其对应的自变量即y1和y2的梯度，将该值赋值到y1.grad和y2.grad。</span></span><br><span class="line"><span class="string">(3) y2和y1根据其自身梯度以及关联的backward方法, 分别计算出其对应的自变量x的梯度，x.grad将其收到的多个梯度值累加。</span></span><br><span class="line"><span class="string">（注意，1,2,3步骤的求梯度顺序和对多个梯度值的累加规则恰好是求导链式法则的程序表述）</span></span><br><span class="line"><span class="string">正因为求导链式法则衍生的梯度累加规则，张量的grad梯度不会自动清零，在需要的时候需要手动置零。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">x = torch.tensor(<span class="number">3.0</span>,requires_grad=<span class="literal">True</span>)</span><br><span class="line">y1 = x+<span class="number">1</span>  <span class="comment"># 梯度:dy1_dx=1</span></span><br><span class="line">y2 = <span class="number">2</span>*x  <span class="comment"># 梯度:dy2_dx=2</span></span><br><span class="line">loss = (y1-y2)**<span class="number">2</span>  </span><br><span class="line"></span><br><span class="line">loss.backward()</span><br><span class="line"><span class="comment"># 会出现UserWarning，y1和y2为非叶子节点，想看梯度需.retain_grad()</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;loss.grad:&quot;</span>, loss.grad)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;y1.grad:&quot;</span>, y1.grad)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;y2.grad:&quot;</span>, y2.grad)</span><br><span class="line"><span class="comment"># 对x的梯度:2(y1-y2)*(dy1_dx-dy2_dx)=2(1-x)*(-1)，带入x=3得4</span></span><br><span class="line"><span class="built_in">print</span>(x.grad)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">loss.grad: None</span></span><br><span class="line"><span class="string">y1.grad: None</span></span><br><span class="line"><span class="string">y2.grad: None</span></span><br><span class="line"><span class="string">tensor(4.)</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="built_in">print</span>(loss.is_leaf)</span><br><span class="line"><span class="built_in">print</span>(y1.is_leaf)</span><br><span class="line"><span class="built_in">print</span>(y2.is_leaf)</span><br><span class="line"><span class="built_in">print</span>(x.is_leaf)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">False</span></span><br><span class="line"><span class="string">False</span></span><br><span class="line"><span class="string">False</span></span><br><span class="line"><span class="string">True</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure></div></div></div><br />
叶子节点和非叶子节点：<br />
反向传播计算过程中，会发现loss.grad并不是我们期望的1，而是None，类似地
y1.grad 以及 y2.grad也是None。<br />
这是为什么呢？这是由于它们不是叶子节点张量。</p>
<p>在反向传播过程中，只有 is_leaf=True
的叶子节点，需要求导的张量的导数结果才会被最后保留下来。<br />
那么什么是叶子节点张量呢？叶子节点张量需要满足两个条件。<br />
（1）叶子节点张量是由用户直接创建的张量，而非由某个Function通过计算得到的张量。<br />
（2）叶子节点张量的 requires_grad 属性必须为True。<br />
Pytorch设计这样的规则主要是为了节约内存或者显存空间，因为几乎所有的时候，用户只会关心他自己直接创建的张量的梯度。<br />
所有依赖于叶子节点张量的张量, 其 requires_grad
属性必定是True的，但其梯度值只在计算过程中被用到，不会最终存储到grad属性中。</p>
<p>如果需要保留中间计算结果的梯度到grad属性中，可以使用<code>retain_grad</code>方法。<br />
如果仅仅是为了调试代码查看梯度值，可以利用<code>register_hook</code>打印日志。<br />
如果不想保留梯度，可使用<code>with torch.no_grad()</code>方法或Tensor的<code>detach()</code>方法。<br />
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#非叶子节点梯度显示控制</span></span><br><span class="line">y1.retain_grad()  <span class="comment"># y1.register_hook(lambda grad: print(&#x27;y1 grad: &#x27;, grad))</span></span><br><span class="line">y2.retain_grad()</span><br><span class="line">loss.retain_grad()</span><br><span class="line"></span><br><span class="line"><span class="comment">#反向传播</span></span><br><span class="line">loss.backward()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;loss.grad:&quot;</span>, loss.grad)</span><br><span class="line"><span class="comment"># 对y1的梯度:2(y1-y2)=2(x+1-2x)，带入x=3得-4</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;y1.grad:&quot;</span>, y1.grad)</span><br><span class="line"><span class="comment"># 对y2的梯度:-2(y1-y2)=-2(x+1-2x)，带入x=3得4</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;y2.grad:&quot;</span>, y2.grad)</span><br><span class="line"><span class="comment"># 对x的梯度:2(y1-y2)*(dy1_dx-dy2_dx)=2(1-x)*(-1)，带入x=3得4</span></span><br><span class="line"><span class="built_in">print</span>(x.grad)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">loss.grad: tensor(1.)</span></span><br><span class="line"><span class="string">y1 grad:  tensor(-4.)</span></span><br><span class="line"><span class="string">y2 grad:  tensor(4.)</span></span><br><span class="line"><span class="string">x.grad: tensor(4.)</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="comment"># 不想保留梯度，torch.no_grad或detach</span></span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    loss = (y1-y2)**<span class="number">2</span> </span><br><span class="line"><span class="built_in">print</span>(loss.requires_grad)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">False</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">loss = (y1-y2)**<span class="number">2</span> </span><br><span class="line">loss.detach()</span><br><span class="line"><span class="built_in">print</span>(loss.requires_grad)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">False</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><br />
计算图在TensorBoard中的可视化：<br />
可以利用 torch.utils.tensorboard 将计算图导出到
TensorBoard进行可视化。<br />
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn </span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Net</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(Net, self).__init__()</span><br><span class="line">        self.w = nn.Parameter(torch.randn(<span class="number">2</span>,<span class="number">1</span>))</span><br><span class="line">        self.b = nn.Parameter(torch.zeros(<span class="number">1</span>,<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        y = x@self.w + self.b</span><br><span class="line">        <span class="keyword">return</span> y</span><br><span class="line"></span><br><span class="line">net = Net()</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> torch.utils.tensorboard <span class="keyword">import</span> SummaryWriter</span><br><span class="line">writer = SummaryWriter(<span class="string">&#x27;../data/tensorboard&#x27;</span>)</span><br><span class="line">writer.add_graph(net,input_to_model = torch.rand(<span class="number">10</span>,<span class="number">2</span>))</span><br><span class="line">writer.close()</span><br><span class="line"></span><br><span class="line">%load_ext tensorboard</span><br><span class="line"><span class="comment">#%tensorboard --logdir ../data/tensorboard</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> tensorboard <span class="keyword">import</span> notebook</span><br><span class="line">notebook.<span class="built_in">list</span>() </span><br><span class="line"></span><br><span class="line"><span class="comment">#在tensorboard中查看模型</span></span><br><span class="line">notebook.start(<span class="string">&quot;--logdir ../data/tensorboard&quot;</span>)</span><br></pre></td></tr></table></figure></p>
<h1 id="dataset和dataloader">Dataset和DataLoader</h1>
<p>Pytorch通常使用Dataset和DataLoader这两个工具类来构建数据管道(<span class="exturl" data-url="aHR0cHM6Ly9weXRvcmNoLm9yZy90dXRvcmlhbHMvYmVnaW5uZXIvYmFzaWNzL2RhdGFfdHV0b3JpYWwuaHRtbA==">官方讲解<i class="fa fa-external-link-alt"></i></span>)。<br />
Dataset定义了数据集的内容，它相当于一个类似列表的数据结构，具有确定的长度，能够用索引获取数据集中的元素。<br />
DataLoader定义了按batch加载数据集的方法，它是一个实现了<code>__iter__</code>方法的可迭代对象，每次迭代输出一个batch的数据。它能够控制batch的大小，batch中元素的采样方法，以及将batch结果整理成模型所需输入形式的方法，能够使用多进程读取数据。<br />
在绝大部分情况下，用户只需实现Dataset的<code>__init__</code>、<code>__len__</code>和<code>__getitem__</code>方法，就可以构建自己的数据集，并用默认数据管道进行加载。</p>
<p>获取一个batch数据的步骤(假定数据集的特征和标签分别表示为张量X和Y，数据集可以表示为(X,Y),
假定batch大小为m)：<br />
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">（1）首先确定数据集的长度n。</span><br><span class="line">（2）然后从0到n-1的范围中抽样出m个数(batch大小)。假定m=4, 拿到的结果是一个列表，类似：indices=[1,4,8,9]。</span><br><span class="line">（3）接着从数据集中去取这m个数对应下标的元素。拿到的结果是一个元组列表，</span><br><span class="line">	 类似：samples=[(X[1],Y[1]),(X[4],Y[4]),(X[8],Y[8]),(X[9],Y[9])]。</span><br><span class="line">（4）最后将结果整理成两个张量作为输出。拿到的结果是两个张量：</span><br><span class="line">	 类似batch=(features,labels)，其中features=torch.stack([X[1],X[4],X[8],X[9]])，</span><br><span class="line">	 labels=torch.stack([Y[1],Y[4],Y[8],Y[9]])。</span><br><span class="line"></span><br><span class="line">第（1）个步骤确定数据集的长度是由 Dataset 的__len__方法实现的。</span><br><span class="line">第（2）个步骤从0到n-1的范围中抽样出m个数的方法是由 DataLoader的 sampler 和 batch_sampler 参数指定的。</span><br><span class="line">  sampler参数指定单个元素抽样方法，一般无需用户设置，程序默认在DataLoader的参数shuffle=True时采用随机抽样，</span><br><span class="line">  shuffle=False时采用顺序抽样，</span><br><span class="line">  batch_sampler参数将多个抽样的元素整理成一个列表，一般无需用户设置，</span><br><span class="line">  默认方法在DataLoader的参数drop_last=True时会丢弃数据集最后一个长度不能被batch大小整除的批次，</span><br><span class="line">  在drop_last=False时保留最后一个批次。</span><br><span class="line">第（3）个步骤的核心逻辑根据下标取数据集中的元素，是由 Dataset 的__getitem__方法实现的。</span><br><span class="line">第（4）个步骤的逻辑由 DataLoader 的参数collate_fn指定。一般情况下也无需用户设置。</span><br></pre></td></tr></table></figure><br />
以下是 Dataset和
DataLoader的核心接口逻辑伪代码，不完全和源码一致：<br />
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch </span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Dataset</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">raise</span> NotImplementedError</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self,index</span>):</span><br><span class="line">        <span class="keyword">raise</span> NotImplementedError</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">DataLoader</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,dataset,batch_size,collate_fn,shuffle = <span class="literal">True</span>,drop_last = <span class="literal">False</span></span>):</span><br><span class="line">        self.dataset = dataset</span><br><span class="line">        self.sampler =torch.utils.data.RandomSampler <span class="keyword">if</span> shuffle <span class="keyword">else</span> \</span><br><span class="line">           torch.utils.data.SequentialSampler</span><br><span class="line">        self.batch_sampler = torch.utils.data.BatchSampler</span><br><span class="line">        self.sample_iter = self.batch_sampler(</span><br><span class="line">            self.sampler(<span class="built_in">range</span>(<span class="built_in">len</span>(dataset))),</span><br><span class="line">            batch_size = batch_size,drop_last = drop_last)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__next__</span>(<span class="params">self</span>):</span><br><span class="line">        indices = <span class="built_in">next</span>(self.sample_iter)</span><br><span class="line">        batch = self.collate_fn([self.dataset[i] <span class="keyword">for</span> i <span class="keyword">in</span> indices])</span><br><span class="line">        <span class="keyword">return</span> batch</span><br></pre></td></tr></table></figure><br />
官网自定义数据集示例：<br />
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> torchvision.io <span class="keyword">import</span> read_image</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">CustomImageDataset</span>(<span class="title class_ inherited__">Dataset</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, annotations_file, img_dir, transform=<span class="literal">None</span>, target_transform=<span class="literal">None</span></span>):</span><br><span class="line">        self.img_labels = pd.read_csv(annotations_file) <span class="comment"># 标签地址</span></span><br><span class="line">        self.img_dir = img_dir  <span class="comment"># 文件地址</span></span><br><span class="line">        self.transform = transform  <span class="comment"># 文件预处理函数</span></span><br><span class="line">        self.target_transform = target_transform <span class="comment"># 标签预处理函数</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.img_labels)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, idx</span>):</span><br><span class="line">        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, <span class="number">0</span>])  <span class="comment"># 第0列为地址</span></span><br><span class="line">        image = read_image(img_path)</span><br><span class="line">        label = self.img_labels.iloc[idx, <span class="number">1</span>]  <span class="comment"># 第1列为标签</span></span><br><span class="line">        <span class="keyword">if</span> self.transform:</span><br><span class="line">            image = self.transform(image)</span><br><span class="line">        <span class="keyword">if</span> self.target_transform:</span><br><span class="line">            label = self.target_transform(label)</span><br><span class="line">        <span class="keyword">return</span> image, label</span><br></pre></td></tr></table></figure></p>
<p><strong>使用Dataset创建数据集</strong><br />
（1）使用 torch.utils.data.TensorDataset
根据Tensor创建数据集（numpy的array，Pandas的DataFrame需要先转换成Tensor）。<br />
（2）使用 torchvision.datasets.ImageFolder
根据图片目录创建图片数据集。<br />
（3）继承 torch.utils.data.Dataset 创建自定义数据集。<br />
torch.utils.data.random_split
将一个数据集分割成多份，常用于分割训练集，验证集和测试集。调用Dataset的加法运算符（+）将多个数据集合并成一个数据集。<br />
<div class="tabs" id="three"><ul class="nav-tabs"><li class="tab active"><a href="#three-1">根据Tensor创建数据集</a></li><li class="tab"><a href="#three-2">根据图片目录创建图片数据集</a></li><li class="tab"><a href="#three-3">自定义数据集</a></li></ul><div class="tab-content"><div class="tab-pane active" id="three-1"><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np </span><br><span class="line"><span class="keyword">import</span> torch </span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> TensorDataset,Dataset,DataLoader,random_split </span><br><span class="line"></span><br><span class="line"><span class="comment"># 根据Tensor创建数据集</span></span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> datasets </span><br><span class="line">iris = datasets.load_iris()</span><br><span class="line">ds_iris = TensorDataset(torch.tensor(iris.data),torch.tensor(iris.target))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 分割成训练集和预测集</span></span><br><span class="line">n_train = <span class="built_in">int</span>(<span class="built_in">len</span>(ds_iris)*<span class="number">0.8</span>)</span><br><span class="line">n_valid = <span class="built_in">len</span>(ds_iris) - n_train</span><br><span class="line">ds_train,ds_valid = random_split(ds_iris,[n_train,n_valid])</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">type</span>(ds_iris))</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">type</span>(ds_train))</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">torch.utils.data.dataset.TensorDataset</span></span><br><span class="line"><span class="string">torch.utils.data.dataset.Subset</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="comment"># 使用DataLoader加载数据集</span></span><br><span class="line">dl_train,dl_valid = DataLoader(ds_train,batch_size=<span class="number">8</span>),DataLoader(ds_valid,batch_size=<span class="number">8</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> features,labels <span class="keyword">in</span> dl_train:</span><br><span class="line">    <span class="built_in">print</span>(features,labels)</span><br><span class="line">    <span class="keyword">break</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">tensor([[5.4000, 3.4000, 1.5000, 0.4000],</span></span><br><span class="line"><span class="string">        [7.2000, 3.0000, 5.8000, 1.6000],</span></span><br><span class="line"><span class="string">        [5.7000, 2.6000, 3.5000, 1.0000],</span></span><br><span class="line"><span class="string">        [6.1000, 2.6000, 5.6000, 1.4000],</span></span><br><span class="line"><span class="string">        [6.7000, 3.1000, 4.4000, 1.4000],</span></span><br><span class="line"><span class="string">        [5.8000, 2.8000, 5.1000, 2.4000],</span></span><br><span class="line"><span class="string">        [5.6000, 2.5000, 3.9000, 1.1000],</span></span><br><span class="line"><span class="string">        [6.2000, 2.2000, 4.5000, 1.5000]], </span></span><br><span class="line"><span class="string">        dtype=torch.float64) tensor([0, 2, 1, 2, 1, 2, 1, 1], dtype=torch.int32)</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="comment"># 演示加法运算符（`+`）的合并作用</span></span><br><span class="line">ds_data = ds_train + ds_valid</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;len(ds_train) = &#x27;</span>,<span class="built_in">len</span>(ds_train))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;len(ds_valid) = &#x27;</span>,<span class="built_in">len</span>(ds_valid))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;len(ds_train+ds_valid) = &#x27;</span>,<span class="built_in">len</span>(ds_data))</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">type</span>(ds_data))</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">len(ds_train) =  120</span></span><br><span class="line"><span class="string">len(ds_valid) =  30</span></span><br><span class="line"><span class="string">len(ds_train+ds_valid) =  150</span></span><br><span class="line"><span class="string">torch.utils.data.dataset.ConcatDataset</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure></div><div class="tab-pane" id="three-2"><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np </span><br><span class="line"><span class="keyword">import</span> torch </span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms,datasets </span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line">img = Image.<span class="built_in">open</span>(<span class="string">&#x27;../data/cat.jpeg&#x27;</span>)</span><br><span class="line">img</span><br><span class="line"></span><br><span class="line"><span class="comment"># 一些常用的图片增强操作</span></span><br><span class="line"><span class="comment">#随机数值翻转</span></span><br><span class="line">transforms.RandomVerticalFlip()(img)</span><br><span class="line"><span class="comment">#随机旋转</span></span><br><span class="line">transforms.RandomRotation(<span class="number">45</span>)(img)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 自定义图片增强操作</span></span><br><span class="line">transform_train = transforms.Compose([</span><br><span class="line">   transforms.RandomHorizontalFlip(), <span class="comment">#随机水平翻转</span></span><br><span class="line">   transforms.RandomVerticalFlip(), <span class="comment">#随机垂直翻转</span></span><br><span class="line">   transforms.RandomRotation(<span class="number">45</span>),  <span class="comment">#随机在45度角度内旋转</span></span><br><span class="line">   transforms.ToTensor() <span class="comment">#转换成张量</span></span><br><span class="line">  ]</span><br><span class="line">) </span><br><span class="line"><span class="comment"># 验证集不需要增强</span></span><br><span class="line">transform_valid = transforms.Compose([</span><br><span class="line">    transforms.ToTensor()</span><br><span class="line">  ]</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 根据图片目录创建数据集</span></span><br><span class="line">ds_train = datasets.ImageFolder(<span class="string">&quot;../data/cifar2/train/&quot;</span>,</span><br><span class="line">            transform = transform_train,target_transform= <span class="keyword">lambda</span> t:torch.tensor([t]).<span class="built_in">float</span>())</span><br><span class="line">ds_valid = datasets.ImageFolder(<span class="string">&quot;../data/cifar2/test/&quot;</span>,</span><br><span class="line">            transform = transform_train,target_transform= <span class="keyword">lambda</span> t:torch.tensor([t]).<span class="built_in">float</span>())</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(ds_train.class_to_idx)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">&#123;&#x27;0_airplane&#x27;: 0, &#x27;1_automobile&#x27;: 1&#125;</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="comment"># 使用DataLoader加载数据集</span></span><br><span class="line">dl_train = DataLoader(ds_train,batch_size = <span class="number">50</span>,shuffle = <span class="literal">True</span>,num_workers=<span class="number">3</span>)</span><br><span class="line">dl_valid = DataLoader(ds_valid,batch_size = <span class="number">50</span>,shuffle = <span class="literal">True</span>,num_workers=<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> features,labels <span class="keyword">in</span> dl_train:</span><br><span class="line">    <span class="built_in">print</span>(features.shape)</span><br><span class="line">    <span class="built_in">print</span>(labels.shape)</span><br><span class="line">    <span class="keyword">break</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">torch.Size([50, 3, 32, 32])</span></span><br><span class="line"><span class="string">torch.Size([50, 1])</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure></div><div class="tab-pane" id="three-3"><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 首先，对训练集文本分词构建词典。然后将训练集文本和测试集文本数据转换成token单词编码。</span></span><br><span class="line"><span class="comment"># 接着将转换成单词编码的训练集数据和测试集数据按样本分割成多个文件，一个文件代表一个样本。</span></span><br><span class="line"><span class="comment"># 最后，我们可以根据文件名列表获取对应序号的样本内容，从而构建Dataset数据集。</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np </span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd </span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> OrderedDict</span><br><span class="line"><span class="keyword">import</span> re,string</span><br><span class="line"></span><br><span class="line">MAX_WORDS = <span class="number">10000</span>  <span class="comment"># 仅考虑最高频的10000个词</span></span><br><span class="line">MAX_LEN = <span class="number">200</span>  <span class="comment"># 每个样本保留200个词的长度</span></span><br><span class="line">BATCH_SIZE = <span class="number">20</span> </span><br><span class="line"></span><br><span class="line">train_data_path = <span class="string">&#x27;data/imdb/train.tsv&#x27;</span></span><br><span class="line">test_data_path = <span class="string">&#x27;data/imdb/test.tsv&#x27;</span></span><br><span class="line">train_token_path = <span class="string">&#x27;data/imdb/train_token.tsv&#x27;</span></span><br><span class="line">test_token_path =  <span class="string">&#x27;data/imdb/test_token.tsv&#x27;</span></span><br><span class="line">train_samples_path = <span class="string">&#x27;data/imdb/train_samples/&#x27;</span></span><br><span class="line">test_samples_path =  <span class="string">&#x27;data/imdb/test_samples/&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 构建词典，并保留最高频的MAX_WORDS个词</span></span><br><span class="line"><span class="comment">#构建词典</span></span><br><span class="line">word_count_dict = &#123;&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">#清洗文本</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">clean_text</span>(<span class="params">text</span>):</span><br><span class="line">    lowercase = text.lower().replace(<span class="string">&quot;\n&quot;</span>,<span class="string">&quot; &quot;</span>)</span><br><span class="line">    stripped_html = re.sub(<span class="string">&#x27;&lt;br /&gt;&#x27;</span>, <span class="string">&#x27; &#x27;</span>,lowercase)</span><br><span class="line">    cleaned_punctuation = re.sub(<span class="string">&#x27;[%s]&#x27;</span>%re.escape(string.punctuation),<span class="string">&#x27;&#x27;</span>,stripped_html)</span><br><span class="line">    <span class="keyword">return</span> cleaned_punctuation</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(train_data_path,<span class="string">&quot;r&quot;</span>,encoding = <span class="string">&#x27;utf-8&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> f:</span><br><span class="line">        label,text = line.split(<span class="string">&quot;\t&quot;</span>)</span><br><span class="line">        cleaned_text = clean_text(text)</span><br><span class="line">        <span class="keyword">for</span> word <span class="keyword">in</span> cleaned_text.split(<span class="string">&quot; &quot;</span>):</span><br><span class="line">            word_count_dict[word] = word_count_dict.get(word,<span class="number">0</span>)+<span class="number">1</span> </span><br><span class="line"></span><br><span class="line">df_word_dict = pd.DataFrame(pd.Series(word_count_dict,name = <span class="string">&quot;count&quot;</span>))</span><br><span class="line">df_word_dict = df_word_dict.sort_values(by = <span class="string">&quot;count&quot;</span>,ascending =<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">df_word_dict = df_word_dict[<span class="number">0</span>:MAX_WORDS-<span class="number">2</span>]  </span><br><span class="line">df_word_dict[<span class="string">&quot;word_id&quot;</span>] = <span class="built_in">range</span>(<span class="number">2</span>,MAX_WORDS) <span class="comment"># 编号0和1分别留给未知词&lt;unkown&gt;和填充&lt;padding&gt;</span></span><br><span class="line"></span><br><span class="line">word_id_dict = df_word_dict[<span class="string">&quot;word_id&quot;</span>].to_dict()</span><br><span class="line"></span><br><span class="line">df_word_dict.head(<span class="number">10</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 利用构建好的词典，将文本转换成token序号</span></span><br><span class="line"><span class="comment">#转换token, 填充文本</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">pad</span>(<span class="params">data_list,pad_length</span>):</span><br><span class="line">    padded_list = data_list.copy()</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(data_list)&gt; pad_length:</span><br><span class="line">         padded_list = data_list[-pad_length:]</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(data_list)&lt; pad_length:</span><br><span class="line">         padded_list = [<span class="number">1</span>]*(pad_length-<span class="built_in">len</span>(data_list))+data_list</span><br><span class="line">    <span class="keyword">return</span> padded_list</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">text_to_token</span>(<span class="params">text_file,token_file</span>):</span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(text_file,<span class="string">&quot;r&quot;</span>,encoding = <span class="string">&#x27;utf-8&#x27;</span>) <span class="keyword">as</span> fin,\</span><br><span class="line">      <span class="built_in">open</span>(token_file,<span class="string">&quot;w&quot;</span>,encoding = <span class="string">&#x27;utf-8&#x27;</span>) <span class="keyword">as</span> fout:</span><br><span class="line">        <span class="keyword">for</span> line <span class="keyword">in</span> fin:</span><br><span class="line">            label,text = line.split(<span class="string">&quot;\t&quot;</span>)</span><br><span class="line">            cleaned_text = clean_text(text)</span><br><span class="line">            word_token_list = [word_id_dict.get(word, <span class="number">0</span>) <span class="keyword">for</span> word <span class="keyword">in</span> cleaned_text.split(<span class="string">&quot; &quot;</span>)]</span><br><span class="line">            pad_list = pad(word_token_list,MAX_LEN)</span><br><span class="line">            out_line = label+<span class="string">&quot;\t&quot;</span>+<span class="string">&quot; &quot;</span>.join([<span class="built_in">str</span>(x) <span class="keyword">for</span> x <span class="keyword">in</span> pad_list])</span><br><span class="line">            fout.write(out_line+<span class="string">&quot;\n&quot;</span>)</span><br><span class="line"></span><br><span class="line">text_to_token(train_data_path,train_token_path)</span><br><span class="line">text_to_token(test_data_path,test_token_path)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 接着将token文本按照样本分割，每个文件存放一个样本的数据</span></span><br><span class="line"><span class="comment">#分割样本</span></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(train_samples_path):</span><br><span class="line">    os.mkdir(train_samples_path)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(test_samples_path):</span><br><span class="line">    os.mkdir(test_samples_path)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">split_samples</span>(<span class="params">token_path,samples_dir</span>):</span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(token_path,<span class="string">&quot;r&quot;</span>, encoding=<span class="string">&#x27;utf-8&#x27;</span>) <span class="keyword">as</span> fin:</span><br><span class="line">        i = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> line <span class="keyword">in</span> fin:</span><br><span class="line">            <span class="keyword">with</span> <span class="built_in">open</span>(samples_dir+<span class="string">&quot;%d.txt&quot;</span>%i,<span class="string">&quot;w&quot;</span>, encoding=<span class="string">&quot;utf-8&quot;</span>) <span class="keyword">as</span> fout:</span><br><span class="line">                fout.write(line)</span><br><span class="line">            i = i+<span class="number">1</span></span><br><span class="line"></span><br><span class="line">split_samples(train_token_path,train_samples_path)</span><br><span class="line">split_samples(test_token_path,test_samples_path)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(os.listdir(train_samples_path)[<span class="number">0</span>:<span class="number">3</span>])</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">[&#x27;11303.txt&#x27;, &#x27;3644.txt&#x27;, &#x27;19987.txt&#x27;]</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="comment"># 创建数据集Dataset, 从文件名称列表中读取文件内容了</span></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">imdbDataset</span>(<span class="title class_ inherited__">Dataset</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,samples_dir</span>):</span><br><span class="line">        self.samples_dir = samples_dir</span><br><span class="line">        self.samples_paths = os.listdir(samples_dir)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.samples_paths)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self,index</span>):</span><br><span class="line">        path = self.samples_dir + self.samples_paths[index]</span><br><span class="line">        <span class="keyword">with</span> <span class="built_in">open</span>(path,<span class="string">&quot;r&quot;</span>,encoding=<span class="string">&quot;utf-8&quot;</span>) <span class="keyword">as</span> f:</span><br><span class="line">            line = f.readline()</span><br><span class="line">            label,tokens = line.split(<span class="string">&quot;\t&quot;</span>)</span><br><span class="line">            label = torch.tensor([<span class="built_in">float</span>(label)], dtype=torch.<span class="built_in">float</span>)</span><br><span class="line">            feature = torch.tensor([<span class="built_in">int</span>(x) <span class="keyword">for</span> x <span class="keyword">in</span> tokens.split(<span class="string">&quot; &quot;</span>)], dtype=torch.long)</span><br><span class="line">            <span class="keyword">return</span>  (feature,label)</span><br><span class="line"></span><br><span class="line">ds_train = imdbDataset(train_samples_path)</span><br><span class="line">ds_test = imdbDataset(test_samples_path)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">len</span>(ds_train))</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">len</span>(ds_test))</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">20000</span></span><br><span class="line"><span class="string">5000</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">dl_train = DataLoader(ds_train, batch_size=BATCH_SIZE, shuffle=<span class="literal">True</span>, num_workers=<span class="number">4</span>)</span><br><span class="line">dl_test = DataLoader(ds_test, batch_size=BATCH_SIZE, num_workers=<span class="number">4</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> features,labels <span class="keyword">in</span> dl_train:</span><br><span class="line">    <span class="built_in">print</span>(features)</span><br><span class="line">    <span class="built_in">print</span>(labels)</span><br><span class="line">    <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 最后构建模型测试一下数据集管道是否可用</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn </span><br><span class="line"><span class="keyword">import</span> importlib </span><br><span class="line"><span class="keyword">from</span> torchkeras <span class="keyword">import</span> Model,summary</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Net</span>(<span class="title class_ inherited__">Model</span>):</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(Net, self).__init__()</span><br><span class="line"></span><br><span class="line">        <span class="comment">#设置padding_idx参数后将在训练过程中将填充的token始终赋值为0向量</span></span><br><span class="line">        self.embedding = nn.Embedding(num_embeddings=MAX_WORDS, embedding_dim=<span class="number">3</span>,padding_idx=<span class="number">1</span>)</span><br><span class="line">        self.conv = nn.Sequential()</span><br><span class="line">        self.conv.add_module(<span class="string">&quot;conv_1&quot;</span>,nn.Conv1d(in_channels=<span class="number">3</span>, out_channels=<span class="number">16</span>, kernel_size=<span class="number">5</span>))</span><br><span class="line">        self.conv.add_module(<span class="string">&quot;pool_1&quot;</span>,nn.MaxPool1d(kernel_size=<span class="number">2</span>))</span><br><span class="line">        self.conv.add_module(<span class="string">&quot;relu_1&quot;</span>,nn.ReLU())</span><br><span class="line">        self.conv.add_module(<span class="string">&quot;conv_2&quot;</span>,nn.Conv1d(in_channels=<span class="number">16</span>, out_channels=<span class="number">128</span>, kernel_size=<span class="number">2</span>))</span><br><span class="line">        self.conv.add_module(<span class="string">&quot;pool_2&quot;</span>,nn.MaxPool1d(kernel_size = <span class="number">2</span>))</span><br><span class="line">        self.conv.add_module(<span class="string">&quot;relu_2&quot;</span>,nn.ReLU())</span><br><span class="line"></span><br><span class="line">        self.dense = nn.Sequential()</span><br><span class="line">        self.dense.add_module(<span class="string">&quot;flatten&quot;</span>,nn.Flatten())</span><br><span class="line">        self.dense.add_module(<span class="string">&quot;linear&quot;</span>,nn.Linear(<span class="number">6144</span>,<span class="number">1</span>))</span><br><span class="line">        self.dense.add_module(<span class="string">&quot;sigmoid&quot;</span>,nn.Sigmoid())</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self,x</span>):</span><br><span class="line">        x = self.embedding(x).transpose(<span class="number">1</span>,<span class="number">2</span>)</span><br><span class="line">        x = self.conv(x)</span><br><span class="line">        y = self.dense(x)</span><br><span class="line">        <span class="keyword">return</span> y</span><br><span class="line"></span><br><span class="line">model = Net()</span><br><span class="line"><span class="built_in">print</span>(model)</span><br><span class="line"></span><br><span class="line">model.summary(input_shape=(<span class="number">200</span>,),input_dtype=torch.LongTensor)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">Net(</span></span><br><span class="line"><span class="string">  (embedding): Embedding(10000, 3, padding_idx=1)</span></span><br><span class="line"><span class="string">  (conv): Sequential(</span></span><br><span class="line"><span class="string">    (conv_1): Conv1d(3, 16, kernel_size=(5,), stride=(1,))</span></span><br><span class="line"><span class="string">    (pool_1): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)</span></span><br><span class="line"><span class="string">    (relu_1): ReLU()</span></span><br><span class="line"><span class="string">    (conv_2): Conv1d(16, 128, kernel_size=(2,), stride=(1,))</span></span><br><span class="line"><span class="string">    (pool_2): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)</span></span><br><span class="line"><span class="string">    (relu_2): ReLU()</span></span><br><span class="line"><span class="string">  )</span></span><br><span class="line"><span class="string">  (dense): Sequential(</span></span><br><span class="line"><span class="string">    (flatten): Flatten()</span></span><br><span class="line"><span class="string">    (linear): Linear(in_features=6144, out_features=1, bias=True)</span></span><br><span class="line"><span class="string">    (sigmoid): Sigmoid()</span></span><br><span class="line"><span class="string">  )</span></span><br><span class="line"><span class="string">)</span></span><br><span class="line"><span class="string">----------------------------------------------------------------</span></span><br><span class="line"><span class="string">        Layer (type)               Output Shape         Param #</span></span><br><span class="line"><span class="string">================================================================</span></span><br><span class="line"><span class="string">         Embedding-1               [-1, 200, 3]          30,000</span></span><br><span class="line"><span class="string">            Conv1d-2              [-1, 16, 196]             256</span></span><br><span class="line"><span class="string">         MaxPool1d-3               [-1, 16, 98]               0</span></span><br><span class="line"><span class="string">              ReLU-4               [-1, 16, 98]               0</span></span><br><span class="line"><span class="string">            Conv1d-5              [-1, 128, 97]           4,224</span></span><br><span class="line"><span class="string">         MaxPool1d-6              [-1, 128, 48]               0</span></span><br><span class="line"><span class="string">              ReLU-7              [-1, 128, 48]               0</span></span><br><span class="line"><span class="string">           Flatten-8                 [-1, 6144]               0</span></span><br><span class="line"><span class="string">            Linear-9                    [-1, 1]           6,145</span></span><br><span class="line"><span class="string">          Sigmoid-10                    [-1, 1]               0</span></span><br><span class="line"><span class="string">================================================================</span></span><br><span class="line"><span class="string">Total params: 40,625</span></span><br><span class="line"><span class="string">Trainable params: 40,625</span></span><br><span class="line"><span class="string">Non-trainable params: 0</span></span><br><span class="line"><span class="string">----------------------------------------------------------------</span></span><br><span class="line"><span class="string">Input size (MB): 0.000763</span></span><br><span class="line"><span class="string">Forward/backward pass size (MB): 0.287796</span></span><br><span class="line"><span class="string">Params size (MB): 0.154972</span></span><br><span class="line"><span class="string">Estimated Total Size (MB): 0.443531</span></span><br><span class="line"><span class="string">----------------------------------------------------------------</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="comment"># 编译模型</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">accuracy</span>(<span class="params">y_pred,y_true</span>):</span><br><span class="line">    y_pred = torch.where(y_pred&gt;<span class="number">0.5</span>,torch.ones_like(y_pred, dtype=torch.float32),</span><br><span class="line">                      torch.zeros_like(y_pred,dtype=torch.float32))</span><br><span class="line">    acc = torch.mean(<span class="number">1</span>-torch.<span class="built_in">abs</span>(y_true-y_pred))</span><br><span class="line">    <span class="keyword">return</span> acc</span><br><span class="line"></span><br><span class="line">model.<span class="built_in">compile</span>(loss_func = nn.BCELoss(),optimizer= torch.optim.Adagrad(model.parameters(),lr=<span class="number">0.02</span>),</span><br><span class="line">             metrics_dict=&#123;<span class="string">&quot;accuracy&quot;</span>:accuracy&#125;)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练模型</span></span><br><span class="line">dfhistory = model.fit(<span class="number">10</span>,dl_train,dl_val=dl_test,log_step_freq=<span class="number">200</span>)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">Start Training ...</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">================================================================================2020-07-11 23:21:53</span></span><br><span class="line"><span class="string">&#123;&#x27;step&#x27;: 200, &#x27;loss&#x27;: 0.956, &#x27;accuracy&#x27;: 0.521&#125;</span></span><br><span class="line"><span class="string">&#123;&#x27;step&#x27;: 400, &#x27;loss&#x27;: 0.823, &#x27;accuracy&#x27;: 0.53&#125;</span></span><br><span class="line"><span class="string">&#123;&#x27;step&#x27;: 600, &#x27;loss&#x27;: 0.774, &#x27;accuracy&#x27;: 0.545&#125;</span></span><br><span class="line"><span class="string">&#123;&#x27;step&#x27;: 800, &#x27;loss&#x27;: 0.747, &#x27;accuracy&#x27;: 0.56&#125;</span></span><br><span class="line"><span class="string">&#123;&#x27;step&#x27;: 1000, &#x27;loss&#x27;: 0.726, &#x27;accuracy&#x27;: 0.572&#125;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"> +-------+-------+----------+----------+--------------+</span></span><br><span class="line"><span class="string">| epoch |  loss | accuracy | val_loss | val_accuracy |</span></span><br><span class="line"><span class="string">+-------+-------+----------+----------+--------------+</span></span><br><span class="line"><span class="string">|   1   | 0.726 |  0.572   |  0.661   |    0.613     |</span></span><br><span class="line"><span class="string">+-------+-------+----------+----------+--------------+</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">================================================================================2020-07-11 23:22:20</span></span><br><span class="line"><span class="string">&#123;&#x27;step&#x27;: 200, &#x27;loss&#x27;: 0.605, &#x27;accuracy&#x27;: 0.668&#125;</span></span><br><span class="line"><span class="string">&#123;&#x27;step&#x27;: 400, &#x27;loss&#x27;: 0.602, &#x27;accuracy&#x27;: 0.674&#125;</span></span><br><span class="line"><span class="string">&#123;&#x27;step&#x27;: 600, &#x27;loss&#x27;: 0.592, &#x27;accuracy&#x27;: 0.681&#125;</span></span><br><span class="line"><span class="string">&#123;&#x27;step&#x27;: 800, &#x27;loss&#x27;: 0.584, &#x27;accuracy&#x27;: 0.687&#125;</span></span><br><span class="line"><span class="string">&#123;&#x27;step&#x27;: 1000, &#x27;loss&#x27;: 0.575, &#x27;accuracy&#x27;: 0.696&#125;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"> +-------+-------+----------+----------+--------------+</span></span><br><span class="line"><span class="string">| epoch |  loss | accuracy | val_loss | val_accuracy |</span></span><br><span class="line"><span class="string">+-------+-------+----------+----------+--------------+</span></span><br><span class="line"><span class="string">|   2   | 0.575 |  0.696   |  0.553   |    0.716     |</span></span><br><span class="line"><span class="string">+-------+-------+----------+----------+--------------+</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">================================================================================2020-07-11 23:25:53</span></span><br><span class="line"><span class="string">&#123;&#x27;step&#x27;: 200, &#x27;loss&#x27;: 0.294, &#x27;accuracy&#x27;: 0.877&#125;</span></span><br><span class="line"><span class="string">&#123;&#x27;step&#x27;: 400, &#x27;loss&#x27;: 0.299, &#x27;accuracy&#x27;: 0.875&#125;</span></span><br><span class="line"><span class="string">&#123;&#x27;step&#x27;: 600, &#x27;loss&#x27;: 0.298, &#x27;accuracy&#x27;: 0.875&#125;</span></span><br><span class="line"><span class="string">&#123;&#x27;step&#x27;: 800, &#x27;loss&#x27;: 0.296, &#x27;accuracy&#x27;: 0.876&#125;</span></span><br><span class="line"><span class="string">&#123;&#x27;step&#x27;: 1000, &#x27;loss&#x27;: 0.298, &#x27;accuracy&#x27;: 0.875&#125;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"> +-------+-------+----------+----------+--------------+</span></span><br><span class="line"><span class="string">| epoch |  loss | accuracy | val_loss | val_accuracy |</span></span><br><span class="line"><span class="string">+-------+-------+----------+----------+--------------+</span></span><br><span class="line"><span class="string">|   10  | 0.298 |  0.875   |  0.464   |    0.795     |</span></span><br><span class="line"><span class="string">+-------+-------+----------+----------+--------------+</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">================================================================================2020-07-11 23:26:19</span></span><br><span class="line"><span class="string">Finished Training...</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure></div></div></div></p>
<p><strong>使用DataLoader加载数据集</strong><br />
DataLoader能够控制batch的大小，batch中元素的采样方法，以及将batch结果整理成模型所需输入形式的方法，并且能够使用多进程读取数据。<br />
DataLoader的函数签名如下：<br />
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">DataLoader(</span><br><span class="line">    dataset,  <span class="comment"># 数据集</span></span><br><span class="line">    batch_size=<span class="number">1</span>,  <span class="comment"># 批次大小</span></span><br><span class="line">    shuffle=<span class="literal">False</span>,  <span class="comment"># 是否乱序，shuffle=True和sampler和batch_sampler自定义会冲突。</span></span><br><span class="line">    sampler=<span class="literal">None</span>,  <span class="comment"># 样本采样函数，一般无需设置。</span></span><br><span class="line">    batch_sampler=<span class="literal">None</span>,  <span class="comment"># 批次采样后处理函数,一般无需设置.若设置了那么batch_size,shuffle,sampler,drop_last都不用设置了。</span></span><br><span class="line">    num_workers=<span class="number">0</span>,  <span class="comment"># 使用多进程读取数据，设置的进程数。</span></span><br><span class="line">    collate_fn=<span class="literal">None</span>,  <span class="comment"># 处理一个批次数据的函数,对采样完的每一个batch数据进行处理,输入batch输出batch.常用于padding等操作。</span></span><br><span class="line">    pin_memory=<span class="literal">False</span>,  <span class="comment"># 是否设置为锁业内存。默认为False,锁业内存不会使用虚拟内存(硬盘),从锁业内存拷贝到GPU上速度会更快。</span></span><br><span class="line">    drop_last=<span class="literal">False</span>,  <span class="comment"># 是否丢弃最后一个样本数量不足batch_size批次数据。</span></span><br><span class="line">    timeout=<span class="number">0</span>,  <span class="comment"># 加载一个数据批次的最长等待时间，一般无需设置。</span></span><br><span class="line">    worker_init_fn=<span class="literal">None</span>,  <span class="comment"># 每个worker中dataset的初始化函数，常用于 IterableDataset。一般不使用。</span></span><br><span class="line">    multiprocessing_context=<span class="literal">None</span>,</span><br><span class="line">)</span><br><span class="line"><span class="comment"># 一般情况下，我们仅仅会配置 dataset, batch_size, shuffle, num_workers, drop_last这五个参数，其他参数使用默认值即可。</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># sampler源码中使用了RandomSample(shuffle=Ture时)和SequentialSample：</span></span><br><span class="line">  <span class="comment"># RandomSample中使用了torch.randperm方法。</span></span><br><span class="line">  <span class="comment"># SequentialSample直接是iter(range(len(self.data.source)))。</span></span><br><span class="line"><span class="comment"># batch_size!=None and batch_sampler==None时用BatchSampler(sample,batch_size,drop_last)。</span></span><br><span class="line"><span class="comment"># collate_fn=None时默认会用default_conllate(batch)，它可以看作什么也没处理。如果自定义要遵循输入batch输出batch的格式。</span></span><br><span class="line"><span class="comment"># 支持迭代方式，__iter__中调用了self._get_iterator(),它又调用了_SingleProcessDataLoaderIter(当num_workers=0时),</span></span><br><span class="line">  <span class="comment"># 多进程用_MultiProcessingDataLoaderIter，不详细介绍此方法了。</span></span><br><span class="line">  <span class="comment"># _SingleProcessDataLoaderIter定义了_next_data方法,调用了_DatasetKind.create_fetcher(...).fetch(index)获取batch数据。</span></span><br><span class="line">  <span class="comment"># _SingleProcessDataLoaderIter继承自_BaseDataLoaderIter类，该类定义了__next__方法调用了_next_data方法，</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><br />
两种加载数据方式：DataLoader除了可以加载
torch.utils.data.Dataset(官网叫<span class="exturl" data-url="aHR0cHM6Ly9weXRvcmNoLm9yZy9kb2NzL3N0YWJsZS9kYXRhLmh0bWwjbWFwLXN0eWxlLWRhdGFzZXRz">map-style
datasets<i class="fa fa-external-link-alt"></i></span>) 外，还能够加载另外一种数据集
torch.utils.data.IterableDataset(官网叫<span class="exturl" data-url="aHR0cHM6Ly9weXRvcmNoLm9yZy9kb2NzL3N0YWJsZS9kYXRhLmh0bWwjaXRlcmFibGUtc3R5bGUtZGF0YXNldHM=">iterable-style
datasets<i class="fa fa-external-link-alt"></i></span>)。和Dataset数据集相当于一种列表结构不同，IterableDataset相当于一种迭代器结构，
它更加复杂，多用于流式读取数据，一般较少使用。<br />
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#构建输入数据管道</span></span><br><span class="line">ds = TensorDataset(torch.arange(<span class="number">1</span>,<span class="number">50</span>))</span><br><span class="line">dl = DataLoader(ds, batch_size=<span class="number">10</span>, shuffle=<span class="literal">True</span>, num_workers=<span class="number">2</span>, drop_last=<span class="literal">True</span>)</span><br><span class="line"><span class="comment">#迭代数据</span></span><br><span class="line"><span class="keyword">for</span> batch, <span class="keyword">in</span> dl:</span><br><span class="line">    <span class="built_in">print</span>(batch)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">tensor([43, 44, 21, 36,  9,  5, 28, 16, 20, 14])</span></span><br><span class="line"><span class="string">tensor([23, 49, 35, 38,  2, 34, 45, 18, 15, 40])</span></span><br><span class="line"><span class="string">tensor([26,  6, 27, 39,  8,  4, 24, 19, 32, 17])</span></span><br><span class="line"><span class="string">tensor([ 1, 29, 11, 47, 12, 22, 48, 42, 10,  7])</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure></p>
<h1 id="nn.functional和-nn.module">nn.functional和 nn.Module</h1>
<p>Pytorch和神经网络相关的功能组件大多都封装在torch.nn模块下。这些功能组件的绝大部分既有函数形式实现，也有类形式实现。其中nn.functional（一般引入后改名为F）有各种功能组件的函数实现。为了便于对参数进行管理，一般通过继承
nn.Module 转换成为类的实现形式，并直接封装在 nn 模块下：<br />
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 激活函数</span></span><br><span class="line">F.relu/nn.ReLU</span><br><span class="line">F.sigmoid/nn.Sigmoid</span><br><span class="line">F.tanh/nn.Tanh</span><br><span class="line">F.softmax/nn.Softmax</span><br><span class="line"></span><br><span class="line"><span class="comment"># 模型层</span></span><br><span class="line">F.linear/nn.Linear</span><br><span class="line">F.conv2d/nn.Conv2d</span><br><span class="line">F.max_pool2d/nn.MaxPool2d</span><br><span class="line">F.dropout2d/nn.Dropout2d</span><br><span class="line">F.embedding/nn.Embedding</span><br><span class="line"></span><br><span class="line"><span class="comment"># 损失函数</span></span><br><span class="line">F.binary_cross_entropy/nn.BCELoss</span><br><span class="line">F.mse_loss/nn.MSELoss</span><br><span class="line">F.cross_entropy/nn.CrossEntropyLoss</span><br></pre></td></tr></table></figure></p>
<p><strong>nn.Modules类的方法</strong>：<br />
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># nn.Modules类是一切模块的基类，创建层或者网络结构都要继承此类。</span></span><br><span class="line">add_module(name, module) <span class="comment"># 添加子模块。</span></span><br><span class="line">apply(fn) <span class="comment"># 对子模块(.children()返回的模块)应用fn函数，可用于初始化。</span></span><br><span class="line"><span class="comment"># fn实际上也作用于parameter和buffer上了。</span></span><br><span class="line"></span><br><span class="line">children() <span class="comment"># 返回所有子模块(迭代器)。</span></span><br><span class="line">modules() <span class="comment"># 返回所有层级模块(迭代器)，包括本身。</span></span><br><span class="line"><span class="comment"># 可用._moudles返回字典。</span></span><br><span class="line">parameters(recurse=<span class="literal">True</span>) <span class="comment"># 返回所有参数(可反向传播)(迭代器)。</span></span><br><span class="line"><span class="comment"># ._parameters只提供模型中显示创建的Parameter类变量(不会遍历子模块)，空字典不代表模型没有参数。</span></span><br><span class="line">buffers(recurse=<span class="literal">True</span>) <span class="comment"># 类似tf.constant，存储常量。Parameter和buffer关系：https://zhuanlan.zhihu.com/p/89442276</span></span><br><span class="line"><span class="comment"># ._buffers和._parameters一样，只提供模型中显示创建的buffers(不会遍历子模块)。</span></span><br><span class="line"></span><br><span class="line">named_children() <span class="comment"># 返回所有子模块名称和值。</span></span><br><span class="line">named_modules(memo=<span class="literal">None</span>, prefix=<span class="string">&#x27;&#x27;</span>, remove_duplicate=<span class="literal">True</span>) <span class="comment"># 返回所有层级模块名称和值，包括本身。</span></span><br><span class="line">named_parameters(prefix=<span class="string">&#x27;&#x27;</span>, recurse=<span class="literal">True</span>) <span class="comment"># 返回所有参数名称和值。</span></span><br><span class="line">named_buffers(prefix=<span class="string">&#x27;&#x27;</span>, recurse=<span class="literal">True</span>) <span class="comment"># 返回所有buffer名称和值。</span></span><br><span class="line"></span><br><span class="line">register_buffer(name, tensor, persistent=<span class="literal">True</span>) <span class="comment"># 注册buffer到model。</span></span><br><span class="line"><span class="comment"># persistent是否作为参数保存到磁盘上，在torch.save中调用model.state_dict()生效。</span></span><br><span class="line">register_parameter(name, param) <span class="comment"># 注册parameter到model。</span></span><br><span class="line"><span class="comment"># param必须是torch.nn.parameter.Parameter类的实例。</span></span><br><span class="line"></span><br><span class="line">get_buffer(target) <span class="comment"># 获取buffer，target为目标名称(字符串)。</span></span><br><span class="line">get_parameter(target) <span class="comment"># 获取parameter，target为目标名称(字符串)。</span></span><br><span class="line">get_submodule(target) <span class="comment"># 获取子模块，target为目标名称(字符串)。</span></span><br><span class="line"></span><br><span class="line">cpu() <span class="comment"># 模型(parameters and buffers)放入cpu运行。</span></span><br><span class="line">cuda(device=<span class="literal">None</span>) <span class="comment"># 模型(parameters and buffers)放入gpu运行。</span></span><br><span class="line"><span class="built_in">eval</span>() <span class="comment"># 设置成验证模式，不进行梯度计算。 </span></span><br><span class="line">train(mode=<span class="literal">True</span>) <span class="comment"># 设置成训练模式。</span></span><br><span class="line"></span><br><span class="line">load_state_dict(state_dict, strict=<span class="literal">True</span>) <span class="comment"># 加载parameter和buffer到模型中。</span></span><br><span class="line">state_dict(destination=<span class="literal">None</span>, prefix=<span class="string">&#x27;&#x27;</span>, keep_vars=<span class="literal">False</span>) <span class="comment"># 返回模型所有参数，字典格式。</span></span><br><span class="line">requires_grad_(requires_grad=<span class="literal">True</span>) <span class="comment"># 参数是否需要梯度更新？</span></span><br><span class="line">zero_grad(set_to_none=<span class="literal">False</span>) <span class="comment"># 参数梯度清零。</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 以下转换都是针对parameter和buffer。</span></span><br><span class="line">bfloat16() <span class="comment"># float转换成bfloat16(只对float有效)。</span></span><br><span class="line">half() <span class="comment"># float转换成半精度float(只对float有效)。</span></span><br><span class="line"><span class="built_in">float</span>() <span class="comment"># float转换成float(只对float有效)。</span></span><br><span class="line">double() <span class="comment"># # float转换成双精度(只对float有效)。</span></span><br><span class="line">to_empty(*, device) <span class="comment"># 拷贝parameter和buffer的空张量，到指定设备。</span></span><br><span class="line">to(*args, **kwargs)：<span class="comment"># 转换，多种选择。</span></span><br><span class="line">    <span class="comment">#to(device=None, dtype=None, non_blocking=False)</span></span><br><span class="line">    <span class="comment">#to(dtype, non_blocking=False)</span></span><br><span class="line">    <span class="comment">#to(tensor, non_blocking=False)</span></span><br><span class="line">    <span class="comment">#to(memory_format=torch.channels_last)</span></span><br></pre></td></tr></table></figure><br />
<strong>使用nn.Module来管理参数</strong>：<br />
在Pytorch中，模型的参数是需要被优化器训练的，因此，通常要设置参数为
requires_grad=True 的张量。<br />
同时，在一个模型中，往往有许多的参数，要手动管理这些参数并不是一件容易的事情。<br />
Pytorch一般将参数用nn.Parameter来表示，并且用nn.Module来管理其结构下的所有参数。<br />
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch </span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn </span><br><span class="line"><span class="keyword">import</span> torch.nn.functional  <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment"># nn.Parameter 具有 requires_grad=True 属性</span></span><br><span class="line">w = nn.Parameter(torch.randn(<span class="number">2</span>,<span class="number">2</span>))</span><br><span class="line"><span class="built_in">print</span>(w)</span><br><span class="line"><span class="built_in">print</span>(w.requires_grad)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">Parameter containing:</span></span><br><span class="line"><span class="string">tensor([[ 0.3544, -1.1643],</span></span><br><span class="line"><span class="string">        [ 1.2302,  1.3952]], requires_grad=True)</span></span><br><span class="line"><span class="string">True</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="comment"># nn.ParameterList 可以将多个nn.Parameter组成一个列表</span></span><br><span class="line">params_list = nn.ParameterList([nn.Parameter(torch.rand(<span class="number">2</span>,i)) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>,<span class="number">3</span>)])</span><br><span class="line"><span class="built_in">print</span>(params_list)</span><br><span class="line"><span class="built_in">print</span>(params_list[<span class="number">0</span>].requires_grad)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">ParameterList(</span></span><br><span class="line"><span class="string">    (0): Parameter containing: [torch.FloatTensor of size 2x1]</span></span><br><span class="line"><span class="string">    (1): Parameter containing: [torch.FloatTensor of size 2x2]</span></span><br><span class="line"><span class="string">)</span></span><br><span class="line"><span class="string">True</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="comment"># nn.ParameterDict 可以将多个nn.Parameter组成一个字典</span></span><br><span class="line">params_dict = nn.ParameterDict(&#123;<span class="string">&quot;a&quot;</span>:nn.Parameter(torch.rand(<span class="number">2</span>,<span class="number">2</span>)),</span><br><span class="line">                                <span class="string">&quot;b&quot;</span>:nn.Parameter(torch.zeros(<span class="number">2</span>))&#125;)</span><br><span class="line"><span class="built_in">print</span>(params_dict)</span><br><span class="line"><span class="built_in">print</span>(params_dict[<span class="string">&quot;a&quot;</span>].requires_grad)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">ParameterDict(</span></span><br><span class="line"><span class="string">    (a): Parameter containing: [torch.FloatTensor of size 2x2]</span></span><br><span class="line"><span class="string">    (b): Parameter containing: [torch.FloatTensor of size 2]</span></span><br><span class="line"><span class="string">)</span></span><br><span class="line"><span class="string">True</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="comment"># 可以用Module将它们管理起来</span></span><br><span class="line"><span class="comment"># module.parameters()返回一个生成器，包括其结构下的所有parameters</span></span><br><span class="line">module = nn.Module()</span><br><span class="line">module.w = w</span><br><span class="line">module.params_list = params_list</span><br><span class="line">module.params_dict = params_dict</span><br><span class="line"></span><br><span class="line">num_param = <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> param <span class="keyword">in</span> module.parameters():</span><br><span class="line">    <span class="built_in">print</span>(param,<span class="string">&quot;\n&quot;</span>)</span><br><span class="line">    num_param = num_param + <span class="number">1</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;number of Parameters =&quot;</span>, num_param)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">Parameter containing:</span></span><br><span class="line"><span class="string">tensor([[ 0.3544, -1.1643],</span></span><br><span class="line"><span class="string">        [ 1.2302,  1.3952]], requires_grad=True) </span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Parameter containing:</span></span><br><span class="line"><span class="string">tensor([[0.9391],</span></span><br><span class="line"><span class="string">        [0.1353]], requires_grad=True) </span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Parameter containing:</span></span><br><span class="line"><span class="string">tensor([[0.8012, 0.9587],</span></span><br><span class="line"><span class="string">        [0.3418, 0.7291]], requires_grad=True) </span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Parameter containing:</span></span><br><span class="line"><span class="string">tensor([[0.7729, 0.2383],</span></span><br><span class="line"><span class="string">        [0.7054, 0.9937]], requires_grad=True) </span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Parameter containing:</span></span><br><span class="line"><span class="string">tensor([0., 0.], requires_grad=True) </span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">number of Parameters = 5</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><br />
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#实践当中，一般通过继承nn.Module来构建模块类，并将所有含有需要学习的参数的部分放在构造函数中。</span></span><br><span class="line"><span class="comment">#以下范例为Pytorch中nn.Linear的源码的简化版本</span></span><br><span class="line"><span class="comment">#可以看到它将需要学习的参数放在了__init__构造函数中，并在forward中调用F.linear函数来实现计算逻辑。</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Linear</span>(nn.Module):</span><br><span class="line">    __constants__ = [<span class="string">&#x27;in_features&#x27;</span>, <span class="string">&#x27;out_features&#x27;</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_features, out_features, bias=<span class="literal">True</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(Linear, self).__init__()</span><br><span class="line">        self.in_features = in_features</span><br><span class="line">        self.out_features = out_features</span><br><span class="line">        self.weight = nn.Parameter(torch.Tensor(out_features, in_features))</span><br><span class="line">        <span class="keyword">if</span> bias:</span><br><span class="line">            self.bias = nn.Parameter(torch.Tensor(out_features))</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.register_parameter(<span class="string">&#x27;bias&#x27;</span>, <span class="literal">None</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, <span class="built_in">input</span></span>):</span><br><span class="line">        <span class="keyword">return</span> F.linear(<span class="built_in">input</span>, self.weight, self.bias)</span><br></pre></td></tr></table></figure></p>
<p><strong>使用nn.Module来管理子模块</strong>：<br />
一般情况下，我们都很少直接使用nn.Parameter来定义参数构建模型，而是通过一些拼装一些常用的模型层来构造模型。这些模型层也是继承自nn.Module的对象，本身也包括参数，属于我们要定义的模块的子模块。nn.Module提供了一些方法可以管理这些子模块：<br />
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">children() <span class="comment"># 返回生成器，包括模块下的所有子模块。</span></span><br><span class="line">named_children() <span class="comment"># 返回一个生成器，包括模块下的所有子模块，以及它们的名字。</span></span><br><span class="line">modules() <span class="comment"># 返回一个生成器，包括模块下的所有各个层级的模块，包括模块本身。</span></span><br><span class="line">named_modules() <span class="comment"># 返回一个生成器，包括模块下的所有各个层级的模块以及它们的名字，包括模块本身。</span></span><br></pre></td></tr></table></figure><br />
其中chidren()方法和named_children()方法较多使用。<br />
modules()方法和named_modules()方法较少使用，其功能可以通过多个named_children()的嵌套使用实现。<br />
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Net</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(Net, self).__init__()</span><br><span class="line"></span><br><span class="line">        self.embedding = nn.Embedding(num_embeddings=<span class="number">10000</span>,embedding_dim=<span class="number">3</span>,padding_idx=<span class="number">1</span>)</span><br><span class="line">        self.conv = nn.Sequential()</span><br><span class="line">        self.conv.add_module(<span class="string">&quot;conv_1&quot;</span>,nn.Conv1d(in_channels=<span class="number">3</span>,out_channels=<span class="number">16</span>,kernel_size=<span class="number">5</span>))</span><br><span class="line">        self.conv.add_module(<span class="string">&quot;pool_1&quot;</span>,nn.MaxPool1d(kernel_size=<span class="number">2</span>))</span><br><span class="line">        self.conv.add_module(<span class="string">&quot;relu_1&quot;</span>,nn.ReLU())</span><br><span class="line">        self.conv.add_module(<span class="string">&quot;conv_2&quot;</span>,nn.Conv1d(in_channels=<span class="number">16</span>,out_channels=<span class="number">128</span>,kernel_size=<span class="number">2</span>))</span><br><span class="line">        self.conv.add_module(<span class="string">&quot;pool_2&quot;</span>,nn.MaxPool1d(kernel_size=<span class="number">2</span>))</span><br><span class="line">        self.conv.add_module(<span class="string">&quot;relu_2&quot;</span>,nn.ReLU())</span><br><span class="line"></span><br><span class="line">        self.dense = nn.Sequential()</span><br><span class="line">        self.dense.add_module(<span class="string">&quot;flatten&quot;</span>,nn.Flatten())</span><br><span class="line">        self.dense.add_module(<span class="string">&quot;linear&quot;</span>,nn.Linear(<span class="number">6144</span>,<span class="number">1</span>))</span><br><span class="line">        self.dense.add_module(<span class="string">&quot;sigmoid&quot;</span>,nn.Sigmoid())</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self,x</span>):</span><br><span class="line">        x = self.embedding(x).transpose(<span class="number">1</span>,<span class="number">2</span>)</span><br><span class="line">        x = self.conv(x)</span><br><span class="line">        y = self.dense(x)</span><br><span class="line">        <span class="keyword">return</span> y</span><br><span class="line"></span><br><span class="line">net = Net()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 打印子模块</span></span><br><span class="line">i = <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> child <span class="keyword">in</span> net.children():</span><br><span class="line">    i+=<span class="number">1</span></span><br><span class="line">    <span class="built_in">print</span>(child,<span class="string">&quot;\n&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;child number&quot;</span>,i)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">Embedding(10000, 3, padding_idx=1) </span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Sequential(</span></span><br><span class="line"><span class="string">  (conv_1): Conv1d(3, 16, kernel_size=(5,), stride=(1,))</span></span><br><span class="line"><span class="string">  (pool_1): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)</span></span><br><span class="line"><span class="string">  (relu_1): ReLU()</span></span><br><span class="line"><span class="string">  (conv_2): Conv1d(16, 128, kernel_size=(2,), stride=(1,))</span></span><br><span class="line"><span class="string">  (pool_2): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)</span></span><br><span class="line"><span class="string">  (relu_2): ReLU()</span></span><br><span class="line"><span class="string">) </span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Sequential(</span></span><br><span class="line"><span class="string">  (flatten): Flatten()</span></span><br><span class="line"><span class="string">  (linear): Linear(in_features=6144, out_features=1, bias=True)</span></span><br><span class="line"><span class="string">  (sigmoid): Sigmoid()</span></span><br><span class="line"><span class="string">) </span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">child number 3</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="comment"># 打印子模块名字和子模块</span></span><br><span class="line">i = <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> name,child <span class="keyword">in</span> net.named_children():</span><br><span class="line">    i+=<span class="number">1</span></span><br><span class="line">    <span class="built_in">print</span>(name,<span class="string">&quot;:&quot;</span>,child,<span class="string">&quot;\n&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;child number&quot;</span>,i)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">embedding : Embedding(10000, 3, padding_idx=1) </span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">conv : Sequential(</span></span><br><span class="line"><span class="string">  (conv_1): Conv1d(3, 16, kernel_size=(5,), stride=(1,))</span></span><br><span class="line"><span class="string">  (pool_1): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)</span></span><br><span class="line"><span class="string">  (relu_1): ReLU()</span></span><br><span class="line"><span class="string">  (conv_2): Conv1d(16, 128, kernel_size=(2,), stride=(1,))</span></span><br><span class="line"><span class="string">  (pool_2): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)</span></span><br><span class="line"><span class="string">  (relu_2): ReLU()</span></span><br><span class="line"><span class="string">) </span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">dense : Sequential(</span></span><br><span class="line"><span class="string">  (flatten): Flatten()</span></span><br><span class="line"><span class="string">  (linear): Linear(in_features=6144, out_features=1, bias=True)</span></span><br><span class="line"><span class="string">  (sigmoid): Sigmoid()</span></span><br><span class="line"><span class="string">) </span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">child number 3</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="comment"># 打印各级模块</span></span><br><span class="line">i = <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> module <span class="keyword">in</span> net.modules():</span><br><span class="line">    i+=<span class="number">1</span></span><br><span class="line">    <span class="built_in">print</span>(module)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;module number:&quot;</span>,i)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">Net(</span></span><br><span class="line"><span class="string">  (embedding): Embedding(10000, 3, padding_idx=1)</span></span><br><span class="line"><span class="string">  (conv): Sequential(</span></span><br><span class="line"><span class="string">    (conv_1): Conv1d(3, 16, kernel_size=(5,), stride=(1,))</span></span><br><span class="line"><span class="string">    (pool_1): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)</span></span><br><span class="line"><span class="string">    (relu_1): ReLU()</span></span><br><span class="line"><span class="string">    (conv_2): Conv1d(16, 128, kernel_size=(2,), stride=(1,))</span></span><br><span class="line"><span class="string">    (pool_2): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)</span></span><br><span class="line"><span class="string">    (relu_2): ReLU()</span></span><br><span class="line"><span class="string">  )</span></span><br><span class="line"><span class="string">  (dense): Sequential(</span></span><br><span class="line"><span class="string">    (flatten): Flatten()</span></span><br><span class="line"><span class="string">    (linear): Linear(in_features=6144, out_features=1, bias=True)</span></span><br><span class="line"><span class="string">    (sigmoid): Sigmoid()</span></span><br><span class="line"><span class="string">  )</span></span><br><span class="line"><span class="string">)</span></span><br><span class="line"><span class="string">Embedding(10000, 3, padding_idx=1)</span></span><br><span class="line"><span class="string">Sequential(</span></span><br><span class="line"><span class="string">  (conv_1): Conv1d(3, 16, kernel_size=(5,), stride=(1,))</span></span><br><span class="line"><span class="string">  (pool_1): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)</span></span><br><span class="line"><span class="string">  (relu_1): ReLU()</span></span><br><span class="line"><span class="string">  (conv_2): Conv1d(16, 128, kernel_size=(2,), stride=(1,))</span></span><br><span class="line"><span class="string">  (pool_2): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)</span></span><br><span class="line"><span class="string">  (relu_2): ReLU()</span></span><br><span class="line"><span class="string">)</span></span><br><span class="line"><span class="string">Conv1d(3, 16, kernel_size=(5,), stride=(1,))</span></span><br><span class="line"><span class="string">MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)</span></span><br><span class="line"><span class="string">ReLU()</span></span><br><span class="line"><span class="string">Conv1d(16, 128, kernel_size=(2,), stride=(1,))</span></span><br><span class="line"><span class="string">MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)</span></span><br><span class="line"><span class="string">ReLU()</span></span><br><span class="line"><span class="string">Sequential(</span></span><br><span class="line"><span class="string">  (flatten): Flatten()</span></span><br><span class="line"><span class="string">  (linear): Linear(in_features=6144, out_features=1, bias=True)</span></span><br><span class="line"><span class="string">  (sigmoid): Sigmoid()</span></span><br><span class="line"><span class="string">)</span></span><br><span class="line"><span class="string">Flatten()</span></span><br><span class="line"><span class="string">Linear(in_features=6144, out_features=1, bias=True)</span></span><br><span class="line"><span class="string">Sigmoid()</span></span><br><span class="line"><span class="string">module number: 13</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><br />
可通过named_children方法找到embedding层，并将其参数设置为不可训练(相当于冻结embedding层)。<br />
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line">children_dict = &#123;name:module <span class="keyword">for</span> name,module <span class="keyword">in</span> net.named_children()&#125;</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(children_dict)</span><br><span class="line">embedding = children_dict[<span class="string">&quot;embedding&quot;</span>]</span><br><span class="line">embedding.requires_grad_(<span class="literal">False</span>) <span class="comment">#冻结其参数</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">&#123;&#x27;embedding&#x27;: Embedding(10000, 3, padding_idx=1), &#x27;conv&#x27;: Sequential(</span></span><br><span class="line"><span class="string">  (conv_1): Conv1d(3, 16, kernel_size=(5,), stride=(1,))</span></span><br><span class="line"><span class="string">  (pool_1): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)</span></span><br><span class="line"><span class="string">  (relu_1): ReLU()</span></span><br><span class="line"><span class="string">  (conv_2): Conv1d(16, 128, kernel_size=(2,), stride=(1,))</span></span><br><span class="line"><span class="string">  (pool_2): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)</span></span><br><span class="line"><span class="string">  (relu_2): ReLU()</span></span><br><span class="line"><span class="string">), &#x27;dense&#x27;: Sequential(</span></span><br><span class="line"><span class="string">  (flatten): Flatten()</span></span><br><span class="line"><span class="string">  (linear): Linear(in_features=6144, out_features=1, bias=True)</span></span><br><span class="line"><span class="string">  (sigmoid): Sigmoid()</span></span><br><span class="line"><span class="string">)&#125;</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="comment">#可以看到其第一层的参数已经不可以被训练了。</span></span><br><span class="line"><span class="keyword">for</span> param <span class="keyword">in</span> embedding.parameters():</span><br><span class="line">    <span class="built_in">print</span>(param.requires_grad)</span><br><span class="line">    <span class="built_in">print</span>(param.numel())</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">False</span></span><br><span class="line"><span class="string">30000</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="comment"># totchsummary包也实现了summary功能，用法一样</span></span><br><span class="line"><span class="keyword">from</span> torchkeras <span class="keyword">import</span> summary</span><br><span class="line">summary(net,input_shape=(<span class="number">200</span>,),input_dtype=torch.LongTensor)</span><br><span class="line"><span class="comment"># 不可训练参数数量增加</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">----------------------------------------------------------------</span></span><br><span class="line"><span class="string">        Layer (type)               Output Shape         Param #</span></span><br><span class="line"><span class="string">================================================================</span></span><br><span class="line"><span class="string">         Embedding-1               [-1, 200, 3]          30,000</span></span><br><span class="line"><span class="string">            Conv1d-2              [-1, 16, 196]             256</span></span><br><span class="line"><span class="string">         MaxPool1d-3               [-1, 16, 98]               0</span></span><br><span class="line"><span class="string">              ReLU-4               [-1, 16, 98]               0</span></span><br><span class="line"><span class="string">            Conv1d-5              [-1, 128, 97]           4,224</span></span><br><span class="line"><span class="string">         MaxPool1d-6              [-1, 128, 48]               0</span></span><br><span class="line"><span class="string">              ReLU-7              [-1, 128, 48]               0</span></span><br><span class="line"><span class="string">           Flatten-8                 [-1, 6144]               0</span></span><br><span class="line"><span class="string">            Linear-9                    [-1, 1]           6,145</span></span><br><span class="line"><span class="string">          Sigmoid-10                    [-1, 1]               0</span></span><br><span class="line"><span class="string">================================================================</span></span><br><span class="line"><span class="string">Total params: 40,625</span></span><br><span class="line"><span class="string">Trainable params: 10,625</span></span><br><span class="line"><span class="string">Non-trainable params: 30,000</span></span><br><span class="line"><span class="string">----------------------------------------------------------------</span></span><br><span class="line"><span class="string">Input size (MB): 0.000763</span></span><br><span class="line"><span class="string">Forward/backward pass size (MB): 0.287796</span></span><br><span class="line"><span class="string">Params size (MB): 0.154972</span></span><br><span class="line"><span class="string">Estimated Total Size (MB): 0.443531</span></span><br><span class="line"><span class="string">----------------------------------------------------------------</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><br />
<strong>创建基础的神经网络：</strong><br />
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> datasets, transforms</span><br><span class="line"></span><br><span class="line">device = <span class="string">&#x27;cuda&#x27;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&#x27;cpu&#x27;</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;Using <span class="subst">&#123;device&#125;</span> device&#x27;</span>)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">Using cuda device</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">NeuralNetwork</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(NeuralNetwork, self).__init__()</span><br><span class="line">        self.flatten = nn.Flatten()</span><br><span class="line">        self.linear_relu_stack = nn.Sequential(</span><br><span class="line">            nn.Linear(<span class="number">28</span>*<span class="number">28</span>, <span class="number">512</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Linear(<span class="number">512</span>, <span class="number">512</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Linear(<span class="number">512</span>, <span class="number">10</span>),</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = self.flatten(x)</span><br><span class="line">        logits = self.linear_relu_stack(x)</span><br><span class="line">        <span class="keyword">return</span> logits</span><br><span class="line"></span><br><span class="line">model = NeuralNetwork().to(device)</span><br><span class="line"><span class="built_in">print</span>(model)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">NeuralNetwork(</span></span><br><span class="line"><span class="string">  (flatten): Flatten(start_dim=1, end_dim=-1)</span></span><br><span class="line"><span class="string">  (linear_relu_stack): Sequential(</span></span><br><span class="line"><span class="string">    (0): Linear(in_features=784, out_features=512, bias=True)</span></span><br><span class="line"><span class="string">    (1): ReLU()</span></span><br><span class="line"><span class="string">    (2): Linear(in_features=512, out_features=512, bias=True)</span></span><br><span class="line"><span class="string">    (3): ReLU()</span></span><br><span class="line"><span class="string">    (4): Linear(in_features=512, out_features=10, bias=True)</span></span><br><span class="line"><span class="string">  )</span></span><br><span class="line"><span class="string">)</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">X = torch.rand(<span class="number">1</span>, <span class="number">28</span>, <span class="number">28</span>, device=device)</span><br><span class="line">logits = model(X)</span><br><span class="line">pred_probab = nn.Softmax(dim=<span class="number">1</span>)(logits)</span><br><span class="line">y_pred = pred_probab.argmax(<span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Predicted class: <span class="subst">&#123;y_pred&#125;</span>&quot;</span>)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">Predicted class: tensor([7], device=&#x27;cuda:0&#x27;)</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure></p>
<h1 id="containers">containers</h1>
<p>Pytorch中除了torch.nn.Module，还有很多<span class="exturl" data-url="aHR0cHM6Ly9weXRvcmNoLm9yZy9kb2NzL3N0YWJsZS9ubi5odG1sI2NvbnRhaW5lcnM=">容器<i class="fa fa-external-link-alt"></i></span>：<br />
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Sequential具有forward方法，而ModuleList等只是容器，所以Sequential用的更多些。</span></span><br><span class="line">torch.nn.Sequential(*args) </span><br><span class="line">torch.nn.ModuleList(modules=<span class="literal">None</span>) <span class="comment"># 模块列表，和python列表不同的是继承了nn.Module方法。默认module名称是从0开始。</span></span><br><span class="line">torch.nn.ModuleDict(modules=<span class="literal">None</span>) <span class="comment"># 模块字典，可自定义模型名称了。</span></span><br><span class="line">torch.nn.ParameterList(parameters=<span class="literal">None</span>)</span><br><span class="line">torch.nn.ParameterDict(parameters=<span class="literal">None</span>)</span><br></pre></td></tr></table></figure></p>
<h1 id="save">save</h1>
<p>Pytorch模型的保存：<br />
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># https://pytorch.org/tutorials/recipes/recipes/saving_and_loading_a_general_checkpoint.html</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Net</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(Net, self).__init__()</span><br><span class="line">        self.conv1 = nn.Conv2d(<span class="number">3</span>, <span class="number">6</span>, <span class="number">5</span>)</span><br><span class="line">        self.pool = nn.MaxPool2d(<span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">        self.conv2 = nn.Conv2d(<span class="number">6</span>, <span class="number">16</span>, <span class="number">5</span>)</span><br><span class="line">        self.fc1 = nn.Linear(<span class="number">16</span> * <span class="number">5</span> * <span class="number">5</span>, <span class="number">120</span>)</span><br><span class="line">        self.fc2 = nn.Linear(<span class="number">120</span>, <span class="number">84</span>)</span><br><span class="line">        self.fc3 = nn.Linear(<span class="number">84</span>, <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = self.pool(F.relu(self.conv1(x)))</span><br><span class="line">        x = self.pool(F.relu(self.conv2(x)))</span><br><span class="line">        x = x.view(-<span class="number">1</span>, <span class="number">16</span> * <span class="number">5</span> * <span class="number">5</span>)</span><br><span class="line">        x = F.relu(self.fc1(x))</span><br><span class="line">        x = F.relu(self.fc2(x))</span><br><span class="line">        x = self.fc3(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">net = Net()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Save the general checkpoint</span></span><br><span class="line">EPOCH = <span class="number">5</span></span><br><span class="line">PATH = <span class="string">&quot;model.pt&quot;</span></span><br><span class="line">LOSS = <span class="number">0.4</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 保存模型</span></span><br><span class="line">torch.save(&#123;</span><br><span class="line">            <span class="string">&#x27;epoch&#x27;</span>: EPOCH,</span><br><span class="line">            <span class="string">&#x27;model_state_dict&#x27;</span>: net.state_dict(), <span class="comment"># 包括parameter和buffer</span></span><br><span class="line">            <span class="string">&#x27;optimizer_state_dict&#x27;</span>: optimizer.state_dict(),</span><br><span class="line">            <span class="string">&#x27;loss&#x27;</span>: LOSS,</span><br><span class="line">            &#125;, PATH)</span><br><span class="line"><span class="comment"># Load the general checkpoint</span></span><br><span class="line">model = Net()</span><br><span class="line">optimizer = optim.SGD(net.parameters(), lr=<span class="number">0.001</span>, momentum=<span class="number">0.9</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载模型</span></span><br><span class="line">checkpoint = torch.load(PATH)</span><br><span class="line">model.load_state_dict(checkpoint[<span class="string">&#x27;model_state_dict&#x27;</span>])</span><br><span class="line">optimizer.load_state_dict(checkpoint[<span class="string">&#x27;optimizer_state_dict&#x27;</span>])</span><br><span class="line">epoch = checkpoint[<span class="string">&#x27;epoch&#x27;</span>]</span><br><span class="line">loss = checkpoint[<span class="string">&#x27;loss&#x27;</span>]</span><br></pre></td></tr></table></figure></p>
<h1 id="layers">layers</h1>
<p>深度学习模型一般由各种模型层组合而成。torch.nn中内置了非常丰富的各种模型层。它们都属于nn.Module的子类，具备参数管理功能。如果这些内置模型层不能够满足需求，也可以通过继承nn.Module基类构建自定义的模型层。实际上，Pytorch不区分模型和模型层，都是通过继承nn.Module进行构建。因此，只要继承nn.Module基类并实现forward方法即可自定义模型层。更多层的定义参考<span class="exturl" data-url="aHR0cHM6Ly9weXRvcmNoLm9yZy9kb2NzL3N0YWJsZS9ubi5odG1s">torch.nn<i class="fa fa-external-link-alt"></i></span>。</p>
<p>基础层：<br />
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">nn.Linear <span class="comment"># 全连接层。参数个数=输入层特征数×输出层特征数(weight)＋输出层特征数(bias)</span></span><br><span class="line"></span><br><span class="line">nn.Flatten <span class="comment"># 压平层，用于将多维张量(从维度1开始)样本压成一维张量样本。</span></span><br><span class="line"></span><br><span class="line">nn.BatchNorm1d <span class="comment"># 一维批标准化层。通过线性变换将输入批次缩放平移到稳定的均值和标准差。</span></span><br><span class="line"><span class="comment"># 可以增强模型对输入不同分布的适应性，加快模型训练速度，有轻微正则化效果。</span></span><br><span class="line"><span class="comment"># 一般在激活函数之前使用。可以用afine参数设置该层是否含有可以训练的参数。</span></span><br><span class="line"></span><br><span class="line">nn.BatchNorm2d <span class="comment"># 二维批标准化层。</span></span><br><span class="line"></span><br><span class="line">nn.BatchNorm3d <span class="comment"># 三维批标准化层。</span></span><br><span class="line"></span><br><span class="line">nn.Dropout <span class="comment"># 一维随机丢弃层。一种正则化手段。</span></span><br><span class="line"></span><br><span class="line">nn.Dropout2d <span class="comment"># 二维随机丢弃层。</span></span><br><span class="line"></span><br><span class="line">nn.Dropout3d <span class="comment"># 三维随机丢弃层。</span></span><br><span class="line"></span><br><span class="line">nn.Threshold <span class="comment"># 限幅层。当输入大于或小于阈值范围时，截断之。</span></span><br><span class="line"></span><br><span class="line">nn.ConstantPad2d <span class="comment"># 二维常数填充层。对二维张量样本填充常数扩展长度。</span></span><br><span class="line"></span><br><span class="line">nn.ReplicationPad1d <span class="comment"># 一维复制填充层。对一维张量样本通过复制边缘值填充扩展长度。</span></span><br><span class="line"></span><br><span class="line">nn.ZeroPad2d <span class="comment"># 二维零值填充层。对二维张量样本在边缘填充0值.</span></span><br><span class="line"></span><br><span class="line">nn.GroupNorm <span class="comment"># 组归一化。一种替代批归一化的方法，将通道分成若干组进行归一。</span></span><br><span class="line"><span class="comment"># 不受batch大小限制，据称性能和效果都优于BatchNorm。</span></span><br><span class="line"></span><br><span class="line">nn.LayerNorm <span class="comment"># 层归一化。较少使用。</span></span><br><span class="line"></span><br><span class="line">nn.InstanceNorm2d <span class="comment"># 样本归一化。较少使用。</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 各种归一化技术参考如下知乎文章:https://zhuanlan.zhihu.com/p/34858971</span></span><br></pre></td></tr></table></figure><br />
卷积网络相关层：<br />
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">nn.Conv1d <span class="comment"># 普通一维卷积，常用于文本。参数个数=输入通道数×卷积核尺寸(如3)×卷积核个数+卷积核尺寸(如3)。</span></span><br><span class="line"></span><br><span class="line">nn.Conv2d <span class="comment"># 普通二维卷积，常用于图像。参数个数=输入通道数×卷积核尺寸(如3乘3)×卷积核个数+卷积核尺寸(如3乘3)。</span></span><br><span class="line"><span class="comment"># 通过调整dilation参数大于1，可以变成空洞卷积，增大卷积核感受野。</span></span><br><span class="line"><span class="comment"># 通过调整groups参数不为1，可以变成分组卷积。分组卷积中不同分组使用相同的卷积核，显著减少参数数量。</span></span><br><span class="line"><span class="comment"># 当groups参数等于通道数时，相当于tensorflow中的二维深度卷积层tf.keras.layers.DepthwiseConv2D。</span></span><br><span class="line"><span class="comment"># 利用分组卷积和1乘1卷积的组合操作，可以构造相当于Keras中的二维深度可分离卷积层tf.keras.layers.SeparableConv2D。</span></span><br><span class="line"></span><br><span class="line">nn.Conv3d <span class="comment"># 普通三维卷积，常用于视频。参数个数=输入通道数×卷积核尺寸(如3乘3乘3)×卷积核个数+卷积核尺寸(如3乘3乘3)。</span></span><br><span class="line"></span><br><span class="line">nn.MaxPool1d <span class="comment"># 一维最大池化。</span></span><br><span class="line"></span><br><span class="line">nn.MaxPool2d <span class="comment"># 二维最大池化。一种下采样方式。没有需要训练的参数。</span></span><br><span class="line"></span><br><span class="line">nn.MaxPool3d <span class="comment"># 三维最大池化。</span></span><br><span class="line"></span><br><span class="line">nn.AdaptiveMaxPool2d <span class="comment"># 二维自适应最大池化。无论输入图像的尺寸如何变化，输出的图像尺寸是固定的。</span></span><br><span class="line"><span class="comment"># 该函数的实现原理，大概是通过输入图像的尺寸和要得到的输出图像的尺寸来反向推算池化算子的padding,stride等参数。</span></span><br><span class="line"></span><br><span class="line">nn.FractionalMaxPool2d <span class="comment"># 二维分数最大池化。普通最大池化通常输入尺寸是输出的整数倍。而分数最大池化则可以不必是整数。</span></span><br><span class="line"><span class="comment"># 分数最大池化使用了一些随机采样策略，有一定的正则效果，可以用它来代替普通最大池化和Dropout层。</span></span><br><span class="line"></span><br><span class="line">nn.AvgPool2d <span class="comment"># 二维平均池化。</span></span><br><span class="line"></span><br><span class="line">nn.AdaptiveAvgPool2d <span class="comment"># 二维自适应平均池化。无论输入的维度如何变化，输出的维度是固定的。</span></span><br><span class="line"></span><br><span class="line">nn.ConvTranspose2d <span class="comment"># 二维卷积转置层，俗称反卷积层。</span></span><br><span class="line"><span class="comment"># 并非卷积的逆操作，但在卷积核相同的情况下，当其输入尺寸是卷积操作输出尺寸的情况下，</span></span><br><span class="line"><span class="comment"># 卷积转置的输出尺寸恰好是卷积操作的输入尺寸。在语义分割中可用于上采样。</span></span><br><span class="line"></span><br><span class="line">nn.Upsample <span class="comment"># 上采样层，操作效果和池化相反。可以通过mode参数控制上采样策略为&quot;nearest&quot;最邻近策略或&quot;linear&quot;线性插值策略。</span></span><br><span class="line"></span><br><span class="line">nn.Unfold <span class="comment"># 滑动窗口提取层。其参数和卷积操作nn.Conv2d相同。</span></span><br><span class="line"><span class="comment"># 实际上，卷积操作可以等价于nn.Unfold和nn.Linear以及nn.Fold的一个组合。</span></span><br><span class="line"><span class="comment"># 其中nn.Unfold操作可以从输入中提取各个滑动窗口的数值矩阵，并将其压平成一维。</span></span><br><span class="line"><span class="comment"># 利用nn.Linear将nn.Unfold的输出和卷积核做乘法后，再使用nn.Fold操作将结果转换成输出图片形状。</span></span><br><span class="line"></span><br><span class="line">nn.Fold <span class="comment"># 逆滑动窗口提取层。</span></span><br></pre></td></tr></table></figure><br />
循环网络相关层：<br />
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">nn.Embedding <span class="comment"># 嵌入层。一种比Onehot更加有效的对离散特征进行编码的方法。</span></span><br><span class="line"><span class="comment"># 一般用于将输入中的单词映射为稠密向量。嵌入层的参数需要学习。</span></span><br><span class="line"></span><br><span class="line">nn.LSTM <span class="comment"># 长短记忆循环网络层【支持多层】。最普遍使用的循环网络层。</span></span><br><span class="line"><span class="comment"># 具有携带轨道，遗忘门，更新门，输出门。可以较为有效地缓解梯度消失问题，从而能够适用长期依赖问题。</span></span><br><span class="line"><span class="comment"># 设置bidirectional=True时可以得到双向LSTM。需要注意的是，</span></span><br><span class="line"><span class="comment"># 默认的输入和输出形状是(seq,batch,feature), 如果需要将batch维度放在第0维，则要设置batch_first参数设置为True。</span></span><br><span class="line"></span><br><span class="line">nn.GRU <span class="comment"># 门控循环网络层【支持多层】。LSTM的低配版，不具有携带轨道，参数数量少于LSTM，训练速度更快。</span></span><br><span class="line"></span><br><span class="line">nn.RNN <span class="comment"># 简单循环网络层【支持多层】。容易存在梯度消失，不能够适用长期依赖问题。一般较少使用。</span></span><br><span class="line"></span><br><span class="line">nn.LSTMCell <span class="comment"># 长短记忆循环网络单元。和nn.LSTM在整个序列上迭代相比，它仅在序列上迭代一步。一般较少使用。</span></span><br><span class="line"></span><br><span class="line">nn.GRUCell <span class="comment"># 门控循环网络单元。和nn.GRU在整个序列上迭代相比，它仅在序列上迭代一步。一般较少使用。</span></span><br><span class="line"></span><br><span class="line">nn.RNNCell <span class="comment"># 简单循环网络单元。和nn.RNN在整个序列上迭代相比，它仅在序列上迭代一步。一般较少使用。</span></span><br></pre></td></tr></table></figure><br />
Transformer相关层(它是目前NLP任务的主流模型的主要构成部分)：<br />
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Transformer网络结构是替代循环网络的一种结构，解决了循环网络难以并行，难以捕捉长期依赖的缺陷。</span></span><br><span class="line"><span class="comment"># Transformer网络结构由TransformerEncoder编码器和TransformerDecoder解码器组成。</span></span><br><span class="line"><span class="comment"># 编码器和解码器的核心是MultiheadAttention多头注意力层。</span></span><br><span class="line"><span class="comment"># Transformer原理介绍可以参考如下知乎文章：https://zhuanlan.zhihu.com/p/48508221</span></span><br><span class="line">nn.Transformer <span class="comment"># Transformer网络结构。</span></span><br><span class="line"></span><br><span class="line">nn.TransformerEncoder <span class="comment"># Transformer编码器结构。由多个 nn.TransformerEncoderLayer编码器层组成。</span></span><br><span class="line"></span><br><span class="line">nn.TransformerDecoder <span class="comment"># Transformer解码器结构。由多个 nn.TransformerDecoderLayer解码器层组成。</span></span><br><span class="line"></span><br><span class="line">nn.TransformerEncoderLayer <span class="comment"># Transformer的编码器层。</span></span><br><span class="line"></span><br><span class="line">nn.TransformerDecoderLayer <span class="comment"># Transformer的解码器层。</span></span><br><span class="line"></span><br><span class="line">nn.MultiheadAttention <span class="comment"># 多头注意力层。</span></span><br></pre></td></tr></table></figure></p>
<h1 id="losses">losses</h1>
<p>一般来说，监督学习的目标函数由损失函数和正则化项组成。(Objective =
Loss + Regularization)</p>
<p>Pytorch中的损失函数一般在训练模型时候指定。注意Pytorch中内置的损失函数的参数和tensorflow不同，是y_pred在前，y_true在后，而Tensorflow是y_true在前，y_pred在后。</p>
<p>对于回归模型，通常使用的内置损失函数是均方损失函数nn.MSELoss
。对于二分类模型，通常使用的是二元交叉熵损失函数nn.BCELoss
(输入已经是sigmoid激活函数之后的结果) 或者 nn.BCEWithLogitsLoss
(输入尚未经过nn.Sigmoid激活函数) 。</p>
<p>对于多分类模型，一般推荐使用交叉熵损失函数
nn.CrossEntropyLoss。(y_true需要是一维的，是类别编码。y_pred未经过nn.Softmax激活)。此外，如果多分类的y_pred经过了nn.LogSoftmax激活，可以使用nn.NLLLoss损失函数(The
negative log likelihood
loss)。这种方法和直接使用nn.CrossEntropyLoss等价。</p>
<p>也可以自定义损失函数，自定义损失函数需要接收两个张量y_pred，y_true作为输入参数，并输出一个标量作为损失函数值。Pytorch中的正则化项一般通过自定义的方式和损失函数一起添加作为目标函数。如果仅仅使用L2正则化，也可以利用优化器的weight_decay参数来实现相同的效果。</p>
<p>常用的一些内置损失函数：<br />
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 更多损失函数：https://zhuanlan.zhihu.com/p/61379965</span></span><br><span class="line"><span class="comment"># 官网：https://pytorch.org/docs/stable/nn.html#loss-functions</span></span><br><span class="line">nn.MSELoss <span class="comment"># 均方误差损失，也叫做L2损失，用于回归</span></span><br><span class="line"></span><br><span class="line">nn.L1Loss <span class="comment"># L1损失，也叫做绝对值误差损失，用于回归</span></span><br><span class="line"></span><br><span class="line">nn.SmoothL1Loss  <span class="comment"># 平滑L1损失，当输入在-1到1之间时，平滑为L2损失，用于回归</span></span><br><span class="line"></span><br><span class="line">nn.BCELoss <span class="comment"># 二元交叉熵，用于二分类，输入已经过nn.Sigmoid激活，对不平衡数据集可以用weigths参数调整类别权重</span></span><br><span class="line"></span><br><span class="line">nn.BCEWithLogitsLoss <span class="comment"># 二元交叉熵，用于二分类，输入未经过nn.Sigmoid激活</span></span><br><span class="line"></span><br><span class="line">nn.CrossEntropyLoss <span class="comment"># 交叉熵，用于多分类。</span></span><br><span class="line"><span class="comment"># 要求label为稀疏编码，输入未经过nn.Softmax激活，对不平衡数据集可以用weigths参数调整类别权重</span></span><br><span class="line"></span><br><span class="line">nn.NLLLoss <span class="comment"># 负对数似然损失，用于多分类，要求label为稀疏编码，输入经过nn.LogSoftmax激活</span></span><br><span class="line"></span><br><span class="line">nn.CosineSimilarity <span class="comment"># 余弦相似度，可用于多分类</span></span><br><span class="line"></span><br><span class="line">nn.AdaptiveLogSoftmaxWithLoss <span class="comment"># 一种适合非常多类别且类别分布很不均衡的损失函数，会自适应地将多个小类别合成一个cluster</span></span><br></pre></td></tr></table></figure></p>
<p><strong>自定义损失函数</strong>：<br />
自定义损失函数接收两个张量 y_pred 和 y_true
作为输入参数，并输出一个标量作为损失函数值。也可以对nn.Module进行子类化，重写forward方法实现损失的计算逻辑，从而得到损失函数的类的实现。</p>
<p>下面是一个Focal Loss的自定义实现示范。Focal
Loss是一种对binary_crossentropy的改进损失函数形式。它在样本不均衡和存在较多易分类的样本时相比binary_crossentropy具有明显的优势。它有两个可调参数，alpha和gamma。其中alpha参数主要用于衰减负样本的权重，gamma参数主要用于衰减容易训练样本的权重。从而让模型更加聚焦在正样本和困难样本上。这就是为什么这个损失函数叫做Focal
Loss。<br />
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 详见https://zhuanlan.zhihu.com/p/80594704</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">FocalLoss</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,gamma=<span class="number">2.0</span>,alpha=<span class="number">0.75</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.gamma = gamma</span><br><span class="line">        self.alpha = alpha</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self,y_pred,y_true</span>):</span><br><span class="line">        bce = torch.nn.BCELoss(reduction = <span class="string">&quot;none&quot;</span>)(y_pred,y_true)</span><br><span class="line">        p_t = (y_true * y_pred) + ((<span class="number">1</span> - y_true) * (<span class="number">1</span> - y_pred))</span><br><span class="line">        alpha_factor = y_true * self.alpha + (<span class="number">1</span> - y_true) * (<span class="number">1</span> - self.alpha)</span><br><span class="line">        modulating_factor = torch.<span class="built_in">pow</span>(<span class="number">1.0</span> - p_t, self.gamma)</span><br><span class="line">        loss = torch.mean(alpha_factor * modulating_factor * bce)</span><br><span class="line">        <span class="keyword">return</span> loss</span><br><span class="line"></span><br><span class="line"><span class="comment">#困难样本</span></span><br><span class="line">y_pred_hard = torch.tensor([[<span class="number">0.5</span>],[<span class="number">0.5</span>]])</span><br><span class="line">y_true_hard = torch.tensor([[<span class="number">1.0</span>],[<span class="number">0.0</span>]])</span><br><span class="line"></span><br><span class="line"><span class="comment">#容易样本</span></span><br><span class="line">y_pred_easy = torch.tensor([[<span class="number">0.9</span>],[<span class="number">0.1</span>]])</span><br><span class="line">y_true_easy = torch.tensor([[<span class="number">1.0</span>],[<span class="number">0.0</span>]])</span><br><span class="line"></span><br><span class="line">focal_loss = FocalLoss()</span><br><span class="line">bce_loss = nn.BCELoss()</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;focal_loss(hard samples):&quot;</span>, focal_loss(y_pred_hard,y_true_hard))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;bce_loss(hard samples):&quot;</span>, bce_loss(y_pred_hard,y_true_hard))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;focal_loss(easy samples):&quot;</span>, focal_loss(y_pred_easy,y_true_easy))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;bce_loss(easy samples):&quot;</span>, bce_loss(y_pred_easy,y_true_easy))</span><br><span class="line"></span><br><span class="line"><span class="comment">#可见 focal_loss让容易样本的权重衰减到原来的 0.0005/0.1054 = 0.00474</span></span><br><span class="line"><span class="comment">#而让困难样本的权重只衰减到原来的 0.0866/0.6931=0.12496</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 因此相对而言，focal_loss可以衰减容易样本的权重。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">focal_loss(hard samples): tensor(0.0866)</span></span><br><span class="line"><span class="string">bce_loss(hard samples): tensor(0.6931)</span></span><br><span class="line"><span class="string">focal_loss(easy samples): tensor(0.0005)</span></span><br><span class="line"><span class="string">bce_loss(easy samples): tensor(0.1054)</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><br />
FocalLoss的使用完整范例，可以参考下面中自定义L1和L2正则化项中的范例，该范例既演示了自定义正则化项的方法，也演示了FocalLoss的使用方法。</p>
<p><strong>自定义L1和L2正则化项</strong>：<br />
通常认为 L1
正则化可以产生稀疏权值矩阵，即产生一个稀疏模型，可以用于特征选择。而 L2
正则化可以防止模型过拟合（overfitting）。一定程度上，L1也可以防止过拟合。</p>
<p>下面以一个二分类问题为例，演示给模型的目标函数添加自定义L1和L2正则化项的方法。这个范例同时演示了上一部分的FocalLoss的使用。<br />
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np </span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd </span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataset,DataLoader,TensorDataset</span><br><span class="line"><span class="keyword">import</span> torchkeras </span><br><span class="line">%matplotlib inline</span><br><span class="line">%config InlineBackend.figure_format = <span class="string">&#x27;svg&#x27;</span></span><br><span class="line"><span class="comment"># 1, 准备数据</span></span><br><span class="line"><span class="comment">#正负样本数量</span></span><br><span class="line">n_positive,n_negative = <span class="number">200</span>,<span class="number">6000</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#生成正样本, 小圆环分布</span></span><br><span class="line">r_p = <span class="number">5.0</span> + torch.normal(<span class="number">0.0</span>,<span class="number">1.0</span>,size = [n_positive,<span class="number">1</span>]) </span><br><span class="line">theta_p = <span class="number">2</span>*np.pi*torch.rand([n_positive,<span class="number">1</span>])</span><br><span class="line">Xp = torch.cat([r_p*torch.cos(theta_p),r_p*torch.sin(theta_p)],axis = <span class="number">1</span>)</span><br><span class="line">Yp = torch.ones_like(r_p)</span><br><span class="line"></span><br><span class="line"><span class="comment">#生成负样本, 大圆环分布</span></span><br><span class="line">r_n = <span class="number">8.0</span> + torch.normal(<span class="number">0.0</span>,<span class="number">1.0</span>,size = [n_negative,<span class="number">1</span>]) </span><br><span class="line">theta_n = <span class="number">2</span>*np.pi*torch.rand([n_negative,<span class="number">1</span>])</span><br><span class="line">Xn = torch.cat([r_n*torch.cos(theta_n),r_n*torch.sin(theta_n)],axis = <span class="number">1</span>)</span><br><span class="line">Yn = torch.zeros_like(r_n)</span><br><span class="line"></span><br><span class="line"><span class="comment">#汇总样本</span></span><br><span class="line">X = torch.cat([Xp,Xn],axis = <span class="number">0</span>)</span><br><span class="line">Y = torch.cat([Yp,Yn],axis = <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#可视化</span></span><br><span class="line">plt.figure(figsize = (<span class="number">6</span>,<span class="number">6</span>))</span><br><span class="line">plt.scatter(Xp[:,<span class="number">0</span>],Xp[:,<span class="number">1</span>],c = <span class="string">&quot;r&quot;</span>)</span><br><span class="line">plt.scatter(Xn[:,<span class="number">0</span>],Xn[:,<span class="number">1</span>],c = <span class="string">&quot;g&quot;</span>)</span><br><span class="line">plt.legend([<span class="string">&quot;positive&quot;</span>,<span class="string">&quot;negative&quot;</span>])</span><br><span class="line"></span><br><span class="line">ds = TensorDataset(X,Y)</span><br><span class="line"></span><br><span class="line">ds_train,ds_valid = torch.utils.data.random_split(ds,[<span class="built_in">int</span>(<span class="built_in">len</span>(ds)*<span class="number">0.7</span>),<span class="built_in">len</span>(ds)-<span class="built_in">int</span>(<span class="built_in">len</span>(ds)*<span class="number">0.7</span>)])</span><br><span class="line">dl_train = DataLoader(ds_train,batch_size = <span class="number">100</span>,shuffle=<span class="literal">True</span>,num_workers=<span class="number">2</span>)</span><br><span class="line">dl_valid = DataLoader(ds_valid,batch_size = <span class="number">100</span>,num_workers=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2, 定义模型</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">DNNModel</span>(torchkeras.Model):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(DNNModel, self).__init__()</span><br><span class="line">        self.fc1 = nn.Linear(<span class="number">2</span>,<span class="number">4</span>)</span><br><span class="line">        self.fc2 = nn.Linear(<span class="number">4</span>,<span class="number">8</span>) </span><br><span class="line">        self.fc3 = nn.Linear(<span class="number">8</span>,<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self,x</span>):</span><br><span class="line">        x = F.relu(self.fc1(x))</span><br><span class="line">        x = F.relu(self.fc2(x))</span><br><span class="line">        y = nn.Sigmoid()(self.fc3(x))</span><br><span class="line">        <span class="keyword">return</span> y</span><br><span class="line"></span><br><span class="line">model = DNNModel()</span><br><span class="line"></span><br><span class="line">model.summary(input_shape =(<span class="number">2</span>,))</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">----------------------------------------------------------------</span></span><br><span class="line"><span class="string">        Layer (type)               Output Shape         Param #</span></span><br><span class="line"><span class="string">================================================================</span></span><br><span class="line"><span class="string">            Linear-1                    [-1, 4]              12</span></span><br><span class="line"><span class="string">            Linear-2                    [-1, 8]              40</span></span><br><span class="line"><span class="string">            Linear-3                    [-1, 1]               9</span></span><br><span class="line"><span class="string">================================================================</span></span><br><span class="line"><span class="string">Total params: 61</span></span><br><span class="line"><span class="string">Trainable params: 61</span></span><br><span class="line"><span class="string">Non-trainable params: 0</span></span><br><span class="line"><span class="string">----------------------------------------------------------------</span></span><br><span class="line"><span class="string">Input size (MB): 0.000008</span></span><br><span class="line"><span class="string">Forward/backward pass size (MB): 0.000099</span></span><br><span class="line"><span class="string">Params size (MB): 0.000233</span></span><br><span class="line"><span class="string">Estimated Total Size (MB): 0.000340</span></span><br><span class="line"><span class="string">----------------------------------------------------------------</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="comment"># 3，训练模型</span></span><br><span class="line"><span class="comment"># 准确率</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">accuracy</span>(<span class="params">y_pred,y_true</span>):</span><br><span class="line">    y_pred = torch.where(y_pred&gt;<span class="number">0.5</span>,torch.ones_like(y_pred,dtype = torch.float32),</span><br><span class="line">                      torch.zeros_like(y_pred,dtype = torch.float32))</span><br><span class="line">    acc = torch.mean(<span class="number">1</span>-torch.<span class="built_in">abs</span>(y_true-y_pred))</span><br><span class="line">    <span class="keyword">return</span> acc</span><br><span class="line"></span><br><span class="line"><span class="comment"># L2正则化</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">L2Loss</span>(<span class="params">model,alpha</span>):</span><br><span class="line">    l2_loss = torch.tensor(<span class="number">0.0</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">for</span> name, param <span class="keyword">in</span> model.named_parameters():</span><br><span class="line">        <span class="keyword">if</span> <span class="string">&#x27;bias&#x27;</span> <span class="keyword">not</span> <span class="keyword">in</span> name: <span class="comment">#一般不对偏置项使用正则</span></span><br><span class="line">            l2_loss = l2_loss + (<span class="number">0.5</span> * alpha * torch.<span class="built_in">sum</span>(torch.<span class="built_in">pow</span>(param, <span class="number">2</span>)))</span><br><span class="line">    <span class="keyword">return</span> l2_loss</span><br><span class="line"></span><br><span class="line"><span class="comment"># L1正则化</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">L1Loss</span>(<span class="params">model,beta</span>):</span><br><span class="line">    l1_loss = torch.tensor(<span class="number">0.0</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">for</span> name, param <span class="keyword">in</span> model.named_parameters():</span><br><span class="line">        <span class="keyword">if</span> <span class="string">&#x27;bias&#x27;</span> <span class="keyword">not</span> <span class="keyword">in</span> name:</span><br><span class="line">            l1_loss = l1_loss +  beta * torch.<span class="built_in">sum</span>(torch.<span class="built_in">abs</span>(param))</span><br><span class="line">    <span class="keyword">return</span> l1_loss</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将L2正则和L1正则添加到FocalLoss损失，一起作为目标函数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">focal_loss_with_regularization</span>(<span class="params">y_pred,y_true</span>):</span><br><span class="line">    focal = FocalLoss()(y_pred,y_true) </span><br><span class="line">    l2_loss = L2Loss(model,<span class="number">0.001</span>) <span class="comment">#注意设置正则化项系数</span></span><br><span class="line">    l1_loss = L1Loss(model,<span class="number">0.001</span>)</span><br><span class="line">    total_loss = focal + l2_loss + l1_loss</span><br><span class="line">    <span class="keyword">return</span> total_loss</span><br><span class="line"></span><br><span class="line">model.<span class="built_in">compile</span>(loss_func =focal_loss_with_regularization,</span><br><span class="line">              optimizer= torch.optim.Adam(model.parameters(),lr = <span class="number">0.01</span>),</span><br><span class="line">             metrics_dict=&#123;<span class="string">&quot;accuracy&quot;</span>:accuracy&#125;)</span><br><span class="line"></span><br><span class="line">dfhistory = model.fit(<span class="number">30</span>,dl_train = dl_train,dl_val = dl_valid,log_step_freq = <span class="number">30</span>)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">Start Training ...</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">================================================================================2020-07-11 23:34:17</span></span><br><span class="line"><span class="string">&#123;&#x27;step&#x27;: 30, &#x27;loss&#x27;: 0.021, &#x27;accuracy&#x27;: 0.972&#125;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"> +-------+-------+----------+----------+--------------+</span></span><br><span class="line"><span class="string">| epoch |  loss | accuracy | val_loss | val_accuracy |</span></span><br><span class="line"><span class="string">+-------+-------+----------+----------+--------------+</span></span><br><span class="line"><span class="string">|   1   | 0.022 |  0.971   |  0.025   |     0.96     |</span></span><br><span class="line"><span class="string">+-------+-------+----------+----------+--------------+</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">================================================================================2020-07-11 23:34:27</span></span><br><span class="line"><span class="string">&#123;&#x27;step&#x27;: 30, &#x27;loss&#x27;: 0.016, &#x27;accuracy&#x27;: 0.984&#125;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"> +-------+-------+----------+----------+--------------+</span></span><br><span class="line"><span class="string">| epoch |  loss | accuracy | val_loss | val_accuracy |</span></span><br><span class="line"><span class="string">+-------+-------+----------+----------+--------------+</span></span><br><span class="line"><span class="string">|   30  | 0.016 |  0.981   |  0.017   |    0.983     |</span></span><br><span class="line"><span class="string">+-------+-------+----------+----------+--------------+</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">================================================================================2020-07-11 23:34:27</span></span><br><span class="line"><span class="string">Finished Training...</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="comment"># 结果可视化</span></span><br><span class="line">fig, (ax1,ax2) = plt.subplots(nrows=<span class="number">1</span>,ncols=<span class="number">2</span>,figsize = (<span class="number">12</span>,<span class="number">5</span>))</span><br><span class="line">ax1.scatter(Xp[:,<span class="number">0</span>],Xp[:,<span class="number">1</span>], c=<span class="string">&quot;r&quot;</span>)</span><br><span class="line">ax1.scatter(Xn[:,<span class="number">0</span>],Xn[:,<span class="number">1</span>],c = <span class="string">&quot;g&quot;</span>)</span><br><span class="line">ax1.legend([<span class="string">&quot;positive&quot;</span>,<span class="string">&quot;negative&quot;</span>]);</span><br><span class="line">ax1.set_title(<span class="string">&quot;y_true&quot;</span>);</span><br><span class="line"></span><br><span class="line">Xp_pred = X[torch.squeeze(model.forward(X)&gt;=<span class="number">0.5</span>)]</span><br><span class="line">Xn_pred = X[torch.squeeze(model.forward(X)&lt;<span class="number">0.5</span>)]</span><br><span class="line"></span><br><span class="line">ax2.scatter(Xp_pred[:,<span class="number">0</span>],Xp_pred[:,<span class="number">1</span>],c = <span class="string">&quot;r&quot;</span>)</span><br><span class="line">ax2.scatter(Xn_pred[:,<span class="number">0</span>],Xn_pred[:,<span class="number">1</span>],c = <span class="string">&quot;g&quot;</span>)</span><br><span class="line">ax2.legend([<span class="string">&quot;positive&quot;</span>,<span class="string">&quot;negative&quot;</span>]);</span><br><span class="line">ax2.set_title(<span class="string">&quot;y_pred&quot;</span>)</span><br></pre></td></tr></table></figure><br />
<strong>通过优化器实现L2正则化</strong>：<br />
如果仅仅需要使用L2正则化，那么也可以利用优化器的weight_decay参数来实现。weight_decay参数可以设置参数在训练过程中的衰减，这和L2正则化的作用效果等价。<br />
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">before L2 regularization:</span><br><span class="line">gradient descent: w = w - lr * dloss_dw </span><br><span class="line"></span><br><span class="line">after L2 regularization:</span><br><span class="line">gradient descent: w = w - lr * (dloss_dw+beta*w) = (<span class="number">1</span>-lr*beta)*w - lr*dloss_dw</span><br><span class="line"></span><br><span class="line">so （<span class="number">1</span>-lr*beta）<span class="keyword">is</span> the weight decay ratio.</span><br></pre></td></tr></table></figure><br />
Pytorch的优化器支持一种称之为Per-parameter
options的操作，就是对每一个参数进行特定的学习率，权重衰减率指定，以满足更为细致的要求。<br />
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">weight_params = [param <span class="keyword">for</span> name, param <span class="keyword">in</span> model.named_parameters() <span class="keyword">if</span> <span class="string">&quot;bias&quot;</span> <span class="keyword">not</span> <span class="keyword">in</span> name]</span><br><span class="line">bias_params = [param <span class="keyword">for</span> name, param <span class="keyword">in</span> model.named_parameters() <span class="keyword">if</span> <span class="string">&quot;bias&quot;</span> <span class="keyword">in</span> name]</span><br><span class="line"></span><br><span class="line">optimizer = torch.optim.SGD([&#123;<span class="string">&#x27;params&#x27;</span>: weight_params, <span class="string">&#x27;weight_decay&#x27;</span>:<span class="number">1e-5</span>&#125;,</span><br><span class="line">                             &#123;<span class="string">&#x27;params&#x27;</span>: bias_params, <span class="string">&#x27;weight_decay&#x27;</span>:<span class="number">0</span>&#125;],</span><br><span class="line">                            lr=<span class="number">1e-2</span>, momentum=<span class="number">0.9</span>)</span><br></pre></td></tr></table></figure></p>
<h1 id="tensorboard可视化">TensorBoard可视化</h1>
<p>TensorBoard是炼丹可视化辅助工具。它原是TensorFlow的小弟，但它也能够很好地和Pytorch进行配合。甚至在Pytorch中使用TensorBoard比TensorFlow中使用TensorBoard还要来的更加简单和自然。Pytorch中利用TensorBoard可视化的大概过程如下：<br />
（1）在Pytorch中指定一个目录创建一个torch.utils.tensorboard.SummaryWriter日志写入器。<br />
（2）根据需要可视化的信息，利用日志写入器将相应信息日志写入我们指定的目录。<br />
（3）传入日志目录作为参数启动TensorBoard，然后就可以在TensorBoard中愉快地看片了。</p>
<p>我们主要介绍Pytorch中利用TensorBoard进行如下方面信息的可视化的方法：<br />
可视化模型结构：writer.add_graph<br />
可视化指标变化：writer.add_scalar<br />
可视化参数分布：writer.add_histogram<br />
可视化原始图像：writer.add_image 或 writer.add_images<br />
可视化人工绘图：writer.add_figure</p>
<p><strong>可视化模型结构</strong>：<br />
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch </span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.utils.tensorboard <span class="keyword">import</span> SummaryWriter</span><br><span class="line"><span class="keyword">from</span> torchkeras <span class="keyword">import</span> Model,summary</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Net</span>(nn.Module):</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(Net, self).__init__()</span><br><span class="line">        self.conv1 = nn.Conv2d(in_channels=<span class="number">3</span>,out_channels=<span class="number">32</span>,kernel_size = <span class="number">3</span>)</span><br><span class="line">        self.pool = nn.MaxPool2d(kernel_size = <span class="number">2</span>,stride = <span class="number">2</span>)</span><br><span class="line">        self.conv2 = nn.Conv2d(in_channels=<span class="number">32</span>,out_channels=<span class="number">64</span>,kernel_size = <span class="number">5</span>)</span><br><span class="line">        self.dropout = nn.Dropout2d(p = <span class="number">0.1</span>)</span><br><span class="line">        self.adaptive_pool = nn.AdaptiveMaxPool2d((<span class="number">1</span>,<span class="number">1</span>))</span><br><span class="line">        self.flatten = nn.Flatten()</span><br><span class="line">        self.linear1 = nn.Linear(<span class="number">64</span>,<span class="number">32</span>)</span><br><span class="line">        self.relu = nn.ReLU()</span><br><span class="line">        self.linear2 = nn.Linear(<span class="number">32</span>,<span class="number">1</span>)</span><br><span class="line">        self.sigmoid = nn.Sigmoid()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self,x</span>):</span><br><span class="line">        x = self.conv1(x)</span><br><span class="line">        x = self.pool(x)</span><br><span class="line">        x = self.conv2(x)</span><br><span class="line">        x = self.pool(x)</span><br><span class="line">        x = self.dropout(x)</span><br><span class="line">        x = self.adaptive_pool(x)</span><br><span class="line">        x = self.flatten(x)</span><br><span class="line">        x = self.linear1(x)</span><br><span class="line">        x = self.relu(x)</span><br><span class="line">        x = self.linear2(x)</span><br><span class="line">        y = self.sigmoid(x)</span><br><span class="line">        <span class="keyword">return</span> y</span><br><span class="line"></span><br><span class="line">net = Net()</span><br><span class="line"><span class="built_in">print</span>(net)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">Net(</span></span><br><span class="line"><span class="string">  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1))</span></span><br><span class="line"><span class="string">  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)</span></span><br><span class="line"><span class="string">  (conv2): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1))</span></span><br><span class="line"><span class="string">  (dropout): Dropout2d(p=0.1, inplace=False)</span></span><br><span class="line"><span class="string">  (adaptive_pool): AdaptiveMaxPool2d(output_size=(1, 1))</span></span><br><span class="line"><span class="string">  (flatten): Flatten()</span></span><br><span class="line"><span class="string">  (linear1): Linear(in_features=64, out_features=32, bias=True)</span></span><br><span class="line"><span class="string">  (relu): ReLU()</span></span><br><span class="line"><span class="string">  (linear2): Linear(in_features=32, out_features=1, bias=True)</span></span><br><span class="line"><span class="string">  (sigmoid): Sigmoid()</span></span><br><span class="line"><span class="string">)</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">summary(net,input_shape= (<span class="number">3</span>,<span class="number">32</span>,<span class="number">32</span>))</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">----------------------------------------------------------------</span></span><br><span class="line"><span class="string">        Layer (type)               Output Shape         Param #</span></span><br><span class="line"><span class="string">================================================================</span></span><br><span class="line"><span class="string">            Conv2d-1           [-1, 32, 30, 30]             896</span></span><br><span class="line"><span class="string">         MaxPool2d-2           [-1, 32, 15, 15]               0</span></span><br><span class="line"><span class="string">            Conv2d-3           [-1, 64, 11, 11]          51,264</span></span><br><span class="line"><span class="string">         MaxPool2d-4             [-1, 64, 5, 5]               0</span></span><br><span class="line"><span class="string">         Dropout2d-5             [-1, 64, 5, 5]               0</span></span><br><span class="line"><span class="string"> AdaptiveMaxPool2d-6             [-1, 64, 1, 1]               0</span></span><br><span class="line"><span class="string">           Flatten-7                   [-1, 64]               0</span></span><br><span class="line"><span class="string">            Linear-8                   [-1, 32]           2,080</span></span><br><span class="line"><span class="string">              ReLU-9                   [-1, 32]               0</span></span><br><span class="line"><span class="string">           Linear-10                    [-1, 1]              33</span></span><br><span class="line"><span class="string">          Sigmoid-11                    [-1, 1]               0</span></span><br><span class="line"><span class="string">================================================================</span></span><br><span class="line"><span class="string">Total params: 54,273</span></span><br><span class="line"><span class="string">Trainable params: 54,273</span></span><br><span class="line"><span class="string">Non-trainable params: 0</span></span><br><span class="line"><span class="string">----------------------------------------------------------------</span></span><br><span class="line"><span class="string">Input size (MB): 0.011719</span></span><br><span class="line"><span class="string">Forward/backward pass size (MB): 0.359634</span></span><br><span class="line"><span class="string">Params size (MB): 0.207035</span></span><br><span class="line"><span class="string">Estimated Total Size (MB): 0.578388</span></span><br><span class="line"><span class="string">----------------------------------------------------------------</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">writer = SummaryWriter(<span class="string">&#x27;../data/tensorboard&#x27;</span>)</span><br><span class="line">writer.add_graph(net,input_to_model = torch.rand(<span class="number">1</span>,<span class="number">3</span>,<span class="number">32</span>,<span class="number">32</span>))</span><br><span class="line">writer.close()</span><br><span class="line"></span><br><span class="line">%load_ext tensorboard</span><br><span class="line"><span class="comment">#%tensorboard --logdir ../data/tensorboard</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> tensorboard <span class="keyword">import</span> notebook</span><br><span class="line"><span class="comment">#查看启动的tensorboard程序</span></span><br><span class="line">notebook.<span class="built_in">list</span>() </span><br><span class="line"></span><br><span class="line"><span class="comment">#启动tensorboard程序</span></span><br><span class="line">notebook.start(<span class="string">&quot;--logdir ../data/tensorboard&quot;</span>)</span><br><span class="line"><span class="comment">#等价于在命令行中执行 tensorboard --logdir ../data/tensorboard</span></span><br><span class="line"><span class="comment">#可以在浏览器中打开 http://localhost:6006/ 查看</span></span><br></pre></td></tr></table></figure><br />
<strong>可视化指标变化</strong>：<br />
有时候在训练过程中，如果能够实时动态地查看loss和各种metric的变化曲线，那么无疑可以帮助我们更加直观地了解模型的训练情况。</p>
<p>注意，writer.add_scalar仅能对标量的值的变化进行可视化。因此它一般用于对loss和metric的变化进行可视化分析。<br />
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np </span><br><span class="line"><span class="keyword">import</span> torch </span><br><span class="line"><span class="keyword">from</span> torch.utils.tensorboard <span class="keyword">import</span> SummaryWriter</span><br><span class="line"></span><br><span class="line"><span class="comment"># f(x) = a*x**2 + b*x + c的最小值</span></span><br><span class="line">x = torch.tensor(<span class="number">0.0</span>,requires_grad = <span class="literal">True</span>) <span class="comment"># x需要被求导</span></span><br><span class="line">a = torch.tensor(<span class="number">1.0</span>)</span><br><span class="line">b = torch.tensor(-<span class="number">2.0</span>)</span><br><span class="line">c = torch.tensor(<span class="number">1.0</span>)</span><br><span class="line"></span><br><span class="line">optimizer = torch.optim.SGD(params=[x],lr = <span class="number">0.01</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">f</span>(<span class="params">x</span>):</span><br><span class="line">    result = a*torch.<span class="built_in">pow</span>(x,<span class="number">2</span>) + b*x + c </span><br><span class="line">    <span class="keyword">return</span>(result)</span><br><span class="line"></span><br><span class="line">writer = SummaryWriter(<span class="string">&#x27;../data/tensorboard&#x27;</span>)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">500</span>):</span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    y = f(x)</span><br><span class="line">    y.backward()</span><br><span class="line">    optimizer.step()</span><br><span class="line">    writer.add_scalar(<span class="string">&quot;x&quot;</span>,x.item(),i) <span class="comment">#日志中记录x在第step i 的值</span></span><br><span class="line">    writer.add_scalar(<span class="string">&quot;y&quot;</span>,y.item(),i) <span class="comment">#日志中记录y在第step i 的值</span></span><br><span class="line"></span><br><span class="line">writer.close()</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;y=&quot;</span>,f(x).data,<span class="string">&quot;;&quot;</span>,<span class="string">&quot;x=&quot;</span>,x.data)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">y= tensor(0.) ; x= tensor(1.0000)</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><br />
<strong>可视化参数分布</strong>：<br />
如果需要对模型的参数(一般非标量)在训练过程中的变化进行可视化，可以使用
writer.add_histogram。</p>
<p>它能够观测张量值分布的直方图随训练步骤的变化趋势。<br />
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np </span><br><span class="line"><span class="keyword">import</span> torch </span><br><span class="line"><span class="keyword">from</span> torch.utils.tensorboard <span class="keyword">import</span> SummaryWriter</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建正态分布的张量模拟参数矩阵</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">norm</span>(<span class="params">mean,std</span>):</span><br><span class="line">    t = std*torch.randn((<span class="number">100</span>,<span class="number">20</span>))+mean</span><br><span class="line">    <span class="keyword">return</span> t</span><br><span class="line"></span><br><span class="line">writer = SummaryWriter(<span class="string">&#x27;../data/tensorboard&#x27;</span>)</span><br><span class="line"><span class="keyword">for</span> step,mean <span class="keyword">in</span> <span class="built_in">enumerate</span>(<span class="built_in">range</span>(-<span class="number">10</span>,<span class="number">10</span>,<span class="number">1</span>)):</span><br><span class="line">    w = norm(mean,<span class="number">1</span>)</span><br><span class="line">    writer.add_histogram(<span class="string">&quot;w&quot;</span>,w, step)</span><br><span class="line">    writer.flush()</span><br><span class="line">writer.close()</span><br></pre></td></tr></table></figure><br />
<strong>可视化原始图像</strong>：<br />
如果我们做图像相关的任务，也可以将原始的图片在tensorboard中进行可视化展示。</p>
<p>如果只写入一张图片信息，可以使用writer.add_image。<br />
如果要写入多张图片信息，可以使用writer.add_images。<br />
也可以用
torchvision.utils.make_grid将多张图片拼成一张图片，然后用writer.add_image写入。</p>
<p>注意，传入的是代表图片信息的Pytorch中的张量数据。<br />
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataset,DataLoader</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms,datasets </span><br><span class="line"></span><br><span class="line"></span><br><span class="line">transform_train = transforms.Compose(</span><br><span class="line">    [transforms.ToTensor()])</span><br><span class="line">transform_valid = transforms.Compose(</span><br><span class="line">    [transforms.ToTensor()])</span><br><span class="line"></span><br><span class="line">ds_train = datasets.ImageFolder(<span class="string">&quot;../data/cifar2/train/&quot;</span>,</span><br><span class="line">            transform = transform_train,target_transform= <span class="keyword">lambda</span> t:torch.tensor([t]).<span class="built_in">float</span>())</span><br><span class="line">ds_valid = datasets.ImageFolder(<span class="string">&quot;../data/cifar2/test/&quot;</span>,</span><br><span class="line">            transform = transform_train,target_transform= <span class="keyword">lambda</span> t:torch.tensor([t]).<span class="built_in">float</span>())</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(ds_train.class_to_idx)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">&#123;&#x27;0_airplane&#x27;: 0, &#x27;1_automobile&#x27;: 1&#125;</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">dl_train = DataLoader(ds_train,batch_size = <span class="number">50</span>,shuffle = <span class="literal">True</span>,num_workers=<span class="number">3</span>)</span><br><span class="line">dl_valid = DataLoader(ds_valid,batch_size = <span class="number">50</span>,shuffle = <span class="literal">True</span>,num_workers=<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">dl_train_iter = <span class="built_in">iter</span>(dl_train)</span><br><span class="line">images, labels = dl_train_iter.<span class="built_in">next</span>()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 仅查看一张图片</span></span><br><span class="line">writer = SummaryWriter(<span class="string">&#x27;../data/tensorboard&#x27;</span>)</span><br><span class="line">writer.add_image(<span class="string">&#x27;images[0]&#x27;</span>, images[<span class="number">0</span>])</span><br><span class="line">writer.close()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将多张图片拼接成一张图片，中间用黑色网格分割</span></span><br><span class="line">writer = SummaryWriter(<span class="string">&#x27;../data/tensorboard&#x27;</span>)</span><br><span class="line"><span class="comment"># create grid of images</span></span><br><span class="line">img_grid = torchvision.utils.make_grid(images)</span><br><span class="line">writer.add_image(<span class="string">&#x27;image_grid&#x27;</span>, img_grid)</span><br><span class="line">writer.close()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将多张图片直接写入</span></span><br><span class="line">writer = SummaryWriter(<span class="string">&#x27;../data/tensorboard&#x27;</span>)</span><br><span class="line">writer.add_images(<span class="string">&quot;images&quot;</span>,images,global_step = <span class="number">0</span>)</span><br><span class="line">writer.close()</span><br></pre></td></tr></table></figure><br />
<strong>可视化人工绘图</strong>：<br />
如果我们将matplotlib绘图的结果再 tensorboard中展示，可以使用
add_figure.</p>
<p>注意，和writer.add_image不同的是，writer.add_figure需要传入matplotlib的figure对象。<br />
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataset,DataLoader</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms,datasets </span><br><span class="line"></span><br><span class="line">transform_train = transforms.Compose(</span><br><span class="line">    [transforms.ToTensor()])</span><br><span class="line">transform_valid = transforms.Compose(</span><br><span class="line">    [transforms.ToTensor()])</span><br><span class="line"></span><br><span class="line">ds_train = datasets.ImageFolder(<span class="string">&quot;../data/cifar2/train/&quot;</span>,</span><br><span class="line">            transform = transform_train,target_transform= <span class="keyword">lambda</span> t:torch.tensor([t]).<span class="built_in">float</span>())</span><br><span class="line">ds_valid = datasets.ImageFolder(<span class="string">&quot;../data/cifar2/test/&quot;</span>,</span><br><span class="line">            transform = transform_train,target_transform= <span class="keyword">lambda</span> t:torch.tensor([t]).<span class="built_in">float</span>())</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(ds_train.class_to_idx)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">&#123;&#x27;0_airplane&#x27;: 0, &#x27;1_automobile&#x27;: 1&#125;</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">%matplotlib inline</span><br><span class="line">%config InlineBackend.figure_format = <span class="string">&#x27;svg&#x27;</span></span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt </span><br><span class="line"></span><br><span class="line">figure = plt.figure(figsize=(<span class="number">8</span>,<span class="number">8</span>)) </span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">9</span>):</span><br><span class="line">    img,label = ds_train[i]</span><br><span class="line">    img = img.permute(<span class="number">1</span>,<span class="number">2</span>,<span class="number">0</span>)</span><br><span class="line">    ax=plt.subplot(<span class="number">3</span>,<span class="number">3</span>,i+<span class="number">1</span>)</span><br><span class="line">    ax.imshow(img.numpy())</span><br><span class="line">    ax.set_title(<span class="string">&quot;label = %d&quot;</span>%label.item())</span><br><span class="line">    ax.set_xticks([])</span><br><span class="line">    ax.set_yticks([]) </span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Tensorboard显示</span></span><br><span class="line">writer = SummaryWriter(<span class="string">&#x27;../data/tensorboard&#x27;</span>)</span><br><span class="line">writer.add_figure(<span class="string">&#x27;figure&#x27;</span>,figure,global_step=<span class="number">0</span>)</span><br><span class="line">writer.close()         </span><br></pre></td></tr></table></figure></p>
<h1 id="参考资料">参考资料</h1>
<p><span class="exturl" data-url="aHR0cHM6Ly9weXRvcmNoLm9yZy90dXRvcmlhbHMvYmVnaW5uZXIvZGVlcF9sZWFybmluZ182MG1pbl9ibGl0ei5odG1s">Pytorch官网<i class="fa fa-external-link-alt"></i></span><br />
<span class="exturl" data-url="aHR0cHM6Ly96aC12Mi5kMmwuYWkvY2hhcHRlcl9wcmVsaW1pbmFyaWVzL25kYXJyYXkuaHRtbA==">动手学深度学习<i class="fa fa-external-link-alt"></i></span><br />
<span class="exturl" data-url="aHR0cHM6Ly9qYWNraWV4aWFvLmdpdGh1Yi5pby9lYXRfcHl0b3JjaF9pbl8yMF9kYXlzLzIuJUU2JUEwJUI4JUU1JUJGJTgzJUU2JUE2JTgyJUU1JUJGJUI1LzItMSUyQyVFNSVCQyVBMCVFOSU4NyU4RiVFNiU5NSVCMCVFNiU4RCVBRSVFNyVCQiU5MyVFNiU5RSU4NC8=">20天吃掉那只Pytorch<i class="fa fa-external-link-alt"></i></span><br />
<span class="exturl" data-url="aHR0cHM6Ly93d3cuYmlsaWJpbGkuY29tL3ZpZGVvL0JWMW92NDExTTd4TD9zcG1faWRfZnJvbT0zMzMuOTk5LjAuMA==">PyTorch视频精讲<i class="fa fa-external-link-alt"></i></span><br />
<span class="exturl" data-url="aHR0cHM6Ly93d3cuam1sci5vcmcvcGFwZXJzL3ZvbHVtZTE4LzE3LTQ2OC8xNy00NjgucGRm">Automatic
Differentiation in Machine Learning: a Survey<i class="fa fa-external-link-alt"></i></span></p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://soundmemories.github.io/2021/02/12/Graph/03.GraphSAGE/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avstar.png">
      <meta itemprop="name" content="SoundMemories">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="SoundMemories">
      <meta itemprop="description" content="今日事，今日毕">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | SoundMemories">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2021/02/12/Graph/03.GraphSAGE/" class="post-title-link" itemprop="url">GraphSAGE</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2021-02-12 00:00:00" itemprop="dateCreated datePublished" datetime="2021-02-12T00:00:00+08:00">2021-02-12</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Graph/" itemprop="url" rel="index"><span itemprop="name">Graph</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>4.3k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>16 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="摘要">摘要</h1>
<p>大型图中节点的低维embedding已被证明在各种预测任务中非常有用，从内容推荐到识别蛋白质功能。然而，大多数现有的方法都要求在embedding训练期间图中的所有节点都存在；这些以前的方法本质上是<strong>transductive</strong>（直推式），不能自然地推广到看不见的节点。这里我们介绍GraphSAGE，这是一个通用<strong>inductive</strong>（归纳式）框架，它利用节点特征信息（例如，文本属性）高效地为以前看不见的数据生成节点embedding。我们学习的不是为每个节点训练单个embedding，而是通过从节点的局部邻域采样和聚合特征来生成嵌入的函数。</p>
<p>GraphSAGE是一个经典的基于空域的算法，它从两个方面对传统的GCN做了改进：<br />
（1）<strong>在训练时的，采样方式将GCN的全图采样优化到部分以节点为中心的邻居抽样，这使得大规模图数据的分布式训练成为可能，并且使得网络可以学习没有见过的节点，这也使得GraphSAGE可以做归纳学习（Inductive
Learning）</strong>。<br />
（2）GraphSAGE研究了若干种<strong>邻居聚合</strong>的方式，并通过实验和理论分析对比了不同聚合方式的优缺点。</p>
<p><strong>归纳学习（Inductive
Learning）</strong>：指可以对训练过程中见不到的数据直接计算而不需要重新对整个图进行学习。<br />
<strong>直推学习（Transductive
Learning）</strong>：指所有的数据都可以在训练的时候拿到，学习的过程是在这个固定的图上进行学习，一旦图中的某些节点发生变化，则需要对整个图进行重新训练和学习。</p>
<h1 id="introduction">Introduction</h1>
<p><img src="/images/GraphSAGE/1.png" width="80%"></p>
<p>GraphSAGE的算法核心是<strong>将整张图的采样优化到当前邻居节点的采样</strong>，因此从邻居<strong>采样</strong>（Sample）和邻居<strong>聚合</strong>（aggregate）两个方面来对GraphSAGE进行解释。具体流程如上图所示，可分为三个步骤：</p>
<ol type="1">
<li>对图中每个顶点邻居顶点进行采样，因为每个节点的度是不一致的，为了计算高效，
为每个节点采样固定数量的邻居。</li>
<li>根据聚合函数聚合邻居顶点蕴含的信息。</li>
<li>得到图中各顶点的向量表示供下游任务使用。</li>
</ol>
<h1 id="embedding-generation">Embedding generation</h1>
<p>GraphSAGE的前向传播算法如下，前向传播描述了如何使用聚合函数对节点的邻居信息进行聚合，从而生成节点embedding：</p>
<ol type="1">
<li>首先确定深度K，代表聚合深度，对每一层 <span
class="math inline">\(k=1...K\)</span> 进行遍历；</li>
<li>在当前层<span
class="math inline">\(k\)</span>，<strong>采样</strong>固定size大小的邻居节点进行<strong>聚合</strong>，得到<span
class="math inline">\(h_{N(v)}^k\)</span>，和自己进行CONCAT，做一个线性变换经过激活得到<span
class="math inline">\(h_{v}^k\)</span>，再归一化。</li>
<li>每层都按（1）（2）处理，最后得到顶点的embedding。</li>
</ol>
<p>即在每次迭代(或搜索深度)，顶点从它们的局部邻居聚合信息，并且随着这个过程的迭代，顶点会从越来越远的地方获得信息。</p>
<p><strong>采样</strong>：出于对计算效率的考虑，对每个顶点采样一定数量的邻居顶点作为待聚合信息的顶点。设采样数量为<span
class="math inline">\(m\)</span>，若顶点邻居数少于<span
class="math inline">\(m\)</span>,则采用有放回的抽样方法，直到采样出<span
class="math inline">\(m\)</span>个顶点。若顶点邻居数大于<span
class="math inline">\(m\)</span>，则采用无放回的抽样。作者给出了建议：在二层
<span class="math inline">\(K=2\)</span> 时，第一次采样数量 <span
class="math inline">\(S_1\)</span> 和第二次采样数量 <span
class="math inline">\(S_2\)</span> 满足公式：<span
class="math inline">\(S_1 · S_2 \leq 500\)</span>。<br />
<strong>聚合</strong>：之后会单独介绍。<br />
<img src="/images/GraphSAGE/2.png" width="80%"></p>
<p>举个例子：<br />
<img src="/images/GraphSAGE/2.1.png" width="80%"></p>
<p>在GraphSAGE之前的GCN模型中，都是采用的全图的训练方式，也就是说每一轮的迭代都要对全图的节点进行更新，当图的规模很大时，这种训练方式无疑是很耗时甚至无法更新的。mini-batch的训练时深度学习一个非常重要的特点，那么能否将mini-batch的思想用到GraphSAGE中呢，GraphSAGE提出了一个解决方案：<strong>只需记录当前mini-batch用到的所有节点（包括邻居节点），存储下来，之后再进行训练，这样每次训练就不需要保存整个图的信息</strong>。<br />
<img src="/images/GraphSAGE/3.png" width="80%"><br />
注意，mini-batch采样是从中心点<span
class="math inline">\(K\)</span>层到最外层<span
class="math inline">\(1\)</span>，在聚合时是从最外层<span
class="math inline">\(1\)</span>到中心层<span
class="math inline">\(K\)</span>的。</p>
<p>举个例子：<br />
<img src="/images/GraphSAGE/3.1.png" width="80%"><br />
<img src="/images/GraphSAGE/3.2.png" width="80%"></p>
<h1 id="aggregator-architectures">Aggregator Architectures</h1>
<p>在图中顶点的邻居是无序的，所以希望构造出的聚合函数是<strong>对称的</strong>（即也就是对它输入的各种排列，函数的输出结果不变）和<strong>可导的</strong>。
聚合函数的对称性（symmetry
property）确保了神经网络模型可以被训练且可以应用于任意顺序的顶点邻居特征集合上。因为要进行反向传播求梯度，需要聚合函数可导。</p>
<p><strong>Mean aggregator</strong><br />
mean aggregator将目标顶点和邻居顶点的第<span
class="math inline">\(k-1\)</span>层向量拼接起来，然后对向量的每个维度进行求均值的操作，将得到的结果做一次非线性变换产生目标顶点的第<span
class="math inline">\(k\)</span>层表示向量。<br />
文中用下面的式子替换算法1中的4行AGGREGATE和5行CONCAT得到GCN的inductive变形：<br />
<img src="/images/GraphSAGE/4.png" width="40%"></p>
<p><strong>LSTM aggregator</strong><br />
LSTM本身是有顺序的，但是通过将输入节点随机排列（对顶点的邻居进行一次乱序操作），使得LSTM可以适用于无序的集合。</p>
<p><strong>Pooling aggregator</strong><br />
pooling聚合器，它既是对称的，又是可训练的。Pooling
aggregator先对目标顶点的邻居顶点的embedding向量进行一次非线性变换，之后进行一次pooling操作(max
pooling or mean
pooling)，将得到结果与目标顶点的表示向量拼接，最后再经过一次非线性变换得到目标顶点的第k层表示向量。<br />
<img src="/images/GraphSAGE/5.png" width="50%"></p>
<h1 id="learning-the-parameters-of-graphsage">Learning the parameters of
GraphSAGE</h1>
<p>GraphSAGE支持无监督训练和有监督训练两种方式。</p>
<p>GraphSAGE的<strong>无监督学习</strong>的理论基于假设：节点 <span
class="math inline">\(u\)</span> 与其邻居 <span
class="math inline">\(v\)</span>
相似，那么其表征的距离也近，而与没有交集的节点 <span
class="math inline">\(v_n\)</span>
不相似，那么其表征的距离也远，损失函数为：<br />
<img src="/images/GraphSAGE/6.png" width="50%"><br />
其中 <span class="math inline">\(z_u\)</span> 为节点 <span
class="math inline">\(u\)</span> 通过GraphSAGE得到的embedding，<span
class="math inline">\(v\)</span> 是节点 <span
class="math inline">\(u\)</span> 通过随机游走得到的邻居，<span
class="math inline">\(v_n \sim P_n(v)\)</span> 表示负采样，<span
class="math inline">\(Q\)</span>为样本数。embedding之间相似度通过向量点积计算得到。<br />
损失函数的目的是拉近正样本距离，拉远负样本距离。正样本是通过随机游走得到的。</p>
<p><strong>有监督学习</strong>比较简单，使用满足预测目标的任务作为损失函数，例如交叉熵等。</p>
<h1 id="experiments">Experiments</h1>
<p>实验使用baselines：<br />
1. Random，随机分类器<br />
2. Raw features，手工特征（非图特征）<br />
3. deepwalk（图拓扑特征）<br />
4. DeepWalk + features， deepwalk + 手工特征</p>
<p><img src="/images/GraphSAGE/7.png" width="80%"></p>
<p><strong>实验结果1：分类准确率（micro-averaged F1
scores）</strong><br />
1. 可以看到GraphSAGE的性能显著优于baseline方法。<br />
2.
三个数据集上的实验结果表明，一般是LSTM或pooling效果比较好，有监督都比无监督好。<br />
3.
无监督版本的GraphSAGE-pool对引文数据和Reddit数据的连接（concatenation）性能分别比DeepWalk
embeddings和raw
features的连接性能好13.8%和29.1%，而有监督版本的连接性能分别提高了19.7%和37.2%。<br />
4.
尽管LSTM是为有序数据而不是无序集设计的，但是基于LSTM的聚合器显示了强大的性能。<br />
5.
最后，可以看到无监督GraphSAGE的性能与完全监督的版本相比具有相当的竞争力，这表明文中的框架可以在不进行特定于任务的微调（task-specific
fine-tuning）的情况下实现强大的性能</p>
<p><strong>实验结果2：运行时间和参数敏感性</strong><br />
<strong>计算时间</strong>：GraphSAGE中LSTM训练速度最慢，但相比DeepWalk，GraphSAGE在预测时间减少100-500倍（因为对于未知节点，DeepWalk要重新进行随机游走以及通过SGD学习embedding）。<br />
<strong>邻居采样数量</strong>：图B中邻居采样数量递增，F1也增大，但计算时间也变大。
为了平衡F1和计算时间，将S1设为25<br />
<strong>聚合K跳内信息</strong>：在GraphSAGE， K=2 相比K=1
有10-15%的提升；但将K设置超过2，效果上只有0-5%的提升，但是计算时间却变大了10-100倍。</p>
<p><strong>实验结果3：不同聚合器之间的比较</strong><br />
1. LSTM和pool的效果较好。<br />
2.
为了更定量地了解这些趋势，实验中将设置六种不同的实验，即(3个数据集)×(非监督、监督)。<br />
3.
GraphSAGE-LSTM比GraphSAGE-pool慢得多(≈2×)，这可能使基于pooling的聚合器在总体上略占优势。<br />
4. LSTM方法和pooling方法之间没有显著差异。<br />
5. 文中使用非参数Wilcoxon
Signed-Rank检验来量化实验中不同聚合器之间的差异，在适用的情况下报告T-statistic和p-value。</p>
<h1 id="总结">总结</h1>
<p>GraphSAGE的一个强大之处是它在一个子集学到的模型也可以应用到其它模型上，原因是因为GraphSAGE的参数是共享的（每层计算时的参数W是共享的）。当有一个新的图或者有一个节点加入到已训练的图中时，我们只需要知道这个新图或者新节点的结构信息，通过共享的参数，便可以得到它们的特征向量。</p>
<p>它最大的贡献在于给图模型赋予了归纳学习的能力，从而大范围的扩大了GNN的落地场景。另外GraphSAGE的预测速度非常之快，因为它只需要选择若干跳的若干个邻居即可，而这两个可以选择的参数往往也比较小，往往取两跳邻居就能得到不错的学习效果。GraphSAGE的第三个优点是它非常好理解，不需要复杂的图理论基础，对于学习图神经网络也是非常好的入门读物。</p>
<h1 id="code">Code</h1>
<p>Pytorch（最基础的实现）：https://github.com/williamleif/graphsage-simple/<br />
Tensorflow（全版本）：https://github.com/williamleif/GraphSAGE<br />
Pytorch（全版本）：https://github.com/twjiang/graphSAGE-pytorch</p>
<p>以Pytorch基础版本为例，节点分类任务：<br />
<figure class="highlight python"><figcaption><span>model.py</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> init</span><br><span class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> f1_score</span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> defaultdict</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> graphsage.encoders <span class="keyword">import</span> Encoder</span><br><span class="line"><span class="keyword">from</span> graphsage.aggregators <span class="keyword">import</span> MeanAggregator</span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">Simple supervised GraphSAGE model as well as examples running the model</span></span><br><span class="line"><span class="string">on the Cora and Pubmed datasets.</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="comment"># 有监督学习下的GraphSage</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">SupervisedGraphSage</span>(nn.Module):</span><br><span class="line"></span><br><span class="line">    <span class="comment"># num_classes类别数，这里是7</span></span><br><span class="line">    <span class="comment"># enc传入的是enc2，看作递归调用，从外层开始计算</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, num_classes, enc</span>):</span><br><span class="line">        <span class="built_in">super</span>(SupervisedGraphSage, self).__init__()</span><br><span class="line">        self.enc = enc</span><br><span class="line">        self.xent = nn.CrossEntropyLoss() <span class="comment"># 交叉熵损失</span></span><br><span class="line"></span><br><span class="line">        self.weight = nn.Parameter(torch.FloatTensor(num_classes, enc.embed_dim)) <span class="comment"># 初始权重</span></span><br><span class="line">        init.xavier_uniform(self.weight)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, nodes</span>):</span><br><span class="line">        embeds = self.enc(nodes)  <span class="comment"># 通过enc拿到nodes的embeddings</span></span><br><span class="line">        scores = self.weight.mm(embeds) <span class="comment"># 通过W得到score</span></span><br><span class="line">        <span class="keyword">return</span> scores.t() <span class="comment"># score转置</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">loss</span>(<span class="params">self, nodes, labels</span>):</span><br><span class="line">        scores = self.forward(nodes)</span><br><span class="line">        <span class="keyword">return</span> self.xent(scores, labels.squeeze()) <span class="comment"># score和labels交叉熵</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">load_cora</span>():</span><br><span class="line">    num_nodes = <span class="number">2708</span></span><br><span class="line">    num_feats = <span class="number">1433</span></span><br><span class="line">    feat_data = np.zeros((num_nodes, num_feats))</span><br><span class="line">    labels = np.empty((num_nodes,<span class="number">1</span>), dtype=np.int64)</span><br><span class="line">    node_map = &#123;&#125;</span><br><span class="line">    label_map = &#123;&#125;</span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&quot;cora/cora.content&quot;</span>) <span class="keyword">as</span> fp:</span><br><span class="line">        <span class="keyword">for</span> i,line <span class="keyword">in</span> <span class="built_in">enumerate</span>(fp):</span><br><span class="line">            info = line.strip().split()</span><br><span class="line">            feat_data[i,:] = <span class="built_in">map</span>(<span class="built_in">float</span>, info[<span class="number">1</span>:-<span class="number">1</span>])</span><br><span class="line">            node_map[info[<span class="number">0</span>]] = i</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> info[-<span class="number">1</span>] <span class="keyword">in</span> label_map:</span><br><span class="line">                label_map[info[-<span class="number">1</span>]] = <span class="built_in">len</span>(label_map)</span><br><span class="line">            labels[i] = label_map[info[-<span class="number">1</span>]]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 设定字典中默认value为set类型，当key不存在时也不会报错</span></span><br><span class="line">    adj_lists = defaultdict(<span class="built_in">set</span>) </span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&quot;cora/cora.cites&quot;</span>) <span class="keyword">as</span> fp:</span><br><span class="line">        <span class="keyword">for</span> i,line <span class="keyword">in</span> <span class="built_in">enumerate</span>(fp):</span><br><span class="line">            info = line.strip().split()</span><br><span class="line">            paper1 = node_map[info[<span class="number">0</span>]] <span class="comment"># paper1的id</span></span><br><span class="line">            paper2 = node_map[info[<span class="number">1</span>]] <span class="comment"># paper2的id</span></span><br><span class="line">            adj_lists[paper1].add(paper2) <span class="comment"># paper1连接paper2</span></span><br><span class="line">            adj_lists[paper2].add(paper1) <span class="comment"># paper2连接paper1</span></span><br><span class="line">    <span class="comment"># feat_data：(2708,1433)</span></span><br><span class="line">    <span class="comment"># labels：(2708,1)</span></span><br><span class="line">    <span class="comment"># adj_lists：存储边的连接 defaultdict(&lt;class &#x27;set&#x27;&gt;,&#123;163:&#123;1536,2563...&#125;,...&#125;)</span></span><br><span class="line">    <span class="keyword">return</span> feat_data, labels, adj_lists</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">run_cora</span>():</span><br><span class="line">    np.random.seed(<span class="number">1</span>)</span><br><span class="line">    random.seed(<span class="number">1</span>)</span><br><span class="line">    num_nodes = <span class="number">2708</span></span><br><span class="line">    feat_data, labels, adj_lists = load_cora()</span><br><span class="line">    features = nn.Embedding(<span class="number">2708</span>, <span class="number">1433</span>)</span><br><span class="line">    <span class="comment"># 权重矩阵</span></span><br><span class="line">    features.weight = nn.Parameter(torch.FloatTensor(feat_data), requires_grad=<span class="literal">False</span>)</span><br><span class="line">   <span class="comment"># features.cuda()</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 使用MeanAggregator和Encoder</span></span><br><span class="line">    agg1 = MeanAggregator(features, cuda=<span class="literal">True</span>)</span><br><span class="line">    enc1 = Encoder(features, <span class="number">1433</span>, <span class="number">128</span>, adj_lists, agg1, gcn=<span class="literal">True</span>, cuda=<span class="literal">False</span>)</span><br><span class="line">    agg2 = MeanAggregator(<span class="keyword">lambda</span> nodes : enc1(nodes).t(), cuda=<span class="literal">False</span>)</span><br><span class="line">    enc2 = Encoder(<span class="keyword">lambda</span> nodes : enc1(nodes).t(), enc1.embed_dim, <span class="number">128</span>, adj_lists, agg2,</span><br><span class="line">            base_model=enc1, gcn=<span class="literal">True</span>, cuda=<span class="literal">False</span>)</span><br><span class="line">    enc1.num_samples = <span class="number">5</span> <span class="comment"># 1层搜索size</span></span><br><span class="line">    enc2.num_samples = <span class="number">5</span> <span class="comment"># 2层搜索size</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># num_classes类别数，这里是7</span></span><br><span class="line">    <span class="comment"># 传入的是enc2，看作递归调用</span></span><br><span class="line">    graphsage = SupervisedGraphSage(<span class="number">7</span>, enc2)</span><br><span class="line"><span class="comment">#    graphsage.cuda()</span></span><br><span class="line">    <span class="comment"># permutation随机排列序列，对2708个节点随机排列</span></span><br><span class="line">    rand_indices = np.random.permutation(num_nodes)</span><br><span class="line">    test = rand_indices[:<span class="number">1000</span>] <span class="comment"># 测试集</span></span><br><span class="line">    val = rand_indices[<span class="number">1000</span>:<span class="number">1500</span>] <span class="comment"># 验证集</span></span><br><span class="line">    train = <span class="built_in">list</span>(rand_indices[<span class="number">1500</span>:]) <span class="comment"># 训练集</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 初始化优化器，并传递相关参数</span></span><br><span class="line">    optimizer = torch.optim.SGD(<span class="built_in">filter</span>(<span class="keyword">lambda</span> p : p.requires_grad, graphsage.parameters()), lr=<span class="number">0.7</span>)</span><br><span class="line">    times = []</span><br><span class="line">    <span class="keyword">for</span> batch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">        batch_nodes = train[:<span class="number">256</span>] <span class="comment"># batch取前256个节点</span></span><br><span class="line">        random.shuffle(train) <span class="comment"># 随即打乱</span></span><br><span class="line">        start_time = time.time() <span class="comment"># 记录开始时间</span></span><br><span class="line">        optimizer.zero_grad() <span class="comment"># 初始化梯度为0</span></span><br><span class="line">        <span class="comment"># 前向传播计算损失</span></span><br><span class="line">        loss = graphsage.loss(batch_nodes, </span><br><span class="line">                Variable(torch.LongTensor(labels[np.array(batch_nodes)])))</span><br><span class="line">        loss.backward() <span class="comment"># 反向传播</span></span><br><span class="line">        optimizer.step() <span class="comment"># 更新梯度</span></span><br><span class="line">        end_time = time.time() <span class="comment"># 记录结束时间</span></span><br><span class="line">        times.append(end_time-start_time) <span class="comment"># 得到时间差</span></span><br><span class="line">        <span class="built_in">print</span> batch, loss.data[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">    val_output = graphsage.forward(val) </span><br><span class="line">    <span class="built_in">print</span> <span class="string">&quot;Validation F1:&quot;</span>, f1_score(labels[val], val_output.data.numpy().argmax(axis=<span class="number">1</span>), average=<span class="string">&quot;micro&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span> <span class="string">&quot;Average batch time:&quot;</span>, np.mean(times)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">load_pubmed</span>():</span><br><span class="line">    <span class="comment">#hardcoded for simplicity...</span></span><br><span class="line">    num_nodes = <span class="number">19717</span></span><br><span class="line">    num_feats = <span class="number">500</span></span><br><span class="line">    feat_data = np.zeros((num_nodes, num_feats))</span><br><span class="line">    labels = np.empty((num_nodes, <span class="number">1</span>), dtype=np.int64)</span><br><span class="line">    node_map = &#123;&#125;</span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&quot;pubmed-data/Pubmed-Diabetes.NODE.paper.tab&quot;</span>) <span class="keyword">as</span> fp:</span><br><span class="line">        fp.readline()</span><br><span class="line">        feat_map = &#123;entry.split(<span class="string">&quot;:&quot;</span>)[<span class="number">1</span>]:i-<span class="number">1</span> <span class="keyword">for</span> i,entry <span class="keyword">in</span> <span class="built_in">enumerate</span>(fp.readline().split(<span class="string">&quot;\t&quot;</span>))&#125;</span><br><span class="line">        <span class="keyword">for</span> i, line <span class="keyword">in</span> <span class="built_in">enumerate</span>(fp):</span><br><span class="line">            info = line.split(<span class="string">&quot;\t&quot;</span>)</span><br><span class="line">            node_map[info[<span class="number">0</span>]] = i</span><br><span class="line">            labels[i] = <span class="built_in">int</span>(info[<span class="number">1</span>].split(<span class="string">&quot;=&quot;</span>)[<span class="number">1</span>])-<span class="number">1</span></span><br><span class="line">            <span class="keyword">for</span> word_info <span class="keyword">in</span> info[<span class="number">2</span>:-<span class="number">1</span>]:</span><br><span class="line">                word_info = word_info.split(<span class="string">&quot;=&quot;</span>)</span><br><span class="line">                feat_data[i][feat_map[word_info[<span class="number">0</span>]]] = <span class="built_in">float</span>(word_info[<span class="number">1</span>])</span><br><span class="line">    adj_lists = defaultdict(<span class="built_in">set</span>)</span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&quot;pubmed-data/Pubmed-Diabetes.DIRECTED.cites.tab&quot;</span>) <span class="keyword">as</span> fp:</span><br><span class="line">        fp.readline()</span><br><span class="line">        fp.readline()</span><br><span class="line">        <span class="keyword">for</span> line <span class="keyword">in</span> fp:</span><br><span class="line">            info = line.strip().split(<span class="string">&quot;\t&quot;</span>)</span><br><span class="line">            paper1 = node_map[info[<span class="number">1</span>].split(<span class="string">&quot;:&quot;</span>)[<span class="number">1</span>]]</span><br><span class="line">            paper2 = node_map[info[-<span class="number">1</span>].split(<span class="string">&quot;:&quot;</span>)[<span class="number">1</span>]]</span><br><span class="line">            adj_lists[paper1].add(paper2)</span><br><span class="line">            adj_lists[paper2].add(paper1)</span><br><span class="line">    <span class="keyword">return</span> feat_data, labels, adj_lists</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">run_pubmed</span>():</span><br><span class="line">    np.random.seed(<span class="number">1</span>)</span><br><span class="line">    random.seed(<span class="number">1</span>)</span><br><span class="line">    num_nodes = <span class="number">19717</span></span><br><span class="line">    feat_data, labels, adj_lists = load_pubmed()</span><br><span class="line">    features = nn.Embedding(<span class="number">19717</span>, <span class="number">500</span>)</span><br><span class="line">    features.weight = nn.Parameter(torch.FloatTensor(feat_data), requires_grad=<span class="literal">False</span>)</span><br><span class="line">   <span class="comment"># features.cuda()</span></span><br><span class="line"></span><br><span class="line">    agg1 = MeanAggregator(features, cuda=<span class="literal">True</span>)</span><br><span class="line">    enc1 = Encoder(features, <span class="number">500</span>, <span class="number">128</span>, adj_lists, agg1, gcn=<span class="literal">True</span>, cuda=<span class="literal">False</span>)</span><br><span class="line">    agg2 = MeanAggregator(<span class="keyword">lambda</span> nodes : enc1(nodes).t(), cuda=<span class="literal">False</span>)</span><br><span class="line">    enc2 = Encoder(<span class="keyword">lambda</span> nodes : enc1(nodes).t(), enc1.embed_dim, <span class="number">128</span>, adj_lists, agg2,</span><br><span class="line">            base_model=enc1, gcn=<span class="literal">True</span>, cuda=<span class="literal">False</span>)</span><br><span class="line">    enc1.num_samples = <span class="number">10</span></span><br><span class="line">    enc2.num_samples = <span class="number">25</span></span><br><span class="line"></span><br><span class="line">    graphsage = SupervisedGraphSage(<span class="number">3</span>, enc2)</span><br><span class="line"><span class="comment">#    graphsage.cuda()</span></span><br><span class="line">    rand_indices = np.random.permutation(num_nodes)</span><br><span class="line">    test = rand_indices[:<span class="number">1000</span>]</span><br><span class="line">    val = rand_indices[<span class="number">1000</span>:<span class="number">1500</span>]</span><br><span class="line">    train = <span class="built_in">list</span>(rand_indices[<span class="number">1500</span>:])</span><br><span class="line"></span><br><span class="line">    optimizer = torch.optim.SGD(<span class="built_in">filter</span>(<span class="keyword">lambda</span> p : p.requires_grad, graphsage.parameters()), lr=<span class="number">0.7</span>)</span><br><span class="line">    times = []</span><br><span class="line">    <span class="keyword">for</span> batch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">200</span>):</span><br><span class="line">        batch_nodes = train[:<span class="number">1024</span>]</span><br><span class="line">        random.shuffle(train)</span><br><span class="line">        start_time = time.time()</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        loss = graphsage.loss(batch_nodes, </span><br><span class="line">                Variable(torch.LongTensor(labels[np.array(batch_nodes)])))</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line">        end_time = time.time()</span><br><span class="line">        times.append(end_time-start_time)</span><br><span class="line">        <span class="built_in">print</span> batch, loss.data[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">    val_output = graphsage.forward(val) </span><br><span class="line">    <span class="built_in">print</span> <span class="string">&quot;Validation F1:&quot;</span>, f1_score(labels[val], val_output.data.numpy().argmax(axis=<span class="number">1</span>), average=<span class="string">&quot;micro&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span> <span class="string">&quot;Average batch time:&quot;</span>, np.mean(times)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    run_cora()</span><br></pre></td></tr></table></figure></p>
<figure class="highlight python"><figcaption><span>encoders.py</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> init</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Encoder</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Encodes a node&#x27;s using &#x27;convolutional&#x27; GraphSage approach</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, features, feature_dim, </span></span><br><span class="line"><span class="params">            embed_dim, adj_lists, aggregator,</span></span><br><span class="line"><span class="params">            num_sample=<span class="number">10</span>,</span></span><br><span class="line"><span class="params">            base_model=<span class="literal">None</span>, gcn=<span class="literal">False</span>, cuda=<span class="literal">False</span>, </span></span><br><span class="line"><span class="params">            feature_transform=<span class="literal">False</span></span>): </span><br><span class="line">        <span class="built_in">super</span>(Encoder, self).__init__()</span><br><span class="line"></span><br><span class="line">        self.features = features <span class="comment"># (2708, 1433)</span></span><br><span class="line">        self.feat_dim = feature_dim <span class="comment"># 输入维度1433</span></span><br><span class="line">        self.adj_lists = adj_lists  <span class="comment"># 边的连接defaultdict(&lt;class &#x27;set&#x27;&gt;,&#123;163:&#123;1536,2563...&#125;,...&#125;)</span></span><br><span class="line">        self.aggregator = aggregator <span class="comment"># MeanAggregator</span></span><br><span class="line">        self.num_sample = num_sample <span class="comment"># 采样邻居节点size</span></span><br><span class="line">        <span class="keyword">if</span> base_model != <span class="literal">None</span>:</span><br><span class="line">            self.base_model = base_model</span><br><span class="line"></span><br><span class="line">        self.gcn = gcn</span><br><span class="line">        self.embed_dim = embed_dim <span class="comment"># 输出维度128</span></span><br><span class="line">        self.cuda = cuda</span><br><span class="line">        self.aggregator.cuda = cuda</span><br><span class="line">        <span class="comment"># 注意concat之后维度是2倍</span></span><br><span class="line">        self.weight = nn.Parameter(</span><br><span class="line">                torch.FloatTensor(embed_dim, self.feat_dim <span class="keyword">if</span> self.gcn <span class="keyword">else</span> <span class="number">2</span> * self.feat_dim))</span><br><span class="line">        init.xavier_uniform(self.weight)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, nodes</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Generates embeddings for a batch of nodes.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        nodes     -- list of nodes</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment"># 使用传递过来的aggregator计算（这里是mean），得到所有邻居节点的aggregator信息</span></span><br><span class="line">        neigh_feats = self.aggregator.forward(nodes, [self.adj_lists[<span class="built_in">int</span>(node)] <span class="keyword">for</span> node <span class="keyword">in</span> nodes], </span><br><span class="line">                self.num_sample)</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> self.gcn:</span><br><span class="line">            <span class="keyword">if</span> self.cuda:</span><br><span class="line">                self_feats = self.features(torch.LongTensor(nodes).cuda())</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                self_feats = self.features(torch.LongTensor(nodes))</span><br><span class="line">            <span class="comment"># CONCAT当前节点和邻居节点</span></span><br><span class="line">            combined = torch.cat([self_feats, neigh_feats], dim=<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            combined = neigh_feats</span><br><span class="line">        <span class="comment"># W*CONCAT</span></span><br><span class="line">        combined = F.relu(self.weight.mm(combined.t()))</span><br><span class="line">        <span class="keyword">return</span> combined</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><figcaption><span>aggregators.py</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">Set of modules for aggregating embeddings of neighbors.</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MeanAggregator</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Aggregates a node&#x27;s embeddings using mean of neighbors&#x27; embeddings</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, features, cuda=<span class="literal">False</span>, gcn=<span class="literal">False</span></span>): </span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Initializes the aggregator for a specific graph.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        features -- function mapping LongTensor of node ids to FloatTensor of feature values.</span></span><br><span class="line"><span class="string">        cuda -- whether to use GPU</span></span><br><span class="line"><span class="string">        gcn --- whether to perform concatenation GraphSAGE-style, or add self-loops GCN-style</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">        <span class="built_in">super</span>(MeanAggregator, self).__init__()</span><br><span class="line"></span><br><span class="line">        self.features = features</span><br><span class="line">        self.cuda = cuda</span><br><span class="line">        self.gcn = gcn</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, nodes, to_neighs, num_sample=<span class="number">10</span></span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        nodes --- list of nodes in a batch</span></span><br><span class="line"><span class="string">        to_neighs --- list of sets, each set is the set of neighbors for node in batch</span></span><br><span class="line"><span class="string">        num_sample --- number of neighbors to sample. No sampling if None.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        nodes：batch节点列表</span></span><br><span class="line"><span class="string">        to_neighs：集合列表，每个集合是batch中一个节点的邻居集合</span></span><br><span class="line"><span class="string">        num_sample：邻居采样size</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment"># Local pointers to functions (speed hack)</span></span><br><span class="line">        _<span class="built_in">set</span> = <span class="built_in">set</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> num_sample <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            _sample = random.sample</span><br><span class="line">            <span class="comment"># sample：截取列表的指定长度的随机数，但是不会改变列表本身的排序</span></span><br><span class="line">            <span class="comment"># 从to_neigh中采样num_sample个（当邻居节点数量to_neigh大于num_sample）</span></span><br><span class="line">            samp_neighs = [_<span class="built_in">set</span>(_sample(to_neigh, </span><br><span class="line">                            num_sample,</span><br><span class="line">                            )) <span class="keyword">if</span> <span class="built_in">len</span>(to_neigh) &gt;= num_sample <span class="keyword">else</span> to_neigh <span class="keyword">for</span> to_neigh <span class="keyword">in</span> to_neighs]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            samp_neighs = to_neighs <span class="comment"># num_sample为None，每个节点都对其所有邻居节点全部计算</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.gcn:</span><br><span class="line">            samp_neighs = [samp_neigh + <span class="built_in">set</span>([nodes[i]]) <span class="keyword">for</span> i, samp_neigh <span class="keyword">in</span> <span class="built_in">enumerate</span>(samp_neighs)]</span><br><span class="line">        <span class="comment"># set.union():返回多个集合的并集，即包含了所有集合的元素，重复的元素只会出现一次</span></span><br><span class="line">        unique_nodes_list = <span class="built_in">list</span>(<span class="built_in">set</span>.union(*samp_neighs))</span><br><span class="line">        <span class="comment"># &#123;节点:编号&#125;</span></span><br><span class="line">        unique_nodes = &#123;n:i <span class="keyword">for</span> i,n <span class="keyword">in</span> <span class="built_in">enumerate</span>(unique_nodes_list)&#125;</span><br><span class="line">        <span class="comment"># (采样节点（包括了batch节点）,独一无二节点)</span></span><br><span class="line">        mask = Variable(torch.zeros(<span class="built_in">len</span>(samp_neighs), <span class="built_in">len</span>(unique_nodes)))</span><br><span class="line">        <span class="comment"># 得到采样节点对应的编号</span></span><br><span class="line">        column_indices = [unique_nodes[n] <span class="keyword">for</span> samp_neigh <span class="keyword">in</span> samp_neighs <span class="keyword">for</span> n <span class="keyword">in</span> samp_neigh]  </span><br><span class="line">        <span class="comment"># 每个节点对应采样数量个i作为行坐标 </span></span><br><span class="line">        row_indices = [i <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(samp_neighs)) <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(samp_neighs[i]))]</span><br><span class="line">        <span class="comment"># 得到邻接矩阵</span></span><br><span class="line">        mask[row_indices, column_indices] = <span class="number">1</span></span><br><span class="line">        <span class="keyword">if</span> self.cuda:</span><br><span class="line">            mask = mask.cuda()</span><br><span class="line">        <span class="comment"># 统计每个节点的邻居数量</span></span><br><span class="line">        num_neigh = mask.<span class="built_in">sum</span>(<span class="number">1</span>, keepdim=<span class="literal">True</span>) </span><br><span class="line">        <span class="comment"># 每个邻居节点占比</span></span><br><span class="line">        mask = mask.div(num_neigh)</span><br><span class="line">        <span class="comment"># 得到每个邻居节点的特征</span></span><br><span class="line">        <span class="keyword">if</span> self.cuda:</span><br><span class="line">            embed_matrix = self.features(torch.LongTensor(unique_nodes_list).cuda())</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            embed_matrix = self.features(torch.LongTensor(unique_nodes_list))</span><br><span class="line">        <span class="comment"># mean aggregators 计算每个邻居的节点特征mean</span></span><br><span class="line">        to_feats = mask.mm(embed_matrix)</span><br><span class="line">        <span class="keyword">return</span> to_feats</span><br></pre></td></tr></table></figure>
<h1 id="参考文献">参考文献</h1>
<p><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzE3MDYuMDIyMTYucGRm">Inductive
Representation Learning on Large Graphs<i class="fa fa-external-link-alt"></i></span><br />
<span class="exturl" data-url="aHR0cHM6Ly96aHVhbmxhbi56aGlodS5jb20vcC8zMzYxOTU4NjI=">GraphSAGE详解<i class="fa fa-external-link-alt"></i></span><br />
<span class="exturl" data-url="aHR0cHM6Ly96aHVhbmxhbi56aGlodS5jb20vcC83OTYzNzc4Nw==">【Graph Neural
Network】GraphSAGE: 算法原理，实现和应用<i class="fa fa-external-link-alt"></i></span><br />
<span class="exturl" data-url="aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3l5bDQyNDUyNS9hcnRpY2xlL2RldGFpbHMvMTAwNTMyODQ5">[论文笔记]：GraphSAGE：Inductive
Representation Learning on Large Graphs 论文详解 NIPS 2017<i class="fa fa-external-link-alt"></i></span></p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




  <nav class="pagination">
    <a class="extend prev" rel="prev" title="上一页" aria-label="上一页" href="/"><i class="fa fa-angle-left"></i></a><a class="page-number" href="/">1</a><span class="page-number current">2</span><a class="page-number" href="/page/3/">3</a><span class="space">&hellip;</span><a class="page-number" href="/page/13/">13</a><a class="extend next" rel="next" title="下一页" aria-label="下一页" href="/page/3/"><i class="fa fa-angle-right"></i></a>
  </nav>

</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="copyright">
    &copy; 2019 – 
    <span itemprop="copyrightYear">2023</span>
    <span class="with-love">
      <i class="fa fa-heart"></i>
    </span>
    <span class="author" itemprop="copyrightHolder">SoundMemories</span>
  </div>
  <div class="powered-by">由 <span class="exturl" data-url="aHR0cHM6Ly9oZXhvLmlv">Hexo</span> & <span class="exturl" data-url="aHR0cHM6Ly90aGVtZS1uZXh0LmpzLm9yZy9tdXNlLw==">NexT.Muse</span> 强力驱动
  </div>

    </div>
  </footer>

  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>
  <div class="sidebar-dimmer"></div>
  <div class="back-to-top" role="button" aria-label="返回顶部">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>
  <a role="button" class="book-mark-link book-mark-link-fixed"></a>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/next-theme-pjax/0.6.0/pjax.min.js" integrity="sha256-vxLn1tSKWD4dqbMRyv940UYw4sXgMtYcK6reefzZrao=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.7.0/jquery.min.js" integrity="sha256-2Pmvv0kuTBOenSvLm6bvfBSSHrUJ+3A7x6P5Ebd07/g=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.js" integrity="sha256-yt2kYMy0w8AbtF89WXb2P1rfjcP/HTHLT7097U8Y5b8=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/medium-zoom/1.0.8/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/muse.js"></script><script src="/js/next-boot.js"></script><script src="/js/bookmark.js"></script><script src="/js/pjax.js"></script>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-generator-searchdb/1.4.1/search.js" integrity="sha256-1kfA5uHPf65M5cphT2dvymhkuyHPQp5A53EGZOnOLmc=" crossorigin="anonymous"></script>
<script src="/js/third-party/search/local-search.js"></script>

  <script class="next-config" data-name="pdf" type="application/json">{"object_url":{"url":"https://cdnjs.cloudflare.com/ajax/libs/pdfobject/2.2.12/pdfobject.min.js","integrity":"sha256-g2xji1rlE3KsGVClvuxTbcR0Kn2+wtQADSff2Tbb4zA="},"url":"/lib/pdf/web/viewer.html"}</script>
  <script src="/js/third-party/tags/pdf.js"></script>

  <script class="next-config" data-name="mermaid" type="application/json">{"enable":true,"theme":{"light":"neutral","dark":"neutral"},"js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mermaid/10.2.3/mermaid.min.js","integrity":"sha256-JFptYy4KzJ5OQP+Q9fubNf3cxpPPmZKqUOovyEONKrQ="}}</script>
  <script src="/js/third-party/tags/mermaid.js"></script>


  <script src="/js/third-party/fancybox.js"></script>



  




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"none","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>


  <script src="https://cdnjs.cloudflare.com/ajax/libs/quicklink/2.3.0/quicklink.umd.js" integrity="sha256-yvJQOINiH9fWemHn0vCA5lsHWJaHs6/ZmO+1Ft04SvM=" crossorigin="anonymous"></script>
  <script class="next-config" data-name="quicklink" type="application/json">{"enable":true,"home":true,"archive":true,"delay":true,"timeout":3000,"priority":true,"url":"https://soundmemories.github.io/page/2/"}</script>
  <script src="/js/third-party/quicklink.js"></script>

</body>
</html>
