<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222" media="(prefers-color-scheme: light)">
<meta name="theme-color" content="#222" media="(prefers-color-scheme: dark)"><meta name="generator" content="Hexo 6.3.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="https://fonts.loli.net/css?family=Monda:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">

<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css" integrity="sha256-HtsXJanqjKTc8vVQjO4YMhiqFoXkfBsjBWcX91T1jr8=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.css" integrity="sha256-Vzbj7sDDS/woiFS3uNKo8eIuni59rjyNGtXfstRzStA=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"soundmemories.github.io","root":"/","images":"/images","scheme":"Muse","darkmode":true,"version":"8.17.1","exturl":true,"sidebar":{"position":"left","display":"always","padding":18,"offset":12},"copycode":{"enable":true,"style":null},"bookmark":{"enable":true,"color":"#222","save":"auto"},"mediumzoom":true,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"},"path":"/search.xml","localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false}}</script><script src="/js/config.js"></script>

    <meta name="description" content="今日事，今日毕">
<meta property="og:type" content="website">
<meta property="og:title" content="SoundMemories">
<meta property="og:url" content="https://soundmemories.github.io/page/2/index.html">
<meta property="og:site_name" content="SoundMemories">
<meta property="og:description" content="今日事，今日毕">
<meta property="og:locale" content="zh_CN">
<meta property="article:author" content="SoundMemories">
<meta name="twitter:card" content="summary">


<link rel="canonical" href="https://soundmemories.github.io/page/2/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":true,"isPost":false,"lang":"zh-CN","comments":"","permalink":"","path":"page/2/index.html","title":""}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>SoundMemories</title>
  








  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <h1 class="site-title">SoundMemories</h1>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="搜索" role="button">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签<span class="badge">10</span></a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类<span class="badge">10</span></a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档<span class="badge">128</span></a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup"><div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" maxlength="80"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close" role="button">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="search-result-container no-result">
  <div class="search-result-icon">
    <i class="fa fa-spinner fa-pulse fa-5x"></i>
  </div>
</div>

    </div>
  </div>

</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-overview-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="SoundMemories"
      src="/images/avstar.png">
  <p class="site-author-name" itemprop="name">SoundMemories</p>
  <div class="site-description" itemprop="description">今日事，今日毕</div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">128</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">10</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">10</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author animated">
      <span class="links-of-author-item">
        <span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL3NvdW5kbWVtb3JpZXM=" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;soundmemories"><i class="fab fa-github fa-fw"></i>GitHub</span>
      </span>
      <span class="links-of-author-item">
        <span class="exturl" data-url="bWFpbHRvOnNvdW5kbWVtb3JpZXNAMTYzLmNvbQ==" title="E-Mail → mailto:soundmemories@163.com"><i class="fa fa-envelope fa-fw"></i>E-Mail</span>
      </span>
  </div>
  <div class="cc-license animated" itemprop="license">
    <span class="exturl cc-opacity" data-url="aHR0cHM6Ly9jcmVhdGl2ZWNvbW1vbnMub3JnL2xpY2Vuc2VzL2J5LW5jLXNhLzQuMC8="><img src="https://cdnjs.cloudflare.com/ajax/libs/creativecommons-vocabulary/2020.11.3/assets/license_badges/small/by_nc_sa.svg" alt="Creative Commons"></span>
  </div>

        </div>
      </div>
    </div>

    
        <div class="pjax">
        </div>
  </aside>


    </div>

    <div class="main-inner index posts-expand">

    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://soundmemories.github.io/2021/07/21/NLP/06.%E5%AF%B9%E8%AF%9D%E7%B3%BB%E7%BB%9F/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avstar.png">
      <meta itemprop="name" content="SoundMemories">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="SoundMemories">
      <meta itemprop="description" content="今日事，今日毕">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | SoundMemories">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2021/07/21/NLP/06.%E5%AF%B9%E8%AF%9D%E7%B3%BB%E7%BB%9F/" class="post-title-link" itemprop="url">对话系统简介</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2021-07-21 00:00:00" itemprop="dateCreated datePublished" datetime="2021-07-21T00:00:00+08:00">2021-07-21</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/NLP/" itemprop="url" rel="index"><span itemprop="name">NLP</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>939</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>3 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2 id="闲聊式">闲聊式</h2>
<p>情感识别，多轮对话中情感向量生成（Emotion
Embedding）、意图识别等。建议参考项目<span class="exturl" data-url="aHR0cHM6Ly9yYXNhLmNvbS8=">Rasa<i class="fa fa-external-link-alt"></i></span>。</p>
<h2 id="检索式">检索式</h2>
<p>涉及到的核心：<br />
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">输入问句：句向量的构建。</span><br><span class="line">粗排：候选问句的范围，重点是速度、召回率。</span><br><span class="line">精排：候选问句基础上精细匹配/深度匹配，重点是精准率。</span><br><span class="line">答案：匹配问句对应的答案。</span><br></pre></td></tr></table></figure></p>
<h2 id="知识问答">知识问答</h2>
<p><img src="/images/对话系统/1.png" width="80%"></p>
<h2 id="任务式">任务式</h2>
<p><img src="/images/对话系统/2.png" width="80%"></p>
<p>1、<strong>自然语言理解</strong>(NLU)：<br />
(1)
将<strong>非结构化</strong>的<strong>文本</strong>转成<strong>结构化</strong>的<strong>语义</strong>(意图分类、词槽序列标注)。<br />
(2) 将无限种可能转成有限的组合。<br />
核心：意图识别(Intent)、实体识别(Entity)。</p>
<p>2、<strong>对话管理</strong>(DM,
meaning&amp;context)：bot需要通过历史对话中的信息来判断，此时处理方案。<br />
(1) 对话跟踪(DST)：记录和更新 对话状态。用户话语中哪些
<strong>变化</strong>、<strong>值</strong>
是我们关心的，对对话处理有影响的要素记录。<br />
(2)
对话策略(DP)：根据<strong>历史和当前</strong>的对话状态选择合适的对话策略。<strong>选择哪种动作</strong>(Action)和<strong>执行动作响应后作什么</strong>，和业务相关，比如查询or定业务？成功or失败后做什么决策？。<br />
<strong>对话策略(Policy)类型</strong>：<br />
(1) 基于规则：意图为X，那么直接输出对应的动作Y。比如打招呼。<br />
(2)
基于记忆：用户当前对话状态和训练数据中某个story状态完全一致时，可以使用story中后序的动作。<br />
(3) 基于神经网络预测：模型分类预测，比如KerasPolicy。<br />
(4) 基于编程：自定义脚本处理规则。<br />
所有对话策略同时预测，按得分高低做动作。<br />
<strong>动作执行</strong>：把对话跟踪和对话策略的结果
发送给执行机构，得到返回结果呈现给用户。</p>
<p>3、<strong>自然语言生成</strong>(NLG)：<br />
(1) 语义转成文本。绝大数情况下，可用模板解决。</p>
<p><strong>90%看不见的工作</strong>：<br />
(1) 对话数据的获取：已有数据or人工造数据。<br />
(2) 对话数据的扩充：样本少，数据不均衡。<br />
(3)
对话数据的标注：算法实现or业务需求、长尾需求、语义歧义、方言/ASR错误识别、需求变更、无意义句子。<br />
(4) 对话数据校验与清洗：错标、漏标。<br />
(5) 数据和模板
版本化管理(可复现)：数据迭代(变更追踪)、算法迭代、参数迭代。<br />
(6) 模型部署：字典转换、序列编码。<br />
(7)
模型效果分析：模型行不行、哪里不行、为什么不行(数据or模型)、怎么解决。</p>
<p>任务型+FAQ：Rasa成熟的开源框架。<br />
Rasa x：不断根据客户反馈改进模型。<br />
Rasa3.0变化：<br />
(1) 支持无向图，可以使用更多的模型。<br />
(2) 不再支持markdown配置，改用YAML配置。<br />
(3) 需要配置中新增recipe字段，如recipe: default.v1。<br />
(4) 训练文件中的version:"2.0"改成version:"3.0"。<br />
(5) 3.0彻底移除了2.0准备要删除的api。<br />
(6) 2.0的slot
mapping是在表单中定义的，现在每个slot的mapping都在slot的配置中指定，原来的slot和实体之间默认是可以进行自动填充的，现在必须显示指定。<br />
(7)
原来是线性的pipeline，现在用的是有向无环图(DAG)。训练和推理的逻辑都发生了变化。</p>
<h2 id="参考文献">参考文献</h2>
<p><span class="exturl" data-url="aHR0cHM6Ly93d3cuc29odS5jb20vYS8xNjMyNzg1ODhfNTAwNjU5">KBQA从入门到放弃—入门篇<i class="fa fa-external-link-alt"></i></span><br />
<span class="exturl" data-url="aHR0cHM6Ly96aHVhbmxhbi56aGlodS5jb20vcC8yMzc0NTI5MTg/dXRtX3NvdXJjZT13ZWNoYXRfc2Vzc2lvbg==">实体关系、实体属性、三元组、SPO三元组及其抽取方案<i class="fa fa-external-link-alt"></i></span><br />
<span class="exturl" data-url="aHR0cHM6Ly93d3cuYmlsaWJpbGkuY29tL3ZpZGVvL0JWMW9yNHkxVTdYUj9zcG1faWRfZnJvbT0zMzMuMzM3LnNlYXJjaC1jYXJkLmFsbC5jbGljayZ2ZF9zb3VyY2U9MjgyZGM3NmY0M2Q0NjFjY2Q3YjMzY2YwZjVlYTY0YjU=">【社区说】一起来聊聊
Rasa 3.0<i class="fa fa-external-link-alt"></i></span><br />
<span class="exturl" data-url="aHR0cHM6Ly9yYXNhLmNvbS9kb2NzL3Jhc2Ev">Rasa NLU &amp; Core<i class="fa fa-external-link-alt"></i></span><br />
<span class="exturl" data-url="aHR0cHM6Ly9yYXNhLmNvbS9kb2NzL2FjdGlvbi1zZXJ2ZXIv">Rasa Action
Server<i class="fa fa-external-link-alt"></i></span><br />
<span class="exturl" data-url="aHR0cHM6Ly9yYXNhLmNvbS9kb2NzL3Jhc2EteC8=">Rasa X<i class="fa fa-external-link-alt"></i></span><br />
<span class="exturl" data-url="aHR0cHM6Ly9ibG9nLnJhc2EuY29tLw==">Rasa Blog<i class="fa fa-external-link-alt"></i></span><br />
<span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL1Jhc2FIUS9yYXNh">Rasa code<i class="fa fa-external-link-alt"></i></span><br />
<span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL2hvd2wtYW5kZXJzb24=">Rasa 开发者<i class="fa fa-external-link-alt"></i></span></p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://soundmemories.github.io/2021/07/20/NLP/05.Bert/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avstar.png">
      <meta itemprop="name" content="SoundMemories">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="SoundMemories">
      <meta itemprop="description" content="今日事，今日毕">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | SoundMemories">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2021/07/20/NLP/05.Bert/" class="post-title-link" itemprop="url">Bert</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2021-07-20 00:00:00" itemprop="dateCreated datePublished" datetime="2021-07-20T00:00:00+08:00">2021-07-20</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/NLP/" itemprop="url" rel="index"><span itemprop="name">NLP</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>9.6k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>35 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="介绍">介绍</h1>
<p>word2vec和GloVe都将相同的预训练向量分配给同一个词，而不考虑词的上下文。</p>
<p>由此出现考虑上下文的ELMo，通过将整个序列作为输入，ELMo是为输入序列中的每个单词分配一个表示的函数。具体来说，ELMo将来自预训练的双向长短期记忆网络的所有中间层表示组合为输出表示。然后，ELMo的表示将作为附加特征添加到下游任务的现有监督模型中，例如通过将ELMo的表示和现有模型中词元的原始表示（例如GloVe）连结起来。一方面，在加入ELMo表示后，冻结了预训练的双向LSTM模型中的所有权值。另一方面，现有的监督模型是专门为给定的任务定制的。利用当时不同任务的不同最佳模型，添加ELMo改进了六种自然语言处理任务的技术水平：情感分析、自然语言推理、语义角色标注、共指消解、命名实体识别和问答。</p>
<p>尽管ELMo显著改进了各种自然语言处理任务的解决方案，但每个解决方案仍然依赖于一个特定任务的结构。然而，为每一个自然语言处理任务设计一个特定的结构实际上并不是一件容易的事。GPT（Generative
Pre
Training）模型为上下文的敏感表示设计了通用的任务无关模型。GPT建立在Transformer解码器的基础上，预训练了一个用于表示文本序列的语言模型。当将GPT应用于下游任务时，语言模型的输出将被送到一个附加的线性输出层，以预测任务的标签。与ELMo冻结预训练模型的参数不同，GPT在下游任务的监督学习过程中对预训练Transformer解码器中的所有参数进行微调。GPT在自然语言推理、问答、句子相似性和分类等12项任务上进行了评估，并在对模型结构进行最小更改的情况下改善了其中9项任务的最新水平。</p>
<p>然而，由于语言模型的自回归特性，GPT只能向前看（从左到右）。在“i went
to the bank to deposit cash”（我去银行存现金）和“i went to the bank to
sit
down”（我去河岸边坐下）的上下文中，由于“bank”对其左边的上下文敏感，GPT将返回“bank”的相同表示，尽管它有不同的含义。</p>
<p>BERT：把两个最好的结合起来。ELMo对上下文进行双向编码，但使用特定于任务的结构；而GPT是任务无关的，但是从左到右编码上下文。BERT（来自Transformers的双向编码器表示）结合了这两个方面的优点。它对上下文进行双向编码，并且对于大多数的自然语言处理任务，只需要最少的结构改变。通过使用预训练的Transformer编码器，BERT能够基于其双向上下文表示任何词元。在下游任务的监督学习过程中，BERT在两个方面与GPT相似。首先，BERT表示将被输入到一个添加的输出层中，根据任务的性质对模型结构进行最小的更改，例如预测每个词元与预测整个序列。其次，对预训练Transformer编码器的所有参数进行微调，而额外的输出层将从头开始训练。</p>
<h1 id="预训练">预训练</h1>
<p>1、将预训练模型用于下游任务有两种策略：</p>
<ul>
<li>基于微调的策略。如
GPT，通过简单微调预训练模型的参数来训练下游任务。该策略在预训练期间通过单向语言模型来学习通用语言representation，而单向语言模型严重限制了预训练模型的表达能力。例如，在token
级别的任务（如：词性标注任务），结合两个方向的上下文对模型性能非常重要。</li>
<li>基于特征的策略。如 ELMo ，将预训练模型的representation
作为下游任务模型的额外特征。该策略虽然是双向语言模型，但是该模型是浅层的。</li>
</ul>
<p>与它们不同，BERT是一个<strong>同时利用了左右双向上下文的、深度的</strong>预训练模型，它在11项
nlp 任务中取得最领先的结果。</p>
<p>2、基于特征的方法具有一定优势：</p>
<ul>
<li>并不是所有的 NLP 任务都可以很容易地用 Transformer encoder
架构来表示，因此需要添加特定于任务的模型架构。</li>
<li>如果通过训练数据预先计算一次昂贵的数据表示，然后在该表示的基础上用廉价的模型来完成许多任务，这会带来很大的计算优势。</li>
</ul>
<p>3、单向语言模型可以是从左到右(LTR, Left to Right) 或者从右到左(RTL,
Right to Left)。BERT 也可以像 ELMO 一样训练独立的 LTR 和 RTL
模型后拼接在一起，但是这么做有两个问题：</p>
<ul>
<li>其训练代价是单个双向模型的两倍。</li>
<li>对于Question - Answer 之类的问题是反直觉的，因为 RTL
模型需要根据答案来反推问题。</li>
<li>BERT 可以自由的组合左侧上下文和右侧上下文。</li>
</ul>
<p>4、BERT
预训练模型包含两个预训练任务：预测被屏蔽的单词(MLM)、预测下一个句子是否和上一个句子相接(NSP)。</p>
<p>5、BERT 的预训练语料库必须使用
document-level的语料库，而不是经过混洗的 sentence-level
的语料库。因为混洗句子会破坏句子预测预训练任务。这里的 “句子”
不一定是真实的句子，而是一段话或者几段话，代表了一个 token 序列。</p>
<ul>
<li>BERT 预训练时，每个 ”句子“ 的 token 长度小于等于 512 。</li>
<li>BERT 的训练语料库经过了 WordPiece 词干化。如：engineer-&gt;engine
er</li>
</ul>
<p>6、BERT 预训练采用 gelu 激活函数，训练一百万步，bath size = 256
。</p>
<h2 id="mlm">MLM</h2>
<p>1、受完形填空任务启发，BERT
通过提出一个新的预训练目标来解决前面提到的单向限制：掩码语言模型<strong>Masked
language model (MLM)</strong>。</p>
<p>2、从直觉上，深度双向模型要比深度单向模型、单层双向模型表达能力更强。<br />
（1）标准的条件语言模型只能从左到右或者从右到左训练，因为双向条件作用允许每个单词在多层上下文中间接“看到自己”，与单向语言模型不同，MLM
结合了左右两侧上下文。<br />
（2）MLM
模型从输入中随机屏蔽一些token，目标是基于上下文预测被屏蔽单词。方法是：将被屏蔽的单词替换为
[MASK] 标记，然后被屏蔽的单词作为真实 label 。</p>
<p>3、为了训练 MLM，模型随机屏蔽15%的token，然后仅预测那些被屏蔽的 token
。这种方式有两个缺陷：<br />
（1）预训练和微调之间的不匹配。因为在微调阶段，模型永远不会看到 [MASK]
标记。为了缓解这种状况，MLM 在预训练时并不总是用真的 [MASK]
标记，而是从输入种<strong>随机选择 15% 的 token（80% 替换为 [MASK]
标记，10% 替换为一个随机单词，10% 保持原样）</strong>。<br />
（2）MLM
并不知道哪些词被替换，因此它总是努力的学习每个单词的正确表达。每个 batch
预测的 token 只有
15%，这意味着模型需要更多的训练步才可能收敛。实验证明MLM
的收敛速度确实比单向语言模型稍慢，但是相对于 MLM
的泛化能力的提升，这种代价是值得的。</p>
<h2 id="nsp">NSP</h2>
<p>许多重要的下游任务，例如知识问答和自然语言推理，都是基于理解两个句子之间的关系，而这种关系不是由语言模型直接捕获的，为了训练理解句子关系的模型，BERT
训练一个二元化的句子预测任务，称作<strong>Next Sentence Prediction
(NSP)</strong>：<br />
（1）每个训练样本由一对句子 A 和 B 组成：50% 的样本中 B 是紧跟在 A
之后的句子，50%的样本中 B 是随机在训练样本中挑选的。<br />
（2）模型需要预测的是： B 是否是 A 的下一个句子？</p>
<p>重要理解点：<br />
（1）正负样本对的构造，类似word2vec的负采样，但是句子级的负采样，如上（1）。<br />
（2）nsp任务的潜在弊端，由于在选择负样本时是随机选择训练样本的句子，导致AB的负样本比如AC或AD的Topic与AB并不一致，模型可能学习的并不是句间上下关系的判断，而是句子是否为同一个Topic的判断，这也是后续有些Bert模型把NSP任务去掉效果反而更好的原因。</p>
<h1 id="结构">结构</h1>
<p>1、参考Transformer的encoder部分，Bert是一个多层双向Transformer编码器。双向
self-attention 的 Transformer 也称作 Transformer encoder，而单向
self-attention 的 Transformer 被称作 Transformer decoder 。</p>
<p>2、作者指明L表示层数，H表示每个隐藏单元的维数大小，A表示self-attention头数。BERT有2种大小的模型，分别是BERT_base(L=12,
H=768, A=12, Total Parameters=110M)、BERT_large(L=24, H=1024, A=16,
Total Parameters=340M)。BERT_base设定为和OpenAI
GPT的模型大小相同，以便作比较。需要重点说明的是，BERT
Transformer使用双向self-attention，而GPT Transformer
使用带约束的self-attention，每个token只能注意到它左边的上下文。</p>
<p>3、BERT 的模型输入能够表达单个句子或者一对句子。</p>
<ul>
<li>每个 token 的输入 representation 由三个部分相加而成：token
embedding、segment embedding、position embedding 。</li>
<li>每个序列的第一个 token
是一个特殊的[CLS]。网络最后一层对应于该位置的一个隐向量作为整个序列的
representation 来用于分类任务。对于非分类任务，该隐向量被忽略。</li>
<li>如果是一对句子，则在句子之间插入特殊的 token：[SEP]
。然后对句子的前后关系学习一个segment
embedding，通过这种方式，模型得以学习和区分两个句子的前后关系：
<ul>
<li>前一个句子的每个 token 学习和使用 A embedding，代表前后关系的
“前关系” 的表达。</li>
<li>后一个句子的每个 token 学习和使用 B embedding，代表前后关系的
“后关系” 的表达。</li>
</ul></li>
<li>对于单个句子，模型仅学习和使用 A embedding 。</li>
<li>position embedding 是模型学习和使用的 input
每个绝对位置的表达。这里不是正弦+余弦方式，而是参数化方式。</li>
<li>token embedding 是模型学习和使用的每个 token 的表达。</li>
</ul>
<p><img src="/images/Bert/1.png" width="90%"></p>
<h1 id="应用">应用</h1>
<p>从input(2种)和output(4种)看：</p>
<ul>
<li>input：
<ul>
<li>one sentence：输入为1个句子。</li>
<li>multiple sentences：输入为多个句子，用[SEP]分隔。</li>
</ul></li>
<li>output：
<ul>
<li>one
class：[CLS]预训练后的向量输入全连接层，直接判断分类结果（当然，你也可以输入sentence全部向量）。</li>
<li>class for each
token：输入的每个token经过预训练后的向量输入全连接层，对每个token分类。</li>
<li>copy from
input：经典Q-A问题，这里设计时使用限制的QA(Answer必须在document中)，输入2个向量（代表Answer在document的起始-结束位置），和预训练（输入Question+document）后的document
token向量点积+softmax，取最大值token位置即可。</li>
<li>general
sequence：作为seq2seq的encoder、作为encoder+decoder（decoder应使用单向Transformer）。</li>
</ul></li>
</ul>
<h1 id="如何fine-tune">如何fine-tune？</h1>
<ol type="1">
<li>固定Bert参数，只训练全连接层。</li>
<li>Bert+全连接层 全部一起训练。</li>
<li>Adaptor：对Bert中添加一部分Layer，训练时只调整这些Layer参数。</li>
</ol>
<p><strong>warmup</strong>：学习率热身。<strong>规定前多少个热身步骤内，对学习率采取逐步递增的过程</strong>。热身步骤之后，会对学习率采用衰减策略。这样训练初期可以避免震荡，后期可以让loss降得更小。<strong>warmup抑制Layer
Normalization对学习率参数的敏感度。</strong></p>
<p>除了 batch size、学习率、训练 epoch
不同之外，<strong>其它训练参数与预训练阶段的训练参数相同</strong>。fine-tune阶段通常很快，因此建议<strong>对超参数进行彻底搜索</strong>并选择在验证集上表现最好的模型。<br />
论文发现：数据集越小，模型性能对超参数的选择越敏感。大数据集（超过10万的标记样本）对超参数的敏感性要低的多。</p>
<p>对于文本分类，详情参考<span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzE5MDUuMDU1ODMucGRm">How to Fine-Tune BERT for
Text Classification?<i class="fa fa-external-link-alt"></i></span>。<br />
BERT在NLP任务中效果十分优秀，这篇文章对于BERT在文本分类的应用上做了非常丰富的实验，介绍了一些调参以及改进的经验，进一步挖掘BERT的潜力。</p>
<ul>
<li>微调（fine-tune）策略
<ul>
<li>对于长文本，尝试了（1）取头部510 tokens（2）尾部510
tokens（3）头部128 tokens + 尾部382
tokens（4）分片并进行最大池化、平均池化、attention。发现方法（3）最好。因为文章的关键信息一般在开头和结尾。</li>
<li>分层训练，上层对fine-tune更加重要，这部分可随机初始化。</li>
<li>灾难性遗忘：在下游finetune可能会遗忘预训练的知识。需要设置较小的学习率，如2e-5.</li>
<li>分层衰减学习率（Layer-wise Decreasing Layer
Rate），对下层设置更小的学习率可以得到更高的准确率，在lr=2e-5，衰减率<span
class="math inline">\(\xi\)</span>=0.95</li>
</ul></li>
<li>继续预训练（Further Pretraining）
<ul>
<li>任务内（within-task）
和同领域（in-domain）的继续预训练可以大大提高准确率，In-domain比within-task要好。</li>
</ul></li>
<li>多任务微调（Multi-task Finetuning）
<ul>
<li>在单任务微调之前的多任务微调有帮助，但是提升效果小于Further
pretraining。</li>
</ul></li>
<li>小数据集
<ul>
<li>BERT对小数据集提升很大，这个大家都知道的。Further
pretraining对小数据集也有帮助。</li>
</ul></li>
</ul>
<p><span class="exturl" data-url="aHR0cHM6Ly96aHVhbmxhbi56aGlodS5jb20vcC81MjQwMzYwODc=">Bert在fine-tune时训练的5种技巧<i class="fa fa-external-link-alt"></i></span><br />
https://github.com/shuxinyin/Chinese-Text-Classification<br />
<span class="exturl" data-url="aHR0cHM6Ly96aHVhbmxhbi56aGlodS5jb20vcC84Njk2NTU5NQ==">深入理解NLP
Subword算法：BPE、WordPiece、ULM<i class="fa fa-external-link-alt"></i></span></p>
<h1 id="bert变种">Bert变种</h1>
<h2 id="bert-wwm">BERT-WWM</h2>
<p><strong>BERT-WWM</strong>：始版本的 BERT 采用了WordPiece tokenize
来预处理，即把每个单词拆解一些 WordPiece token（比如，loved-&gt;lov ed）
，最初是为了解决谷歌语音识别系统遇到的日语/韩语分割问题。该模型是数据驱动，并且确保任何可能的字符序列的确定性分割。这种预处理为
BERT 带来一个严重的问题：<strong>有可能仅仅对一个单词的某个部分
wordpiece token 执行了 mask。此时 MLM
模型预测的只是单词的一部分，相当于模型对一个错误的目标进行预测。这非常不利于模型的学习</strong>。有鉴于此，谷歌后续发布了
BERT 的 Whole Word Masking:WWM 版本
<strong>BERT-WWM</strong>。在该版本中，一个单词要么没有被
mask、要么该单词所有的 workpiece token 都被 mask
。类似的想法还有ERNIE模型的phrase-level mask和entity-level mask。</p>
<p><strong>BERT-wwm-ext</strong>：是 BERT-WWM
的中文版本(哈工大讯飞联合实验室发布)，该模型对中文的<strong>整个单词</strong>而不是单个字进行mask
，在最新的中文维基百科上训练。</p>
<h2 id="ernie">ERNIE</h2>
<p><strong>ERNIE</strong>：BERT 的 MLM 任务在执行 mask
的时候，未能考虑先验知识，如果有学习到与Mask的单词相关的先验知识，则无需借助很长的上下文就可以很容易的预测出Mask的单词。但是
ERNIE 并没有直接添加先验知识，而是通过引入 <strong>entity-level
mask</strong> 和 <strong>phrase-level mask</strong>
来引入先验知识，隐式的学习实体间的关系、实体的属性等知识：</p>
<ul>
<li><strong>phrase-level mask</strong>：将一个 phrase
作为一个单元，每个单元通常由几个 token
组成。在训练期间，同一个单元中的所有 token 都会被屏蔽，而不是只有一个
token 被屏蔽。</li>
<li><strong>entity-level mask</strong>：将一个 entity
作为一个单元，...。</li>
</ul>
<p>对于英语，论文使用单词分析和分块工具来获取短语的边界；对于中文，论文使用<strong>分词工具</strong>来获取词的边界。<br />
命名实体 entity
包括人名、地名、组织名、产品名等。需要使用NER的方法。</p>
<p><strong>ERNIE</strong> 通过三阶段学习来学得短语或实体的先验知识：</p>
<ul>
<li>第一阶段：Basic-level masking，使用基本的掩码策略，做法与 BERT
完全相同。这个阶段是在基本语言单元的随机 mask
上训练，因此很难对高级语义知识建模。对于英文，基本语言单元是词word；对于中文，基本语言单元是汉字
char 。</li>
<li>第二阶段：Phrase-level
masking，使用基本语言单元作为训练输入，但是使用 phrase-level
的掩码策略。这个阶段模型屏蔽和预测同一个短语的所有基本语言单元。</li>
<li>第三阶段：Entity-level
masking，使用基本语言单元作为训练输入，但是使用 entity-level
的掩码策略。这个阶段模型屏蔽和预测同一个命名实体的所有基本语言单元
。</li>
</ul>
<p>ERNIE 编码器和 BERT 相同，但是对于中文语料，ERNIE 把 CJK
编码范围内的字符都添加空格来分隔，然后使用 WordPiece（loved-&gt;lov ed）
来对中文语句词干化，WordPiece参考<span class="exturl" data-url="aHR0cHM6Ly93d3cuY25ibG9ncy5jb20vaHVhbmd5Yy9wLzEwMjIzMDc1Lmh0bWw=">一文读懂BERT中的WordPiece<i class="fa fa-external-link-alt"></i></span>。</p>
<h2 id="spanbert">SpanBert</h2>
<p><strong>SpanBert</strong>：一个新的分词级别的预训练方法，提出基于Bert掩码方式，采用不同长度token不同概率的方式（越短的token概率越大）Mask。
其在现有任务中的表现优于 BERT
，并在问答、指代消解等分词选择任务中取得了较大的进展。<br />
论文地址：<span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzE5MDcuMTA1MjkucGRm">SpanBERT:
Improving Pre-training by Representing and Predicting Spans<i class="fa fa-external-link-alt"></i></span></p>
<p>主要优化点：</p>
<ul>
<li>Span Masking。</li>
<li>MLM + SBO。</li>
<li>Single-Sequence Training。</li>
</ul>
<p>模型原理如图所示：<br />
<img src="/images/Bert/10.png" width="90%"></p>
<p><strong>Span Masking</strong><br />
提出了更好的 Span Mask 方案，SpanBERT 不再对随机的单个 token 添加
mask，而是对随机对邻接分词添加mask。根据几何分布，先随机选择一段（span）的长度，之后再根据均匀分布随机选择这一段的起始位置，最后按照长度mask。作者设定几何分布取
p=0.2，并裁剪最大长度只能是 10（不应当是长度 10
以上修剪，而应当为丢弃），利用此方案获得平均采样长度分布。因此分词的平均长度为
3.8
。作者还测量了词语（word）中的分词程度，使得添加mask的分词更长。如图，展示了分词mask长度的分布情况。<br />
<img src="/images/Bert/11.png" width="40%"><br />
和在 BERT 中一样，作者将 Y 的规模设定为 X 的15%，其中 80% 使用 [MASK]
进行替换，10%
使用随机单词替换，10%保持不变。与之不同的是，作者是在分词级别进行的这一替换，而非将每个单词单独替换。</p>
<p>不同mask方案对比：<br />
<img src="/images/Bert/17.png" width="80%"></p>
<p><strong>SBO</strong><br />
Span Boundary
Objective。分词选择模型一般使用其边界词创建一个固定长度的分词表示。为了于该模型相适应，作者希望结尾分词的表示的总和与中间分词的内容尽量相同。为此，作者引入了
SBO ，其仅使用观测到的边界词来预测带msak的分词的内容。</p>
<p>具体做法是，在训练时取 Span 前后边界的两个词，值得指出，这两个词不在
Span 内，然后用这<strong>两个词向量</strong>加上 Span
中被mask掉词的<strong>位置向量</strong>，来预测当前被mask的词。详细做法是将<strong>词向量</strong>和<strong>位置向量</strong>拼接起来，作者使用一个两层的前馈神经网络作为表示函数，该网络使用
GeLu 激活函数，并使用层正则化。和 MLM 一样使用交叉熵作为损失函数，就是
SBO 目标的损失，之后将这个损失和 BERT 的 MLM
的损失加起来，一起用于训练模型。<br />
<img src="/images/Bert/14.png" width="20%"><br />
<img src="/images/Bert/12.png" width="30%"><br />
<img src="/images/Bert/13.png" width="30%"></p>
<p><strong>Single-Sequence Training</strong><br />
没有segment embedding，只有一个长的句子，类似RoBERTa。它没用 Next
Sentence Prediction (NSP) 任务，而是直接用 Single-Sequence
Training，也就是根本不加入 NSP
任务来判断是否两句是上下句，直接用一句来训练。作者推测其可能原因如下：（a）更长的语境对模型更有利，模型可以获得更长上下文（类似
XLNet 的一部分效果；（b）加入另一个文本的语境信息会给MLM
语言模型带来噪音。</p>
<p>因此，SpanBERT 就没采用 NSP
任务，仅采样一个单独的邻接片段，该片段长度最多为512个单词，其长度与 BERT
使用的两片段的最大长度总和相同，然后 MLM 加上 SBO 任务来进行预训练。</p>
<p>其中主要训练细节是：</p>
<ul>
<li>训练时用了 Dynamic Masking 而不是像 BERT 在预处理时做 Mask；</li>
<li>取消 BERT 中随机采样短句的策略；</li>
<li>对 Adam 优化器中一些参数改变。</li>
</ul>
<p><img src="/images/Bert/17.png" width="80%"></p>
<p>实验结果：<br />
<img src="/images/Bert/15.png" width="40%"><br />
<img src="/images/Bert/16.png" width="80%"></p>
<h2 id="albert">Albert</h2>
<p>Albert主要对Bert参数进行优化，在任务结果下降很低的情况的下，使模型更小更适合部署。<br />
论文地址：<span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzE5MDkuMTE5NDIucGRm">ALBERT: A LITE
BERT FOR SELF-SUPERVISED LEARNING OF LANGUAGE REPRESENTATIONS<i class="fa fa-external-link-alt"></i></span></p>
<p>主要优化点：</p>
<ul>
<li>Token Embedding projection block
<ul>
<li>Parameter size <span class="math inline">\(O(V \times
E)\)</span></li>
</ul></li>
<li>Attention feed-forward block
<ul>
<li>Parameter size <span class="math inline">\(O(12 \times L \times H
\times H)\)</span></li>
</ul></li>
</ul>
<p><strong>优化一：Embedding维度分解</strong><br />
Token Embedding 采用 <span class="math inline">\(O(V\times E + E \times
H)\)</span> 且 <span class="math inline">\(E \ll
H\)</span>。由于模型结构的限制，WordePiece embedding的大小 <span
class="math inline">\(E\)</span> 总是与隐层大小 <span
class="math inline">\(H\)</span>
相同。从建模的角度考虑，词嵌入学习的是单词与上下文无关的表示，而隐层则是学习与上下文相关的表示。显然后者更加复杂，需要更多的参数，也就是说模型应当增大隐层大小
<span class="math inline">\(H\)</span> ，或者说满足 <span
class="math inline">\(H \gg E\)</span> 。但实际上词汇表的大小 <span
class="math inline">\(V\)</span> 通常非常大，如果 <span
class="math inline">\(E=H\)</span> 的话，增加隐层大小 <span
class="math inline">\(H\)</span> 后将会使embedding matrix的维度 <span
class="math inline">\(V \times E\)</span> 非常巨大。</p>
<p>因此Albert想要打破 <span class="math inline">\(E\)</span> 与 <span
class="math inline">\(H\)</span>
之间的绑定关系，从而减小模型的参数量，同时提升模型表现。具体做法是将embedding
matrix分解为两个大小分别为 <span class="math inline">\(V \times
E\)</span> 和 <span class="math inline">\(E \times H\)</span>
矩阵，也就是说先将单词投影到一个低维的embedding空间 <span
class="math inline">\(E\)</span> ，再将其投影到高维的隐藏空间 <span
class="math inline">\(H\)</span> 。这使得embedding matrix的维度从 <span
class="math inline">\(O(V \times E)\)</span> 减小到 <span
class="math inline">\(O(V\times E + E \times H)\)</span> 。当 <span
class="math inline">\(E \ll H\)</span>
时，参数量减少非常明显。在实现时，随机初始化 <span
class="math inline">\(V \times E\)</span> 和 <span
class="math inline">\(E \times H\)</span>
两个矩阵，计算某个单词的表示需用一个单词的one-hot向量乘以 <span
class="math inline">\(V \times E\)</span>
维的矩阵（也就是lookup），再用得到的结果乘 <span class="math inline">\(E
\times H\)</span> 维的矩阵即可。两个矩阵的参数通过模型学习。</p>
<p>从下图实验结果可见，对于不共享参数的情况， <span
class="math inline">\(E\)</span> 几乎是越大越好；而共享参数之后， <span
class="math inline">\(E\)</span> 太大反而会使模型表现变差， <span
class="math inline">\(E=128\)</span>
模型表现最好，因此ALBERT的默认参数设置为此。<br />
<img src="/images/Bert/2.png" width="90%"></p>
<p><strong>优化二：层权重共享</strong><br />
另一个减少参数量的方法就是层之间的参数共享，即多个层使用相同的参数。因为attention-feedforward操作是重复的，都是自注意力机制，所以考虑使用相同的权重来减少参数量，使得block参数变为<span
class="math inline">\(O(12 \times H \times H)\)</span>。</p>
<p>参数共享有三种方式：</p>
<ul>
<li>只共享feed-forward network的参数。</li>
<li>只共享attention的参数。</li>
<li>共享全部参数。</li>
</ul>
<p>ALBERT默认是共享全部参数的，在后续实验结果中我们可以看到几种方式的模型表现。</p>
<p>如下图所示，实验表明加入参数共享之后，每一层的输入embedding和输出embedding的L2距离和余弦相似度都比BERT稳定了很多，ALBERT
的结果更加平滑。这证明参数共享能够使模型参数更加稳定。<br />
<img src="/images/Bert/3.png" width="90%"></p>
<p>如下图所示，可以看出参数共享几乎也是对模型结果有负面影响的。但是考虑到其能够大幅削减参数量，并且对结果影响不是特别大，因此权衡之下选择了参数共享。<br />
<img src="/images/Bert/4.png" width="90%"></p>
<p><strong>优化三：SOP替代NSP</strong><br />
除了减少模型参数外，本外还对BERT的预训练任务<strong>Next-sentence
prediction</strong>(NSP)进行了改进。在BERT中，NSP任务的正例是文章中连续的两个句子，而负例则是从两篇文档中各选一个句子构造而成。在先前的研究中，已经证明NSP是并不是一个合适的预训练任务。本文推测其原因是模型在判断两个句子的关系时不仅考虑了两个句子之间的连贯性（coherence），还会考虑到两个句子的话题（topic）。而两篇文档的话题通常不同，模型会更多的通过话题去分析两个句子的关系，而不是句子间的连贯性，这使得NSP任务变成了一个相对简单的任务。</p>
<p>因此本文提出了<strong>Sentence-order
prediction</strong>(SOP)来取代NSP。具体来说，其正例与NSP相同，但负例是通过选择一篇文档中的两个连续的句子并将它们的顺序交换构造的。这样两个句子就会有相同的话题，模型学习到的就更多是句子间的连贯性。</p>
<p>如下图实验结果所示，如果不使用NSP或SOP作为预训练任务的话，模型在NSP和SOP两个任务上表现都很差；如果使用NSP作为预训练任务的话，模型确实能很好的解决NSP问题，但是在SOP问题上表现却很差，几乎相当于随机猜，因此说明NSP任务确实很难学到句子间的连贯性；而如果用SOP作为预训练任务，则模型也可以较好的解决NSP问题，同时模型在下游任务上表现也更好。说明SOP确实是更好的预训练任务。<br />
<img src="/images/Bert/5.png" width="90%"></p>
<h2 id="roberta">RoBerta</h2>
<p>论⽂地址：<span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzE5MDcuMTE2OTIucGRm">RoBERTa: A
Robustly Optimized BERT Pretraining Approach<i class="fa fa-external-link-alt"></i></span></p>
<p>预训练模型能够显著的提升任务效果，但是不同预训练模型的比较非常困难。首先，每个预训练模型的训练成本很高，无法一一训练并进行比较。其次，不同预训练模型通常是在不同规模大小的数据集上训练的，难以评估效果的好坏是预训练模型引起的还是预训练数据引起的。最后，超参数的选择对预训练模型的表现影响很大。同一个预训练模型的不同超参数，其比较结果会有很大不同。</p>
<p>RoBERTa 主要工作是复现 BERT，然后对 BERT
的模型架构、训练目标、训练细节（如数据集大小、训练时间）的重要性进行探索，从而提出了改进方案，这个改进方案称为
RoBERTa 。主要修改：</p>
<ul>
<li>更大的 batch
size、更多的数据、更长的输入序列、更长的预训练时间。</li>
<li>移除 NSP 任务。</li>
<li><strong>使用动态mask(dynamic masking)</strong>。</li>
<li>使用BPE方式，减少UNK单词出现次数（英文）。</li>
<li>调整Adam优化器的参数，<span
class="math inline">\(\epsilon\)</span>由1e-6改成1e-8，<span
class="math inline">\(\beta_2\)</span>由0.999改成0.98。</li>
</ul>
<p><strong>更大的 batch size</strong><br />
原本的BERTbase 的batch size是256，训练1M个steps。RoBERTa的batch
size为8k。为什么要用更大的batch
size呢？（除了因为他们有钱玩得起外）作者借鉴了在机器翻译中，用更大的batch
size配合更大学习率能提升模型优化速率和模型性能的现象，并且也用实验证明了确实Bert还能用更大的batch
size。<br />
<img src="/images/Bert/6.png" width="40%"></p>
<p><strong>更多的数据、更长的预训练时间</strong><br />
借鉴XLNet用了比Bert多10倍的数据，RoBERTa也用了更多的数据。性能确实再次彪升。当然，也需要配合更长时间的训练。<br />
<img src="/images/Bert/8.png" width="80%"></p>
<p><strong>移除 NSP 任务 + 使用更长的输入序列</strong><br />
原本的Bert为了捕捉句子之间的关系，使用了NSP任务进行预训练，就是输入一对句子A和B，判断这两个句子是否是连续的。在训练的数据中，50%的B是A的下一个句子，50%的B是随机抽取的。</p>
<p>而RoBERTa去除了NSP，而是每次输入连续的多个句子，直到最大长度512（可以跨文章）。这种训练方式叫做（FULL
-
SENTENCES），而原来的Bert每次只输入两个句子。实验表明在MNLI这种推断句子关系的任务上RoBERTa也能有更好性能。<br />
<img src="/images/Bert/7.png" width="80%"></p>
<p><strong>使用动态mask</strong><br />
原来Bert对每一个序列随机选择15%的Tokens替换成[MASK]，为了消除与下游任务的不匹配，还对这15%的Tokens进行（1）80%的时间替换成[MASK]；（2）10%的时间不变；（3）10%的时间替换成其他词。但整个训练过程，这15%的Tokens一旦被选择就不再改变，也就是说从一开始随机选择了这15%的Tokens，之后的N个epoch里都不再改变了。这就叫做静态Masking。</p>
<p>而RoBERTa一开始把预训练的数据复制10份，每一份都随机选择15%的Tokens进行Masking，也就是说，同样的一句话有10种不同的mask方式。然后每份数据都训练N/10个epoch。这就相当于在这N个epoch的训练中，每个序列的被mask的tokens是会变化的。这就叫做动态Masking。</p>
<p>那么这样改变是否真的有效果？作者在只将静态Masking改成动态Masking，其他参数不变的情况下做了实验，动态Masking确实能提高性能。<br />
<img src="/images/Bert/9.png" width="40%"></p>
<p><strong>使用BPE方式</strong><br />
针对的是英文，BERT原型使用的是 character-level BPE vocabulary of size
30K, RoBERTa使用了GPT2的 byte BPE 实现，使用的是byte而不是unicode
characters作为subword的单位。</p>
<p>这些针对Bert的预训练优化都使用起来，最终在GLUE, RACE,
SQuAD上都达到了SOTA的性能。</p>
<p>RoBERTa 采用 160 G 训练文本，远超 BERT 的 16G 文本，其中包括：</p>
<ul>
<li>BOOKCORPUS 和英文维基百科：原始 BERT 的训练集，大小 16GB 。</li>
<li>CC-NEWS：包含2016年9月到2019年2月爬取的6300万篇英文新闻，大小 76
GB（经过过滤之后）。</li>
<li>OPENWEBTEXT：从 Reddit 上共享的 URL
（至少3个点赞）中提取的网页内容，大小 38 GB 。</li>
<li>STORIES：CommonCrawl 数据集的一个子集，包含 Winograd
模式的故事风格，大小 31GB 。</li>
</ul>
<h2 id="macbert">Macbert</h2>
<p>MLM as
correction，使用校正作为Mask的语言模型，通过<strong>相似的单词mask</strong>而不是mask标记，减轻了预训练和微调阶段两者之间的差距。<br />
论文地址：<span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzIwMDQuMTM5MjIucGRm">Revisiting
Pre-trained Models for Chinese Natural Language Processing<i class="fa fa-external-link-alt"></i></span></p>
<p><img src="/images/Bert/macbert1.jpg" width="90%"></p>
<p>使用不同的mask策略，常见的有wwm和n-gram，而Macbert使用相似词作为mask，缓解预训练和微调阶段之间任务不一致导致的差距。</p>
<p>总结一下mask类任务依赖三点：<br />
（1）选择被mask的token：随机、针对特殊词、组合形式比如bert的80%+10%+10%。<br />
（2）替换方法：中文bert是字、还有词比如wwm，词组比如n-gram等方式，相似词也可以。<br />
（3）mask时机：静态或动态mask策略，静态是在训练前就处理好的，动态是每个epoch都重新处理一次。<br />
根据下游任务设计合适的策略，进行定制化是一项重要能力，比如n-gram在文本分类更有效，相似词对阅读理解效果更好。</p>
<h2 id="electra">ELECTRA</h2>
<p>ELECTRA最主要的贡献是提出了新的预训练任务和框架，把这种Masked
language model(MLM)预训练任务改成了判别式的Replaced token
detection(RTD)任务，判断当前token是否被语⾔模型替换过。<br />
论⽂地址：<span class="exturl" data-url="aHR0cHM6Ly9vcGVucmV2aWV3Lm5ldC9wZGY/aWQ9cjF4TUgxQnR2Qg==">ELECTRA:
PRE-TRAINING TEXT ENCODERS AS DISCRIMINATORS RATHER THAN
GENERATORS<i class="fa fa-external-link-alt"></i></span></p>
<p><img src="/images/Bert/ELECTRA1.jpg" width="90%"></p>
<p>ELECTRA提出了⼀套新的预训练框架，其中包括两个部分：Generator 和
Discriminator。可看作两个Bert模型。<br />
<strong>Generator</strong>：⼀个⼩的MLM模型，在[MASK]的位置预测原来的词。Generator将⽤来把输⼊⽂本做部分词的替换，<br />
<strong>Discriminator</strong>：判断输⼊句⼦中的每个词是否被替换，即使⽤Replaced
Token Detection (RTD)预训练任务，取代了BERT原始的Masked Language Model
(MLM)。需要注意的是这⾥并没有使⽤Next Sentence Prediction
(NSP)任务，在预训练阶段结束之后，只使⽤Discriminator作为下游任务精调的基模型。</p>
<p>需要注意的是：<br />
（1）由于Generator生成的词是离散的，所以梯度到它俩之间是断掉的，Discriminator的梯度无法传递到Generator，所以Generator还是训练MLM任务，Discriminator训练的是序列标注任务（判断每个token是真是假），两者同时训练但Discriminator的梯度不会传给Generator。<br />
（2）因为Discriminator任务相对容易，所以的到的loss会比Generator小，因此在Discriminator的loss前乘系数50，最终loss是它俩相加。<br />
（3）在优化Discriminator时计算了所有token上的loss，而以往计算BERT的MLM
loss时会忽略没被mask的token。</p>
<p>这个模型在同级别的参数量下，阅读理解上任务非常优秀！</p>
<h2 id="sbert">SBERT</h2>
<p>虽然BERT和RoBERTa在很多句子对形式的回归任务（例如文本语义相似度）上达到了SOTA效果，但是它们还存在一些缺点：在这些任务中，它们均需要将比较的两个句子都传入到模型中计算，计算开销过大。BERT模型在一个1W句子集合中，找出最相近的一个句子对，需要5千万次推断计算（约65小时）才能完成，所以BERT并不适合语义相似度搜索等任务。</p>
<p>在该论文中，作者提出了一个新的模型，Sentence-BERT（简称SBERT）。SBERT采用双重或三重BERT网络结构，具体结构介绍会在后文中详细介绍。如果使用的是基于RoBERTa模型，则改造后的模型简称为SRoBERTa。</p>
<p>通过SBERT模型获取到的句子embedding，可以直接通过cos相似度计算两个句子的相似度，这样就大大减少了计算量。因为在使用BERT模型进行句子间相似度的判断时，需要从句子集合中，选出两个句子进行组合，传入BERT中进行计算，而使用SBERT模型，只需要将集合中每个句子单独传入到模型中，得到每个句子的embeding，计算相似度只需要使用cos函数计算两两embeding的cos距离即可。因此，使用BERT/RoBERTa模型需要65h才能完成的寻找最相似句子对任务，SBERT模型完成仅需5s。</p>
<p>作者在一些STS任务和迁移学习任务上评估SBERT模型，该模型达到了新的SOTA水平。</p>
<p>论文地址：<span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzE5MDguMTAwODQucGRm">Sentence-BERT: Sentence
Embeddings using Siamese BERT-Networks<i class="fa fa-external-link-alt"></i></span>。<br />
模型地址：<span class="exturl" data-url="aHR0cHM6Ly93d3cuc2JlcnQubmV0Lw==">Sentence-Transformers<i class="fa fa-external-link-alt"></i></span>。<br />
详解参考：<span class="exturl" data-url="aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2Zlbmd4aW5saW51eC9hcnRpY2xlL2RldGFpbHMvMTA5MTk1NzYy">论文阅读Sentence-BERT:
Sentence Embeddings using Siamese BERT-Networks<i class="fa fa-external-link-alt"></i></span>。</p>
<h2 id="xlnet">XLNet</h2>
<p><strong>XLNet</strong>：使用Transformer-XL模型。</p>
<p>当Bert作为seq2seq的encoder时，输入也要Mask：</p>
<ul>
<li>随机Mask。</li>
<li>删除Delete。</li>
<li>多个句子，换顺序Permuttion。</li>
<li>打乱token，比如后面词放在前面。</li>
<li>插入Mask，Text Infilling。这个效果最好。</li>
</ul>
<h2 id="gpt">GPT</h2>
<p>只使用Transformer-Decoder端进行序列生成(预测下一个词)。<br />
训练使用Few-shot
Learning：先给出问题描述和部分示例学习，再给出prompt(引子/问题)，能给出解答。<br />
训练时只给出一个示例(one-shot)或不给示例(zero-shot)进行学习。</p>
<p>GPT-2的学习目标是使用无监督的预训练模型做有监督的任务。作者认为，当一个语言模型的容量足够大时，它就足以覆盖所有的有监督任务，也就是说所有的有监督学习都是无监督语言模型的一个子集。GPT-2去掉了fine-tuning层。</p>
<p>GPT-3沿用了GPT-2的结构，但是在网络容量上做了很大的提升。</p>
<p><span class="exturl" data-url="aHR0cHM6Ly96aHVhbmxhbi56aGlodS5jb20vcC8zNTAwMTc0NDM=">词向量之GPT-1，GPT-2和GPT-3<i class="fa fa-external-link-alt"></i></span><br />
<span class="exturl" data-url="aHR0cHM6Ly96aHVhbmxhbi56aGlodS5jb20vcC8yMDA5Nzg1Mzg=">GPT-3阅读笔记：Language
Models are Few-Shot Learners<i class="fa fa-external-link-alt"></i></span></p>
<h2 id="mass">MASS</h2>
<p>MASS针对的是seq2seq任务。<br />
MASS在encoder端对句子随机mask一个长度为k的连续片段，然后通过decoder预测生被mask片段。<br />
mask方式：随机删除、打乱词序、旋转词序、插入mask等方式。</p>
<h2 id="unilm">UniLM</h2>
<p>UniLM
1.0通过设计不同掩码，支持4种不同的训练目标：从左往右单向LM，从右往左单向LM，双向LM，序列到序列LM。<br />
UniLM 2.0支持更多样的factorization
order且无需重复构建训练实体。训练时部分自回归使用pseudo mask LM
(PMLM)，直译为伪掩码，作为部分自回归训练时占位符，和自编码的<code>[M]</code>任务作为区分。</p>
<p><span class="exturl" data-url="aHR0cHM6Ly96aHVhbmxhbi56aGlodS5jb20vcC8xMjI5OTkxNTM=">微软统一预训练语言模型UniLM
2.0解读<i class="fa fa-external-link-alt"></i></span></p>
<h1 id="参考文献">参考文献</h1>
<p><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzE4MTAuMDQ4MDU=">BERT: Pre-training of Deep
Bidirectional Transformers for Language Understanding<i class="fa fa-external-link-alt"></i></span><br />
<span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzE5MDUuMDU1ODMucGRm">How to Fine-Tune BERT for
Text Classification?<i class="fa fa-external-link-alt"></i></span><br />
<span class="exturl" data-url="aHR0cHM6Ly9jbG91ZC50ZW5jZW50LmNvbS9kZXZlbG9wZXIvYXJ0aWNsZS8xNTE5MTgw">BERT论文解读<i class="fa fa-external-link-alt"></i></span><br />
<span class="exturl" data-url="aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2RlbmRpX2h1c3QvYXJ0aWNsZS9kZXRhaWxzLzEwNDQ2NTMzNw==">【调优方法】——warmup<i class="fa fa-external-link-alt"></i></span><br />
<span class="exturl" data-url="aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2xpb24xOTkzMDkyNC9hcnRpY2xlL2RldGFpbHMvMTA0NDY5OTQ0">Bert微调技巧实验大全-How
to Fine-Tune BERT for Text Classification<i class="fa fa-external-link-alt"></i></span><br />
<span class="exturl" data-url="aHR0cHM6Ly96aHVhbmxhbi56aGlodS5jb20vcC84NzU2MjkyNg==">【论文阅读】ALBERT<i class="fa fa-external-link-alt"></i></span><br />
<span class="exturl" data-url="aHR0cHM6Ly93d3cuamlhbnNodS5jb20vcC9lZGRmMDRiYTg1NDU=">改进版的RoBERTa到底改进了什么？<i class="fa fa-external-link-alt"></i></span><br />
<span class="exturl" data-url="aHR0cHM6Ly9tcC53ZWl4aW4ucXEuY29tL3MvTUhtN0F4bWN1RWdGUl9vTmJOcUZrUQ==">BERT模型的优化改进方法！<i class="fa fa-external-link-alt"></i></span></p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://soundmemories.github.io/2021/07/19/NLP/04.Transformer/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avstar.png">
      <meta itemprop="name" content="SoundMemories">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="SoundMemories">
      <meta itemprop="description" content="今日事，今日毕">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | SoundMemories">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2021/07/19/NLP/04.Transformer/" class="post-title-link" itemprop="url">Transformer</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2021-07-19 00:00:00" itemprop="dateCreated datePublished" datetime="2021-07-19T00:00:00+08:00">2021-07-19</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/NLP/" itemprop="url" rel="index"><span itemprop="name">NLP</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>1.8k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>6 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="结构">结构</h1>
<p>论文中的 Transformer 架构包含了 encoder 和 decoder
两部分，其架构如下图所示。<br />
<img src="/images/Transformer/1.png" width="60%"></p>
<ul>
<li>编码器 encoder 包含一组 6 个相同的层 Layer(layer间串行连接)
，每层包含两个子层 SubLayer。
<ul>
<li>第一个子层是一个多头自注意力 multi-head self-attention 层。</li>
<li>第二个子层是一个简单的全连接层。</li>
<li>每个子层都使用残差直连，并且残差直连之后跟随一个layer
normalization。</li>
<li>假设子层的输入为 <span class="math inline">\(h\)</span>，则经过 LN
之后整体的输出为 <span
class="math inline">\(LayerNorm(h+Sublayer(h))\)</span> 。为了 Add
直连，论文将内部所有层的输入、输出的向量维度设置为512维。</li>
</ul></li>
<li>解码器 decoder 也包含一组 6 个相同的层 Layer，但是每层包含三个子层
SubLayer 。
<ul>
<li>第一个子层也是一个多头自注意力 multi-head self-attention
层。但是，在计算位置 <span class="math inline">\(i\)</span> 的
self-attention 时屏蔽掉了位置 <span class="math inline">\(i\)</span>
之后的序列值，这意味着：位置 <span class="math inline">\(i\)</span> 的
attention 只能依赖于它之前的结果，不能依赖它之后的结果。因此，这种
self-attention 也被称作 masked self-attention。</li>
<li>第二个子层是一个多头注意力multi-head attention 层，用于捕获 decoder
output 和 encoder output 之间的 attention 。</li>
<li>第三个子层是一个简单的全连接层。</li>
<li>和 encoder
一样：每个子层都使用残差直连，并且残差直连之后跟随一个layer
normalization:LN 。decoder 所有层的输入、输出的向量维度也是512维。</li>
</ul></li>
</ul>
<h1 id="attention">attention</h1>
<p>编码器和解码器的 attention 都是采用 scaled dot attention
(点积)。计算点积后除以 <span class="math inline">\(\sqrt{d_k}\)</span>
是为了降低 <span class="math inline">\(score\)</span>
的数值，防止它落入到 softmax 函数的饱和区间，因为 softmax
函数的饱和区梯度几乎为 0 ，容易发生梯度消失。</p>
<p>一组多个attention 的效果要优于单个 attention，这称作multi-head
attention 。将整个 attention 空间拆分成多个 attention 子空间，最后将多个
head
的输出进行拼接，并再经过一个线性映射即可得到多头attention的结果。线性映射目的是确保
multi-head attention 前后的输入输出维度一致。论文中选择 attention
都为512维，为了保证 multi-head attention 的表达空间与 single-head
attention 一致。<br />
multi-head
attention表达能力更强，相当于在整体计算代价几乎保持不变的条件下，引入了更多的非线性从而增强了模型的表达能力。。</p>
<p>在论文中，有三种方式使用多头注意力机制：<br />
- encoder-decoder attention：query 来自前一个 decoder
层的输出，keys,values 来自 encoder 的输出。其意义是： decoder
的每个位置去查询它与 encoder 的哪些位置相关，并用 encoder 的这些位置的
value 来表示。<br />
- encoder self-attention：query,key,value 都来自前一层 encoder
的输出。这允许 encoder 的每个位置关注 encoder 前一层的所有位置。<br />
- decoder masked self-attention：query,key,value 都来自前一层 decoder
的输出。这允许 decoder 的每个位置关注 decoder
前一层的、在该位置之前的所有位置。</p>
<h1 id="全连接层">全连接层</h1>
<p>encoder 和 decoder 还包含有全连接层。对 encoder/decoder 的每个
attention 输出，全连接层通过一个 ReLU 激活函数和一个线性映射。</p>
<p>对于同一个 multi-head attention
的所有输出，采用相同的参数；对于不同的 multi-head attention
的输出，采用不同的参数。</p>
<p>输入和输出的维度保持为512，但是中间向量的维度是2048，这是为了扩充中间层的表示能力，从而抵抗
ReLU 带来的表达能力的下降。</p>
<h1 id="embedding-层">embedding 层</h1>
<p>网络涉及三个 embedding 层：<br />
- encoder 输入 embedding 层：将 encoder 输入 token
转化为512维的向量。<br />
- decoder 输入 embedding 层：将 decoder 输入 token
转化为512维的向量。<br />
- decoder 输出 embedding 层：将 decoder 输出 token
转化为512维的向量。</p>
<p>在论文中这三个 embedding 矩阵是共享的，并且论文中在 embedding
层将该矩阵乘以一个常量 <span
class="math inline">\(\sqrt{d_{model}}\)</span> 来放大每个权重。</p>
<h1 id="position-embedding">position embedding</h1>
<p>从 attention 的计算公式可知：调整输入的顺序对 attention
的结果没有任何影响，attention 的输出中不包含任何顺序信息。<br />
论文通过将位置编码添加到 encoder 和 decoder 底部的输入 embedding
来解决问题。对于同一个输入序列如果打乱序列顺序，则不同 token 的
attention 权重发生改变使得 attention 的结果不同。</p>
<p>位置编码有两种选择：<br />
- <strong>functional(函数式)</strong>：论文中使用
<strong>正弦+余弦</strong> 方式设定position embedding。<br />
- 不同的维度对应不同的波长的正弦曲线，波长从 <span
class="math inline">\(2\pi\)</span> 到 <span
class="math inline">\(2000\pi\)</span> 。<br />
- 选择这样的函数是因为：不同位置之间的embedding
可以简单的相互表示。这意味着模型可以捕获到位置之间的相对位置关系。<br />
- <strong>parametric(参数式)</strong>：可以自己学习，比如bert。</p>
<h1 id="补充阅读">补充阅读</h1>
<p><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvYWJzLzIwMDIuMDQ3NDU=">On Layer Normalization in
the Transformer Architecture<i class="fa fa-external-link-alt"></i></span>：<br />
我们知道，在原始的Transformer中，Layer
Norm在跟在Residual之后的，我们把这个称为Post-LN Transformer。</p>
<p>而且用Transformer调过参的也知道，Post-LN
Transformer对参数非常敏感，需要很仔细地调参才能取得好的结果，比如必备的warm-up学习率策略，这会非常耗时间。<br />
所以现在问题来了，为什么warm-up是必须的？能不能把它去掉？<br />
本文的出发点是：既然warm-up是训练的初始阶段使用的，那肯定是训练的初始阶段优化有问题，包括模型的初始化。</p>
<p>从而，作者发现，Post-LN
Transformer在训练的初始阶段，输出层附近的期望梯度非常大，所以，如果没有warm-up，模型优化过程就会炸裂，非常不稳定。<br />
既然如此，本文作者尝试把LayerNorm换个位置，比如放在Residual的过程之中（称为Pre-LN
Transformer），再观察训练初始阶段的梯度变化，发现比Post-LN
Transformer不知道好到哪里去了，甚至不需要warm-up，从而进一步减少训练时间，这一结果的确令人震惊。</p>
<p>本文别出心裁，用实验和理论验证了Pre-LN
Transformer结构不需要使用warm-up的可能性，其根源是LN层的位置导致层次梯度范数的增长，进而导致了Post-LN
Transformer训练的不稳定性。<br />
本文第一次将warm-up、LayerNorm、gradient和initialization联系起来，非常值得一读！</p>
<p><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvYWJzLzIwMDMuMDc4NDU=">PowerNorm: Rethinking
Batch Normalization in Transformers<i class="fa fa-external-link-alt"></i></span>：<br />
本文探讨了Transformer为什么使用layer normalization，为什么不用batch
normalization，然后根据batch normalization出现的问题提出了power
normalization代替layer normalization。<span class="exturl" data-url="aHR0cHM6Ly96aHVhbmxhbi56aGlodS5jb20vcC8xMjY3NDkzMTE/ZnJvbV92b3RlcnNfcGFnZT10cnVl">BatchNorm在NLP任务中的问题与改进<i class="fa fa-external-link-alt"></i></span>。</p>
<h1 id="参考文献">参考文献</h1>
<p><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzE3MDYuMDM3NjIucGRm">Attention is All You
Need<i class="fa fa-external-link-alt"></i></span><br />
<span class="exturl" data-url="aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L25vY21sL2FydGljbGUvZGV0YWlscy8xMDMwODI2MDA=">Transformer--论文翻译：Attention
Is All You Need 中文版<i class="fa fa-external-link-alt"></i></span><br />
<span class="exturl" data-url="aHR0cHM6Ly9ubHAuc2Vhcy5oYXJ2YXJkLmVkdS8yMDE4LzA0LzAzL2F0dGVudGlvbi5odG1s">The
Annotated Transformer<i class="fa fa-external-link-alt"></i></span><br />
<span class="exturl" data-url="aHR0cDovL2phbGFtbWFyLmdpdGh1Yi5pby9pbGx1c3RyYXRlZC10cmFuc2Zvcm1lci8=">The
Illustrated Transformer<i class="fa fa-external-link-alt"></i></span><br />
<span class="exturl" data-url="aHR0cHM6Ly9tcC53ZWl4aW4ucXEuY29tL3Mvc05Uc3ZVamNmRVJ3VlA2RFRwLVR2QQ==">Transformer十问十答<i class="fa fa-external-link-alt"></i></span><br />
<span class="exturl" data-url="aHR0cHM6Ly96aC12Mi5kMmwuYWkvY2hhcHRlcl9hdHRlbnRpb24tbWVjaGFuaXNtcy9zZWxmLWF0dGVudGlvbi1hbmQtcG9zaXRpb25hbC1lbmNvZGluZy5odG1s">动手学-自注意力和位置编码<i class="fa fa-external-link-alt"></i></span><br />
<span class="exturl" data-url="aHR0cHM6Ly96aC12Mi5kMmwuYWkvY2hhcHRlcl9hdHRlbnRpb24tbWVjaGFuaXNtcy90cmFuc2Zvcm1lci5odG1s">动手学-Transformer<i class="fa fa-external-link-alt"></i></span><br />
<span class="exturl" data-url="aHR0cHM6Ly93d3cuYmlsaWJpbGkuY29tL3ZpZGVvL0JWMVd2NDExaDdrTj9wPTIz">(强推)李宏毅2021春机器学习课程<i class="fa fa-external-link-alt"></i></span><br />
<span class="exturl" data-url="aHR0cHM6Ly9tcC53ZWl4aW4ucXEuY29tL3MvUkx4V2V2VldIWGdYLVVjb3hEUzcwdw==">细讲 |
Attention Is All You Need<i class="fa fa-external-link-alt"></i></span><br />
<span class="exturl" data-url="aHR0cHM6Ly96aHVhbmxhbi56aGlodS5jb20vcC80NDEyMTM3OA==">【NLP】Transformer模型原理详解<i class="fa fa-external-link-alt"></i></span></p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://soundmemories.github.io/2021/07/19/Python/22.Flask/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avstar.png">
      <meta itemprop="name" content="SoundMemories">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="SoundMemories">
      <meta itemprop="description" content="今日事，今日毕">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | SoundMemories">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2021/07/19/Python/22.Flask/" class="post-title-link" itemprop="url">Flask</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2021-07-19 00:00:00" itemprop="dateCreated datePublished" datetime="2021-07-19T00:00:00+08:00">2021-07-19</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Python/" itemprop="url" rel="index"><span itemprop="name">Python</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>4.8k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>17 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="flask介绍">Flask介绍</h1>
<p>Flask诞生于2010年，是Armin ronacher（人名）用 Python 语言基于
Werkzeug 工具箱编写的轻量级Web开发框架。</p>
<p>Flask
本身相当于一个内核，其他几乎所有的功能都要用到扩展（邮件扩展Flask-Mail，用户认证Flask-Login，数据库Flask-SQLAlchemy），都需要用第三方的扩展来实现。比如可以用
Flask 扩展加入ORM、窗体验证工具，文件上传、身份验证等。Flask
没有默认使用的数据库，你可以选择 MySQL，也可以用 NoSQL。</p>
<p>其<strong>WSGI工具箱</strong>采用
Werkzeug（路由模块），<strong>模板引擎</strong>则使用 Jinja2。这两个也是
Flask 框架的核心。</p>
<p>最新版本：1.0.2、2.1.2</p>
<h2 id="框架对比">框架对比</h2>
<p>重量级的框架：为方便业务程序的开发，提供了丰富的工具、组件，如Django<br />
轻量级的框架：只提供Web框架的核心功能，自由、灵活、高度定制，如Flask、Tornado</p>
<p>django提供了：<br />
1. django-admin快速创建项目工程目录<br />
2. manage.py 管理项目工程<br />
3. orm模型（数据库抽象层）<br />
4. admin后台管理站点<br />
5. 缓存机制<br />
6. 文件存储系统<br />
7. 用户认证系统</p>
<p>而这些，flask都没有，都需要扩展包来提供。</p>
<h2 id="常用扩展包">常用扩展包</h2>
<p>扩展列表：http://flask.pocoo.org/extensions/</p>
<p>Flask-SQLalchemy：操作数据库；<br />
Flask-script：插入脚本；<br />
Flask-migrate：管理迁移数据库；<br />
Flask-Session：Session存储方式指定；<br />
Flask-WTF：表单；<br />
Flask-Mail：邮件；<br />
Flask-Bable：提供国际化和本地化支持，翻译；<br />
Flask-Login：认证用户状态；<br />
Flask-OpenID：认证；<br />
<strong>Flask-RESTful：开发REST API的工具；</strong><br />
Flask-Bootstrap：集成前端Twitter Bootstrap框架；<br />
Flask-Moment：本地化日期和时间；<br />
Flask-Admin：简单而可扩展的管理接口的框架</p>
<h2 id="安装">安装</h2>
<p>建议初学者创建修环境<code>mkvirtualenv flask -p python3</code>，再<code>pip install flask</code>。</p>
<p>虚拟环境命令：<br />
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 虚拟环境</span></span><br><span class="line">mkvirtualenv  <span class="comment"># 创建虚拟环境</span></span><br><span class="line">rmvirtualenv  <span class="comment"># 删除虚拟环境</span></span><br><span class="line">workon  <span class="comment"># 进入虚拟环境、查看所有虚拟环境</span></span><br><span class="line">deactivate  <span class="comment"># 退出虚拟环境</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># pip</span></span><br><span class="line">pip install  <span class="comment"># 安装依赖包</span></span><br><span class="line">pip uninstall  <span class="comment"># 卸载依赖包</span></span><br><span class="line">pip <span class="built_in">list</span>  <span class="comment"># 查看已安装的依赖包</span></span><br><span class="line">pip freeze  <span class="comment"># 冻结当前环境的依赖包</span></span><br></pre></td></tr></table></figure></p>
<h1 id="工程搭建">工程搭建</h1>
<h2 id="hello-world程序">Hello World程序</h2>
<figure class="highlight python"><figcaption><span>app.py</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 导入Flask类</span></span><br><span class="line"><span class="keyword">from</span> flask <span class="keyword">import</span> Flask</span><br><span class="line"></span><br><span class="line"><span class="comment">#Flask类接收一个参数__name__</span></span><br><span class="line">app = Flask(__name__)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 装饰器的作用是将路由映射到视图函数index</span></span><br><span class="line"><span class="meta">@app.route(<span class="params"><span class="string">&#x27;/&#x27;</span></span>)</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">index</span>():</span><br><span class="line">    <span class="keyword">return</span> <span class="string">&#x27;Hello World&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Flask应用程序实例的run方法启动WEB服务器</span></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    app.run()</span><br></pre></td></tr></table></figure>
<p>1、首先我们导入了 Flask 类。 该类的实例将会成为我们的 WSGI
应用。<br />
2、接着我们创建一个该类的实例<code>Flask(__name__)</code>。第一个参数是应用模块或者包的名称。这个参数是必需的，这样
Flask
才能知道在哪里可以找到模板和静态文件等东西。<code>Flask(__name__)</code>就是告诉Flask框架根目录位置，从而Flask可以查找其他配置文件。比如创建的flask项目目录如下：<br />
<figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">- test</span><br><span class="line">  - static</span><br><span class="line">  - tmplates</span><br><span class="line">  - app.py</span><br></pre></td></tr></table></figure><br />
通过<code>Flask(__name__)</code>，Flask知道了根目录test，从而知道本目录下的其他配置文件或文件夹(static、tmplates)，不像Django需要手动指定配置目录。</p>
<p>3、然后我们使用 <code>route()</code> 装饰器来告诉 Flask 触发函数的
URL 。</p>
<p><code>Flask</code>类实例化时可传递的参数：<br />
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">import_name      <span class="comment"># Flask程序所在的包(模块)，传 __name__ 就可以</span></span><br><span class="line">                 <span class="comment"># 其可以决定 Flask 在访问静态文件时查找的路径</span></span><br><span class="line">static_url_path  <span class="comment"># 静态文件访问路径，可以不传，默认为：/ + static_folder</span></span><br><span class="line">static_folder    <span class="comment"># 静态文件存储的文件夹，可以不传，默认为 static</span></span><br><span class="line">template_folder  <span class="comment"># 模板文件存储的文件夹，可以不传，默认为 templates</span></span><br></pre></td></tr></table></figure></p>
<h2 id="应用程序配置参数">应用程序配置参数</h2>
<p><code>Flask(__name__)</code>对象初始化参数仅仅设置的是Flask本身的属性，比如Flask从哪里读取静态文件、模板文件。</p>
<p>应用程序配置参数设置的是一个Web应用工程的相关信息，比如：数据库的连接信息、日志的配置信息、自定义的配置信息。<br />
这就需要集中管理项目的所有配置信息：<br />
1. Django将所有配置信息都放到了settings.py文件中，而Flask则不同。<br />
2.
Flask将配置信息保存到了<strong>app.config</strong>属性中，该属性可以<strong>按照字典类型进行操作</strong>。</p>
<p>Flask应用程序配置参数：<br />
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 读取</span></span><br><span class="line">app.config.get(name)</span><br><span class="line">app.config[name]</span><br></pre></td></tr></table></figure><br />
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 设置</span></span><br><span class="line"><span class="comment"># 1. 从配置“对象”中加载，配置对象可以写一个类</span></span><br><span class="line">app.config.from_object(配置对象)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. 从配置文件中加载</span></span><br><span class="line">app.config.from_pyfile(<span class="string">&#x27;setting.py&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3. 从环境变量中加载(使用环境变量加载不想出现在代码中的敏感配置信息)</span></span><br><span class="line">app.config.from_envvar(<span class="string">&#x27;环境变量名&#x27;</span>)</span><br></pre></td></tr></table></figure><br />
优缺点：<br />
1.
从配置“对象”中加载：可以继承，适用于配置多变动的情况；缺点是敏感信息暴露。<br />
2.
从配置文件中加载：不会暴露敏感信息；缺点是不能继承，配置地址写死，不易变动。<br />
3.
从环境变量中加载：不会暴露敏感信息，配置地址灵活可变；缺点是不能继承，设置不方便。</p>
<p>一般选择2种方式一起，来配置参数。</p>
<div class="tabs" id="1"><ul class="nav-tabs"><li class="tab active"><a href="#1-1">从配置“对象”中加载</a></li><li class="tab"><a href="#1-2">从配置文件中加载</a></li><li class="tab"><a href="#1-3">从环境变量中加载</a></li></ul><div class="tab-content"><div class="tab-pane active" id="1-1"><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">DefaultConfig</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;默认配置&quot;&quot;&quot;</span></span><br><span class="line">    SECRET_KEY = <span class="string">&#x27;TPmi4aLWRbyVq8zu9v82dWYW1&#x27;</span></span><br><span class="line"></span><br><span class="line">app = Flask(__name__)</span><br><span class="line"><span class="comment"># 加入SECRET_KEY配置</span></span><br><span class="line">app.config.from_object(DefaultConfig)</span><br><span class="line"></span><br><span class="line"><span class="meta">@app.route(<span class="params"><span class="string">&quot;/&quot;</span></span>)</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">index</span>():</span><br><span class="line">    <span class="built_in">print</span>(app.config[<span class="string">&#x27;SECRET_KEY&#x27;</span>])</span><br><span class="line">    <span class="keyword">return</span> <span class="string">&quot;hello world&quot;</span></span><br></pre></td></tr></table></figure></div><div class="tab-pane" id="1-2"><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 新建一个配置文件setting.py，写入下文</span></span><br><span class="line">SECRET_KEY = <span class="string">&#x27;TPmi4aLWRbyVq8zu9v82dWYW1&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 在Flask程序</span></span><br><span class="line">app = Flask(__name__)</span><br><span class="line"><span class="comment"># 增加配置</span></span><br><span class="line">app.config.from_pyfile(<span class="string">&#x27;setting.py&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="meta">@app.route(<span class="params"><span class="string">&quot;/&quot;</span></span>)</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">index</span>():</span><br><span class="line">    <span class="built_in">print</span>(app.config[<span class="string">&#x27;SECRET_KEY&#x27;</span>])</span><br><span class="line">    <span class="keyword">return</span> <span class="string">&quot;hello world&quot;</span></span><br></pre></td></tr></table></figure></div><div class="tab-pane" id="1-3"><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 先在终端中执行如下命令</span></span><br><span class="line">export PROJECT_SETTING=<span class="string">&#x27;~/setting.py&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 在Flask程序</span></span><br><span class="line">app = Flask(__name__)</span><br><span class="line"><span class="comment"># 增加配置：setting.py指定为配置文件，只不过这个文件地址存储在PROJECT_SETTING环境变量中</span></span><br><span class="line">app.config.from_envvar(<span class="string">&#x27;PROJECT_SETTING&#x27;</span>, silent=<span class="literal">True</span>) </span><br><span class="line"><span class="comment"># silent=False没有值时报错通知，默认为False</span></span><br><span class="line"><span class="comment"># True 表示安静的处理，即时没有值也让Flask正常的运行下去</span></span><br><span class="line"></span><br><span class="line"><span class="meta">@app.route(<span class="params"><span class="string">&quot;/&quot;</span></span>)</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">index</span>():</span><br><span class="line">    <span class="built_in">print</span>(app.config[<span class="string">&#x27;SECRET_KEY&#x27;</span>])</span><br><span class="line">    <span class="keyword">return</span> <span class="string">&quot;hello world&quot;</span></span><br></pre></td></tr></table></figure></div></div></div>
<p><strong>环境变量</strong><br />
通俗的理解，环境变量就是我们设置在操作系统中，由操作系统代为保存的变量值。<br />
在Linux系统中设置和读取环境变量的方式如下：<br />
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">export 变量名=变量值  # 设置</span><br><span class="line">echo $变量名  # 读取</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">例如</span></span><br><span class="line">export ITCAST=python</span><br><span class="line">echo $ITCAST</span><br></pre></td></tr></table></figure><br />
Flask使用环境变量加载配置的本质是通过环境变量值找到配置文件。</p>
<p>项目中的常用方式：使用工厂模式创建Flask
app，并结合使用配置对象与环境变量加载配置。<br />
（1）使用<strong>配置对象</strong>加载默认配置。<br />
（2）使用<strong>环境变量</strong>加载不想出现在代码中的敏感配置信息。<br />
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">create_flask_app</span>(<span class="params">config</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    创建Flask应用</span></span><br><span class="line"><span class="string">    :param config: 配置对象</span></span><br><span class="line"><span class="string">    :return: Flask应用</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    app = Flask(__name__)</span><br><span class="line">    app.config.from_object(config)</span><br><span class="line">    <span class="comment"># 3. 从环境变量中加载</span></span><br><span class="line">    <span class="comment"># 从环境变量指向的配置文件中读取的配置信息会覆盖掉从配置对象中加载的同名参数</span></span><br><span class="line">    app.config.from_envvar(<span class="string">&quot;PROJECT_SETTING&quot;</span>, silent=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">return</span> app</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 1. 从配置“对象”中加载，配置对象可以写一个类</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">DefaultConfig</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;默认配置&quot;&quot;&quot;</span></span><br><span class="line">    SECRET_KEY = <span class="string">&#x27;itcast1&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 继承</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">DevelopmentConfig</span>(<span class="title class_ inherited__">DefaultConfig</span>):</span><br><span class="line">    DEBUG=<span class="literal">True</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># app = create_flask_app(DefaultConfig)</span></span><br><span class="line">app = create_flask_app(DevelopmentConfig)</span><br><span class="line"></span><br><span class="line"><span class="meta">@app.route(<span class="params"><span class="string">&quot;/&quot;</span></span>)</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">index</span>():</span><br><span class="line">    <span class="built_in">print</span>(app.config[<span class="string">&#x27;SECRET_KEY&#x27;</span>])</span><br><span class="line">    <span class="keyword">return</span> <span class="string">&quot;hello world&quot;</span></span><br></pre></td></tr></table></figure></p>
<h2 id="app.run-参数">app.run 参数</h2>
<p>可以指定运行的主机IP地址，端口，是否开启调试模式。<br />
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">app.run(host=<span class="string">&quot;0.0.0.0&quot;</span>, port=<span class="number">5000</span>, debug = <span class="literal">True</span>)</span><br></pre></td></tr></table></figure><br />
关于DEBUG调试模式：<br />
（1）程序代码修改后可以自动重启服务器。<br />
（2）在服务器出现相关错误的时候可以直接将错误信息返回到前端进行展示。</p>
<p>在1.0版本之后，Flask调整了开发服务器的启动方式，由代码编写<code>app.run()</code>语句调整为命令<code>flask run</code>启动。<br />
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> flask <span class="keyword">import</span> Flask</span><br><span class="line"></span><br><span class="line">app = Flask(__name__)</span><br><span class="line"></span><br><span class="line"><span class="meta">@app.route(<span class="params"><span class="string">&#x27;/&#x27;</span></span>)</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">index</span>():</span><br><span class="line">    <span class="keyword">return</span> <span class="string">&#x27;Hello World&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 程序中不用再写app.run()</span></span><br></pre></td></tr></table></figure><br />
终端启动：<br />
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash"><span class="built_in">export</span> FLASK_APP=helloworld</span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">flask run</span></span><br><span class="line"> * Running on http://127.0.0.1:5000/</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">flask run 等价于 python -m flask run</span></span><br></pre></td></tr></table></figure><br />
说明：<br />
（1）环境变量 <code>FLASK_APP</code> 指明flask的启动实例。<br />
（2）<code>flask run -h 0.0.0.0 -p 8000</code> 绑定地址 端口。<br />
（3）<code>flask run --help</code>获取帮助。<br />
（4）生产模式与开发模式的控制。通过<code>FLASK_ENV</code>环境变量指明：<br />
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">export FLASK_ENV=production # 运行在生产模式，未指明则默认为此方式</span><br><span class="line">export FLASK_ENV=development #  运行在开发模式</span><br></pre></td></tr></table></figure></p>
<h1 id="路由">路由</h1>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@app.route(<span class="params"><span class="string">&quot;/itcast&quot;</span></span>)</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">view_func</span>():</span><br><span class="line">    <span class="keyword">return</span> <span class="string">&quot;hello world&quot;</span></span><br></pre></td></tr></table></figure>
<h2 id="查询路由信息">查询路由信息</h2>
<p>1、命令行方式<br />
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">$ flask routes</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">Endpoint  Methods  Rule</span></span><br><span class="line"><span class="string">--------  -------  -----------------------</span></span><br><span class="line"><span class="string">index     GET      /</span></span><br><span class="line"><span class="string">static    GET      /static/</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><br />
2、在程序中获取<br />
在应用中的<code>url_map</code>属性中保存着整个Flask应用的路由映射信息，可以通过读取这个属性获取路由信息。<br />
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(app.url_map)</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">Map([&lt;Rule &#x27;/&#x27; (GET, HEAD, OPTIONS) -&gt; hello_world&gt;,</span></span><br><span class="line"><span class="string"> &lt;Rule &#x27;/static/&lt;filename&gt;&#x27; (GET, HEAD, OPTIONS) -&gt; static&gt;])</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure><br />
如果想在程序中遍历路由信息，可以采用如下方式：<br />
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># rule为Rule对象，其属性endpoint为路由名字，rule为路由路径。</span></span><br><span class="line"><span class="keyword">for</span> rule <span class="keyword">in</span> app.url_map.iter_rules():</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;name=&#123;&#125; path=&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(rule.endpoint, rule.rule))</span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">name=hello_world path=/</span></span><br><span class="line"><span class="string">name=static path=/static/&lt;path:filename&gt;</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure><br />
实例：<br />
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 通过访问/地址，以json的方式返回应用内的所有路由信息</span></span><br><span class="line"><span class="meta">@app.route(<span class="params"><span class="string">&#x27;/&#x27;</span></span>)</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">route_map</span>():</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    主视图，返回所有视图网址</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    rules_iterator = app.url_map.iter_rules()</span><br><span class="line">    <span class="keyword">return</span> json.dumps(&#123;rule.endpoint: rule.rule <span class="keyword">for</span> rule <span class="keyword">in</span> rules_iterator&#125;)</span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">&#123;&quot;route_map&quot;: &quot;/rule&quot;, &quot;hello_world&quot;: &quot;/&quot;, &quot;static&quot;: &quot;/static/&quot;&#125;</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure></p>
<h2 id="指定请求方式">指定请求方式</h2>
<p>在 Flask 中，定义路由其默认的请求方式为：<br />
（1）<code>GET</code><br />
（2）<code>OPTIONS</code>
(自带)：简化版的GET请求，用于询问服务器接口信息的。比如，接口允许的请求方式、允许的请求源头域名（cors跨域时会出现）。<br />
（3）<code>HEAD</code>
(自带)：简化版的GET请求，只返回GET请求处理时的响应头，不返回响应体。<br />
利用<code>methods</code>参数可以自己指定一个接口的请求方式。<br />
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@app.route(<span class="params"><span class="string">&quot;/itcast1&quot;</span>, methods=[<span class="string">&quot;POST&quot;</span>]</span>)  </span><span class="comment"># 只支持POST请求</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">view_func_1</span>():</span><br><span class="line">    <span class="keyword">return</span> <span class="string">&quot;hello world 1&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="meta">@app.route(<span class="params"><span class="string">&quot;/itcast2&quot;</span>, methods=[<span class="string">&quot;GET&quot;</span>, <span class="string">&quot;POST&quot;</span>]</span>)  </span><span class="comment"># 支持GET、POST请求</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">view_func_2</span>():</span><br><span class="line">    <span class="keyword">return</span> <span class="string">&quot;hello world 2&quot;</span></span><br></pre></td></tr></table></figure></p>
<h1 id="蓝图">蓝图</h1>
<p>在一个Flask
应用项目中，如果业务视图过多，可否将以某种方式划分出的业务单元单独维护，将每个单元用到的视图、静态文件、模板文件等独立分开？<br />
例如从业务角度上，可将整个应用划分为用户模块单元、商品模块单元、订单模块单元，如何分别开发这些不同单元，并最终整合到一个项目应用中？<br />
在Django中这种需求是如何实现的？</p>
<p>在Flask中，使用蓝图 Blueprint 来分模块组织管理。<br />
蓝图实际可以理解为是一个存储一组视图方法的容器对象，其具有如下特点：<br />
（1）一个应用可以具有多个Blueprint。<br />
（2）可以将一个Blueprint注册到任何一个未使用的URL下比如
“/user”、“/goods”。<br />
（3）Blueprint可以单独具有自己的模板、静态文件或者其它的通用操作方法，它并不是必须要实现应用的视图和函数。<br />
（4）在一个应用初始化时，就应该要注册需要使用的Blueprint<br />
但是一个Blueprint并不是一个完整的应用，它不能独立于应用运行，而必须要注册到某一个应用中。</p>
<h2 id="蓝图创建">蓝图创建</h2>
<p>1、创建一个蓝图对象<br />
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">user_bp = Blueprint(<span class="string">&#x27;user&#x27;</span>,__name__)</span><br></pre></td></tr></table></figure><br />
2、在这个蓝图对象上进行操作，注册路由，指定静态文件夹，注册模版过滤器<br />
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@user_bp.route(<span class="params"><span class="string">&#x27;/&#x27;</span></span>)</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">user_profile</span>():</span><br><span class="line">    <span class="keyword">return</span> <span class="string">&#x27;user_profile&#x27;</span></span><br></pre></td></tr></table></figure><br />
3、在应用对象上注册这个蓝图对象<br />
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">app.register_blueprint(user_bp)</span><br></pre></td></tr></table></figure><br />
<strong>单文件蓝图</strong><br />
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> flask <span class="keyword">import</span> Flask, Blueprint</span><br><span class="line"></span><br><span class="line">app = Flask(__name__)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建一个蓝图对象</span></span><br><span class="line">user_bp = Blueprint(<span class="string">&#x27;user&#x27;</span>,__name__)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 注册路由</span></span><br><span class="line"><span class="meta">@user_bp.route(<span class="params"><span class="string">&#x27;/&#x27;</span></span>)</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">user_profile</span>():</span><br><span class="line">    <span class="keyword">return</span> <span class="string">&#x27;user_profile&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 应用对象上注册这个蓝图对象</span></span><br><span class="line">app.register_blueprint(user_bp)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    app.run()</span><br></pre></td></tr></table></figure><br />
<strong>目录蓝图</strong><br />
对于一个打算包含多个文件的蓝图，通常将创建蓝图对象放到Python包的<code>__init__.py</code>文件中。<br />
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">--------- project <span class="comment"># 工程目录</span></span><br><span class="line">  |------ main.py <span class="comment"># 启动文件</span></span><br><span class="line">  |------ user  <span class="comment"># 用户蓝图</span></span><br><span class="line">  |  |--- __init__.py  <span class="comment"># 此处创建蓝图对象</span></span><br><span class="line">  |  |--- passport.py  </span><br><span class="line">  |  |--- profile.py</span><br><span class="line">  |  |--- ...</span><br><span class="line">  |</span><br><span class="line">  |------ goods <span class="comment"># 商品蓝图</span></span><br><span class="line">  |  |--- __init__.py  <span class="comment"># 此处创建蓝图对象</span></span><br><span class="line">  |  |--- views.py  <span class="comment"># 此处定义视图</span></span><br><span class="line">  |...</span><br></pre></td></tr></table></figure><br />
以goods为例：<br />
<div class="tabs" id="2"><ul class="nav-tabs"><li class="tab active"><a href="#2-1">__init__.py</a></li><li class="tab"><a href="#2-2">views.py</a></li><li class="tab"><a href="#2-3">main.py</a></li></ul><div class="tab-content"><div class="tab-pane active" id="2-1"><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> flask <span class="keyword">import</span> Flask, Blueprint</span><br><span class="line"></span><br><span class="line">goods_bp = Blueprint(<span class="string">&#x27;goods&#x27;</span>,__name__)</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> . <span class="keyword">import</span> views  <span class="comment"># 导入视图，不然没有视图</span></span><br></pre></td></tr></table></figure></div><div class="tab-pane" id="2-2"><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> . <span class="keyword">import</span> goods_bp</span><br><span class="line"></span><br><span class="line"><span class="meta">@goods_bp.route(<span class="params"><span class="string">&#x27;/goods&#x27;</span></span>)</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_goods</span>():</span><br><span class="line">    <span class="keyword">return</span> <span class="string">&#x27;get_goods&#x27;</span></span><br></pre></td></tr></table></figure></div><div class="tab-pane" id="2-3"><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> flask <span class="keyword">import</span> Flask, Blueprint</span><br><span class="line"></span><br><span class="line">app = Flask(__name__)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 主程序注册即可</span></span><br><span class="line"><span class="keyword">from</span> goods <span class="keyword">import</span> goods_bp</span><br><span class="line">app.register_blueprint(goods_bp)</span><br></pre></td></tr></table></figure></div></div></div></p>
<h2 id="扩展用法">扩展用法</h2>
<p>1、指定蓝图的url前缀：在应用中注册蓝图时使用url_prefix参数指定<br />
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">app.register_blueprint(user_bp, url_prefix=<span class="string">&#x27;/user&#x27;</span>)</span><br><span class="line">app.register_blueprint(goods_bp, url_prefix=<span class="string">&#x27;/goods&#x27;</span>)</span><br></pre></td></tr></table></figure><br />
2、蓝图内部静态文件<br />
和应用对象不同，蓝图对象创建时不会默认注册静态目录的路由。需要我们在
创建时指定 <code>static_folder</code> 参数。<br />
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 蓝图所在目录下的static_admin目录设置为静态目录</span></span><br><span class="line">admin = Blueprint(<span class="string">&quot;admin&quot;</span>,__name__,static_folder=<span class="string">&#x27;static_admin&#x27;</span>)</span><br><span class="line">app.register_blueprint(admin, url_prefix=<span class="string">&#x27;/admin&#x27;</span>)</span><br><span class="line"><span class="comment"># 注册完，可使用 /admin/static_admin/&lt;filename&gt; 访问static_admin目录下的静态文件</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 也可通过static_url_path改变访问路径</span></span><br><span class="line">admin = Blueprint(<span class="string">&quot;admin&quot;</span>,__name__,static_folder=<span class="string">&#x27;static_admin&#x27;</span>,static_url_path=<span class="string">&#x27;/lib&#x27;</span>)</span><br><span class="line">app.register_blueprint(admin,url_prefix=<span class="string">&#x27;/admin&#x27;</span>)</span><br></pre></td></tr></table></figure><br />
3、蓝图内部模板目录<br />
蓝图对象默认的模板目录为系统的模版目录，可以在创建蓝图对象时使用
<code>template_folder</code> 关键字参数设置模板目录。<br />
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">admin = Blueprint(<span class="string">&#x27;admin&#x27;</span>,__name__,template_folder=<span class="string">&#x27;my_templates&#x27;</span>)</span><br></pre></td></tr></table></figure></p>
<h1 id="请求">请求</h1>
<p>在视图编写中需要读取客户端请求携带的数据时，如何才能正确的取出数据呢？</p>
<p>请求携带的数据可能出现在HTTP报文中的不同位置，需要使用不同的方法来获取参数。</p>
<h2 id="url路径参数动态路由">URL路径参数（动态路由）</h2>
<p>例如，有一个请求访问的接口地址为/users/123，其中123实际上为具体的请求参数，表明请求123号用户的信息。此时如何从url中提取出123的数据？</p>
<p>Flask不同于Django直接在定义路由时编写正则表达式的方式，而是采用转换器语法：<br />
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@app.route(<span class="params"><span class="string">&#x27;/users/&lt;user_id&gt;&#x27;</span></span>)  </span><span class="comment"># 提取路径中user_id参数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">user_info</span>(<span class="params">user_id</span>):</span><br><span class="line">    <span class="built_in">print</span>(<span class="built_in">type</span>(user_id))</span><br><span class="line">    <span class="keyword">return</span> <span class="string">&#x27;hello user &#123;&#125;&#x27;</span>.<span class="built_in">format</span>(user_id)</span><br></pre></td></tr></table></figure><br />
此处的<code>&lt;&gt;</code>即是一个转换器，<strong>默认为字符串类型</strong>，即将该位置的数据以字符串格式进行匹配、并以字符串为数据类型类型、
user_id为参数名传入视图。</p>
<p>Flask也提供其他数据类型的转换器：<br />
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">DEFAULT_CONVERTERS = &#123;</span><br><span class="line">    <span class="string">&#x27;default&#x27;</span>:          UnicodeConverter,</span><br><span class="line">    <span class="string">&#x27;string&#x27;</span>:           UnicodeConverter,</span><br><span class="line">    <span class="string">&#x27;any&#x27;</span>:              AnyConverter,</span><br><span class="line">    <span class="string">&#x27;path&#x27;</span>:             PathConverter,</span><br><span class="line">    <span class="string">&#x27;int&#x27;</span>:              IntegerConverter,</span><br><span class="line">    <span class="string">&#x27;float&#x27;</span>:            FloatConverter,</span><br><span class="line">    <span class="string">&#x27;uuid&#x27;</span>:             UUIDConverter,</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><br />
比如，以整型匹配数据：<br />
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@app.route(<span class="params"><span class="string">&#x27;/users/&lt;int:user_id&gt;&#x27;</span></span>)  </span><span class="comment"># 整型转换</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">user_info</span>(<span class="params">user_id</span>):</span><br><span class="line">    <span class="built_in">print</span>(<span class="built_in">type</span>(user_id))</span><br><span class="line">    <span class="keyword">return</span> <span class="string">&#x27;hello user &#123;&#125;&#x27;</span>.<span class="built_in">format</span>(user_id)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">@app.route(<span class="params"><span class="string">&#x27;/users/&lt;int(min=1):user_id&gt;&#x27;</span></span>)  </span><span class="comment"># 整型转换、限制最小值</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">user_info</span>(<span class="params">user_id</span>):</span><br><span class="line">    <span class="built_in">print</span>(<span class="built_in">type</span>(user_id))</span><br><span class="line">    <span class="keyword">return</span> <span class="string">&#x27;hello user &#123;&#125;&#x27;</span>.<span class="built_in">format</span>(user_id)</span><br></pre></td></tr></table></figure></p>
<h2 id="自定义转换器">自定义转换器</h2>
<p>如果遇到需要匹配提取/sms_codes/15645678901
中的手机号数据，Flask内置的转换器就无法满足需求，此时需要自定义转换器。</p>
<p>1、创建转换器类，保存匹配时的正则表达式。<br />
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> werkzeug.routing <span class="keyword">import</span> BaseConverter</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MobileConverter</span>(<span class="title class_ inherited__">BaseConverter</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    手机号格式</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    regex = <span class="string">r&#x27;1[3-9]\d&#123;9&#125;&#x27;</span></span><br></pre></td></tr></table></figure><br />
2、将自定义的转换器告知Flask应用。相当于注册自定义类型到converters字典中，converters保存了所有类型。<br />
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">app = Flask(__name__)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将自定义转换器添加到转换器字典中，并指定转换器使用时名字为: mobile</span></span><br><span class="line">app.url_map.converters[<span class="string">&#x27;mobile&#x27;</span>] = MobileConverter</span><br></pre></td></tr></table></figure><br />
3、在使用转换器的地方定义使用。<br />
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@app.route(<span class="params"><span class="string">&#x27;/sms_codes/&lt;mobile:mob_num&gt;&#x27;</span></span>)  </span><span class="comment"># mobile为自定义类型</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">send_sms_code</span>(<span class="params">mob_num</span>):</span><br><span class="line">    <span class="keyword">return</span> <span class="string">&#x27;send sms code to &#123;&#125;&#x27;</span>.<span class="built_in">format</span>(mob_num)</span><br></pre></td></tr></table></figure></p>
<h2 id="其他参数">其他参数</h2>
<p>如果想要获取其他地方传递的参数，可以通过Flask提供的request对象来读取。</p>
<p>不同位置的参数都存放在request的不同属性中：<br />
<figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">属性	说明	                        类型</span><br><span class="line">data	记录请求的数据，并转换为字符串      *</span><br><span class="line">form	记录请求中的表单数据	        MultiDict</span><br><span class="line">args	记录请求中的查询参数	        MultiDict</span><br><span class="line">cookies	记录请求中的cookie信息	        Dict</span><br><span class="line">headers	记录请求中的报文头              EnvironHeaders</span><br><span class="line">method	记录请求使用的HTTP方法	        GET/POST</span><br><span class="line">url     记录请求的URL地址               string</span><br><span class="line">files	记录请求上传的文件                 *</span><br></pre></td></tr></table></figure><br />
data 对应请求中 body 的原始数据。<br />
form 对应请求中 body 的form数据。<br />
args 对应请求中 path 后面的查询参数。<br />
files 对应请求中 body 的files数据。</p>
<p>例如
想要获取请求/articles?channel_id=1中channel_id的参数，可以按如下方式使用：<br />
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> flask <span class="keyword">import</span> request</span><br><span class="line"></span><br><span class="line"><span class="meta">@app.route(<span class="params"><span class="string">&#x27;/articles&#x27;</span></span>)</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_articles</span>():</span><br><span class="line">    channel_id = request.args.get(<span class="string">&#x27;channel_id&#x27;</span>)</span><br><span class="line">    <span class="keyword">return</span> <span class="string">&#x27;you wanna get articles of channel &#123;&#125;&#x27;</span>.<span class="built_in">format</span>(channel_id)</span><br></pre></td></tr></table></figure><br />
例如，客户端上传图片到服务器，并保存到服务器中：<br />
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> flask <span class="keyword">import</span> request</span><br><span class="line"></span><br><span class="line"><span class="meta">@app.route(<span class="params"><span class="string">&#x27;/upload&#x27;</span>, methods=[<span class="string">&#x27;POST&#x27;</span>]</span>)</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">upload_file</span>():</span><br><span class="line">    f = request.files[<span class="string">&#x27;pic&#x27;</span>]  <span class="comment"># pic为上传文件的字段名</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 1、python方式保存文件</span></span><br><span class="line">    <span class="comment"># with open(&#x27;./demo.png&#x27;, &#x27;wb&#x27;) as new_file:</span></span><br><span class="line">    <span class="comment">#     new_file.write(f.read())</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 2、自带的保存方式</span></span><br><span class="line">    f.save(<span class="string">&#x27;./demo.png&#x27;</span>)</span><br><span class="line">    <span class="keyword">return</span> <span class="string">&#x27;ok&#x27;</span></span><br></pre></td></tr></table></figure></p>
<h1 id="响应">响应</h1>
<p>如何在不同的场景里返回不同的响应信息？</p>
<h2 id="返回模板">返回模板</h2>
<p>使用render_template方法渲染模板并返回。</p>
<p>例如，新建一个模板index.html：<br />
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&lt;!DOCTYPE <span class="keyword">html</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">html</span> <span class="attr">lang</span>=<span class="string">&quot;en&quot;</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">head</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">meta</span> <span class="attr">charset</span>=<span class="string">&quot;UTF-8&quot;</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">title</span>&gt;</span>Title<span class="tag">&lt;/<span class="name">title</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">head</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">body</span>&gt;</span></span><br><span class="line">我的模板html内容</span><br><span class="line"><span class="tag">&lt;<span class="name">br</span>/&gt;</span>&#123;&#123; my_str &#125;&#125;</span><br><span class="line"><span class="tag">&lt;<span class="name">br</span>/&gt;</span>&#123;&#123; my_int &#125;&#125;</span><br><span class="line"><span class="tag">&lt;/<span class="name">body</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">html</span>&gt;</span></span><br></pre></td></tr></table></figure><br />
后端视图：<br />
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> flask <span class="keyword">import</span> render_template</span><br><span class="line"></span><br><span class="line"><span class="comment"># 返回html</span></span><br><span class="line"><span class="meta">@app.route(<span class="params"><span class="string">&#x27;/&#x27;</span></span>)</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">index</span>():</span><br><span class="line">    mstr = <span class="string">&#x27;Hello World&#x27;</span></span><br><span class="line">    mint = <span class="number">10</span></span><br><span class="line">    <span class="comment"># html模板中的变量名作为参数名，前面定义的变量mstr、mint赋值给这些参数。</span></span><br><span class="line">    <span class="keyword">return</span> render_template(<span class="string">&#x27;index.html&#x27;</span>, my_str=mstr, my_int=mint)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 返回dict格式 </span></span><br><span class="line"><span class="meta">@app.route(<span class="params"><span class="string">&#x27;/&#x27;</span></span>)</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">index</span>():</span><br><span class="line">    <span class="comment"># 注意，参数名要和html一致</span></span><br><span class="line">    data = <span class="built_in">dict</span>(</span><br><span class="line">        my_str = <span class="string">&#x27;Hello World&#x27;</span>  </span><br><span class="line">        my_int = <span class="number">10</span></span><br><span class="line">    )</span><br><span class="line">    <span class="comment"># 解包</span></span><br><span class="line">    <span class="keyword">return</span> render_template(<span class="string">&#x27;index.html&#x27;</span>, **data)</span><br></pre></td></tr></table></figure></p>
<h2 id="重定向">重定向</h2>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> flask <span class="keyword">import</span> redirect</span><br><span class="line"></span><br><span class="line"><span class="meta">@app.route(<span class="params"><span class="string">&#x27;/demo2&#x27;</span></span>)</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">demo2</span>():</span><br><span class="line">    <span class="keyword">return</span> redirect(<span class="string">&#x27;http://www.baidu.com&#x27;</span>)</span><br></pre></td></tr></table></figure>
<h2 id="返回json">返回JSON</h2>
<p><code>jsonify</code>：转换成json格式，设置了响应头<code>Content-Type:application/json</code>。<br />
<code>json.dumps</code>：只会转换成json格式，无设置响应头信息。<br />
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> flask <span class="keyword">import</span> jsonify</span><br><span class="line"></span><br><span class="line"><span class="meta">@app.route(<span class="params"><span class="string">&#x27;/demo3&#x27;</span></span>)</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">demo3</span>():</span><br><span class="line">    json_dict = &#123;</span><br><span class="line">        <span class="string">&quot;user_id&quot;</span>: <span class="number">10</span>,</span><br><span class="line">        <span class="string">&quot;user_name&quot;</span>: <span class="string">&quot;laowang&quot;</span></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> jsonify(json_dict)</span><br></pre></td></tr></table></figure></p>
<h2 id="自定义状态码和响应头">自定义状态码和响应头</h2>
<p>1、元祖方式<br />
可以返回一个元组，这样的元组必须是
<code>(response, status, headers)</code> 的形式，且至少包含一个元素。
<code>status</code> 值会覆盖状态代码， <code>headers</code>
可以是一个列表或字典，作为额外的消息标头值。<br />
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@app.route(<span class="params"><span class="string">&#x27;/demo4&#x27;</span></span>)</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">demo4</span>():</span><br><span class="line">    <span class="comment"># return &#x27;状态码为 666&#x27;, 666</span></span><br><span class="line">    <span class="comment"># return &#x27;状态码为 666&#x27;, 666, [(&#x27;Itcast&#x27;, &#x27;Python&#x27;)]</span></span><br><span class="line">    <span class="keyword">return</span> <span class="string">&#x27;状态码为 666&#x27;</span>, <span class="number">666</span>, &#123;<span class="string">&#x27;Itcast&#x27;</span>: <span class="string">&#x27;Python&#x27;</span>&#125;</span><br></pre></td></tr></table></figure><br />
2、make_response方式<br />
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@app.route(<span class="params"><span class="string">&#x27;/demo5&#x27;</span></span>)</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">demo5</span>():</span><br><span class="line">    resp = make_response(<span class="string">&#x27;make response测试&#x27;</span>)  <span class="comment"># 这里是body</span></span><br><span class="line">        resp.headers[“Itcast”] = “Python”      <span class="comment"># 这里是响应头</span></span><br><span class="line">        resp.status = “<span class="number">404</span> <span class="keyword">not</span> found”          <span class="comment"># 这里是状态，一定是完整的状态(状态码+描述信息)！</span></span><br><span class="line">    <span class="keyword">return</span> resp</span><br></pre></td></tr></table></figure></p>
<h2 id="cookie">Cookie</h2>
<p>1、设置<br />
cookie只能在响应对象中设置，所以要先拿到响应体，所以只能用make_response方法构造响应。<br />
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> flask <span class="keyword">import</span> Flask, make_response</span><br><span class="line"></span><br><span class="line">app = Flask(__name__)</span><br><span class="line"></span><br><span class="line"><span class="meta">@app.route(<span class="params"><span class="string">&#x27;/cookie&#x27;</span></span>)</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">set_cookie</span>():</span><br><span class="line">    resp = make_response(<span class="string">&#x27;set cookie ok&#x27;</span>)  <span class="comment"># 先拿到响应体</span></span><br><span class="line">    resp.set_cookie(<span class="string">&#x27;username&#x27;</span>, <span class="string">&#x27;itcast&#x27;</span>)  <span class="comment"># 再设置cookie，默认有效期为窗口关闭</span></span><br><span class="line">    <span class="keyword">return</span> resp</span><br></pre></td></tr></table></figure><br />
设置有效期：<br />
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@app.route(<span class="params"><span class="string">&#x27;/cookie&#x27;</span></span>)</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">set_cookie</span>():</span><br><span class="line">    response = make_response(<span class="string">&#x27;hello world&#x27;</span>)</span><br><span class="line">    response.set_cookie(<span class="string">&#x27;username&#x27;</span>, <span class="string">&#x27;itheima&#x27;</span>, max_age=<span class="number">3600</span>)</span><br><span class="line">    <span class="keyword">return</span> response</span><br></pre></td></tr></table></figure><br />
2、读取<br />
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> flask <span class="keyword">import</span> request</span><br><span class="line"></span><br><span class="line"><span class="meta">@app.route(<span class="params"><span class="string">&#x27;/get_cookie&#x27;</span></span>)</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_cookie</span>():</span><br><span class="line">    resp = request.cookies.get(<span class="string">&#x27;username&#x27;</span>)</span><br><span class="line">    <span class="keyword">return</span> resp</span><br></pre></td></tr></table></figure><br />
3、删除<br />
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> flask <span class="keyword">import</span> request</span><br><span class="line"></span><br><span class="line"><span class="meta">@app.route(<span class="params"><span class="string">&#x27;/delete_cookie&#x27;</span></span>)</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">delete_cookie</span>():</span><br><span class="line">    response = make_response(<span class="string">&#x27;hello world&#x27;</span>)</span><br><span class="line">    response.delete_cookie(<span class="string">&#x27;username&#x27;</span>)  <span class="comment"># 不是真删除，是设置过早的有效期达到结束目的</span></span><br><span class="line">    <span class="keyword">return</span> response</span><br></pre></td></tr></table></figure></p>
<h2 id="session">Session</h2>
<p>Django把session数据存储到redis中。<br />
flask将session数据保存到了哪里？放到了浏览器缓存中。<br />
为了防止用户修改这个session，那么需要设置签名，这就需要SECRET_KEY了。</p>
<p>需要先设置SECRET_KEY：<br />
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">DefaultConfig</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">    SECRET_KEY = <span class="string">&#x27;fih9fh9eh9gh2&#x27;</span></span><br><span class="line"></span><br><span class="line">app.config.from_object(DefaultConfig)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 或者直接设置</span></span><br><span class="line">app.secret_key=<span class="string">&#x27;xihwidfw9efw&#x27;</span></span><br></pre></td></tr></table></figure><br />
1、设置<br />
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> flask <span class="keyword">import</span> session</span><br><span class="line"></span><br><span class="line"><span class="meta">@app.route(<span class="params"><span class="string">&#x27;/set_session&#x27;</span></span>)</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">set_session</span>():</span><br><span class="line">    session[<span class="string">&#x27;username&#x27;</span>] = <span class="string">&#x27;itcast&#x27;</span></span><br><span class="line">    <span class="keyword">return</span> <span class="string">&#x27;set session ok&#x27;</span></span><br></pre></td></tr></table></figure><br />
2、读取<br />
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@app.route(<span class="params"><span class="string">&#x27;/get_session&#x27;</span></span>)</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_session</span>():</span><br><span class="line">    username = session.get(<span class="string">&#x27;username&#x27;</span>)</span><br><span class="line">    <span class="keyword">return</span> <span class="string">&#x27;get session username &#123;&#125;&#x27;</span>.<span class="built_in">format</span>(username)</span><br></pre></td></tr></table></figure></p>
<h1 id="参考文献">参考文献</h1>
<p><span class="exturl" data-url="aHR0cHM6Ly9mbGFzay1yZXN0ZnVsLnJlYWR0aGVkb2NzLmlvL2VuL2xhdGVzdC9xdWlja3N0YXJ0Lmh0bWw=">Flask-RESTful<i class="fa fa-external-link-alt"></i></span><br />
<span class="exturl" data-url="aHR0cDovL3d3dy5weXRob25kb2MuY29tL0ZsYXNrLVJFU1RmdWwvcXVpY2tzdGFydC5odG1s">Flask-RESTful中文<i class="fa fa-external-link-alt"></i></span><br />
<span class="exturl" data-url="aHR0cDovL3d3dy5iamhlZS5jb20vZmxhc2stMS5odG1s">Flask入门系列(一)–Hello
World<i class="fa fa-external-link-alt"></i></span><br />
<span class="exturl" data-url="aHR0cDovL3d3dy5iamhlZS5jb20vZmxhc2stYWQxLmh0bWw=">Flask进阶系列(一)–上下文环境<i class="fa fa-external-link-alt"></i></span><br />
<span class="exturl" data-url="aHR0cHM6Ly9mbGFzay5wYWxsZXRzcHJvamVjdHMuY29tL2VuLzIuMi54Lw==">Flask
Documentation (2.2.x)<i class="fa fa-external-link-alt"></i></span><br />
<span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL3BhbGxldHMvZmxhc2s=">pallets/flask<i class="fa fa-external-link-alt"></i></span></p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://soundmemories.github.io/2021/07/18/Paper/03.Get%20To%20The%20Point.%20Summarization%20with%20Pointer-Generator%20Networks/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avstar.png">
      <meta itemprop="name" content="SoundMemories">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="SoundMemories">
      <meta itemprop="description" content="今日事，今日毕">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | SoundMemories">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2021/07/18/Paper/03.Get%20To%20The%20Point.%20Summarization%20with%20Pointer-Generator%20Networks/" class="post-title-link" itemprop="url">Get To The Point. Summarization with Pointer-Generator Networks</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2021-07-18 00:00:00" itemprop="dateCreated datePublished" datetime="2021-07-18T00:00:00+08:00">2021-07-18</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Paper/" itemprop="url" rel="index"><span itemprop="name">Paper</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>1.2k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>4 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="摘要">摘要</h1>
<p>把seq2seq模型应用于摘要生成时存在两个主要的问题：<br />
（1）难以准确复述原文的事实细节、无法处理原文中的未登录词(OOV)；<br />
（2）生成的摘要中存在重复的片段。</p>
<p>针对存在的问题提出对应解决之道，在标准seq2seq+attention的基础之上做了改进：<br />
（1）采用Pointer-Generator
Network（抽取+生成），在保留生成新词的同时，还可以从原文中抽取内容，促使生成更准确的摘要。<br />
（2）覆盖率机制(coverage
mechanism)，使用Coverage记录已经生成的内容，从而减少内容重复。</p>
<p><img src="/images/PGN/结构.png" width="90%"></p>
<h1 id="基础模型">基础模型</h1>
<p>标准的seq2seq模型+Attention机制：<br />
encoder使用Bi-LSTM，序列单词按顺序喂给encoder中，产生一系列编码器的隐状态
<span class="math inline">\(h_i\)</span>。decoder使用LSTM，在每个时间步
<span class="math inline">\(t\)</span>，接收前一个词（训练时采用teacher
forcing方式，预测时是前一时刻decoder输出词），decoder隐状态为 <span
class="math inline">\(s_t\)</span>。Attention计算采用Bahdanau方式，但这里采用decoder的当前时刻
<span class="math inline">\(t\)</span> 的隐状态 <span
class="math inline">\(s_t\)</span> 作为 query 计算attention，得到context
vector 和
当前时刻的隐状态拼接后作为全连接层输入。也可使用context上一时刻隐状态作为query，但得到context
vector要与输入拼接作为decoder输入(Bahdanau Attention)。</p>
<p><img src="/images/PGN/1+2.png" width="40%"></p>
<p>编码器隐藏状态的加权和，称为上下文向量（context vector） <span
class="math inline">\(h_t^*\)</span>。<br />
<img src="/images/PGN/3.png" width="30%"></p>
<p>上下文向量可以被看作是在当前时间步 <span
class="math inline">\(t\)</span> 看encoder的哪些部分，与解码器隐状态
<span class="math inline">\(s_t\)</span>
拼接，通过两个线性层产生词汇分布 <span
class="math inline">\(P_{\text{vocab}}\)</span>。<br />
<img src="/images/PGN/4.png" width="40%"></p>
<p><span class="math inline">\(P_{\text{vocab}}\)</span>
是词汇表中所有单词的概率分布，它提供了预测单词 <span
class="math inline">\(w\)</span> 的最终分布。<br />
<img src="/images/PGN/5.png" width="30%"></p>
<p>在训练过程中，时间步长 <span class="math inline">\(t\)</span>
的损失是 <span class="math inline">\(t\)</span> 的目标词 <span
class="math inline">\(w_t^*\)</span> 的负对数似然。<br />
<img src="/images/PGN/6.png" width="30%"></p>
<p>整个序列的损失是：<br />
<img src="/images/PGN/7.png" width="30%"></p>
<h1 id="pointer-generator-network">Pointer-Generator Network</h1>
<p>通过Pointer-Generator可以从输入中复制单词，需要一个软开关 <span
class="math inline">\(P_{\text{gen}}\)</span> 来融合baseline 和
Pointer-Generator。<br />
根据上下文向量 <span class="math inline">\(h_t^*\)</span>，解码器隐状态
<span class="math inline">\(s_t\)</span>，解码器输入 <span
class="math inline">\(x_t\)</span>，计算得到时间步 <span
class="math inline">\(t\)</span> 的生成概率 <span
class="math inline">\(P_{\text{gen}}\in [0,1]\)</span>。<br />
<img src="/images/PGN/8.png" width="40%"></p>
<p>使用软开关 <span class="math inline">\(P_{\text{gen}}\)</span>
来融合baseline 和 Pointer-Generator。需要维护一个扩展词表（extended
vocabulary）表示原始词表和出现在source中的所有词汇的联合，在扩展词表上得到以下概率分布，<span
class="math inline">\(a_i^t\)</span> 表示通过attention
weight来确定从source中拷贝词的概率分布。因为attention
weight包括了所有source中单词出现的概率，即使是原始词表中oov的词(该词在src_oov中能找到)，也能通过attention
weight从source扩展词表src_oov中抽取复制。<br />
<img src="/images/PGN/9.png" width="40%"><br />
注意，如果单词 <span class="math inline">\(w\)</span>
是原始词表OOV单词，那么 <span
class="math inline">\(P_{\text{vocab}}\)</span>
一定为零，要是这个词也没出现在source中(即也没在扩展词表)，那么 <span
class="math inline">\(a_i^t\)</span> 也是零。</p>
<h1 id="coverage-mechanism">Coverage mechanism</h1>
<p>重复序列是一个常见问题，本文维护一个覆盖向量（coverage vector） <span
class="math inline">\(c^t\)</span>，它是所有先前decoder时间步的Attention值
<span class="math inline">\(a_i^t\)</span> 的累加和：<br />
<img src="/images/PGN/10.png" width="30%"><br />
目的是用先前的注意力权重决策来影响当前注意力权重的决策（累加的某个值大代表一直关注它，要减少这个值，不让模型总关注它），这样就避免在同一位置重复，从而避免重复生成文本。</p>
<p><span class="math inline">\(c^t\)</span>
是source中单词的分布，表示这些单词从注意力机制到目前时间步的覆盖程度。
<span class="math inline">\(c^0\)</span>
是一个零向量。覆盖向量被引入到注意力机制中：<br />
<img src="/images/PGN/11.png" width="40%"><br />
作者认为有必要额外定义覆盖损失（coverage
loss）处罚重复出现的参考位置，使注意力更分散，即对过往时刻或当前时刻受到注意力较多的单词进行惩罚：<br />
<img src="/images/PGN/12.png" width="35%"><br />
最后融合为复合损失函数：<br />
<img src="/images/PGN/13.png" width="45%"></p>
<h1 id="总结">总结</h1>
<p>根据评价指标来看，提升效果明显，普遍提高2个百分点。</p>
<p>作者还说了直接加coverage效果并不好，所以实际应用时采用fine-tuning训练。</p>
<p>实际使用时建议增加weight_tying和scheduled_sampling尝试效果。</p>
<h1 id="参考文献">参考文献</h1>
<p><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzE3MDQuMDQzNjgucGRm">Get To The Point:
Summarization with Pointer-Generator Networks<i class="fa fa-external-link-alt"></i></span><br />
<span class="exturl" data-url="aHR0cHM6Ly96aHVhbmxhbi56aGlodS5jb20vcC8zMjMwMDg0Ng==">Get To The Point:
Summarization with Pointer-Generator Networks 译文<i class="fa fa-external-link-alt"></i></span></p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://soundmemories.github.io/2021/07/16/NLP/03.%E5%9F%BA%E4%BA%8ELSTM%E7%9A%84%E6%9C%BA%E5%99%A8%E7%BF%BB%E8%AF%91/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avstar.png">
      <meta itemprop="name" content="SoundMemories">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="SoundMemories">
      <meta itemprop="description" content="今日事，今日毕">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | SoundMemories">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2021/07/16/NLP/03.%E5%9F%BA%E4%BA%8ELSTM%E7%9A%84%E6%9C%BA%E5%99%A8%E7%BF%BB%E8%AF%91/" class="post-title-link" itemprop="url">基于LSTM的机器翻译</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2021-07-16 00:00:00" itemprop="dateCreated datePublished" datetime="2021-07-16T00:00:00+08:00">2021-07-16</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/NLP/" itemprop="url" rel="index"><span itemprop="name">NLP</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>1.5k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>5 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <figure class="highlight python"><figcaption><span>utils.py</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">import</span> spacy</span><br><span class="line"><span class="keyword">from</span> torchtext.data <span class="keyword">import</span> Field, BucketIterator</span><br><span class="line"><span class="keyword">from</span> torchtext.datasets <span class="keyword">import</span> Multi30k</span><br><span class="line"></span><br><span class="line"><span class="comment"># manual create date ( token 2 index , index to token)</span></span><br><span class="line"><span class="comment"># dataset dataloader   PADDING BATCH SHUFFLE</span></span><br><span class="line"><span class="comment"># torchtext</span></span><br><span class="line"><span class="comment"># ALLENNLP (Field)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># [&quot;&lt;sos&gt;&quot; 3 ,&quot;word&quot;1 ,&quot;peace&quot; 2,&quot;&lt;eos&gt;&quot; 4 ]</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">load_dataset</span>(<span class="params">batch_size</span>):</span><br><span class="line">    spacy_de = spacy.load(<span class="string">&#x27;de&#x27;</span>)</span><br><span class="line">    spacy_en = spacy.load(<span class="string">&#x27;en&#x27;</span>)</span><br><span class="line">    url = re.<span class="built_in">compile</span>(<span class="string">&#x27;(&lt;url&gt;.*&lt;/url&gt;)&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">tokenize_de</span>(<span class="params">text</span>):</span><br><span class="line">        <span class="keyword">return</span> [tok.text <span class="keyword">for</span> tok <span class="keyword">in</span> spacy_de.tokenizer(url.sub(<span class="string">&#x27;@URL@&#x27;</span>, text))]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">tokenize_en</span>(<span class="params">text</span>):</span><br><span class="line">        <span class="keyword">return</span> [tok.text <span class="keyword">for</span> tok <span class="keyword">in</span> spacy_en.tokenizer(url.sub(<span class="string">&#x27;@URL@&#x27;</span>, text))]</span><br><span class="line"></span><br><span class="line">    DE = Field(tokenize=tokenize_de, include_lengths=<span class="literal">True</span>,</span><br><span class="line">               init_token=<span class="string">&#x27;&lt;sos&gt;&#x27;</span>, eos_token=<span class="string">&#x27;&lt;eos&gt;&#x27;</span>)</span><br><span class="line">    EN = Field(tokenize=tokenize_en, include_lengths=<span class="literal">True</span>,</span><br><span class="line">               init_token=<span class="string">&#x27;&lt;sos&gt;&#x27;</span>, eos_token=<span class="string">&#x27;&lt;eos&gt;&#x27;</span>)</span><br><span class="line">    train, val, test = Multi30k.splits(exts=(<span class="string">&#x27;.de&#x27;</span>, <span class="string">&#x27;.en&#x27;</span>), fields=(DE, EN))</span><br><span class="line">    DE.build_vocab(train.src, min_freq=<span class="number">2</span>)</span><br><span class="line">    EN.build_vocab(train.trg, max_size=<span class="number">10000</span>)</span><br><span class="line">    train_iter, val_iter, test_iter = BucketIterator.splits(</span><br><span class="line">            (train, val, test), batch_size=batch_size, repeat=<span class="literal">False</span>)</span><br><span class="line">    <span class="keyword">return</span> train_iter, val_iter, test_iter, DE, EN</span><br><span class="line"></span><br><span class="line">load_dataset(<span class="number">8</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><figcaption><span>model.py</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Encoder</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, input_size, embed_size, hidden_size,</span></span><br><span class="line"><span class="params">                 n_layers=<span class="number">1</span>, dropout=<span class="number">0.5</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(Encoder, self).__init__()</span><br><span class="line">        self.input_size = input_size</span><br><span class="line">        self.hidden_size = hidden_size</span><br><span class="line">        self.embed_size = embed_size</span><br><span class="line">        self.embed = nn.Embedding(input_size, embed_size)</span><br><span class="line">        self.gru = nn.GRU(embed_size, hidden_size, n_layers,</span><br><span class="line">                          dropout=dropout, bidirectional=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, src, hidden=<span class="literal">None</span></span>):   <span class="comment">#A  ---&gt; B</span></span><br><span class="line">        embedded = self.embed(src)</span><br><span class="line">        outputs, hidden = self.gru(embedded, hidden)</span><br><span class="line">        <span class="comment"># sum bidirectional outputs</span></span><br><span class="line">        outputs = (outputs[:, :, :self.hidden_size] +</span><br><span class="line">                   outputs[:, :, self.hidden_size:])</span><br><span class="line">        <span class="keyword">return</span> outputs, hidden</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Attention</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, hidden_size</span>):</span><br><span class="line">        <span class="built_in">super</span>(Attention, self).__init__()</span><br><span class="line">        self.hidden_size = hidden_size</span><br><span class="line">        self.attn = nn.Linear(self.hidden_size * <span class="number">2</span>, hidden_size)</span><br><span class="line">        self.v = nn.Parameter(torch.rand(hidden_size))</span><br><span class="line">        stdv = <span class="number">1.</span> / math.sqrt(self.v.size(<span class="number">0</span>))</span><br><span class="line">        self.v.data.uniform_(-stdv, stdv)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, hidden, encoder_outputs</span>):</span><br><span class="line">        timestep = encoder_outputs.size(<span class="number">0</span>)</span><br><span class="line">        h = hidden.repeat(timestep, <span class="number">1</span>, <span class="number">1</span>).transpose(<span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line">        encoder_outputs = encoder_outputs.transpose(<span class="number">0</span>, <span class="number">1</span>)  <span class="comment"># [B*T*H]</span></span><br><span class="line">        attn_energies = self.score(h, encoder_outputs)</span><br><span class="line">        <span class="keyword">return</span> F.softmax(attn_energies, dim=<span class="number">1</span>).unsqueeze(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">score</span>(<span class="params">self, hidden, encoder_outputs</span>):</span><br><span class="line">        <span class="comment"># [B*T*2H]-&gt;[B*T*H]</span></span><br><span class="line">        energy = F.relu(self.attn(torch.cat([hidden, encoder_outputs], <span class="number">2</span>)))</span><br><span class="line">        energy = energy.transpose(<span class="number">1</span>, <span class="number">2</span>)  <span class="comment"># [B*H*T]</span></span><br><span class="line">        v = self.v.repeat(encoder_outputs.size(<span class="number">0</span>), <span class="number">1</span>).unsqueeze(<span class="number">1</span>)  <span class="comment"># [B*1*H]</span></span><br><span class="line">        energy = torch.bmm(v, energy)  <span class="comment"># [B*1*T]</span></span><br><span class="line">        <span class="keyword">return</span> energy.squeeze(<span class="number">1</span>)  <span class="comment"># [B*T]</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Decoder</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, embed_size, hidden_size, output_size,</span></span><br><span class="line"><span class="params">                 n_layers=<span class="number">1</span>, dropout=<span class="number">0.2</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(Decoder, self).__init__()</span><br><span class="line">        self.embed_size = embed_size</span><br><span class="line">        self.hidden_size = hidden_size</span><br><span class="line">        self.output_size = output_size</span><br><span class="line">        self.n_layers = n_layers</span><br><span class="line"></span><br><span class="line">        self.embed = nn.Embedding(output_size, embed_size)</span><br><span class="line">        self.dropout = nn.Dropout(dropout, inplace=<span class="literal">True</span>)</span><br><span class="line">        self.attention = Attention(hidden_size)</span><br><span class="line">        self.gru = nn.GRU(hidden_size + embed_size, hidden_size,</span><br><span class="line">                          n_layers, dropout=dropout)</span><br><span class="line">        self.out = nn.Linear(hidden_size * <span class="number">2</span>, output_size)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, <span class="built_in">input</span>, last_hidden, encoder_outputs</span>):</span><br><span class="line">        <span class="comment"># Get the embedding of the current input word (last output word)</span></span><br><span class="line">        embedded = self.embed(<span class="built_in">input</span>).unsqueeze(<span class="number">0</span>)  <span class="comment"># (1,B,N)</span></span><br><span class="line">        embedded = self.dropout(embedded)</span><br><span class="line">        <span class="comment"># Calculate attention weights and apply to encoder outputs</span></span><br><span class="line">        attn_weights = self.attention(last_hidden[-<span class="number">1</span>], encoder_outputs)</span><br><span class="line">        context = attn_weights.bmm(encoder_outputs.transpose(<span class="number">0</span>, <span class="number">1</span>))  <span class="comment"># (B,1,N)</span></span><br><span class="line">        context = context.transpose(<span class="number">0</span>, <span class="number">1</span>)  <span class="comment"># (1,B,N)</span></span><br><span class="line">        <span class="comment"># Combine embedded input word and attended context, run through RNN</span></span><br><span class="line">        rnn_input = torch.cat([embedded, context], <span class="number">2</span>)</span><br><span class="line">        output, hidden = self.gru(rnn_input, last_hidden)</span><br><span class="line">        output = output.squeeze(<span class="number">0</span>)  <span class="comment"># (1,B,N) -&gt; (B,N)</span></span><br><span class="line">        context = context.squeeze(<span class="number">0</span>)</span><br><span class="line">        output = self.out(torch.cat([output, context], <span class="number">1</span>))</span><br><span class="line">        output = F.log_softmax(output, dim=<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> output, hidden, attn_weights</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Seq2Seq</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, encoder, decoder</span>):</span><br><span class="line">        <span class="built_in">super</span>(Seq2Seq, self).__init__()</span><br><span class="line">        self.encoder = encoder</span><br><span class="line">        self.decoder = decoder</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, src, trg, teacher_forcing_ratio=<span class="number">0.5</span></span>):</span><br><span class="line">        batch_size = src.size(<span class="number">1</span>)</span><br><span class="line">        max_len = trg.size(<span class="number">0</span>)</span><br><span class="line">        vocab_size = self.decoder.output_size</span><br><span class="line">        outputs = Variable(torch.zeros(max_len, batch_size, vocab_size))</span><br><span class="line"></span><br><span class="line">        encoder_output, hidden = self.encoder(src)</span><br><span class="line">        hidden = hidden[:self.decoder.n_layers]</span><br><span class="line">        output = Variable(trg.data[<span class="number">0</span>, :])  <span class="comment"># sos   EOS</span></span><br><span class="line">        <span class="keyword">for</span> t <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, max_len):</span><br><span class="line">            output, hidden, attn_weights = self.decoder(</span><br><span class="line">                    output, hidden, encoder_output)</span><br><span class="line">            outputs[t] = output</span><br><span class="line">            is_teacher = random.random() &lt; teacher_forcing_ratio</span><br><span class="line">            top1 = output.data.<span class="built_in">max</span>(<span class="number">1</span>)[<span class="number">1</span>]</span><br><span class="line">            output = Variable(trg.data[t] <span class="keyword">if</span> is_teacher <span class="keyword">else</span> top1)</span><br><span class="line">        <span class="keyword">return</span> outputs</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><figcaption><span>train.py</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> optim</span><br><span class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</span><br><span class="line"><span class="keyword">from</span> torch.nn.utils <span class="keyword">import</span> clip_grad_norm</span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">from</span> model <span class="keyword">import</span> Encoder, Decoder, Seq2Seq</span><br><span class="line"><span class="keyword">from</span> utils <span class="keyword">import</span> load_dataset</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">parse_arguments</span>():</span><br><span class="line">    p = argparse.ArgumentParser(description=<span class="string">&#x27;Hyperparams&#x27;</span>)</span><br><span class="line">    p.add_argument(<span class="string">&#x27;-epochs&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">int</span>, default=<span class="number">100</span>,</span><br><span class="line">                   <span class="built_in">help</span>=<span class="string">&#x27;number of epochs for train&#x27;</span>)</span><br><span class="line">    p.add_argument(<span class="string">&#x27;-batch_size&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">int</span>, default=<span class="number">32</span>,</span><br><span class="line">                   <span class="built_in">help</span>=<span class="string">&#x27;number of epochs for train&#x27;</span>)</span><br><span class="line">    p.add_argument(<span class="string">&#x27;-lr&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">float</span>, default=<span class="number">0.0001</span>,</span><br><span class="line">                   <span class="built_in">help</span>=<span class="string">&#x27;initial learning rate&#x27;</span>)</span><br><span class="line">    p.add_argument(<span class="string">&#x27;-grad_clip&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">float</span>, default=<span class="number">10.0</span>,</span><br><span class="line">                   <span class="built_in">help</span>=<span class="string">&#x27;in case of gradient explosion&#x27;</span>)</span><br><span class="line">    <span class="comment"># p.add_argument(&#x27;-hidden_size&#x27;,type=int,default=10,help=&quot; the size of hidden tensor&quot;)</span></span><br><span class="line">    <span class="keyword">return</span> p.parse_args()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">evaluate</span>(<span class="params">model, val_iter, vocab_size, DE, EN</span>):</span><br><span class="line">    model.<span class="built_in">eval</span>()</span><br><span class="line">    pad = EN.vocab.stoi[<span class="string">&#x27;&lt;pad&gt;&#x27;</span>]</span><br><span class="line">    total_loss = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> b, batch <span class="keyword">in</span> <span class="built_in">enumerate</span>(val_iter):</span><br><span class="line">        src, len_src = batch.src</span><br><span class="line">        trg, len_trg = batch.trg</span><br><span class="line">        src = Variable(src.data, volatile=<span class="literal">True</span>)</span><br><span class="line">        trg = Variable(trg.data, volatile=<span class="literal">True</span>)</span><br><span class="line">        output = model(src, trg, teacher_forcing_ratio=<span class="number">0.0</span>)</span><br><span class="line">        loss = F.nll_loss(output[<span class="number">1</span>:].view(-<span class="number">1</span>, vocab_size),</span><br><span class="line">                               trg[<span class="number">1</span>:].contiguous().view(-<span class="number">1</span>),</span><br><span class="line">                               ignore_index=pad)</span><br><span class="line">        total_loss += loss.data.item()</span><br><span class="line">    <span class="keyword">return</span> total_loss / <span class="built_in">len</span>(val_iter)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">train</span>(<span class="params">e, model, optimizer, train_iter, vocab_size, grad_clip, DE, EN</span>):</span><br><span class="line">    model.train()</span><br><span class="line">    total_loss = <span class="number">0</span></span><br><span class="line">    pad = EN.vocab.stoi[<span class="string">&#x27;&lt;pad&gt;&#x27;</span>]</span><br><span class="line">    <span class="keyword">for</span> b, batch <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_iter):</span><br><span class="line">        src, len_src = batch.src</span><br><span class="line">        trg, len_trg = batch.trg</span><br><span class="line">        src, trg = src, trg</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        output = model(src, trg)</span><br><span class="line">        loss = F.nll_loss(output[<span class="number">1</span>:].view(-<span class="number">1</span>, vocab_size),</span><br><span class="line">                               trg[<span class="number">1</span>:].contiguous().view(-<span class="number">1</span>),</span><br><span class="line">                               ignore_index=pad)</span><br><span class="line"></span><br><span class="line">        loss.backward()</span><br><span class="line">        clip_grad_norm(model.parameters(), grad_clip)</span><br><span class="line">        optimizer.step()</span><br><span class="line">        total_loss += loss.data.item()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> b % <span class="number">100</span> == <span class="number">0</span> <span class="keyword">and</span> b != <span class="number">0</span>:</span><br><span class="line">            total_loss = total_loss / <span class="number">100</span></span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;[%d][loss:%5.2f][pp:%5.2f]&quot;</span> %</span><br><span class="line">                  (b, total_loss, math.exp(total_loss)))</span><br><span class="line">            total_loss = <span class="number">0</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">main</span>():</span><br><span class="line">    args = parse_arguments()</span><br><span class="line">    hidden_size = <span class="number">512</span></span><br><span class="line">    embed_size = <span class="number">256</span></span><br><span class="line">    <span class="comment"># assert torch.cuda.is_available()</span></span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;[!] preparing dataset...&quot;</span>)</span><br><span class="line">    train_iter, val_iter, test_iter, DE, EN = load_dataset(args.batch_size)</span><br><span class="line">    de_size, en_size = <span class="built_in">len</span>(DE.vocab), <span class="built_in">len</span>(EN.vocab)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;[TRAIN]:%d (dataset:%d)\t[TEST]:%d (dataset:%d)&quot;</span></span><br><span class="line">          % (<span class="built_in">len</span>(train_iter), <span class="built_in">len</span>(train_iter.dataset),</span><br><span class="line">             <span class="built_in">len</span>(test_iter), <span class="built_in">len</span>(test_iter.dataset)))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;[DE_vocab]:%d [en_vocab]:%d&quot;</span> % (de_size, en_size))</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;[!] Instantiating models...&quot;</span>)</span><br><span class="line">    encoder = Encoder(de_size, embed_size, hidden_size,</span><br><span class="line">                      n_layers=<span class="number">2</span>, dropout=<span class="number">0.5</span>)</span><br><span class="line">    decoder = Decoder(embed_size, hidden_size, en_size,</span><br><span class="line">                      n_layers=<span class="number">1</span>, dropout=<span class="number">0.5</span>)</span><br><span class="line">    seq2seq = Seq2Seq(encoder, decoder)</span><br><span class="line">    optimizer = optim.Adam(seq2seq.parameters(), lr=args.lr)</span><br><span class="line">    <span class="built_in">print</span>(seq2seq)</span><br><span class="line"></span><br><span class="line">    best_val_loss = <span class="literal">None</span></span><br><span class="line">    <span class="keyword">for</span> e <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, args.epochs+<span class="number">1</span>):</span><br><span class="line">        train(e, seq2seq, optimizer, train_iter,</span><br><span class="line">              en_size, args.grad_clip, DE, EN)</span><br><span class="line">        val_loss = evaluate(seq2seq, val_iter, en_size, DE, EN)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;[Epoch:%d] val_loss:%5.3f | val_pp:%5.2fS&quot;</span></span><br><span class="line">              % (e, val_loss, math.exp(val_loss)))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Save the model if the validation loss is the best we&#x27;ve seen so far.</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> best_val_loss <span class="keyword">or</span> val_loss &lt; best_val_loss:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;[!] saving model...&quot;</span>)</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> os.path.isdir(<span class="string">&quot;.save&quot;</span>):</span><br><span class="line">                os.makedirs(<span class="string">&quot;.save&quot;</span>)</span><br><span class="line">            torch.save(seq2seq.state_dict(), <span class="string">&#x27;./.save/seq2seq_%d.pt&#x27;</span> % (e))</span><br><span class="line">            best_val_loss = val_loss</span><br><span class="line">    test_loss = evaluate(seq2seq, test_iter, en_size, DE, EN)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;[TEST] loss:%5.2f&quot;</span> % test_loss)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        main()</span><br><span class="line">    <span class="keyword">except</span> KeyboardInterrupt <span class="keyword">as</span> e:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;[STOP]&quot;</span>, e)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><figcaption><span>bleu.py</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding:utf-8 -*-</span></span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="keyword">import</span> codecs</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> operator</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">from</span> nltk <span class="keyword">import</span> bleu</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">fetch_data</span>(<span class="params">cand, ref</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot; Store each reference and candidate sentences as a list &quot;&quot;&quot;</span></span><br><span class="line">    references = []</span><br><span class="line">    <span class="keyword">if</span> <span class="string">&#x27;.txt&#x27;</span> <span class="keyword">in</span> ref:</span><br><span class="line">        reference_file = codecs.<span class="built_in">open</span>(ref, <span class="string">&#x27;r&#x27;</span>, <span class="string">&#x27;utf-8&#x27;</span>)</span><br><span class="line">        references.append(reference_file.readlines())</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">for</span> root, dirs, files <span class="keyword">in</span> os.walk(ref):</span><br><span class="line">            <span class="keyword">for</span> f <span class="keyword">in</span> files:</span><br><span class="line">                reference_file = codecs.<span class="built_in">open</span>(os.path.join(root, f), <span class="string">&#x27;r&#x27;</span>, <span class="string">&#x27;utf-8&#x27;</span>)</span><br><span class="line">                references.append(reference_file.readlines())</span><br><span class="line">    candidate_file = codecs.<span class="built_in">open</span>(cand, <span class="string">&#x27;r&#x27;</span>, <span class="string">&#x27;utf-8&#x27;</span>)</span><br><span class="line">    candidate = candidate_file.readlines()</span><br><span class="line">    <span class="keyword">return</span> candidate, references</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># candidate = [[&quot;word peace],[&#x27;make china great again !&#x27;]]</span></span><br><span class="line"><span class="comment"># reference [[&quot;world war&quot;],[&#x27;make USA great again&#x27;]]</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">count_ngram</span>(<span class="params">candidate, references, n</span>):</span><br><span class="line">    clipped_count = <span class="number">0</span></span><br><span class="line">    count = <span class="number">0</span></span><br><span class="line">    r = <span class="number">0</span>   <span class="comment">#用来记录reference</span></span><br><span class="line">    c = <span class="number">0</span>  <span class="comment">#用来记录 candidates的长度</span></span><br><span class="line">    <span class="keyword">for</span> si <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(candidate)):  <span class="comment"># 遍历每一个CANDIDATES</span></span><br><span class="line">        <span class="comment"># Calculate precision for each sentence</span></span><br><span class="line">        <span class="comment"># print si</span></span><br><span class="line">        ref_counts = []   <span class="comment">#统计ref 中的，每个n-gram 的数字</span></span><br><span class="line">        ref_lengths = []  <span class="comment">#统计 REF 的长度，length</span></span><br><span class="line">        <span class="comment"># print references</span></span><br><span class="line">        <span class="comment"># Build dictionary of ngram counts</span></span><br><span class="line">        <span class="keyword">for</span> reference <span class="keyword">in</span> references:  <span class="comment"># 遍历每一个REFERENCE</span></span><br><span class="line">            <span class="comment"># print &#x27;reference&#x27; + reference</span></span><br><span class="line">            ref_sentence = reference[si]</span><br><span class="line">            ngram_d = &#123;&#125;</span><br><span class="line">            words = ref_sentence.strip().split()</span><br><span class="line">            ref_lengths.append(<span class="built_in">len</span>(words))</span><br><span class="line">            limits = <span class="built_in">len</span>(words) - n + <span class="number">1</span>      <span class="comment"># [1,2,3,4,5,6,7]</span></span><br><span class="line">            <span class="comment"># loop through the sentance consider the ngram length</span></span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(limits):</span><br><span class="line">                ngram = <span class="string">&#x27; &#x27;</span>.join(words[i:i + n]).lower()</span><br><span class="line">                <span class="keyword">if</span> ngram <span class="keyword">in</span> ngram_d.keys():</span><br><span class="line">                    ngram_d[ngram] += <span class="number">1</span></span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    ngram_d[ngram] = <span class="number">1</span></span><br><span class="line">            ref_counts.append(ngram_d)</span><br><span class="line">        <span class="comment"># candidate</span></span><br><span class="line">        cand_sentence = candidate[si]  <span class="comment"># 遍历 CANDIDATE</span></span><br><span class="line">        cand_dict = &#123;&#125;</span><br><span class="line">        words = cand_sentence.strip().split()</span><br><span class="line">        limits = <span class="built_in">len</span>(words) - n + <span class="number">1</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, limits):</span><br><span class="line">            ngram = <span class="string">&#x27; &#x27;</span>.join(words[i:i + n]).lower()</span><br><span class="line">            <span class="keyword">if</span> ngram <span class="keyword">in</span> cand_dict:</span><br><span class="line">                cand_dict[ngram] += <span class="number">1</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                cand_dict[ngram] = <span class="number">1</span></span><br><span class="line">        clipped_count += clip_count(cand_dict, ref_counts)</span><br><span class="line">        count += limits</span><br><span class="line">        r += best_length_match(ref_lengths, <span class="built_in">len</span>(words))</span><br><span class="line">        c += <span class="built_in">len</span>(words)</span><br><span class="line">    <span class="keyword">if</span> clipped_count == <span class="number">0</span>:</span><br><span class="line">        pr = <span class="number">0</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        pr = <span class="built_in">float</span>(clipped_count) / count</span><br><span class="line">    bp = brevity_penalty(c, r)</span><br><span class="line">    <span class="keyword">return</span> pr, bp</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">clip_count</span>(<span class="params">cand_d, ref_ds</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Count the clip count for each ngram considering all references&quot;&quot;&quot;</span></span><br><span class="line">    count = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> m <span class="keyword">in</span> cand_d.keys():</span><br><span class="line">        m_w = cand_d[m]</span><br><span class="line">        m_max = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> ref <span class="keyword">in</span> ref_ds:</span><br><span class="line">            <span class="keyword">if</span> m <span class="keyword">in</span> ref:</span><br><span class="line">                m_max = <span class="built_in">max</span>(m_max, ref[m])</span><br><span class="line">        m_w = <span class="built_in">min</span>(m_w, m_max)</span><br><span class="line">        count += m_w</span><br><span class="line">    <span class="keyword">return</span> count</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">best_length_match</span>(<span class="params">ref_l, cand_l</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Find the closest length of reference to that of candidate&quot;&quot;&quot;</span></span><br><span class="line">    least_diff = <span class="built_in">abs</span>(cand_l - ref_l[<span class="number">0</span>])</span><br><span class="line">    best = ref_l[<span class="number">0</span>]</span><br><span class="line">    <span class="keyword">for</span> ref <span class="keyword">in</span> ref_l:</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">abs</span>(cand_l - ref) &lt; least_diff:</span><br><span class="line">            least_diff = <span class="built_in">abs</span>(cand_l - ref)</span><br><span class="line">            best = ref</span><br><span class="line">    <span class="keyword">return</span> best</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">brevity_penalty</span>(<span class="params">c, r</span>):</span><br><span class="line">    <span class="keyword">if</span> c &gt; r:</span><br><span class="line">        bp = <span class="number">1</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        bp = math.exp(<span class="number">1</span> - (<span class="built_in">float</span>(r) / c))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> bp</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">geometric_mean</span>(<span class="params">precisions</span>):</span><br><span class="line">    <span class="keyword">return</span> (reduce(operator.mul, precisions)) ** (<span class="number">1.0</span> / <span class="built_in">len</span>(precisions))</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">BLEU</span>(<span class="params">candidate, references</span>):</span><br><span class="line">    precisions = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">4</span>):</span><br><span class="line">        pr, bp = count_ngram(candidate, references, i + <span class="number">1</span>)</span><br><span class="line">        precisions.append(pr)</span><br><span class="line">        <span class="built_in">print</span></span><br><span class="line">        <span class="string">&#x27;P&#x27;</span> + <span class="built_in">str</span>(i + <span class="number">1</span>), <span class="string">&#x27; = &#x27;</span>, <span class="built_in">round</span>(pr, <span class="number">2</span>)</span><br><span class="line">    <span class="built_in">print</span></span><br><span class="line">    <span class="string">&#x27;BP = &#x27;</span>, <span class="built_in">round</span>(bp, <span class="number">2</span>)</span><br><span class="line">    bleu = geometric_mean(precisions) * bp</span><br><span class="line">    <span class="keyword">return</span> bleu</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    candidate, references = fetch_data(sys.argv[<span class="number">1</span>], sys.argv[<span class="number">2</span>])</span><br><span class="line">    bleu = BLEU(candidate, references)</span><br><span class="line">    <span class="built_in">print</span></span><br><span class="line">    <span class="string">&#x27;BLEU = &#x27;</span>, <span class="built_in">round</span>(bleu, <span class="number">4</span>)</span><br><span class="line">    out = <span class="built_in">open</span>(<span class="string">&#x27;bleu_out.txt&#x27;</span>, <span class="string">&#x27;w&#x27;</span>)</span><br><span class="line">    out.write(<span class="built_in">str</span>(bleu))</span><br><span class="line">    out.close()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> nltk.translate.bleu_score <span class="keyword">import</span> sentence_bleu</span><br><span class="line">reference = [[<span class="string">&#x27;this&#x27;</span>, <span class="string">&#x27;is&#x27;</span>, <span class="string">&#x27;a&#x27;</span>, <span class="string">&#x27;test&#x27;</span>], [<span class="string">&#x27;this&#x27;</span>, <span class="string">&#x27;is&#x27;</span> <span class="string">&#x27;test&#x27;</span>]]</span><br><span class="line">candidate = [<span class="string">&#x27;this&#x27;</span>, <span class="string">&#x27;is&#x27;</span>, <span class="string">&#x27;a&#x27;</span>, <span class="string">&#x27;test&#x27;</span>]</span><br><span class="line">score = sentence_bleu(reference, candidate)</span><br><span class="line"><span class="built_in">print</span>(score)</span><br></pre></td></tr></table></figure>
<h1 id="参考文献">参考文献</h1>
<p><span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL2tlb24vc2VxMnNlcQ==">mini seq2seq<i class="fa fa-external-link-alt"></i></span></p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://soundmemories.github.io/2021/07/15/NLP/02.%E5%9F%BA%E4%BA%8ELSTM%E7%9A%84%E6%83%85%E6%84%9F%E5%88%86%E7%B1%BB/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avstar.png">
      <meta itemprop="name" content="SoundMemories">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="SoundMemories">
      <meta itemprop="description" content="今日事，今日毕">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | SoundMemories">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2021/07/15/NLP/02.%E5%9F%BA%E4%BA%8ELSTM%E7%9A%84%E6%83%85%E6%84%9F%E5%88%86%E7%B1%BB/" class="post-title-link" itemprop="url">基于LSTM的情感分类</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2021-07-15 00:00:00" itemprop="dateCreated datePublished" datetime="2021-07-15T00:00:00+08:00">2021-07-15</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/NLP/" itemprop="url" rel="index"><span itemprop="name">NLP</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>2.5k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>9 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p><strong>本项目使用了word2vec的中文预训练向量</strong><br />
<strong>模型分别有BiLSTM-attention和普通的LSTM两种，自行选择</strong></p>
<p><strong>使用说明</strong>：<br />
1、在<strong>Config</strong>中配置相关参数</p>
<p>2、然后运行<strong>DataProcess.py</strong>，生成相应的word2id，word2vec等文件</p>
<p>3、运行主函数<strong>main.py</strong>，得到训练好的模型，并保存模型</p>
<p>4、运行<strong>eval.py</strong>，读取模型，并得到评价</p>
<p>5、模型<strong>准确率平均85%左右</strong></p>
<figure class="highlight python"><figcaption><span>Sentiment_Analysis_Config.py</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Config</span>():</span><br><span class="line">    update_w2v = <span class="literal">True</span>          <span class="comment"># 是否在训练中更新w2v</span></span><br><span class="line">    vocab_size = <span class="number">54848</span>          <span class="comment"># 词汇量，与word2id中的词汇量一致</span></span><br><span class="line">    n_class = <span class="number">2</span>                 <span class="comment"># 分类数：分别为pos和neg</span></span><br><span class="line">    max_sen_len = <span class="number">65</span>           <span class="comment"># 句子最大长度</span></span><br><span class="line">    embedding_dim = <span class="number">50</span>          <span class="comment"># 词向量维度</span></span><br><span class="line">    batch_size =<span class="number">64</span>            <span class="comment"># 批处理尺寸</span></span><br><span class="line">    hidden_dim=<span class="number">100</span>           <span class="comment"># 隐藏层节点数</span></span><br><span class="line">    n_epoch = <span class="number">30</span>            <span class="comment"># 训练迭代周期，即遍历整个训练样本的次数</span></span><br><span class="line">    lr = <span class="number">0.0001</span>               <span class="comment"># 学习率；若opt=‘adadelta&#x27;，则不需要定义学习率</span></span><br><span class="line">    drop_keep_prob = <span class="number">0.2</span>        <span class="comment"># dropout层，参数keep的比例</span></span><br><span class="line">    num_layers = <span class="number">2</span>              <span class="comment"># LSTM层数</span></span><br><span class="line">    bidirectional=<span class="literal">True</span>         <span class="comment">#是否使用双向LSTM</span></span><br><span class="line">    train_path = <span class="string">&#x27;./word2vec_data/train.txt&#x27;</span></span><br><span class="line">    val_path = <span class="string">&#x27;./word2vec_data/validation.txt&#x27;</span></span><br><span class="line">    test_path = <span class="string">&#x27;./word2vec_data/test.txt&#x27;</span></span><br><span class="line">    pre_path =<span class="string">&#x27;./word2vec_data/pre.txt&#x27;</span></span><br><span class="line">    word2id_path = <span class="string">&#x27;./word2vec_data/word2id.txt&#x27;</span></span><br><span class="line">    pre_word2vec_path = <span class="string">&#x27;./word2vec_data/wiki_word2vec_50.bin&#x27;</span></span><br><span class="line">    corpus_word2vec_path = <span class="string">&#x27;./word2vec_data/word_vec.txt&#x27;</span></span><br><span class="line">    model_state_dict_path=<span class="string">&#x27;./word2vec_data/sen_model.pkl&#x27;</span><span class="comment"># 训练模型保存的地址</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><figcaption><span>Sentiment_Analysis_DataProcess.py</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> unicode_literals, print_function, division</span><br><span class="line"><span class="keyword">from</span> io <span class="keyword">import</span> <span class="built_in">open</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">import</span>  numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> gensim</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataset</span><br><span class="line"><span class="keyword">from</span> Sentiment_Analysis_Config <span class="keyword">import</span> Config</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Data_set</span>(<span class="title class_ inherited__">Dataset</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, Data, Label</span>):</span><br><span class="line">        self.Data = Data</span><br><span class="line">        <span class="keyword">if</span> Label <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:<span class="comment">#考虑对测试集的使用</span></span><br><span class="line">            self.Label = Label</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.Data)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, index</span>):</span><br><span class="line">        <span class="keyword">if</span> self.Label <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            data = torch.from_numpy(self.Data[index])</span><br><span class="line">            label = torch.from_numpy(self.Label[index])</span><br><span class="line">            <span class="keyword">return</span> data, label</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            data = torch.from_numpy(self.Data[index])</span><br><span class="line">            <span class="keyword">return</span> data</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">stopwordslist</span>():<span class="comment">#创建停用词表</span></span><br><span class="line">    stopwords = [line.strip() <span class="keyword">for</span> line <span class="keyword">in</span> <span class="built_in">open</span>(<span class="string">&#x27;word2vec_data/stopword.txt&#x27;</span>,encoding=<span class="string">&#x27;UTF-8&#x27;</span>).readlines()]</span><br><span class="line">    <span class="keyword">return</span> stopwords</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">build_word2id</span>(<span class="params">file</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    :param file: word2id保存地址</span></span><br><span class="line"><span class="string">    :return: None</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment">#num_50=0#统计长度大于50的句子数</span></span><br><span class="line">    stopwords = stopwordslist()</span><br><span class="line">    word2id = &#123;<span class="string">&#x27;_PAD_&#x27;</span>: <span class="number">0</span>&#125;</span><br><span class="line">    path = [Config.train_path, Config.val_path]</span><br><span class="line">    <span class="comment">#print(path)</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> _path <span class="keyword">in</span> path:</span><br><span class="line">        <span class="keyword">with</span> <span class="built_in">open</span>(_path, encoding=<span class="string">&#x27;utf-8&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">            <span class="keyword">for</span> line <span class="keyword">in</span> f.readlines():</span><br><span class="line">                out_list = []</span><br><span class="line">                <span class="comment"># 去停用词</span></span><br><span class="line">                sp = line.strip().split()</span><br><span class="line">                <span class="keyword">for</span> word <span class="keyword">in</span> sp[<span class="number">1</span>:]:</span><br><span class="line">                    <span class="keyword">if</span> word <span class="keyword">not</span> <span class="keyword">in</span> stopwords:</span><br><span class="line">                        rt = re.findall(<span class="string">&#x27;[a-zA-Z]+&#x27;</span>, word)</span><br><span class="line">                        <span class="keyword">if</span> word != <span class="string">&#x27;\t&#x27;</span>:</span><br><span class="line">                            <span class="comment"># if is_number(word):</span></span><br><span class="line">                            <span class="comment"># continue</span></span><br><span class="line">                            <span class="keyword">if</span> <span class="built_in">len</span>(rt) == <span class="number">1</span>:</span><br><span class="line">                                <span class="keyword">continue</span></span><br><span class="line">                            <span class="keyword">else</span>:</span><br><span class="line">                                out_list.append(word)</span><br><span class="line">                <span class="keyword">for</span> word <span class="keyword">in</span> out_list:</span><br><span class="line">                    <span class="keyword">if</span> word <span class="keyword">not</span> <span class="keyword">in</span> word2id.keys():</span><br><span class="line">                        word2id[word] = <span class="built_in">len</span>(word2id)</span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(file, <span class="string">&#x27;w&#x27;</span>, encoding=<span class="string">&#x27;utf-8&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">        <span class="keyword">for</span> w <span class="keyword">in</span> word2id:</span><br><span class="line">            f.write(w+<span class="string">&#x27;\t&#x27;</span>)</span><br><span class="line">            f.write(<span class="built_in">str</span>(word2id[w]))</span><br><span class="line">            f.write(<span class="string">&#x27;\n&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">build_word2vec</span>(<span class="params">fname, word2id, save_to_path=<span class="literal">None</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    :param fname: 预训练的word2vec.</span></span><br><span class="line"><span class="string">    :param word2id: 语料文本中包含的词汇集.</span></span><br><span class="line"><span class="string">    :param save_to_path: 保存训练语料库中的词组对应的word2vec到本地</span></span><br><span class="line"><span class="string">    :return: 语料文本中词汇集对应的word2vec向量&#123;id: word2vec&#125;.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    n_words = <span class="built_in">max</span>(word2id.values()) + <span class="number">1</span></span><br><span class="line">    model = gensim.models.KeyedVectors.load_word2vec_format(fname, binary=<span class="literal">True</span>)</span><br><span class="line">    word_vecs = np.array(np.random.uniform(-<span class="number">1.</span>, <span class="number">1.</span>, [n_words, model.vector_size]))</span><br><span class="line">    <span class="keyword">for</span> word <span class="keyword">in</span> word2id.keys():</span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            word_vecs[word2id[word]] = model[word]</span><br><span class="line">        <span class="keyword">except</span> KeyError:</span><br><span class="line">            <span class="keyword">pass</span></span><br><span class="line">    <span class="keyword">if</span> save_to_path:</span><br><span class="line">        <span class="keyword">with</span> <span class="built_in">open</span>(save_to_path, <span class="string">&#x27;w&#x27;</span>, encoding=<span class="string">&#x27;utf-8&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">            <span class="keyword">for</span> vec <span class="keyword">in</span> word_vecs:</span><br><span class="line">                vec = [<span class="built_in">str</span>(w) <span class="keyword">for</span> w <span class="keyword">in</span> vec]</span><br><span class="line">                f.write(<span class="string">&#x27; &#x27;</span>.join(vec))</span><br><span class="line">                f.write(<span class="string">&#x27;\n&#x27;</span>)</span><br><span class="line">    <span class="keyword">return</span> word_vecs</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">text_to_array</span>(<span class="params">word2id,seq_lenth ,path</span>):  <span class="comment"># 文本转为索引数字模式,</span></span><br><span class="line"></span><br><span class="line">    lable_array=[]</span><br><span class="line">    i=<span class="number">0</span></span><br><span class="line">    sa=[]</span><br><span class="line">    <span class="comment">#获取句子个数</span></span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(path, encoding=<span class="string">&#x27;utf-8&#x27;</span>) <span class="keyword">as</span> f1:</span><br><span class="line">        <span class="keyword">for</span> l1 <span class="keyword">in</span> f1.readlines():</span><br><span class="line">            s= l1.strip().split()</span><br><span class="line">            s1=s[<span class="number">1</span>:]</span><br><span class="line">            new_s = [word2id.get(word, <span class="number">0</span>) <span class="keyword">for</span> word <span class="keyword">in</span> s1]  <span class="comment"># 单词转索引数字</span></span><br><span class="line">            sa.append(new_s)</span><br><span class="line">        <span class="comment">#print(len(sa))</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(path, encoding=<span class="string">&#x27;utf-8&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">        sentences_array=np.zeros(shape=(<span class="built_in">len</span>(sa),seq_lenth))<span class="comment">#行：句子个数 列：句子长度</span></span><br><span class="line">        <span class="keyword">for</span> line <span class="keyword">in</span> f.readlines():</span><br><span class="line">            sl1 = line.strip().split()</span><br><span class="line">            sen=sl1[<span class="number">1</span>:]</span><br><span class="line">            new_sen = [word2id.get(word, <span class="number">0</span>) <span class="keyword">for</span> word <span class="keyword">in</span> sen]  <span class="comment"># 单词转索引数字,不存在则为0</span></span><br><span class="line">            new_sen_np=np.array(new_sen).reshape(<span class="number">1</span>,-<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">            <span class="comment">#补齐每个句子长度，多余补零，少了就直接赋值,0填在前面。</span></span><br><span class="line">            <span class="keyword">if</span> np.size(new_sen_np,<span class="number">1</span>)&lt;seq_lenth:</span><br><span class="line">                sentences_array[i,seq_lenth-np.size(new_sen_np,<span class="number">1</span>):]=new_sen_np[<span class="number">0</span>,:]</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                sentences_array[i, <span class="number">0</span>:seq_lenth]=new_sen_np[<span class="number">0</span>,<span class="number">0</span>:seq_lenth]</span><br><span class="line"></span><br><span class="line">            i=i+<span class="number">1</span></span><br><span class="line">            lable=<span class="built_in">int</span>(sl1[<span class="number">0</span>])<span class="comment">#标签</span></span><br><span class="line">            lable_array.append(lable)</span><br><span class="line">    <span class="keyword">return</span> np.array(sentences_array),lable_array</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">text_to_array_nolable</span>(<span class="params">word2id,seq_lenth ,path</span>):  <span class="comment"># 文本转为索引数字模式,</span></span><br><span class="line"></span><br><span class="line">    i=<span class="number">0</span></span><br><span class="line">    sa=[]</span><br><span class="line">    <span class="comment">#获取句子个数</span></span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(path, encoding=<span class="string">&#x27;utf-8&#x27;</span>) <span class="keyword">as</span> f1:</span><br><span class="line">        <span class="keyword">for</span> l1 <span class="keyword">in</span> f1.readlines():</span><br><span class="line">            s= l1.strip().split()</span><br><span class="line">            s1=s[<span class="number">1</span>:]</span><br><span class="line">            new_s = [word2id.get(word, <span class="number">0</span>) <span class="keyword">for</span> word <span class="keyword">in</span> s1]  <span class="comment"># 单词转索引数字</span></span><br><span class="line">            sa.append(new_s)</span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(path, encoding=<span class="string">&#x27;utf-8&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">        sentences_array=np.zeros(shape=(<span class="built_in">len</span>(sa),seq_lenth))<span class="comment">#行：句子个数 列：句子长度</span></span><br><span class="line">        <span class="keyword">for</span> line <span class="keyword">in</span> f.readlines():</span><br><span class="line">            sl1 = line.strip().split()</span><br><span class="line">            sen=sl1[<span class="number">1</span>:]</span><br><span class="line">            new_sen = [word2id.get(word, <span class="number">0</span>) <span class="keyword">for</span> word <span class="keyword">in</span> sen]  <span class="comment"># 单词转索引数字,不存在则为0</span></span><br><span class="line">            new_sen_np=np.array(new_sen).reshape(<span class="number">1</span>,-<span class="number">1</span>)</span><br><span class="line">            <span class="keyword">if</span> np.size(new_sen_np,<span class="number">1</span>)&lt;seq_lenth:</span><br><span class="line">                sentences_array[i,seq_lenth-np.size(new_sen_np,<span class="number">1</span>):]=new_sen_np[<span class="number">0</span>,:]</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                sentences_array[i, <span class="number">0</span>:seq_lenth]=new_sen_np[<span class="number">0</span>,<span class="number">0</span>:seq_lenth]</span><br><span class="line">            i=i+<span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> np.array(sentences_array)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">to_categorical</span>(<span class="params">y, num_classes=<span class="literal">None</span></span>):<span class="comment">#将类别转化为one-hot编码</span></span><br><span class="line">    y = np.array(y, dtype=<span class="string">&#x27;int&#x27;</span>)</span><br><span class="line">    input_shape = y.shape</span><br><span class="line">    <span class="keyword">if</span> input_shape <span class="keyword">and</span> input_shape[-<span class="number">1</span>] == <span class="number">1</span> <span class="keyword">and</span> <span class="built_in">len</span>(input_shape) &gt; <span class="number">1</span>:</span><br><span class="line">        input_shape = <span class="built_in">tuple</span>(input_shape[:-<span class="number">1</span>])</span><br><span class="line">    y = y.ravel()</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> num_classes:</span><br><span class="line">        num_classes = np.<span class="built_in">max</span>(y) + <span class="number">1</span></span><br><span class="line">    n = y.shape[<span class="number">0</span>]</span><br><span class="line">    categorical = np.zeros((n, num_classes))</span><br><span class="line">    categorical[np.arange(n), y] = <span class="number">1</span></span><br><span class="line">    output_shape = input_shape + (num_classes,)</span><br><span class="line">    categorical = np.reshape(categorical, output_shape)</span><br><span class="line">    <span class="keyword">return</span> categorical</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">prepare_data</span>(<span class="params">w2id, train_path,val_path,test_path,seq_lenth</span>):<span class="comment">#得到数字索引表示的句子和标签</span></span><br><span class="line">    train_array,train_lable = text_to_array(w2id,seq_lenth= seq_lenth,path=train_path)</span><br><span class="line">    val_array,val_lable  = text_to_array(w2id,seq_lenth=seq_lenth,path= val_path)</span><br><span class="line">    test_array,test_lable=text_to_array(w2id,seq_lenth=seq_lenth,path=test_path)</span><br><span class="line"></span><br><span class="line">    <span class="comment">#标签为[1, 1, 1, 1, 1, 1, 1, 1, 0, 0...]将标签转为onehot</span></span><br><span class="line">    <span class="comment">#train_lable=to_categorical(train_lable,num_classes=2)</span></span><br><span class="line">    <span class="comment">#val_lable=to_categorical(val_lable,num_classes=2)</span></span><br><span class="line"></span><br><span class="line">    <span class="string">&quot;&quot;&quot;for i in train_lable:</span></span><br><span class="line"><span class="string">        np.array([i])&quot;&quot;&quot;</span></span><br><span class="line">    train_lable=np.array([train_lable]).T</span><br><span class="line">    val_lable=np.array([val_lable]).T</span><br><span class="line">    test_lable=np.array([test_lable]).T</span><br><span class="line">    <span class="string">&quot;&quot;&quot;转换后标签</span></span><br><span class="line"><span class="string">            [[0. 1.]</span></span><br><span class="line"><span class="string">            [0. 1.]</span></span><br><span class="line"><span class="string">            [0. 1.]</span></span><br><span class="line"><span class="string">            ...</span></span><br><span class="line"><span class="string">            [1. 0.]</span></span><br><span class="line"><span class="string">            [1. 0.]</span></span><br><span class="line"><span class="string">            [1. 0.]]&quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment">#print(train_lab,&quot;\nval\n&quot;,val_lab)</span></span><br><span class="line">    <span class="keyword">return</span> train_array ,train_lable,val_array,val_lable,test_array,test_lable</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#建立word2id</span></span><br><span class="line">build_word2id(<span class="string">&#x27;./word2vec_data/word2id.txt&#x27;</span>)<span class="comment">#建立词toid</span></span><br><span class="line">splist=[]</span><br><span class="line">word2id=&#123;&#125;</span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;./word2vec_data/word2id.txt&#x27;</span>, encoding=<span class="string">&#x27;utf-8&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">        <span class="keyword">for</span> line <span class="keyword">in</span> f.readlines():</span><br><span class="line">            sp = line.strip().split()<span class="comment">#去掉\n \t 等</span></span><br><span class="line">            splist.append(sp)</span><br><span class="line">        word2id=<span class="built_in">dict</span>(splist)<span class="comment">#转成字典</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> key <span class="keyword">in</span> word2id:<span class="comment"># 将字典的值，从str转成int</span></span><br><span class="line">    word2id[key]=<span class="built_in">int</span>(word2id[key])</span><br><span class="line"></span><br><span class="line">id2word=&#123;&#125;<span class="comment">#得到id2word</span></span><br><span class="line"><span class="keyword">for</span> key,val <span class="keyword">in</span> word2id.items():</span><br><span class="line">    id2word[val]=key</span><br><span class="line"><span class="comment">#建立word2vec</span></span><br><span class="line">w2vec=build_word2vec(Config.pre_word2vec_path,word2id,Config.corpus_word2vec_path)</span><br><span class="line"></span><br><span class="line"><span class="comment">#得到句子id表示和标签</span></span><br><span class="line">train_array,train_lable,val_array,val_lable,test_array,test_label=prepare_data(word2id,</span><br><span class="line">                                                         train_path=Config.train_path,</span><br><span class="line">                                                         val_path=Config.val_path,</span><br><span class="line">                                                         test_path=Config.test_path,seq_lenth=Config.max_sen_len)</span><br><span class="line"></span><br><span class="line">np.savetxt(<span class="string">&#x27;./word2vec_data/train_data.txt&#x27;</span>, train_array,fmt=<span class="string">&#x27;%d&#x27;</span>)</span><br><span class="line">np.savetxt(<span class="string">&#x27;./word2vec_data/val_data.txt&#x27;</span>, val_array,fmt=<span class="string">&#x27;%d&#x27;</span>)</span><br><span class="line">np.savetxt(<span class="string">&#x27;./word2vec_data/test_data.txt&#x27;</span>, test_array,fmt=<span class="string">&#x27;%d&#x27;</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><figcaption><span>Sentiment_model.py</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> unicode_literals, print_function, division</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">LSTMModel</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, vocab_size, embedding_dim,pretrained_weight, update_w2v,hidden_dim,</span></span><br><span class="line"><span class="params">                 num_layers,drop_keep_prob,n_class,bidirectional, **kwargs</span>):</span><br><span class="line">        <span class="built_in">super</span>(LSTMModel, self).__init__()</span><br><span class="line">        self.hidden_dim = hidden_dim</span><br><span class="line">        self.num_layers = num_layers</span><br><span class="line">        self.n_class = n_class</span><br><span class="line"></span><br><span class="line">        self.bidirectional = bidirectional</span><br><span class="line">        self.embedding = nn.Embedding.from_pretrained(pretrained_weight)</span><br><span class="line">        self.embedding.weight.requires_grad = update_w2v</span><br><span class="line">        self.encoder = nn.LSTM(input_size=embedding_dim, hidden_size=self.hidden_dim,</span><br><span class="line">                               num_layers=num_layers, bidirectional=self.bidirectional,</span><br><span class="line">                               dropout=drop_keep_prob)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.bidirectional:</span><br><span class="line">            self.decoder1 = nn.Linear(hidden_dim * <span class="number">4</span>, hidden_dim)</span><br><span class="line">            self.decoder2 = nn.Linear(hidden_dim,n_class)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.decoder1 = nn.Linear(hidden_dim * <span class="number">2</span>, hidden_dim)</span><br><span class="line">            self.decoder2 = nn.Linear(hidden_dim,n_class)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, inputs</span>):</span><br><span class="line">        embeddings = self.embedding(inputs)<span class="comment"># [batch, seq_len] =&gt; [batch, seq_len, embed_dim][64,75,50]</span></span><br><span class="line">        states, hidden = self.encoder(embeddings.permute([<span class="number">1</span>, <span class="number">0</span>, <span class="number">2</span>]))<span class="comment">#[75,32,50],[seq_len, batch, embed_dim]</span></span><br><span class="line"></span><br><span class="line">        encoding = torch.cat([states[<span class="number">0</span>], states[-<span class="number">1</span>]], dim=<span class="number">1</span>)<span class="comment">#张量拼接[32,512]</span></span><br><span class="line">        outputs = self.decoder1(encoding)</span><br><span class="line">        <span class="comment">#outputs = F.softmax(outputs, dim=1)</span></span><br><span class="line">        outputs=self.decoder2(outputs)</span><br><span class="line">        <span class="keyword">return</span> outputs</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">LSTM_attention</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, vocab_size, embedding_dim,pretrained_weight, update_w2v,hidden_dim,</span></span><br><span class="line"><span class="params">                 num_layers,drop_keep_prob,n_class,bidirectional, **kwargs</span>):</span><br><span class="line">        <span class="built_in">super</span>(LSTM_attention, self).__init__()</span><br><span class="line">        self.hidden_dim = hidden_dim</span><br><span class="line">        self.num_layers = num_layers</span><br><span class="line">        self.n_class = n_class</span><br><span class="line"></span><br><span class="line">        self.bidirectional = bidirectional</span><br><span class="line">        self.embedding = nn.Embedding.from_pretrained(pretrained_weight)</span><br><span class="line">        self.embedding.weight.requires_grad = update_w2v</span><br><span class="line">        self.encoder = nn.LSTM(input_size=embedding_dim, hidden_size=self.hidden_dim,</span><br><span class="line">                               num_layers=num_layers, bidirectional=self.bidirectional,</span><br><span class="line">                               dropout=drop_keep_prob)</span><br><span class="line"></span><br><span class="line">        <span class="comment">#TODO</span></span><br><span class="line">        <span class="comment"># What is nn. Parameter ? Explain</span></span><br><span class="line">        self.weight_W = nn.Parameter(torch.Tensor(<span class="number">2</span>*hidden_dim, <span class="number">2</span>*hidden_dim))</span><br><span class="line">        self.weight_proj = nn.Parameter(torch.Tensor(<span class="number">2</span>*hidden_dim, <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.bidirectional:</span><br><span class="line">            <span class="comment">#self.decoder1 = nn.Linear(hidden_dim * 2, n_class)</span></span><br><span class="line">            self.decoder1 = nn.Linear(hidden_dim * <span class="number">2</span>, hidden_dim)</span><br><span class="line">            self.decoder2 = nn.Linear(hidden_dim,n_class)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.decoder1 = nn.Linear(hidden_dim * <span class="number">2</span>, hidden_dim)</span><br><span class="line">            self.decoder2 = nn.Linear(hidden_dim,n_class)</span><br><span class="line"></span><br><span class="line">        nn.init.uniform_(self.weight_W, -<span class="number">0.1</span>, <span class="number">0.1</span>)</span><br><span class="line">        nn.init.uniform_(self.weight_proj, -<span class="number">0.1</span>, <span class="number">0.1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, inputs</span>):<span class="number">0</span></span><br><span class="line">        embeddings = self.embedding(inputs)<span class="comment"># [batch, seq_len] =&gt; [batch, seq_len, embed_dim][64,75,50]</span></span><br><span class="line"></span><br><span class="line">        states, hidden = self.encoder(embeddings.permute([<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>]))<span class="comment">#[batch, seq_len, embed_dim]</span></span><br><span class="line">        <span class="comment">#attention</span></span><br><span class="line"></span><br><span class="line">        u = torch.tanh(torch.matmul(states, self.weight_W))</span><br><span class="line">        att = torch.matmul(u, self.weight_proj)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        att_score = F.softmax(att, dim=<span class="number">1</span>)</span><br><span class="line">        scored_x = states * att_score</span><br><span class="line">        encoding = torch.<span class="built_in">sum</span>(scored_x, dim=<span class="number">1</span>)</span><br><span class="line">        outputs = self.decoder1(encoding)</span><br><span class="line">        outputs=self.decoder2(outputs)</span><br><span class="line">        <span class="keyword">return</span> outputs</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><figcaption><span>Sentiment_Analysis_main.py</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> unicode_literals, print_function, division</span><br><span class="line"><span class="keyword">from</span> io <span class="keyword">import</span> <span class="built_in">open</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> optim</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader,Dataset</span><br><span class="line"><span class="keyword">import</span> tqdm</span><br><span class="line"><span class="keyword">from</span> Sentiment_Analysis_DataProcess <span class="keyword">import</span> prepare_data,build_word2vec,Data_set</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> confusion_matrix,f1_score,recall_score</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">from</span> Sentiment_model <span class="keyword">import</span> LSTMModel,LSTM_attention</span><br><span class="line"><span class="keyword">from</span> Sentiment_Analysis_Config <span class="keyword">import</span> Config</span><br><span class="line"><span class="keyword">from</span> Sentiment_Analysis_eval <span class="keyword">import</span> val_accuary</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">train</span>(<span class="params">train_dataloader,model, device, epoches, lr</span>):</span><br><span class="line"></span><br><span class="line">        model.train()</span><br><span class="line">        model = model.to(device)</span><br><span class="line">        <span class="built_in">print</span>(model)</span><br><span class="line">        optimizer = optim.Adam(model.parameters(), lr=lr)</span><br><span class="line">        criterion = nn.CrossEntropyLoss()</span><br><span class="line">        <span class="comment"># scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.2)  # 学习率调整</span></span><br><span class="line">        best_acc = <span class="number">0.85</span></span><br><span class="line">        <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(epoches):  <span class="comment"># 一个epoch可以认为是一次训练循环</span></span><br><span class="line">            train_loss = <span class="number">0.0</span></span><br><span class="line">            correct = <span class="number">0</span></span><br><span class="line">            total = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">            train_dataloader = tqdm.tqdm(train_dataloader)</span><br><span class="line">            <span class="comment"># train_dataloader.set_description(&#x27;[%s%04d/%04d %s%f]&#x27; % </span></span><br><span class="line">            <span class="comment">#                                 (&#x27;Epoch:&#x27;, epoch + 1, epoches, &#x27;lr:&#x27;, scheduler.get_last_lr()[0]))</span></span><br><span class="line">            <span class="keyword">for</span> i, data_ <span class="keyword">in</span> (<span class="built_in">enumerate</span>(train_dataloader)):</span><br><span class="line"></span><br><span class="line">                optimizer.zero_grad()</span><br><span class="line">                input_, target = data_[<span class="number">0</span>], data_[<span class="number">1</span>]</span><br><span class="line">                input_=input_.<span class="built_in">type</span>(torch.LongTensor)</span><br><span class="line">                target=target.<span class="built_in">type</span>(torch.LongTensor)</span><br><span class="line">                input_=input_.to(device)</span><br><span class="line">                target=target.to(device)</span><br><span class="line">                output= model(input_)</span><br><span class="line">                <span class="comment"># 经过模型对象就产生了输出</span></span><br><span class="line">                target=target.squeeze(<span class="number">1</span>)</span><br><span class="line">                loss = criterion(output, target)</span><br><span class="line">                loss.backward()</span><br><span class="line">                optimizer.step()</span><br><span class="line">                train_loss+= loss.item()</span><br><span class="line">                _, predicted = torch.<span class="built_in">max</span>(output, <span class="number">1</span>)</span><br><span class="line">                <span class="comment">#print(predicted.shape)</span></span><br><span class="line">                total += target.size(<span class="number">0</span>)  <span class="comment"># 此处的size()类似numpy的shape: np.shape(train_images)[0]</span></span><br><span class="line">                <span class="comment">#print(target.shape)</span></span><br><span class="line">                correct += (predicted == target).<span class="built_in">sum</span>().item()</span><br><span class="line">                F1=f1_score(target.cpu(),predicted.cpu(),average=<span class="string">&#x27;weighted&#x27;</span>)</span><br><span class="line">                Recall=recall_score(target.cpu(),predicted.cpu(),average=<span class="string">&#x27;micro&#x27;</span>)</span><br><span class="line">                <span class="comment">#CM=confusion_matrix(target.cpu(),predicted.cpu())</span></span><br><span class="line">                postfix = &#123;<span class="string">&#x27;train_loss: &#123;:.5f&#125;,train_acc:&#123;:.3f&#125;%&#x27;</span></span><br><span class="line">                           <span class="string">&#x27;,F1: &#123;:.3f&#125;%,Recall:&#123;:.3f&#125;%&#x27;</span> .<span class="built_in">format</span>(train_loss / (i + <span class="number">1</span>),</span><br><span class="line">                                                                        <span class="number">100</span> * correct / total, <span class="number">100</span>*F1 , <span class="number">100</span>* Recall)&#125;</span><br><span class="line">                train_dataloader.set_postfix(log=postfix)</span><br><span class="line"></span><br><span class="line">            acc=val_accuary(model,val_dataloader,device,criterion)</span><br><span class="line">            <span class="keyword">if</span> acc&gt;best_acc:</span><br><span class="line">                best_acc = acc</span><br><span class="line">                <span class="keyword">if</span> os.path.exists(Config.model_state_dict_path) == <span class="literal">False</span>:</span><br><span class="line">                    os.mkdir(Config.model_state_dict_path)</span><br><span class="line">                torch.save(model,<span class="string">&#x27;./word2vec_data/sen_model_best.pkl&#x27;</span> )</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    splist=[]</span><br><span class="line">    word2id=&#123;&#125;</span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(Config.word2id_path, encoding=<span class="string">&#x27;utf-8&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">            <span class="keyword">for</span> line <span class="keyword">in</span> f.readlines():</span><br><span class="line">                sp = line.strip().split()<span class="comment">#去掉\n \t 等</span></span><br><span class="line">                splist.append(sp)</span><br><span class="line">            word2id=<span class="built_in">dict</span>(splist)<span class="comment">#转成字典</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> key <span class="keyword">in</span> word2id:<span class="comment"># 将字典的值，从str转成int</span></span><br><span class="line">        word2id[key]=<span class="built_in">int</span>(word2id[key])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    id2word=&#123;&#125;<span class="comment">#得到id2word</span></span><br><span class="line">    <span class="keyword">for</span> key,val <span class="keyword">in</span> word2id.items():</span><br><span class="line">        id2word[val]=key</span><br><span class="line"></span><br><span class="line">    device = torch.device(<span class="string">&quot;cuda&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    train_array,train_lable,val_array,val_lable,test_array,test_lable=prepare_data(word2id,</span><br><span class="line">                                                             train_path=Config.train_path,</span><br><span class="line">                                                             val_path=Config.val_path,</span><br><span class="line">                                                             test_path=Config.test_path,seq_lenth=Config.max_sen_len)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    train_loader = Data_set(train_array, train_lable)</span><br><span class="line">    train_dataloader = DataLoader(train_loader,</span><br><span class="line">                                 batch_size=Config.batch_size,</span><br><span class="line">                                 shuffle=<span class="literal">True</span>,</span><br><span class="line">                                 num_workers=<span class="number">0</span>)<span class="comment">#用了workers反而变慢了</span></span><br><span class="line"></span><br><span class="line">    val_loader = Data_set(val_array, val_lable)</span><br><span class="line">    val_dataloader = DataLoader(val_loader,</span><br><span class="line">                                 batch_size=Config.batch_size,</span><br><span class="line">                                 shuffle=<span class="literal">True</span>,</span><br><span class="line">                                 num_workers=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    test_loader = Data_set(test_array, test_lable)</span><br><span class="line">    test_dataloader = DataLoader(test_loader,</span><br><span class="line">                                 batch_size=Config.batch_size,</span><br><span class="line">                                 shuffle=<span class="literal">True</span>,</span><br><span class="line">                                 num_workers=<span class="number">0</span>)</span><br><span class="line">    w2vec=build_word2vec(Config.pre_word2vec_path,word2id,<span class="literal">None</span>)<span class="comment">#生成word2vec</span></span><br><span class="line">    w2vec=torch.from_numpy(w2vec)</span><br><span class="line">    w2vec=w2vec.<span class="built_in">float</span>()<span class="comment">#CUDA接受float32，不接受float64</span></span><br><span class="line">    model=LSTM_attention(Config.vocab_size,Config.embedding_dim,w2vec,Config.update_w2v,</span><br><span class="line">                    Config.hidden_dim,Config.num_layers,Config.drop_keep_prob,Config.n_class,Config.bidirectional)</span><br><span class="line"></span><br><span class="line"><span class="comment">#训练</span></span><br><span class="line">    train(train_dataloader,model=model,device=device,epoches=Config.n_epoch,lr=Config.lr)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#保存模型</span></span><br><span class="line">    <span class="keyword">if</span> os.path.exists(Config.model_state_dict_path) == <span class="literal">False</span>:</span><br><span class="line">           os.mkdir(Config.model_state_dict_path)</span><br><span class="line">    torch.save(model, Config.model_state_dict_path)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><figcaption><span>Sentiment_Analysis_eval.py</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> unicode_literals, print_function, division</span><br><span class="line"><span class="keyword">from</span> io <span class="keyword">import</span> <span class="built_in">open</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> confusion_matrix,f1_score,recall_score,precision_score</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">from</span> Sentiment_model <span class="keyword">import</span> LSTMModel,LSTM_attention</span><br><span class="line"><span class="keyword">from</span> Sentiment_Analysis_Config <span class="keyword">import</span> Config</span><br><span class="line"><span class="keyword">from</span> Sentiment_Analysis_DataProcess <span class="keyword">import</span> prepare_data,build_word2vec,text_to_array_nolable,Data_set</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">val_accuary</span>(<span class="params">model,val_dataloader,device,criterion</span>):</span><br><span class="line">    model = model.to(device)</span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        correct1 = <span class="number">0</span></span><br><span class="line">        total1 = <span class="number">0</span></span><br><span class="line">        val_loss=<span class="number">0.0</span></span><br><span class="line">        <span class="keyword">for</span> j, data_1 <span class="keyword">in</span> (<span class="built_in">enumerate</span>(val_dataloader, <span class="number">0</span>)):</span><br><span class="line">            input1, target1 = data_1[<span class="number">0</span>], data_1[<span class="number">1</span>]</span><br><span class="line">            input1= input1.<span class="built_in">type</span>(torch.LongTensor)</span><br><span class="line">            target1 = target1.<span class="built_in">type</span>(torch.LongTensor)</span><br><span class="line">            target1=target1.squeeze(<span class="number">1</span>)<span class="comment">#从[64,1]到[64]</span></span><br><span class="line">            input1 = input1.to(device)</span><br><span class="line">            target1 = target1.to(device)</span><br><span class="line">            output1 = model(input1)</span><br><span class="line">            loss1 = criterion(output1, target1)</span><br><span class="line">            val_loss += loss1.item()</span><br><span class="line">            _, predicted1 = torch.<span class="built_in">max</span>(output1, <span class="number">1</span>)</span><br><span class="line">            total1 += target1.size(<span class="number">0</span>)<span class="comment"># 此处的size()类似numpy的shape: np.shape(train_images)[0]</span></span><br><span class="line">            correct1 += (predicted1 == target1).<span class="built_in">sum</span>().item()</span><br><span class="line">            F1 = f1_score(target1.cpu(), predicted1.cpu(), average=<span class="string">&#x27;weighted&#x27;</span>)</span><br><span class="line">            Recall = recall_score(target1.cpu(), predicted1.cpu(), average=<span class="string">&#x27;micro&#x27;</span>)</span><br><span class="line">            <span class="comment">#CM = confusion_matrix(target1.cpu(), predicted1.cpu())</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;\nVal accuracy : &#123;:.3f&#125;%,val_loss:&#123;:.3f&#125;, F1_score：&#123;:.3f&#125;%, Recall：&#123;:.3f&#125;%&#x27;</span>.<span class="built_in">format</span>(<span class="number">100</span>*correct1/total1,val_loss,<span class="number">100</span>*F1,<span class="number">100</span>*Recall))</span><br><span class="line">        <span class="keyword">return</span> <span class="number">100</span>*correct1/total1</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">test_accuary</span>(<span class="params">model,test_dataloader,device</span>):</span><br><span class="line">    model = model.to(device)</span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        correct = <span class="number">0</span></span><br><span class="line">        total = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> k, data_test <span class="keyword">in</span> (<span class="built_in">enumerate</span>(test_dataloader, <span class="number">0</span>)):</span><br><span class="line">            input_test, target_ = data_test[<span class="number">0</span>], data_test[<span class="number">1</span>]</span><br><span class="line">            input_test= input_test.<span class="built_in">type</span>(torch.LongTensor)</span><br><span class="line">            target_ = target_.<span class="built_in">type</span>(torch.LongTensor)</span><br><span class="line">            target_=target_.squeeze(<span class="number">1</span>)<span class="comment">#从[64,1]到[64]</span></span><br><span class="line">            input_test = input_test.to(device)</span><br><span class="line">            target_ = target_.to(device)</span><br><span class="line">            output2 = model(input_test)</span><br><span class="line">            _, predicted_test = torch.<span class="built_in">max</span>(output2, <span class="number">1</span>)</span><br><span class="line">            total += target_.size(<span class="number">0</span>)<span class="comment"># 此处的size()类似numpy的shape: np.shape(train_images)[0]</span></span><br><span class="line">            correct += (predicted_test == target_).<span class="built_in">sum</span>().item()</span><br><span class="line">            F1 = f1_score(target_.cpu(), predicted_test.cpu(), average=<span class="string">&#x27;weighted&#x27;</span>)</span><br><span class="line">            Recall = recall_score(target_.cpu(), predicted_test.cpu(), average=<span class="string">&#x27;micro&#x27;</span>)</span><br><span class="line">            CM = confusion_matrix(target_.cpu(), predicted_test.cpu())</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;test accuracy : &#123;:.3f&#125;%, F1_score：&#123;:.3f&#125;%, Recall：&#123;:.3f&#125;%,Confusion_matrix：&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(<span class="number">100</span>*correct/total,<span class="number">100</span>*F1,<span class="number">100</span>*Recall,CM))</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">pre</span>(<span class="params">word2id,model,seq_lenth ,path</span>):</span><br><span class="line">    model.cpu()</span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        input_array=text_to_array_nolable(word2id,seq_lenth,path)</span><br><span class="line">        <span class="comment">#sen_p = sen_p.type(torch.LongTensor)</span></span><br><span class="line">        sen_p = torch.from_numpy(input_array)</span><br><span class="line">        sen_p=sen_p.<span class="built_in">type</span>(torch.LongTensor)</span><br><span class="line">        output_p = model(sen_p)</span><br><span class="line">        _, pred = torch.<span class="built_in">max</span>(output_p, <span class="number">1</span>)</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> pred:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&#x27;预测类别为&#x27;</span>,i.item())</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    splist = []</span><br><span class="line">    word2id = &#123;&#125;</span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(Config.word2id_path, encoding=<span class="string">&#x27;utf-8&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">        <span class="keyword">for</span> line <span class="keyword">in</span> f.readlines():</span><br><span class="line">            sp = line.strip().split()  <span class="comment"># 去掉\n \t 等</span></span><br><span class="line">            splist.append(sp)</span><br><span class="line">        word2id = <span class="built_in">dict</span>(splist)  <span class="comment"># 转成字典</span></span><br><span class="line">    <span class="keyword">for</span> key <span class="keyword">in</span> word2id:  <span class="comment"># 将字典的值，从str转成int</span></span><br><span class="line">        word2id[key] = <span class="built_in">int</span>(word2id[key])</span><br><span class="line"></span><br><span class="line">    device = torch.device(<span class="string">&quot;cuda&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span>)</span><br><span class="line">    train_array, train_lable, val_array, val_lable, test_array, test_lable = prepare_data(word2id,</span><br><span class="line">                                                                                          train_path=Config.train_path,</span><br><span class="line">                                                                                          val_path=Config.val_path,</span><br><span class="line">                                                                                          test_path=Config.test_path,seq_lenth=Config.max_sen_len)</span><br><span class="line">    test_loader = Data_set(test_array, test_lable)</span><br><span class="line">    test_dataloader = DataLoader(test_loader,</span><br><span class="line">                                 batch_size=Config.batch_size,</span><br><span class="line">                                 shuffle=<span class="literal">True</span>,</span><br><span class="line">                                 num_workers=<span class="number">0</span>)</span><br><span class="line">    w2vec = build_word2vec(Config.pre_word2vec_path,</span><br><span class="line">                           word2id,</span><br><span class="line">                          <span class="literal">None</span>)  <span class="comment"># 生成word2vec</span></span><br><span class="line">    w2vec = torch.from_numpy(w2vec)</span><br><span class="line">    w2vec = w2vec.<span class="built_in">float</span>()  <span class="comment"># CUDA接受float32，不接受float64</span></span><br><span class="line"></span><br><span class="line">    model=LSTM_attention(Config.vocab_size,Config.embedding_dim,w2vec,Config.update_w2v,</span><br><span class="line">                        Config.hidden_dim,Config.num_layers,Config.drop_keep_prob,Config.n_class,Config.bidirectional)</span><br><span class="line">    <span class="comment"># 读取模型</span></span><br><span class="line">    <span class="comment">#model1 = torch.load(Config.model_state_dict_path)</span></span><br><span class="line">    model = torch.load(<span class="string">&#x27;./word2vec_data/sen_model_best.pkl&#x27;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">#model.load_state_dict(torch.load(Config.model_state_dict_path)) #仅保存参数</span></span><br><span class="line">    <span class="comment">#验证</span></span><br><span class="line">    <span class="comment">#val_accuary(model1, val_dataloader, device)</span></span><br><span class="line">    <span class="comment">#测试</span></span><br><span class="line">    test_accuary(model,test_dataloader,device)</span><br><span class="line">    <span class="comment">#预测</span></span><br><span class="line">    pre(word2id,model,Config.max_sen_len,Config.pre_path)</span><br></pre></td></tr></table></figure>
<h1 id="参考文献">参考文献</h1>
<p><span class="exturl" data-url="aHR0cHM6Ly9naXRlZS5jb20vaHUteWFuZ2dhbmcvU2VudGltZW50LUFuYWx5c2lzLUNoaW5lc2UtcHl0b3JjaCMlRTUlOEUlOUYlRTUlODglOUIlRTQlQjglOEQlRTYlOTglOTMlRTUlQTYlODIlRTYlOUUlOUMlRTUlQTUlQkQlRTclOTQlQTglRTglQUYlQjclRTclQkIlOTklRTQlQjglQUFzdGFyJUU4JUIwJUEyJUU4JUIwJUEyJUU0JUJBJTg2LSVFNCVCRCU5QyVFOCU4MCU4NSVFNiU5RCU4RSVFNyU4QiU5NyVFNSU5NyVBOA==">Sentiment-Analysis-Chinese-pytorch<i class="fa fa-external-link-alt"></i></span></p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://soundmemories.github.io/2021/06/25/Paper/02.XGBoost%20A%20Scalable%20Tree%20Boosting%20System/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avstar.png">
      <meta itemprop="name" content="SoundMemories">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="SoundMemories">
      <meta itemprop="description" content="今日事，今日毕">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | SoundMemories">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2021/06/25/Paper/02.XGBoost%20A%20Scalable%20Tree%20Boosting%20System/" class="post-title-link" itemprop="url">XGBoost A Scalable Tree Boosting System</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2021-06-25 00:00:00" itemprop="dateCreated datePublished" datetime="2021-06-25T00:00:00+08:00">2021-06-25</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Paper/" itemprop="url" rel="index"><span itemprop="name">Paper</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>4.7k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>17 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="摘要">摘要</h1>
<p>提出名为XGBoost的树提升系统。<br />
提出一种新颖的稀疏数据感知算法用于稀疏数据，一种带权值的分位数略图(weighted
quantile sketch) 来近似实现树的学习。<br />
提出有关缓存访问模式，数据压缩和分片的见解，以构建有延展性的提升树系统。</p>
<h1 id="导读">导读</h1>
<p>机器学习方法里，GradientTree
Boosting（GBDT）是一个在很多应用里都很出彩的技术。提升树方法在很多有标准分类基准的情况下表现很出色。本文提出了一个可扩展的提升树机器学习系统（XGBoost）。XGBoost在2015年的29场比赛获胜队伍中，有17个都使用了XGBoost。</p>
<p>主要贡献：<br />
1、设计和构建高度可扩展的端到端提升树系统。（树的个数能灵活的增加或减少）<br />
2、提出了一个理论上合理的加权分位法。（推荐分割点的时候用，能不用遍历所有的点，只用部分点就行）<br />
3、引入了一种新颖的稀疏感知算法用于并行树学习。（令缺失值有默认方向，稀疏数据处理方法和并行计算）<br />
4、提出了一个有效的用于核外树形学习的缓存感知块结构。（有效使用缓存块处理数据）</p>
<h1 id="内容">内容</h1>
<h2 id="regularized-learning-objective">Regularized Learning
Objective</h2>
<p>给定一个数据集<span class="math inline">\(\mathcal{D}\)</span>，<span
class="math inline">\(n\)</span>个样本，每个样本有<span
class="math inline">\(m\)</span>个特征：<br />
<span class="math display">\[
\mathcal{D}= \{(x_i,y_i)\}(|\mathcal{D}|= n,x_i\in \Bbb{R}^m, y_i\in
\Bbb{R})
\]</span><br />
第 <span class="math inline">\(k\)</span> 棵树对于输入 <span
class="math inline">\(x_i\)</span> 的样本预测结果为<span
class="math inline">\(f_k(x_i)\)</span>：<br />
<span class="math display">\[
\hat{y}_i=\varnothing(x_i)=\sum\limits_{k=1}^{K}f_k(x_i),\quad f_k\in
\mathcal{F}
\]</span><br />
其中，<span class="math inline">\(\mathcal{F}=
\{f(x)=w_{q(x)}\}(q:\Bbb{R}^m\to T,w\in \Bbb{R}^T)\)</span>
是CART回归树。<span class="math inline">\(q(x)\)</span> 表示映射样本
<span class="math inline">\(x\)</span> 到叶子节点的下标。<span
class="math inline">\(T\)</span>是叶子节点数量。<span
class="math inline">\(w\)</span> 是叶子节点最优解（<span
class="math inline">\(w_i\)</span>表示第<span
class="math inline">\(i\)</span>个叶子节点的最优解）。每一棵树都有独立的
<span class="math inline">\(q\)</span> 和$ w$。</p>
<h2 id="gradient-tree-boosting">Gradient Tree Boosting</h2>
<p>以上定义了树模型的预测函数，那么接下来定义整个损失函数：<br />
<span class="math display">\[
\mathcal{L}(\varnothing)= \sum\limits_i l(\hat{y}_i,y_i)+\sum\limits_k
\Omega(f_k)\\
\]</span></p>
<p><span class="math display">\[
\text{where} \quad \Omega( f)=\gamma  T + \frac{1}{2}\lambda||w||^2
\]</span></p>
<p>其中，<span class="math inline">\(l\)</span>
函数是一个可导的凸函数，用来表示预测值 <span
class="math inline">\(\hat{y}_i\)</span> 和真实值 <span
class="math inline">\(y_i\)</span> 之间的差异。<span
class="math inline">\(\Omega\)</span>
是是惩罚项（正则化），用来防止树的结构过于复杂。<br />
损失函数 <span class="math inline">\(\mathcal{L}\)</span>
参数中包含了函数，所以不能用传统的优化算法来优化。假设 <span
class="math inline">\(\hat{y}_i^{t}\)</span> 是 <span
class="math inline">\(x_i\)</span> 在第 <span
class="math inline">\(t\)</span> 次迭代中的预测值。那么则有：<br />
<span class="math display">\[
\mathcal{L}^{( t)}= \sum\limits_{i=
1}^{n}l(y_i,\hat{y}_i^{(t-1)}+f_t(x_i))+\Omega(f_t)
\]</span></p>
<p>把 <span class="math inline">\(y_i,\hat{y}_i^{(t-1)}\)</span> 看成
<span class="math inline">\(x\)</span> ，把 <span
class="math inline">\(f_t(x_i)\)</span> 看成 <span
class="math inline">\(\Delta x\)</span>
，对上式近似为二阶泰勒级展开：<br />
<span class="math display">\[
\mathcal{L}^{( t)}\simeq \sum\limits_{i=
1}^{n}[l(y_i,\hat{y}_i^{(t-1)})+g_if_t(x_i)+\frac{1}{2} h_if_t^{
2}(x_i)]+\Omega(f_t)
\]</span></p>
<p><span class="math display">\[
\text{where}
\quad  g_i=\partial_{\hat{y}^{(t-1)}}l(y_i,\hat{y}_i^{(t-1)})\quad
\text{and} \quad h_i=\partial_{\hat{y}^{(t-1)}}^{
2}l(y_i,\hat{y}_i^{(t-1)})
\]</span></p>
<p>其中，<span class="math inline">\(g_i\)</span>和<span
class="math inline">\(h_i\)</span>是一阶偏导和二阶偏导。由于<span
class="math inline">\(l(y_i,\hat{y}_i^{(t-1)})\)</span>为常数项，可以去除。<br />
定义 <span class="math inline">\(I_j=\{i|q(x_i)=j\}\)</span> 作为样本
<span class="math inline">\(x_i\)</span> 被分割到第 <span
class="math inline">\(j\)</span>
个叶子节点下的样本下标集合（样本集合）。那么上面公式可以由<strong>对每棵树的样本求和</strong>转成<strong>对每棵树的叶子节点求和</strong>：<br />
<span class="math display">\[
\begin{aligned}
\tilde{\mathcal{L}}^{( t)}&amp;= \sum\limits_{i=
1}^{n}[g_if_t(x_i)+\frac{1}{2} h_if_t^{ 2}(x_i)]+\Omega(f_t)\\
&amp;= \sum\limits_{i= 1}^{n}[g_if_t(x_i)+\frac{1}{2} h_if_t^{
2}(x_i)]+\gamma  T + \frac{1}{2}\lambda \sum\limits_{ j= 1}^{T}w_j^{
2}\\
&amp;= \sum\limits_{i= 1}^{n}[g_iw_{q(x_i)}+\frac{1}{2}
h_i(w_{q(x_i)})^{ 2}]+\gamma  T + \frac{1}{2}\lambda \sum\limits_{ j=
1}^{T}w_j^{ 2}\\
&amp;= \sum\limits_{j= 1}^{T}[\sum\limits_{i\in
I_j}g_iw_{j}+\frac{1}{2}\sum\limits_{i\in  I_j} h_i(w_{j})^{
2}]+\gamma  T + \frac{1}{2}\lambda \sum\limits_{ j= 1}^{T}w_j^{ 2}\\
&amp;= \sum\limits_{j= 1}^{T}[(\sum\limits_{i\in
I_j}g_i)w_{j}+\frac{1}{2}(\sum\limits_{i\in  I_j} h_i+\lambda)w_{j}^{
2}]+\gamma  T
\end{aligned}
\]</span></p>
<p><strong>1、如何求出每个叶子节点的最优解？</strong><br />
对上式求极值点（它是凸函数，求的是极小值），即对 <span
class="math inline">\(w_j\)</span>
求一阶导数等于零（也可以看成二元一次方程求解），解得：<br />
<span class="math display">\[
w_j^*=-\frac{\sum_{i\in I_j}g_i}{\sum_{i\in I_j}h_i+\lambda}
\]</span><br />
带入 <span class="math inline">\(\tilde{\mathcal{L}}^{( t)}\)</span>
求得最优值（最小值）：<br />
<span class="math display">\[
\tilde{\mathcal{L}}^{( t)}=-\frac{1}{2}\sum\limits_{j=
1}^{T}\frac{(\sum_{i\in I_j}g_i)^{ 2}}{\sum_{i\in
I_j}h_i+\lambda}+\gamma  T
\]</span><br />
可以用这个公式来来衡量决策树的质量。有点像决策树信息熵一个道理。</p>
<p><strong>2、对当前决策树做子树分裂时，如何选择哪个特征和特征值进行分裂，使损失函数最小？</strong><br />
如果想要划分后损失函数得到最小值，意味这在做每次划分的时候，要尽量保证划分后的score比划分前的score要更小。那么应该找到
(划分前的score - 划分后的score)
这个差最大的切分点作为我们这一次的划分点。假设 <span
class="math inline">\(I_L\)</span>和 <span
class="math inline">\(I_R\)</span> 为一个节点 <span
class="math inline">\(I\)</span> 划分后的左子集和右子集，节点 <span
class="math inline">\(I=I_L \cup I_R\)</span> ，则得到以下公式：<br />
<span class="math display">\[
\begin{aligned}
\mathcal{L}_{split}&amp;=- \dfrac{1}{2}[\frac{(\sum_{i\in I}g_i)^{
2}}{\sum_{i\in I}h_i+\lambda}-\frac{(\sum_{i\in I_L}g_i)^{
2}}{\sum_{i\in I_L}h_i+\lambda}-\frac{(\sum_{i\in I_R}g_i)^{
2}}{\sum_{i\in I_R}h_i+\lambda}]-\gamma\\
&amp;= \dfrac{1}{2}  [\frac{(\sum_{i\in I_L}g_i)^{ 2}}{\sum_{i\in
I_L}h_i+\lambda}+\frac{(\sum_{i\in I_R}g_i)^{ 2}}{\sum_{i\in
I_R}h_i+\lambda}-\frac{(\sum_{i\in I}g_i)^{ 2}}{\sum_{i\in
I}h_i+\lambda}]-\gamma
\end{aligned}
\]</span><br />
可以用这个公式来衡量是否当前节点是否再应该继续划分下去。每次用不同的特征，计算分数，然后用最大的值那个特征，作为当前树节点的划分点。</p>
<p>相对于GBDT，XGBoost一次性求解出<strong>最优解叶子节点区域</strong>和<strong>每个叶子节点区域最优解</strong>。而GBDT是基于残差（一阶泰勒）拟合一颗CART书，得到<strong>最优叶子节点区域</strong>，再求出<strong>每个叶子节点区域最优解</strong>。</p>
<h2 id="shrinkage-and-column-subsampling">Shrinkage and Column
Subsampling</h2>
<p>除了之前提过的添加正则化的项来防止模型的过拟合之外，还可以用两种方式来防止过拟合：<br />
（1）添加类似梯度下降优化问题中的学习率 <span
class="math inline">\(\eta\)</span>
，这个可以收缩每棵树的权重，让每棵树的生长更加稳定。<br />
（2）对样本的特征子采样（随机森林用到过）。每次生成树的时候，只用其中一部分抽样的特征。这样子也能降低过拟合的风险。</p>
<p><strong>精准的贪心算法</strong><br />
贪心算法就是每次都希望找到最优的结果，但这样需每次都遍历所有的特征，对每个特征，又遍历所有划分的可能。然后通过
$ _{split}$
的分数，计算每次划分后的score，取最大的score的对应的特征来进行划分。<br />
<img src="/images/XGB/01.png" width="60%"></p>
<p><strong>近似的贪心算法</strong><br />
用上面贪心算法来寻找最佳划分点，准确度非常不错，但是时间复杂度和空间复杂度都太高了，特别是对于连续值的变量来说，简直是一个大灾难。作者提出一种近似法分位法，先对数据进行分桶(Bucket)，然后桶内的数据相加起来，作为一个代表来进行计算。</p>
<p>那我们应该在什么时候对数据进行分桶呢？有两种方式。<br />
（1）全局分桶（Global
Bucket），可以一开始就对全部的数据进行分桶。后面进行划分的时候只需要使用分桶数据就可以了。<br />
（2）局部分桶（Local Bucket），每次需要对当前leaf
node进行划分的时候，对当前节点里面的数据进行分桶。然后再划分。当然这个时间复杂度也会变的比较高。</p>
<p>具体划分的伪代码如下：<br />
（1）先计算1到M个特征，找出每个特征的分桶的候选点。<br />
（2）然后将候选点之间的数据的g和h求和，装入到Bucket里面，代表这些数据。<br />
（3）后面流程就跟精确的弹性分割算法一样。只是将每个Bucket看成一个x。<br />
<img src="/images/XGB/02.png" width="60%"></p>
<p>下面是作者测试对几种不同的切分算法的AUC结果比较图。可以看得出，当eps=0.05，也就是将数据分成20个Bucket的时候，AUC的分数跟精准的贪心算法一样。<br />
<img src="/images/XGB/03.png" width="60%"></p>
<h2 id="split-finding-algorithms">SPLIT FINDING ALGORITHMS</h2>
<p>提出一种新的分桶的方法<strong>Weighted Quantile
Sketch</strong>（加权分位法）。</p>
<p><strong>加权分位法</strong><br />
上面我们讨论了对数据分成Bucket，再来计算他的节点的split分数。来减少我们的计算量。那么我们如何对数据分桶，才能够比较合理呢？</p>
<p>作者按照对loss的影响权重来进行分桶，让数据分桶后，每个桶对loss的影响权重相同。<br />
定义一个数据集 <span
class="math inline">\(\mathcal{D}_k=\{(x_{1k},h_1),(x_{2k},h_2),...,(x_{nk},h_n)\}\)</span>
，<span class="math inline">\(k\)</span> 为样本 <span
class="math inline">\(x\)</span> 的特征数。<br />
定义一个排序函数 <span class="math inline">\(r_k:\Bbb{R}\sim
[0,\infty)\)</span>，则：<br />
<span class="math display">\[
r_k(z)=\frac{1}{\sum_{(x,h)\in \mathcal{D}_k}h}\sum_{(x,h)\in
\mathcal{D}_k,x&lt;z}h
\]</span><br />
切分点集合 <span class="math inline">\(\{s_{k1},
s_{k2},...,s_{kl}\}\)</span>，满足：<br />
<span class="math display">\[
|r_k(s_{k,j})-r_k(s_{k,j+1})|&lt;\epsilon, \quad s_{k1}=\min\limits_i
x_{ik},s_{kl}=\max\limits_i x_{ik}
\]</span><br />
这里 <span class="math inline">\(\epsilon\)</span>
用来衡量划分区间的大小（每个Bucket不能太大，以免错过最优切分点）。这个意味这数据集大约被分为
<span class="math inline">\(\frac{1}{\epsilon}\)</span> 个候选点。</p>
<p>那么 <span class="math inline">\(h_i\)</span> 为什么能够代表 <span
class="math inline">\(x_i\)</span> 的权重来进行排序呢？将公式 <span
class="math inline">\({\tilde{\mathcal{L}}}^{(t)}\)</span> 对 <span
class="math inline">\(h_i\)</span> 进行提取，可得到：<br />
<span class="math display">\[
\begin{aligned}
\tilde{\mathcal{L}}^{( t)}&amp;= \sum\limits_{i=
1}^{n}[g_if_t(x_i)+\frac{1}{2} h_if_t^{ 2}(x_i)]+\Omega(f_t)\\
&amp;= \sum\limits_{i= 1}^{n}[\frac{1}{2} h_i\frac{ 2*
g_if_t(x_i)}{h_i}+\frac{1}{2} h_if_t^{ 2}(x_i)]+\Omega(f_t)\\
&amp;= \sum\limits_{i= 1}^{n}\frac{1}{2} h_i[ 2\times
\frac{g_i}{h_i}f_t(x_i)+f_t^{ 2}(x_i)]+\Omega(f_t)\\
&amp;= \sum\limits_{i= 1}^{n}\frac{1}{2} h_i[ 2\times
\frac{g_i}{h_i}f_t(x_i)+f_t^{
2}(x_i)+(\frac{g_i}{h_i})^2-(\frac{g_i}{h_i})^2]+\Omega(f_t)\\
&amp;= \sum\limits_{i= 1}^{n}\frac{1}{2} h_i(f_t(x_i)+\frac{g_i}{h_i})^{
2}+\Omega(f_t)\\
&amp;= \sum\limits_{i= 1}^{n}\frac{1}{2}
h_i(f_t(x_i)-(-\frac{g_i}{h_i}))^{ 2}+\Omega(f_t)+\text{constant}
\end{aligned}
\]</span><br />
发现 <span class="math inline">\(h_i\)</span>
对结果影响最大，所以用它来进行排序。</p>
<h2 id="sparsity-aware-split-finding">Sparsity-aware Split Finding</h2>
<p>对稀疏数据的处理，重点是针对特征为空时的处理，对<strong>精准的贪心算法</strong>做了改进：<br />
<img src="/images/XGB/04.png" width="60%"></p>
<p>简单来讲，通过两轮遍历可以确保稀疏值位于左子树和右子树的情形，就是对该特征划分为左节点还是右节点做了一次比较，哪个效果好就把它放在哪。</p>
<h2 id="system-design">SYSTEM DESIGN</h2>
<p>重点是优化：<br />
（1）<strong>预排序</strong>：这个算法大量时间消耗在排序上。只需在最开始对每个特征排一次序即可。<br />
这里XGB将所有的列数据都预先排了序。以压缩形式分别存到block里，不同的block可以分布式存储，甚至存到硬盘里。在特征选择的时候，可以并行的处理这些列数据，XGB就是在这实现的并行化，用多线程来实现加速。同时这里还用cache加了一个底层优化：当数据排序后，索引值是乱序的，可能指向了不同的内存地址，找的时候数据是不连续的，这里加了个缓存，让以后找的时候能找到小批量的连续地址，以实现加速！这里是在每个线程里申请了一个internal
buffer来实现的！这个优化在小数据下看不出来，数据越多越明显。</p>
<p>（2）<strong>预取</strong>：尽可能的把数据保存在缓存中，这样不用去磁盘进行读取，减少时间开销。并且给出了对比结果：<br />
<img src="/images/XGB/05.png" width="100%"></p>
<p>（3）内存块的大小也会影响缓存速率。<br />
<img src="/images/XGB/06.png" width="50%"></p>
<p>针对磁盘存储优化。磁盘block不大的情况下：<br />
1.把block数据进行压缩，让它没有那么大。<br />
2.把block数据放在多个磁盘，增大磁盘带宽，让读取速度更。</p>
<h1 id="信息论">信息论</h1>
<h2 id="熵">熵</h2>
<p>如果<span
class="math inline">\(\textbf{X}\)</span>是一个离散型随机变量，取空间值为<span
class="math inline">\(\Bbb{R}\)</span>，其概率分布为<span
class="math inline">\(p(x)=P(\textbf{X}=x),x\in\Bbb{R}\)</span>。那么，<span
class="math inline">\(\textbf{X}\)</span>的熵<span
class="math inline">\(H(\textbf{X})\)</span>定义为：<br />
<span class="math display">\[
H(\textbf{X})=-\sum\limits_{x\in\Bbb{R}} p(x) log p(x)
\]</span><br />
其中，<span class="math inline">\(H(\textbf{X})\)</span>可以写成<span
class="math inline">\(H( p)\)</span>。</p>
<p><strong>熵又称为子信息（self-information），可以视为描述一个随机变量的不确定性的数量</strong>。它表示信源<span
class="math inline">\(\textbf{X}\)</span>每发一个符号（不论发什么符号）所提供的平均信息量。<strong>一个随机变量的熵越大，它的不确定性越大，那么，正确估计其值的可能性就越小。越不确定的随机变量越需要大的信息量用以确定其值。</strong></p>
<p><strong>熵定义了随机变量的不确定性，当熵最大时，随机变量最不确定，最难准确地预测其行为。也就是说，在已知部分知识的前提下，关于未知分布最合理的推断应该是符合已知知识最不确定或最大随机的推断。</strong></p>
<h2 id="联合熵和条件熵">联合熵和条件熵</h2>
<p>如果<span
class="math inline">\(\textbf{X},\textbf{Y}\)</span>是一对离散型随机变量<span
class="math inline">\(\textbf{X},\textbf{Y}\sim p(x,y)\)</span>，<span
class="math inline">\(\textbf{X},\textbf{Y}\)</span>的联合熵（joint
entropy）<span
class="math inline">\(H(\textbf{X},\textbf{Y})\)</span>定义为：<br />
<span class="math display">\[
H(\textbf{X},\textbf{Y})=-\sum\limits_{ x\in\textbf{X}}\sum\limits_{
y\in\textbf{Y}} p(x,y) log p(x,y)
\]</span><br />
联合熵实际上就是描述一对随机变量平均所需要的信息量。</p>
<p>给定随机变量<span class="math inline">\(\textbf{X}\)</span>的情况下，
随机变量<span
class="math inline">\(\textbf{Y}\)</span>的条件熵（conditionalentropy）：<br />
<span class="math display">\[
\begin{aligned}
H(\textbf{Y}|\textbf{X})&amp;=\sum\limits_{ x\in\textbf{X}} p(x)
H(\textbf{Y}|\textbf{X}= x)\\
&amp;=\sum\limits_{ x\in\textbf{X}} p(x)[-\sum\limits_{ y\in\textbf{Y}}
p(y|x) log p(y|x)]\\
&amp;=-\sum\limits_{ x\in\textbf{X}}\sum\limits_{ y\in\textbf{Y}} p(x,y)
log p(y|x)
\end{aligned}
\]</span><br />
将其中的联合概率<span class="math inline">\(log
p(x,y)\)</span>展开，可得：<br />
<span class="math display">\[
\begin{aligned}
H(\textbf{X},\textbf{Y})&amp;=-\sum\limits_{
x\in\textbf{X}}\sum\limits_{ y\in\textbf{Y}} p(x,y) log[ p(x)p(y|x)]\\
&amp;=-\sum\limits_{ x\in\textbf{X}}\sum\limits_{ y\in\textbf{Y}} p(x,y)
[log p(x) + log p(y|x)]\\
&amp;=-\sum\limits_{ x\in\textbf{X}}\sum\limits_{ y\in\textbf{Y}} p(x,y)
log p(x)-\sum\limits_{ x\in\textbf{X}}\sum\limits_{ y\in\textbf{Y}}
p(x,y) log p(y|x)\\
&amp;=-\sum\limits_{ x\in\textbf{X}} p(x) log p(x)-\sum\limits_{
x\in\textbf{X}}\sum\limits_{ y\in\textbf{Y}} p(x,y) log p(y|x)\\
&amp;=H(\textbf{X})+H(\textbf{Y}|\textbf{X})
\end{aligned}
\]</span><br />
我们称上式为熵的连锁规则。推广到一般情况，有：<br />
<span class="math display">\[
H(\textbf{X}_1,\textbf{X}_2,...,\textbf{X}_n)=H(\textbf{X}_1)+H(\textbf{X}_2|\textbf{X}_1)+...+H(\textbf{X}_n|\textbf{X}_1,\textbf{X}_2,...,\textbf{X}_{n-1})
\]</span></p>
<h2 id="互信息">互信息</h2>
<p>根据熵的连锁规则， 有：<br />
<span class="math display">\[
H(\textbf{X},\textbf{Y})=H(\textbf{X})+H(\textbf{Y}|\textbf{X})=H(\textbf{Y})+H(\textbf{X}|\textbf{Y})
\]</span><br />
因此：<br />
<span class="math display">\[
H(\textbf{X})-H(\textbf{X}|\textbf{Y})=H(\textbf{Y})-H(\textbf{Y}|\textbf{X})
\]</span><br />
这个差叫做<span class="math inline">\(\textbf{X}\)</span>和<span
class="math inline">\(\textbf{Y}\)</span>的互信息（mutual information,
MI），记作<span
class="math inline">\(I(\textbf{X};\textbf{Y})\)</span>。<br />
或者定义为：如果<span class="math inline">\((\textbf{X},\textbf{Y})\sim
p(x,y)\)</span>，则<span
class="math inline">\(\textbf{X},\textbf{Y}\)</span>之间的互信息<span
class="math inline">\(I(\textbf{X};\textbf{Y})=H(\textbf{X})-H(\textbf{X}|\textbf{Y})\)</span>。</p>
<p><strong><span
class="math inline">\(I(\textbf{X};\textbf{Y})\)</span>反映的是在知道了<span
class="math inline">\(\textbf{Y}\)</span>的值以后<span
class="math inline">\(\textbf{X}\)</span>的不确定性的减少量。可以理解为<span
class="math inline">\(\textbf{Y}\)</span>的值透露了多少关于<span
class="math inline">\(\textbf{X}\)</span>的信息量。</strong><br />
互信息和熵之间的关系：<br />
<img src="/images/XGB/互信息.png" width="40%"></p>
<p>如果将定义中的<span
class="math inline">\(H(\textbf{X})\)</span>和<span
class="math inline">\(H(\textbf{X}|\textbf{Y})\)</span>展开，可得：<br />
<span class="math display">\[
\begin{aligned}
I(\textbf{X};\textbf{Y})&amp;=H(\textbf{X})-H(\textbf{X}|\textbf{Y})\\
&amp;=H(\textbf{X})+H(\textbf{Y})-H(\textbf{X},\textbf{Y})\\
&amp;=\sum\limits_{ x} p(x) log \frac{1}{p(x)}  + \sum\limits_{ y} p(y)
log \frac{1}{p(y)}  + \sum\limits_{ x,y} p(x,y) log p(x,y) \\
&amp;=\sum\limits_{ x,y} p(x,y) log \frac{p(x,y)}{p(x)p(y)} \\
\end{aligned}
\]</span><br />
由于<span class="math inline">\(H(\textbf{X}|\textbf{X})=0\)</span>，
因此，<br />
<span class="math display">\[
H(\textbf{X})=H(\textbf{X})-H(\textbf{X}|\textbf{X})=I(\textbf{X};\textbf{X})
\]</span><br />
这一方面说明了为什么熵又称为自信息，另一方面说明了两个完全相互依赖的变量之间的互信息并不是一个常量，而是取决于它们的熵。</p>
<p>实际上，互信息体现了两变量之间的依赖程度：<br />
如果<span
class="math inline">\(I(\textbf{X};\textbf{Y})≫0\)</span>，表明<span
class="math inline">\(\textbf{X}\)</span>和<span
class="math inline">\(\textbf{Y}\)</span>是高度相关的；<br />
如果<span
class="math inline">\(I(\textbf{X};\textbf{Y})=0\)</span>，表明<span
class="math inline">\(\textbf{X}\)</span>和<span
class="math inline">\(\textbf{Y}\)</span>是相互独立的；<br />
如果<span
class="math inline">\(I(\textbf{X};\textbf{Y})≪0\)</span>，表明<span
class="math inline">\(\textbf{Y}\)</span>的出现不但未使<span
class="math inline">\(\textbf{X}\)</span>的不确定性减小，反而增大了<span
class="math inline">\(\textbf{X}\)</span>的不确定性，是非常是不利的。</p>
<h2 id="相对熵">相对熵</h2>
<p>相对熵（relative entropy）又称Kullback-Leibler差异（KullbackLeibler
divergence），或简称KL距离，是<strong>衡量相同事件空间里两个概率分布相对差距的测度</strong>。两个概率分布<span
class="math inline">\(p(x)\)</span>和<span
class="math inline">\(q(x)\)</span>的相对熵定义为：<br />
<span class="math display">\[
\begin{aligned}
D( p||q)&amp;=\sum\limits_{ x\in\textbf{X}} p(x) log\frac{ p(x)}{
q(x)}\\
&amp;=\sum\limits_{ x\in\textbf{X}} p(x) log  p(x)  - \sum\limits_{
x\in\textbf{X}} p(x) log q(x)\\
&amp;=-H( p)+H( p,q)
\end{aligned}
\]</span><br />
其中，<span
class="math inline">\(H(p)\)</span>恒不变，只需考虑交叉熵<span
class="math inline">\(H(p,q)\)</span>即可。<span
class="math inline">\(q(x)\)</span> 分布越接近 <span
class="math inline">\(p(x)\)</span>，那么散度值越小。<br />
有时会将KL散度称为KL距离，但它并不满足距离的性质：KL散度不是对称的；KL散度不满足三角不等式。</p>
<h2 id="交叉熵">交叉熵</h2>
<p>根据前面熵的定义，知道熵是一个不确定性的测度，也就是说，我们对于某件事情知道得越多，那么，熵就越小，因而对于试验的结果我们越不感到意外。<strong>交叉熵的概念就是用来衡量估计模型与真实概率分布之间差异情况的。</strong><br />
如果一个随机变量<span class="math inline">\(\textbf{X}\sim
p(x)\)</span>，<span class="math inline">\(q(x)\)</span>为用于近似<span
class="math inline">\(p(x)\)</span>的概率分布，那么，随机变量<span
class="math inline">\(\textbf{X}\)</span>和模型$
p(x)$之间的交叉熵（cross entropy）定义为：<br />
<span class="math display">\[
\begin{aligned}
H(p,q)&amp;=H(p)+D(p||q)\\
&amp;=-\sum\limits_{ x\in\textbf{X}} p(x) log  q(x)\\
\end{aligned}
\]</span></p>
<h1 id="参考文献">参考文献</h1>
<p><span class="exturl" data-url="aHR0cHM6Ly9kbC5hY20ub3JnL2RvaS9wZGYvMTAuMTE0NS8yOTM5NjcyLjI5Mzk3ODU=">XGBoost:
A Scalable Tree Boosting System<i class="fa fa-external-link-alt"></i></span><br />
<span class="exturl" data-url="aHR0cHM6Ly93d3cuY25ibG9ncy5jb20vcGluYXJkL3AvMTA5Nzk4MDguaHRtbA==">XGBoost算法原理小结<i class="fa fa-external-link-alt"></i></span><br />
<span class="exturl" data-url="aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FkYnN6c2ovYXJ0aWNsZS9kZXRhaWxzLzc5NjE1NzEy">XGBoost
论文翻译+个人注释<i class="fa fa-external-link-alt"></i></span><br />
<span class="exturl" data-url="aHR0cHM6Ly96aHVhbmxhbi56aGlodS5jb20vcC84OTU4OTIyMg==">XGBoost论文详解<i class="fa fa-external-link-alt"></i></span><br />
[统计自然语言处理（第二版），宗成庆]<br />
# 扩展<br />
<span class="exturl" data-url="aHR0cHM6Ly9tcC53ZWl4aW4ucXEuY29tL3MvRHk0cnRiX0JqMmI3Q0FCVEVWNFRuUQ==">Xgboost ·
十三问十三答<i class="fa fa-external-link-alt"></i></span></p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://soundmemories.github.io/2021/06/20/NLP/01.%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%92%8C%E8%AF%8D%E5%90%91%E9%87%8F/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avstar.png">
      <meta itemprop="name" content="SoundMemories">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="SoundMemories">
      <meta itemprop="description" content="今日事，今日毕">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | SoundMemories">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2021/06/20/NLP/01.%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%92%8C%E8%AF%8D%E5%90%91%E9%87%8F/" class="post-title-link" itemprop="url">语言模型和词向量</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2021-06-20 00:00:00" itemprop="dateCreated datePublished" datetime="2021-06-20T00:00:00+08:00">2021-06-20</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/NLP/" itemprop="url" rel="index"><span itemprop="name">NLP</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>18k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>1:04</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="语言模型">语言模型</h1>
<p><strong>模型</strong>指的是对事物的数学抽象，那么<strong>语言模型</strong>指的就是对语言现象的数学抽象。准确的讲，给定一个句子
<span class="math inline">\(w\)</span> ，语言模型就是计算句子的出现概率
<span class="math inline">\(p(w)\)</span>
的模型，而统计的对象就是人工标注而成的语料库。</p>
<p>假设构建如下的小型语料库：<br />
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">商品 和 服务</span><br><span class="line">商品 和服 物美价廉</span><br><span class="line">服务 和 货币</span><br></pre></td></tr></table></figure><br />
每个句子出现的概率都是 <span
class="math inline">\(\dfrac{1}{3}\)</span>，因为样本空间为 3 ，这 3
次基数平均分给了 3 个句子，所以它们的概率都为<span
class="math inline">\(\dfrac{1}{3}\)</span>，既然它们的概率之和为 1
，那么其他句子的概率自然为 0 了，这就是语言模型。然而 <span
class="math inline">\(p(w)\)</span>
的计算非常难：句子数量无穷无尽，无法枚举。即便是大型语料库，也只能“枚举”有限的数百万个句子。实际遇到的句子大部分都在语料库之外，意味着它们的概率都被当作
0，这种现象被称为<strong>数据稀疏</strong>。枚举不可行，我们需要一种可计算的、更合理的概率估计方法。</p>
<p>考虑到句子由单词构成，句子无限，单词有限。于是我们从单词构成句子的角度出发去建模句子，把句子表示为单词列表
<span class="math inline">\(\textbf{w}=w_1w_2...w_k\)</span>，每个 <span
class="math inline">\(w_t,
t\in[1,k]\)</span>都是一个单词，然后定义<strong>语言模型</strong>：<br />
<span class="math display">\[
\begin{aligned}
p(\textbf{w})&amp;=p(w_1w_2...w_k)\\&amp;=p(w_1|w_0)\times
p(w_2|w_0w_1)\times...\times p(w_{k+1}|w_0w_1...w_k)\\&amp;=
\prod\limits _{t=1}^{k+1}p(w_{t}|w_0w_1...w_{t-1})
\end{aligned}
\]</span><br />
其中，<span class="math inline">\(w_0\)</span>=BOS（begin of
sentence，或&lt;s&gt;），<span
class="math inline">\(w_{k+1}\)</span>=EOS（end of
sentence，或&lt;/s&gt;），用来标记句子首尾两个特殊“单词”。<br />
也就是说，语言模型模拟说话顺序：给定已经说出口的词语序列，预测下一个词语的后验概率。一个单词一个单词地乘上后验概率，我们就能估计任意一句话的概率。以极大似然估计来计算每个后验概率，即：<br />
<span class="math display">\[
\begin{aligned}
p(w_{t}|w_0w_1...w_{t-1})=p_{MLE}(w_{t}|w_0w_1...w_{t-1})=\dfrac{c(w_0...w_t)}{c(w_0...w_{t-1})}
\end{aligned}
\]</span><br />
其中，<span class="math inline">\(c(w_0...w_t)\)</span>表示<span
class="math inline">\(w_0...w_t\)</span>的计数。</p>
<p>以上面小型语料库为例，计算<span class="math inline">\(p(商品 和
服务)\)</span>出现的概率？<br />
（1）<span class="math inline">\(p(商品|BOS)
=\frac{2}{3}\)</span>（“商品”作为第一个词出现的次数为2，所有单词作为第一个词出现的次数为3）；<br />
（2）<span class="math inline">\(p(和|BOS 商品)
=\frac{1}{2}\)</span>（“BOS 商品 和”出现的次数为1，“BOS
商品”出现的次数为2）；<br />
（3）<span class="math inline">\(p(服务|BOS 商品 和)
=\frac{1}{1}\)</span>（“BOS 商品 和 服务”出现的次数为1，“BOS 商品
和”出现的次数为1）；<br />
（4）<span class="math inline">\(p(EOS|BOS 商品 和 服务)
=\frac{1}{1}\)</span>（“BOS 商品 和 服务 EOS”出现的次数为1，“BOS 商品 和
服务”出现的次数为1）；<br />
整个句子的概率是4者乘积：<span class="math inline">\(p(商品 和
服务)=\frac{2}{3}\times \frac{1}{2}\times \frac{1}{1}\times
\frac{1}{1}=\frac{1}{3}\)</span>。</p>
<p>但是随着句子长度增大，语言模型会遇到如下问题：<br />
（1）<strong>数据稀疏</strong>。指长度越大的句子越难出现，语料库中极有可能统计不到长句子的频次，导致<span
class="math inline">\(p(w_{t}|w_0w_1...w_{t-1})\)</span>为0。<br />
（2）<strong>计算代价大</strong>。t越大，需要存储的<span
class="math inline">\(p(w_{t}|w_0w_1...w_{t-1})\)</span>就越多。</p>
<h2 id="n元语法">n元语法</h2>
<p>为了解决上面两个问题，使用<strong>马尔可夫假设</strong>（Markov
Assumption）来简化语言模型：给定时间线上有一串事件顺序发生，假设每个事件的发生概率只取决于前一个事件，那么这串事件构成的因果链被称作<strong>马尔可夫链</strong>。</p>
<p>在语言模型中，第t个事件指的是<span
class="math inline">\(w_t\)</span>作为第t个单词出现。也就是说，马尔科夫链假设每个单词出现的概率只取决于前一个单词：<br />
<span class="math display">\[
\begin{aligned}
p(w_{t}|w_0w_1...w_{t-1})=p(w_t|w_{t-1})
\end{aligned}
\]</span><br />
基于此假设，需要计算的量一下子减少了不少，由于每次计算只涉及连续两个单词的二元接续，所以此语言模型称为<strong>二元语法</strong>（bi-gram），也叫<strong>一阶马尔科夫链</strong>：<br />
<span class="math display">\[
\begin{aligned}
p(\textbf{w})&amp;=p(w_1w_2...w_k)\\&amp;=p(w_1|w_0)\times
p(w_2|w_1)\times...\times p(w_{k+1}|w_k)\\&amp;=
\prod\limits _{t=1}^{k+1}p(w_{t}|w_{t-1})
\end{aligned}
\]</span></p>
<p>那么根据这个思路推广下，可以得到<strong>n元语法</strong>（n-gram）的定义：每个单词出现的概率，仅取决于该单词之前n个单词，即：<br />
<span class="math display">\[
\begin{aligned}
p(\textbf{w})=\prod\limits_{t=1}^{k+n-1}p(w_{t}|w_{t-(n-1)}...w_{t-1})
\end{aligned}
\]</span><br />
当<span
class="math inline">\(\text{n=1}\)</span>时（即出现在第t位的词<span
class="math inline">\(w_t\)</span>独立于历史时），称为<strong>一元语法</strong>（uni-gram）；当<span
class="math inline">\(\text{n=2}\)</span>时（即出现在第t位的词<span
class="math inline">\(w_t\)</span>仅与它前面的一个历史词<span
class="math inline">\(w_{t-1}\)</span>有关），称为<strong>二元语法</strong>（bi-gram），也叫<strong>一阶马尔科夫链</strong>；当<span
class="math inline">\(\text{n=3}\)</span>时（即出现在第t位的词<span
class="math inline">\(w_t\)</span>仅与它前面的两个历史词<span
class="math inline">\(w_{t-1}w_{t-2}\)</span>有关），称为<strong>三元语法</strong>（tri-gram），也叫<strong>二阶马尔科夫链</strong>。当<span
class="math inline">\(n\geqslant
4\)</span>时数据稀疏和计算代价又变的显著了，实际工程中几乎不使用。另外，深度学习带了一种递归神经网络语言模型（RNN
Language
Model），理论上可以记忆无限个单词，可以看作“无穷元语法”（∞-gram）。</p>
<p>以<strong>二元语法</strong>（bi-gram）为例，计算<span
class="math inline">\(p(商品 和 服务)\)</span>出现的概率？<br />
（1）<span class="math inline">\(p(商品|BOS)
=\frac{2}{3}\)</span>；<br />
（2）<span class="math inline">\(p(和|商品)
=\frac{1}{2}\)</span>；<br />
（3）<span class="math inline">\(p(服务|和)
=\frac{1}{2}\)</span>；<br />
（4）<span class="math inline">\(p(EOS|服务)
=\frac{1}{1}\)</span>；<br />
整个句子的概率是4者乘积：<span class="math inline">\(p(商品 和
服务)=\frac{2}{3}\times \frac{1}{2}\times \frac{1}{2}\times
\frac{1}{1}=\frac{1}{6}\)</span>。</p>
<p>这次的概率比上次的<span
class="math inline">\(\dfrac{1}{3}\)</span>要小一半，剩下的概率到哪里去了呢？来算算语料库之外的新句子<span
class="math inline">\(p(商品 和 货币)\)</span>就知道了：<br />
（1）<span class="math inline">\(p(商品|BOS)
=\frac{2}{3}\)</span>；<br />
（2）<span class="math inline">\(p(和|商品)
=\frac{1}{2}\)</span>；<br />
（3）<span class="math inline">\(p(货币|和)
=\frac{1}{2}\)</span>；<br />
（4）<span class="math inline">\(p(EOS|货币)
=\frac{1}{1}\)</span>；<br />
整个句子的概率是4者乘积：<span class="math inline">\(p(商品 和
货币)=\frac{2}{3}\times \frac{1}{2}\times \frac{1}{2}\times
\frac{1}{1}=\frac{1}{6}\)</span>。<br />
原来剩下的<span
class="math inline">\(\dfrac{1}{6}\)</span>分配给了语料库之外的句子，它们的概率终于不是0了，这样就缓解了一部分数据稀疏的问题。</p>
<h2 id="模型评估困惑度">模型评估：困惑度</h2>
<p>在理想情况下，对两个语言模型A，B进行评估，选定一个特定的任务比如拼写纠错系统，把两个模型A，B都应用在此任务中，最后比较准确率，从而判断A，B的表现。这种评估方法是以应用为中心的度量方法，通过在下游任务中的性能来进行评估。那有没有更简单的评估方法？不需要放在特定的任务中验证？——————<strong>困惑度</strong>（Perplexity）。</p>
<p><strong>困惑度</strong>（Perplexity）是一种信息论测度，用来测量一个概率模型预测样本的好坏，困惑度越低越好。给定一个包含k个词的文本预料<span
class="math inline">\(\textbf{w}=w_1w_2...w_k\)</span>，和一个基于历史行为的语言模型，其预测结果为<span
class="math inline">\(p(\textbf{w})\)</span>，则这个语言模型在这个语料的困惑度是：<br />
<span class="math display">\[
\begin{aligned}
pp(\textbf{w})=2^{-\dfrac{1}{k}\log p(\textbf{w})}
\end{aligned}
\]</span><br />
以二元语法（bi-gram）为例，使用平均交叉熵，此时困惑度可以表示为：<br />
<span class="math display">\[
\begin{aligned}
pp(\textbf{w})&amp;=2^{-\dfrac{1}{k}\log
p(\textbf{w})}\\&amp;=2^{-\dfrac{1}{k}\log \prod\limits
_{t=1}^{k+1}p(w_{t}|w_{t-1})}\\&amp;=
2^{-\dfrac{1}{k}\sum\limits_{t=1}^{k+1}\log p(w_{t}|w_{t-1})}
\end{aligned}
\]</span><br />
模型的困惑度越小越好。</p>
<p>计算"商品 和 服务"的困惑度？<br />
（1）<span class="math inline">\(p(商品|BOS)
=\frac{2}{3}\)</span>；<br />
（2）<span class="math inline">\(p(和|商品)
=\frac{1}{2}\)</span>；<br />
（3）<span class="math inline">\(p(服务|和)
=\frac{1}{2}\)</span>；<br />
（4）<span class="math inline">\(p(EOS|服务)
=\frac{1}{1}\)</span>；<br />
整个句子的困惑度：<span class="math inline">\(pp(商品 和
服务)=2^{-\dfrac{1}{3}(log\frac{2}{3}+log\frac{1}{2}+log\frac{1}{2}+log\frac{1}{1})}\)</span>。</p>
<h2 id="数据平滑">数据平滑</h2>
<p>n元语法虽然有效，但它有一大不足，以二元语法为例，如果<span
class="math inline">\(c(w_tw_{t-1})\text{=0}\)</span>，因为计算句子概率时的乘法计算，导致整个语料的0-概率分配。0概率会造成非常大的困惑度，这是一种很糟糕的情况。一种避免0-概率事件的方法是使用平滑技术，确保为每个可能的情况都分配一个概率（可能非常小）。</p>
<h3 id="加法平滑">加法平滑</h3>
<p>最简单的一类方法是<strong>加法平滑</strong>（Additive
Smoothing），以下公式都以二元语法（bi-gram）为例。</p>
<p>不加平滑时：<br />
<span class="math display">\[
\begin{aligned}
p_{ MLE}(w_tw_{t-1})=\dfrac{c(w_{t-1}w_t)}{c(w_t)}
\end{aligned}
\]</span></p>
<p><strong>加一平滑</strong>（Add-one Smoothing/Laplace
Smoothing）：<br />
<span class="math display">\[
\begin{aligned}
p_{
add-one}(w_t|w_{t-1})=\dfrac{c(w_{t-1}w_t)+\textcolor{red}{1}}{c(w_t)+\textcolor{red}{V}},
\quad \textcolor{red}{V}是词表大小，即语料库中所有单词去重的总数
\end{aligned}
\]</span><br />
<strong>加K平滑</strong>（Add-K Smoothing/Laplace Smoothing）：<br />
<span class="math display">\[
\begin{aligned}
p_{
add-k}(w_t|w_{t-1})=\dfrac{c(w_{t-1}w_t)+\textcolor{red}{k}}{c(w_t)+\textcolor{red}{kV}},
\quad \textcolor{red}{V}是词表大小，即语料库中所有单词去重的总数
\end{aligned}
\]</span></p>
<h3 id="插值法">插值法</h3>
<p>另外一类方法使用back-off策略，即如果没有观测到n元语法，那么就基于n-1元语法计算，利用低阶n元语法平滑高阶n元语法，这就产生了很多方案，最简单的一种是<strong>线性插值法</strong>（Linear
Interpolation）。</p>
<p>三元语法（tri-gram）的线性插值法：<br />
<span class="math display">\[
\begin{aligned}
p_{tiny Int}(w_t|w_{t-2}w_{t-1})=\lambda_1
p(w_t|w_{t-2}w_{t-1})+\lambda_2p(w_t|w_{t-1})+\lambda_3p(w_t),\quad
\lambda_1+\lambda_2+\lambda_3=1
\end{aligned}
\]</span><br />
二元语法（bi-gram）的线性插值法：<br />
<span class="math display">\[
\begin{aligned}
p_{ Int}(w_t|w_{t-1})=\lambda_1 p(w_t|w_{t-1})+\lambda_2 p(w_t),\quad
\lambda_1+\lambda_2=1
\end{aligned}
\]</span><br />
一元语法（uni-gram）的线性插值法：<br />
<span class="math display">\[
\begin{aligned}
p_{ Int}(w_t)=\lambda_1 p(w_t)+\lambda_2\frac{1}{V},\quad
\lambda_1+\lambda_2=1
\end{aligned}
\]</span><br />
其中，V是词表大小，即语料库中所有单词去重的总数。</p>
<h3 id="古德-图灵平滑">古德-图灵平滑</h3>
<p><strong>古德-图灵平滑</strong>（Good-Turing
Smoothing）：对于任何一个出现 <span class="math inline">\(r\)</span>
次n元语法，都假设它出现了<span
class="math inline">\(r^*\)</span>次：<br />
<span class="math display">\[
\begin{aligned}
r^*=\frac{(r+1)N_{r+1}}{N_r} ,\quad  N_r表示训练预料中出现
r次的n元语法的数目
\end{aligned}
\]</span><br />
要把整个统计数转化为概率，只需要进行归一化处理：对于统计数为<span
class="math inline">\(r\)</span>的n元语法，其概率为：<br />
<span class="math display">\[
\begin{aligned}
p_r=\frac{r^*}{N}=\frac{(r+1)N_{r+1}}{N_r*N},\quad  N=\sum\limits_{r=1}^{\infty}r^*
N_r
\end{aligned}
\]</span><br />
注意到：<br />
<span class="math display">\[
\begin{aligned}
N=\sum\limits_{r=1}^{\infty}r^*
N_r=\sum\limits_{r=1}^{\infty}(r+1)N_{r+1}=\sum\limits_{r=1}^{\infty}rN_r
\end{aligned}
\]</span><br />
也就是说，N等于整个分布中的最初的计数。这样，样本中所有事件的概率之和为：<br />
<span class="math display">\[
\begin{aligned}
\sum\limits_{r&gt;0}N_rp_r=1-\frac{N_1}{N}&lt;1
\end{aligned}
\]</span><br />
因此，有$ N_1/N<span
class="math inline">\(的概率剩余量可以分配给所有未见事件（\)</span>r=0$的事件）。</p>
<p>但古德-图灵平滑也有其缺陷，比如某一个 $ N_{r+1}$ 为0，此时就无法计算
<span class="math inline">\(p_{r}\)</span>
了，一般这种情况，我们使用机器学习算法去拟合 $
N_{r}$，这样就可以把缺失的部分补上。</p>
<h1 id="词向量">词向量</h1>
<p>主要考虑单词或句子，甚至是文章的表示，一般把它们进行向量化处理。</p>
<h2 id="相似度">相似度</h2>
<h3 id="距离">距离</h3>
<p>单词/句子用向量表示后，可以计算它们的<strong>距离</strong>来判断相似度（距离越大，相似度越低），<span
class="math inline">\(i\)</span>为向量下标：<br />
（1）<strong>欧氏距离</strong>：<span
class="math inline">\(d=||A-B||_2=\sqrt{\sum \limits_{i=1}^n(A_{i} -
B_{i})^2}\)</span>，两个点的直线距离。<br />
（2）<strong>曼哈顿距离</strong>：<span
class="math inline">\(d=||A-B||_1=\sum
\limits_{i=1}^{n}|A_{i}-B_{i}|\)</span>，各个维度的长度差进行累加。常用计算城市间到达距离计算。<br />
（3）<strong>闵科夫斯基距离</strong>：<span
class="math inline">\(d=||A-B||_P=\sqrt[p]{\sum
\limits_{i=1}^n|A_{i}-B_{i}|^p}\)</span>，可以根据p来决定距离，如果p=1就是曼哈顿距离；p=2就是欧氏距离；当p趋近无穷时，就会变为长度差最大那个距离。</p>
<h3 id="方向">方向</h3>
<p>但是向量不光有大小还有<strong>方向</strong>的，两个向量之间是有夹角的，从这个角度发现了<strong>余弦相似度</strong>（值越大，相似度越高），<span
class="math inline">\(i\)</span>为向量下标：<br />
<span class="math display">\[
\begin{aligned}
cos(A,B)=\frac{A \cdot B}{|A||B|}=\frac{\sum
\limits_{i=1}^{n}A_iB_i}{\sqrt{\sum \limits_{i=1}^{n}A_i^2}\sqrt{\sum
\limits_{i=1}^{n}B_i^2}}
\end{aligned}
\]</span><br />
余弦相似度的<strong>取值范围是[-1,
1]，相同的两个向量之间的相似度为1</strong>。</p>
<p>当一对文本相似度的长度差距很大、但内容相近时，如果使用词频/词向量作为特征：<br />
（1）如果使用欧氏距离的话，它们在特定空间中的欧氏距离通常很大，因而相似度低；<br />
（2）而如果使用余弦相似度的话，它们之间的夹角可能很小，因而相似度高。<br />
在高维情况下：<br />
（1）余弦相似度依然保持“<strong>相同时为1，正交时为0，相反时为-1</strong>”的性质；<br />
（2）而欧式距离的数值则受维度的影响，范围不固定，并且含义也比较模糊。</p>
<p>如果希望得到类似于距离的表示，使用<span
class="math inline">\(\textbf{1-cos(A,B)}\)</span>即为<strong>余弦距离</strong>，<strong>其取值范围是[0,
2]，相同的两个向量余弦距离为0</strong>。<br />
在一些场景，比如word2vec中，其向量的模长是经过归一化的，此时欧式距离与余弦距离有着单调的关系：<br />
<span class="math display">\[
\begin{aligned}
||A-B||_2=\sqrt{2(1-cos(A,B))}
\end{aligned}
\]</span><br />
其中，<span class="math inline">\(||A-B||_2\)</span>表示欧氏距离，<span
class="math inline">\(cos(A,B)\)</span>表示余弦相似度，<span
class="math inline">\(1-cos(A,B)\)</span>表示余弦距离。此时，如果选择距离小的（相似度最大）的近邻，那么使用余弦相似度和欧氏距离的结果是相同的。<br />
总的来说：<strong>欧氏距离体现数值上的绝对差异，余弦距离体现方向上的相对差异。</strong></p>
<h2 id="one-hot">One-Hot</h2>
<p><strong>词袋模型</strong>（Bag-of-words
model）：将所有词语装进一个袋子里，不考虑其词法和语序的问题，即每个词语都是独立的。最简单的一种就是<strong>独热表示</strong>（One-Hot
Representation）。<br />
假设，词典：[是，天空，蓝色，的]。<strong>每个单词的表示</strong>：<br />
“是”　——&gt;[1, 0, 0, 0]<br />
“天空”——&gt;[0, 1, 0, 0]<br />
“蓝色”——&gt;[0, 0, 1, 0]<br />
“的”　——&gt;[0, 0, 0, 1]<br />
向量的维度等于词典的的大小。<br />
<div class="note info"><p>利用One-Hot表示法无法表达<strong>单词</strong>之间的相似度！不管用欧氏距离（任意两个词的相似度计算结果都相同）还是余弦相似度（任意两个词的相似度计算结果都是0）。</p>
<p>One-Hot表示单词/句子的缺点：<br />
（1）<strong>稀疏性</strong>（Sparsity）：如果词典非常大，维度就会很大，而一个句子可能只有很少的词，导致出现很多0，造成稀疏问题。核心问题是维度太大。<br />
（2）<strong>弱语义</strong>（Semantically
Weak）：无法表达词与词之间（语义）的相似度，因为One-Hot表示的单词向量是正交的。核心问题是每个单词的向量只能有一个有效值（local
representation），且取值只能是{0,1}。</p>
</div></p>
<h3 id="boolean">boolean</h3>
<p><strong>每个句子的表示（boolean）</strong>，向量对应的下标与字典的下标相匹配，其值为该词语出现为1，没出现为0：<br />
“天空 是 蓝色”——&gt;[1, 1, 1, 0]<br />
“蓝色 是 蓝色”——&gt;[1, 0, 1, 0]</p>
<h3 id="count">count</h3>
<p><strong>每个句子的表示（count）</strong>，向量对应的下标与字典的下标相匹配，其值为该词语出现的次数：<br />
“天空 是 蓝色”——&gt;[1, 1, 1, 0]<br />
“蓝色 是 蓝色”——&gt;[1, 0, 2, 0]</p>
<h3 id="tf-idf">TF-IDF</h3>
<p>一句话中每个词的重要程度是不同的，但boolean（每个单词权重相同）和count（出现次数越多不一定越重要）都不合理。由此考虑到新的计算方式————<strong>TF-IDF</strong>。<br />
<strong>TF-IDF</strong>（term frequency–inverse document
frequency）是一种用于信息检索与文本挖掘的常用加权技术。用以评估一个词，对于一个文件集或一个语料库中的其中一份文件的重要程度。词的重要性随着它在文件中出现的次数成正比增加，但同时会随着它在语料库中出现的频率成反比下降。其公式为：<br />
<span class="math display">\[
\begin{aligned}
\text{TF-IDF(t,d)}=\text{TF(t,d)}\times \text{IDF(t)}
\end{aligned}
\]</span></p>
<p>在一份给定的文件里，<strong>词频</strong>（term
frequency，TF）指的是某一个给定的词语在该文件中出现的频率。这个数字是对<strong>词数</strong>（term
count）的归一化，以防止它偏向长的文件。对于某一特定文件 <span
class="math inline">\(d_j\)</span> 里的词语 <span
class="math inline">\(t_i\)</span>
来说，它的词频（在本文件的重要程度）可表示为：<br />
<span class="math display">\[
\begin{aligned}
\text{TF}(t_i,d_j)=\frac{n_{i,j}}{\sum_k n_{k,j}}
\end{aligned}
\]</span><br />
其中，<span class="math inline">\(n_{i,j}\)</span> 是该词在文件 <span
class="math inline">\(d_j\)</span> 中的出现次数，而分母则是在文件 <span
class="math inline">\(d_j\)</span> 中所有字词的出现次数之和。<br />
有时 <span class="math inline">\(\text{TF}(t_i,d_j)\)</span>
也可以直接采用词频 <span class="math inline">\(n_{i,j}\)</span>
计算，不进行归一化处理。</p>
<p><strong>逆向文件频率</strong>（inverse document
frequency，IDF）是一个词语普遍重要性的度量（在整体文件的重要程度，和文件频率反比关系）。某一特定词语的IDF———总文件数目除以包含该词语的文件数目，再取对数（防止它的值过大）：<br />
<span class="math display">\[
\begin{aligned}
\text{IDF}(t_i)=log\frac{|D|}{|1+\{j:t_i\in d_j\}|}
\end{aligned}
\]</span><br />
其中，<span class="math inline">\(|D|\)</span> 是语料库中文件总数，<span
class="math inline">\(\{j:t_i\in d_j\}\)</span> 是包含词语 <span
class="math inline">\(t_i\)</span>
的文件数目（如果词语不存在资料库中，按 1 处理）。</p>
<p>假设，词典：[是，天空，蓝色，的]，语料库：[“天空 是 蓝色”, “蓝色 是
蓝色”]。<br />
<strong>每个句子的表示（TF-IDF）</strong>，向量对应的下标与字典的下标相匹配，其值为该词语的TF-IDF（TF按词频计算）：<br />
“天空 是 蓝色”——&gt;<span class="math inline">\([1·\log\frac{2}{1},
1·\log\frac{2}{2}, 1·\log\frac{2}{2}, 0]\)</span><br />
“蓝色 是 蓝色”——&gt;<span class="math inline">\([1\log\frac{2}{1}, 0,
2·\log\frac{2}{2}, 0]\)</span></p>
<h2 id="word2vec">word2vec</h2>
<p>之前我们说了One-Hot表示方法有<strong>稀疏性</strong>、<strong>弱语义</strong>缺点，那么如何解决这些问题？————分布式表示。</p>
<p><strong>分布式表示</strong>（Distributed
Representation）的思路是：通过训练，将每个词用<strong>低维度</strong>的向量表示（解决稀疏性/高维度问题，维度不再依赖字典长度），并且每个单词的向量<strong>有多个有效值</strong>（global
representation），每个维度上的有效值不再是{0,1}，而是介于[0,1]的值（解决弱语义问题，可计算相似度）。这种把词映射到低维的向量表示，也叫做<strong>词嵌入</strong>（word
embedding）。对于句子表示，可以使用平均策略，即句子中所有词的向量求和，再取均值。</p>
<p><strong><span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL3RtaWtvbG92L3dvcmQydmVj">word2vec<i class="fa fa-external-link-alt"></i></span></strong>
就是分布式表示方法的一种，它将词的语义表示为训练语料库中上下文的向量。它根据输入和输出的不同分为<strong>CBOW</strong>（Continuous
Bag-Of-Words）和<strong>Skip-Gram</strong>两种模型。而且word2vec对这两种方法进行了优化，从而得到<strong>Hierarchical
Softmax</strong>模型和<strong>Negative Sampling</strong>模型。<br />
<img src="/images/语言模型和词向量/CBOW和Skip-gram.png" width="80%" height="80%"></p>
<p><strong>CBOW</strong>：基于上下文词（输入）预测中心词（输出）。<br />
<strong>Skip-Gram</strong>：基于中心词（输入）预测上下文词（输出）。</p>
<h3 id="skip-gram">Skip-Gram</h3>
<p><strong>Skip-Gram</strong>核心思想是：用中心词（输入）预测上下文词（输出）。输入的中心词使用One-Hot向量表示，引入一个大小为
<span class="math inline">\(c\)</span>
的窗口，那么上下文词就是由中心词左 <span
class="math inline">\(c\)</span> 个词和右 <span
class="math inline">\(c\)</span> 个词构成（一般用<span
class="math inline">\(2c\)</span>表示上下文）我们希望模型输出的就是这些上下文词，而通过<strong>神经网络输出+softmax</strong>计算得到的是所有词的概率，那么只需要优化上下文词的概率最大即可达到我们的目的。</p>
<p>把这个核心思想转化成数学表示，假设有如下一句话：<br />
<span class="math display">\[
[w_1...w_{t-1}w_tw_{t+1}...w_V]
\]</span><br />
其中 <span class="math inline">\(w_t\)</span> 代表第 <span
class="math inline">\(t\)</span> 个词，总共有$
V$个词。我们要计算的就是：<br />
<span class="math display">\[
\prod\limits_{t=1}^Vp(\text{context}(w_t)|w_t)
\]</span><br />
每一个 <span class="math inline">\(p(\text{context}(w_t)|w_t)\)</span>
是相互独立的。<br />
此时引入窗口参数 <span class="math inline">\(i\in
\text{[-c,c]}\)</span>，<span class="math inline">\(c\)</span>
为窗口大小。中心词 <span class="math inline">\(w_t\)</span> 的上下文词
<span class="math inline">\(\text{context}(w_t)\)</span> 就是其左 <span
class="math inline">\(c\)</span> 个词和右 <span
class="math inline">\(c\)</span> 个词构成，上式变为：<br />
<span class="math display">\[
\begin{aligned}
\prod\limits_{t=1}^V \prod\limits_{i=-c}^c p(w_{t+i}|w_t)
\end{aligned}
\]</span><br />
每一个 <span class="math inline">\(p(\text{context}(w_i)|w_t)\)</span>
是相互独立且同分布的。<br />
为了方便计算，转成<span
class="math inline">\(log\)</span>（其实就是对数损失函数），并且取均值：<br />
<span class="math display">\[
\begin{aligned}
\dfrac{1}{\text{V}} \sum\limits_{t=1}^V \sum\limits_{i=-c}^c \log
p(w_{t+i}|w_t)
\end{aligned}
\]</span><br />
我们的目标函数就是引入参数 <span
class="math inline">\(\theta\)</span>，使整个式子最大化：<br />
<span class="math display">\[
\begin{aligned}
\mathop{argmax}\limits_{\theta}\dfrac{1}{\text{V}} \sum\limits_{t=1}^V
\sum\limits_{i=-c}^c \log p(w_{t+i}|w_t;\theta)
=\mathop{argmin}\limits_{\theta}-\dfrac{1}{\text{V}} \sum\limits_{t=1}^V
\sum\limits_{i=-c}^c \log p(w_{t+i}|w_t;\theta)
\end{aligned}
\]</span><br />
上式其实就是我们的优化函数<span
class="math inline">\(J(\theta)\)</span>，这里为了方便优化计算，最大化转成了最小化，而引入的参数
<span class="math inline">\(\theta\)</span>
其实就是我们要找的<strong>词向量</strong>。</p>
<hr />
<p>那么如何计算呢？一般采用的方法是一个三层的神经网络结构，分为输入层，隐藏层和输出层（softmax层）。<br />
这个神经网络计算的是<strong>一个词</strong>（输入）预测<strong>所有词</strong>（输出）的情况：<br />
<img src="/images/语言模型和词向量/one-word.png" width="60%" height="60%"></p>
<p><span class="math inline">\(\text{V}\)</span>：词汇表的长度;<br />
<span
class="math inline">\(\text{N}\)</span>：隐层神经元个数（词向量维度，需要我们自己指定）;<br />
<span
class="math inline">\(\text{W}\)</span>：输入层到隐层的权重矩阵（词向量矩阵，每一行代表一个词的词向量），维度是<span
class="math inline">\(\small [V,N]\)</span>;<br />
<span
class="math inline">\(\text{W}^\prime\)</span>：隐层到输出层的权重矩阵（词向量矩阵，每一列代表一个词的词向量），维度是<span
class="math inline">\(\small [N,V]\)</span>;</p>
<p>我们需要做的是用输入的词去预测输出的词（方便书写这里用行向量表示一个词）：<br />
（1）输入层的一个单词 <span class="math inline">\(w_t\)</span>
使用One-Hot表示：<br />
<span class="math display">\[
w_t=[x_1...x_t...x_V]
\]</span><br />
其中，只有 <span class="math inline">\(x_t\)</span>
为1，其余为0，其中t是输入单词在词汇表中的索引下标，它的维度是<span
class="math inline">\(\small [1,V]\)</span>。<br />
（2）输入的词 <span class="math inline">\(w_t\)</span> 和词向量矩阵 $ W$
相乘，得到一个维度为<span class="math inline">\(\small
[1,N]\)</span>的隐层向量 <span
class="math inline">\(h\)</span>。此过程可看作从词向量矩阵 $
W$取对应的词向量。<br />
<span class="math display">\[
\begin{aligned}
h=w_t\cdot\text{W}
\end{aligned}
\]</span><br />
（3）隐层向量 <span class="math inline">\(h\)</span> 和 词向量矩阵 <span
class="math inline">\(W^\prime\)</span> 相乘，得到一个维度为<span
class="math inline">\(\small [1,V]\)</span>的输出向量 <span
class="math inline">\(y\)</span>。此过程可看作计算当前词向量和所有词向量的相似度。从这个过程可看出<strong>word2vec中隐藏层没有用激活函数</strong>。<br />
<span class="math display">\[
\begin{aligned}
y=h\cdot\text{W}^\prime
\end{aligned}
\]</span><br />
（4）输出向量 <span class="math inline">\(y\)</span>
再通过softmax计算，从而得到概率。此过程可看作把相似度转成概率，即向量
<span class="math inline">\(y\)</span>
的每个值是当前词向量和另一个词向量相似的概率。下面公式是预测一个词的概率：<br />
<span class="math display">\[
\normalsize p(w_{i,i\ne k}|w_t)=p(w_{i,i\ne
k}|y)=\dfrac{e^{y_i}}{\sum\limits_{j=1}^V e^{y_j}}
\]</span><br />
其中，<span class="math inline">\(w_i\)</span> 代表词表中第 <span
class="math inline">\(i\)</span> 个词且<span
class="math inline">\(i\mathbb{\ne} t\)</span>（非中心词）。<span
class="math inline">\(y_i\)</span>为在原始输出向量<span
class="math inline">\(y\)</span>中，与单词<span
class="math inline">\(w_i\)</span>所对应的维度取值。<span
class="math inline">\(y\)</span>向量通过softamx计算出来就是当前词<span
class="math inline">\(w_t\)</span>和所有词的相似概率。<br />
为什么是softmax？因为其值域是<span
class="math inline">\([0,1]\)</span>，且所有结果的和为<span
class="math inline">\(1\)</span>。符合我们想要得到概率的目的。</p>
<hr />
<p><img src="/images/语言模型和词向量/skip-gram.png" width="40%"></p>
<p>那么<strong>Skip-Gram</strong>是怎么计算的呢？回到核心思想：用<strong>中心词</strong>（输入）预测<strong>上下文词</strong>（输出）。<br />
它引入了窗口 <span class="math inline">\(c\)</span> ，上下文词就是<span
class="math inline">\(2c\)</span>（左 <span
class="math inline">\(c\)</span> 个词和右 <span
class="math inline">\(c\)</span> 个词构成），目标是给定一个词<span
class="math inline">\(w_t\)</span>预测上下文词的概率最大化。以下是一个词<span
class="math inline">\(w_t\)</span>的优化公式：<br />
<span class="math display">\[
\normalsize
p(w_{t+i,i\in[-c,c]}|w_t)=\dfrac{e^{y_i}}{\sum\limits_{j=1}^V e^{y_j}}
\]</span><br />
<span class="math display">\[
\normalsize \mathop{argmax}\limits_{W,W^{\prime}}
\prod\limits_{i=-c}^{c}
p(w_{t+i}|w_t;\scriptsize{W,W^{\prime}})\normalsize
=\mathop{min}\limits_{W,W^{\prime}} \sum\limits_{i=-c}^{c}
-logp(w_{t+i}|w_t;\scriptsize{W,W^{\prime}})
\]</span><br />
把所有中心词都训练一遍，就是以下公式：<br />
<span class="math display">\[
\begin{aligned}
\mathop{argmax}\limits_{W,W^{\prime}}\dfrac{1}{\text{V}}
\sum\limits_{t=1}^V \sum\limits_{i=-c}^c \log
p(w_{t+i}|w_t;\scriptsize{W,W^{\prime}})\normalsize
=\mathop{argmin}\limits_{W,W^{\prime}}-\dfrac{1}{\text{V}}
\sum\limits_{t=1}^V \sum\limits_{i=-c}^c \log
p(w_{t+i}|w_t;\scriptsize{W,W^{\prime}})
\end{aligned}
\]</span><br />
其中，损失函数选择对数损失函数（<span
class="math inline">\(log\)</span>），全局损失定义为所有训练样本上的平均损失。<br />
上式损失函数优化过程就可以通过反向传播方法优化（基于梯度的优化），这里不再赘述。所有的中心词都训练一遍后，得到的<span
class="math inline">\(\text{W}\)</span>和<span
class="math inline">\(\text{W}^\prime\)</span>（论文中是<span
class="math inline">\(v\)</span>和<span
class="math inline">\(u\)</span>，代表中心词向量和上下文词向量）就是我们需要的词向量，可选其中一个作为V个词的N维向量表示（word2vec中一般选择<span
class="math inline">\(\text{W}\)</span>作为词向量）。</p>
<h3 id="cbow">CBOW</h3>
<p><img src="/images/语言模型和词向量/cbow.png" width="40%"></p>
<p><strong>CBOW</strong>核心思想：用上下文词（输入）预测中心词（输出）。<br />
（1）输入是多个词（上下文词）的One-Hot表示。在计算隐层的向量前，对输入向量和取均值即可。<br />
<span class="math display">\[
\begin{aligned}
h=\frac{\sum\limits_{i=-c}^c w_{t+i}\cdot
W}{2c}=\frac{\sum\limits_{i=-c}^c h_{t+i}}{2c}
\end{aligned}
\]</span><br />
（2）输出是上下文词向量和某一个词向量相似的概率，使中心词概率最大即可。以下是一个词<span
class="math inline">\(w_t\)</span>的优化公式：<br />
<span class="math display">\[
\normalsize \mathop{argmax}\limits_{W,W^{\prime}}
p(w_t|w_{t+i,i\in[-c,c]};\scriptsize{W,W^{\prime}})\normalsize
=\mathop{argmin}\limits_{W,W^{\prime}}
-logp(w_t|w_{t+i,i\in[-c,c]};\scriptsize{W,W^{\prime}})
\]</span><br />
把所有中心词的上下文词都训练一遍，就是以下公式：<br />
<span class="math display">\[
\begin{aligned}
\mathop{argmax}\limits_{W,W^{\prime}}\dfrac{1}{\text{V}}
\sum\limits_{t=1}^V \sum\limits_{i=-c}^c \log
p(w_t|w_{t+i};\scriptsize{W,W^{\prime}})\normalsize
=\mathop{argmin}\limits_{W,W^{\prime}}-\dfrac{1}{\text{V}}
\sum\limits_{t=1}^V \sum\limits_{i=-c}^c \log
p(w_t|w_{t+i};\scriptsize{W,W^{\prime}})
\end{aligned}
\]</span></p>
<h3 id="hierarchical-softmax">Hierarchical Softmax</h3>
<p>word2vec也是用了CBOW与Skip-Gram来训练模型与得到词向量，但没有使用神经网络结构，而是使用<strong>霍夫曼树</strong>（Huffman）来替代隐藏层到输出层的过程。</p>
<hr />
<p>我们先来复习下<strong>霍夫曼树</strong>，其特点是<strong>带权路径最短</strong>。首先明确一些概念：<br />
（1）<strong>路径</strong>：指从树种一个结点到另一个结点的分支所构成的路线。<br />
（2）<strong>路径长度</strong>：指路径上的分支数目。<br />
（3）<strong>树的路径长度</strong>：指从根到每个结点的路径长度之和。<br />
（4）<strong>带权路径长度</strong>：结点具有权值，从该结点到根之间的路径长度乘以结点的权值，就是该结点的带权路径长度。<br />
（5）<strong>树的带权路径长度</strong>（WPL）：指树中所有叶子结点的带权路径长度之和。</p>
<p><strong>霍夫曼树的构造方法</strong>（霍夫曼树可以是n叉树，我们主要以二叉树为例）<br />
给定<span class="math inline">\(n\)</span>个权值，用这<span
class="math inline">\(n\)</span>个权值构造霍夫曼树的算法如下：<br />
（1）将这个<span
class="math inline">\(n\)</span>个权值分别看作只有根节点的n棵二叉树，这些二叉树构成的集合记为<span
class="math inline">\(\small F\)</span>。<br />
（2）从<span class="math inline">\(\small
F\)</span>中选出两棵根节点的权值最小的数（假设为<span
class="math inline">\(a\)</span>、<span
class="math inline">\(b\)</span>），作为左、右子树，构造一棵新的二叉树（假设为<span
class="math inline">\(c\)</span>），新的二叉树的根节点权值为左、右子树根节点权值之和。<br />
（3）从F中删除<span class="math inline">\(a\)</span>、<span
class="math inline">\(b\)</span>，加入新构造的树<span
class="math inline">\(c\)</span>。<br />
（4）重复（2）（3）两步，直到<span class="math inline">\(\small
F\)</span>中只剩下一棵树为止，这棵树就是霍夫曼树。</p>
<p>一个简单的例子：“this is an example of a huffman tree”
中得到的字母频率（权重）来建构霍夫曼树。<br />
<img src="/images/语言模型和词向量/huffman.png" width="60%"></p>
<p><strong>霍夫曼树特点</strong>：<br />
（1）权重（频率）越大的结点，距离根结点越近。<br />
（2）树的带权路径长度最短。</p>
<p><strong>霍夫曼编码</strong>：<br />
一般得到霍夫曼树后我们会对叶子节点进行霍夫曼编码，由于权重高的叶子节点越靠近根节点，而权重低的叶子节点会远离根节点，这样我们的高权重节点编码值较短，而低权重值编码值较长。这保证的树的带权路径最短，也符合我们的信息论，即我们希望越常用的词拥有更短的编码。如何编码呢？一般对于一个霍夫曼树的节点（根节点除外），可以约定<strong>左子树编码为0</strong>，<strong>右子树编码为1</strong>。如下图所示：<br />
<img src="/images/语言模型和词向量/huffman1.png" width="60%"></p>
<hr />
<p><strong>word2vec中，霍夫曼编码方式和正常的相反，即约定沿着左子树走编码为1（负类），沿着右子树走编码为0（正类），同时约定左子树的权重不小于右子树的权重。</strong></p>
<p><strong>Hierarchical
Softmax</strong>的<strong>隐藏层</strong>到<strong>输出概率</strong>的计算过程（CBOW）：<br />
首先按照<strong>词频</strong>建立一棵霍夫曼树，叶子结点就是词典中的每个单词，但顺序和词典中不一定相同。假设预测的词（叶子节点）为<span
class="math inline">\(w\)</span>，定义一些符号：<br />
（1）<span class="math inline">\(p^w\)</span>：从根节点到<span
class="math inline">\(w\)</span>对应叶子节点的路径。<br />
（2）<span class="math inline">\(n^w\)</span>：路径<span
class="math inline">\(p^w\)</span>中包含结点个数。<br />
（3）<span
class="math inline">\(p_1^w,p_2^w,...,p_{n^w}^w\)</span>：路径<span
class="math inline">\(p^w\)</span>中的<span
class="math inline">\(n^w\)</span>个结点，<span
class="math inline">\(p_1^w\)</span>表示根节点，<span
class="math inline">\(p_{n^w}^w\)</span>表示词<span
class="math inline">\(w\)</span>对应的叶子结点。<br />
（4）<span
class="math inline">\(d_2^w,d_3^w,...,d_{n^w}^w\)</span>：路径<span
class="math inline">\(p^w\)</span>中的<span
class="math inline">\(n^w\)</span>个结点的编码，总共有<span
class="math inline">\(n^w\text{-1}\)</span>个，根结点不对应编码，每个编码值为<span
class="math inline">\(\{0,1\}\)</span>。<br />
（5）<span
class="math inline">\(\theta_1^w,\theta_2^w,...,\theta_{n^w-1}^w\)</span>：路径<span
class="math inline">\(p^w\)</span>中的<span
class="math inline">\(n^w\text{-1}\)</span>个结点的向量（维度为<span
class="math inline">\(N,1\)</span>），不包括叶子结点。</p>
<p>对于词典中任意的词<span
class="math inline">\(w\)</span>，霍夫曼树中必存在一条从根结点到词<span
class="math inline">\(w\)</span>叶子节点的路径<span
class="math inline">\(p^w\)</span>（路径唯一）。路径<span
class="math inline">\(p^w\)</span>上存在<span
class="math inline">\(n^w\text{-1}\)</span>个分支(边)，每个分支看做一次二分类，每一次分类就产生一个概率，将这些概率乘起来就是预测词<span
class="math inline">\(w\)</span>的概率，即<span
class="math inline">\(p(w|context(w))\)</span>。<br />
由此可以给出<span class="math inline">\(w\)</span>的条件概率：<br />
<span class="math display">\[
p(w|context(w))=\prod\limits_{j=2}^{n^w}p(d_j^w|h_w;\theta_{j-1}^w)
\]</span><br />
从根节点到叶节点经过了<span
class="math inline">\(n^w\text{-1}\)</span>个节点，编码从下标2开始（根节点无编码），对应的参数向量下标从1开始（根节点为1）。其中<span
class="math inline">\(h_w\)</span>是隐藏层向量(context向量加权和)。<br />
其中<strong>每个</strong> <span class="math inline">\(\small
p(d_j^w|h_w;\theta_{j-1}^w)\)</span> 都是一个逻辑回归二分类：<br />
<span class="math display">\[
p(d_j^w|h_w;\theta_{j-1}^w)=
\begin{cases}
   \sigma(h_w\cdot\theta_{j-1}^w), &amp;d_j^w=0\quad\text{(正类)}\\
   1-\sigma(h_w\cdot\theta_{j-1}^w), &amp;d_j^w=1\quad\text{(负类)}
\end{cases}
\]</span><br />
其中<span
class="math inline">\(h_w\)</span>是隐藏层向量(context向量加权和)，<span
class="math inline">\(\sigma\)</span>是sigmoid函数。<br />
考虑到<span
class="math inline">\(d\)</span>只有0和1两种取值，我们可以用指数形式方便地将其写到一起：<br />
<span class="math display">\[
p(d_j^w|h_w;\theta_{j-1}^w)=[\sigma(h_w\cdot\theta_{j-1}^w)]^{1-d_j^w}\cdot[1-\sigma(h_w\cdot\theta_{j-1}^w)]^{d_j^w}
\]</span><br />
所以对于<span
class="math inline">\(p(w|context(w))\)</span>，目标函数取对数似然，引入窗口<span
class="math inline">\(C\)</span>代表<span
class="math inline">\(context(w)\)</span>：<br />
<span class="math display">\[
\begin{aligned}
p(w|context(w))&amp;=\sum\limits_{w\in
C}log\prod\limits_{j=2}^{n^w}\{[\sigma(h_w\cdot\theta_{j-1}^w)]^{1-d_j^w}\cdot[1-\sigma(h_w\cdot\theta_{j-1}^w)]^{d_j^w}\}\\
&amp;=\sum\limits_{w\in C}\sum\limits_{j=2}^{n^w}\{(1-d_j^w)\cdot
log[\sigma(h_w\cdot\theta_{j-1}^w)]+d_j^w\cdot
log[1-\sigma(h_w\cdot\theta_{j-1}^w)]\}
\end{aligned}
\]</span><br />
其中<span
class="math inline">\(C\)</span>是上下文单词。接下来只需要对<span
class="math inline">\(h_w\)</span>和<span
class="math inline">\(\theta_{j-1}^w\)</span>求梯度，然后用随机梯度上升法优化即可，这个过程和逻辑回归梯度优化类似。</p>
<p>以 <strong>CBOW：上下文词（输入）预测中心词（输出）</strong>
为例，从输入层到隐藏层计算方式不变，最后得到维度为<span
class="math inline">\([1,N]\)</span>的隐藏层向量<span
class="math inline">\(h\)</span>：<br />
<img src="/images/语言模型和词向量/hs.png"></p>
<p>使用<span class="math inline">\(w\)</span>表示图中<span
class="math inline">\(w_2\)</span>，其计算过程如下：<br />
第1次：<span
class="math inline">\(p(d_2^w|h_w;\theta_1^w)=1-\sigma(h_w\cdot\theta_1^w)\)</span><br />
第2次：<span
class="math inline">\(p(d_3^w|h_w;\theta_2^w)=1-\sigma(h_w\cdot\theta_2^w)\)</span><br />
第3次：<span
class="math inline">\(p(d_4^w|h_w;\theta_3^w)=\sigma(h_w\cdot\theta_3^w)\)</span></p>
<p><strong>基于Hierarchical
Softmax的CBOW</strong>模型算法流程，梯度迭代使用了随机梯度上升法：<br />
输入：基于CBOW的语料训练样本，词典大小<span
class="math inline">\(V\)</span>，中心词<span
class="math inline">\(w\)</span>，词向量的维度大小<span
class="math inline">\(N\)</span>，上下文<span
class="math inline">\(C\)</span>大小<span
class="math inline">\(2c\)</span>，步长<span
class="math inline">\(\eta\)</span>。<br />
输出：霍夫曼树的内部节点模型参数<span
class="math inline">\(\theta\)</span>，所有的词向量<span
class="math inline">\(W=w_1,w_2,...,w_V\)</span>。<br />
（1）基于语料训练样本建立霍夫曼树。<br />
（2）随机初始化所有的模型参数<span
class="math inline">\(\theta\)</span>和所有的词向量<span
class="math inline">\(W\)</span>。<br />
（3）进行梯度上升迭代过程，对于训练集中的每一个样本<span
class="math inline">\((context(w),w)\)</span>做如下处理：</p>
<ul>
<li>e=0，计算<span
class="math inline">\(h_w=\frac{1}{2c}\sum\limits_{i=-c}^cw_{i}\)</span></li>
<li>for <span class="math inline">\(\text{j=2}\)</span> to <span
class="math inline">\(n^w\)</span>，计算：
<ul>
<li><span
class="math inline">\(f=\sigma(h_w\theta_{j-1}^w)\)</span></li>
<li><span class="math inline">\(g=\eta(1-d_j^w-f)\)</span></li>
<li><span class="math inline">\(e=e+g\cdot\theta_{j-1}^w\)</span></li>
<li><span class="math inline">\(\theta_{j-1}^w=\theta_{j-1}^w+g\cdot
h_w\)</span></li>
</ul></li>
<li>对于<span
class="math inline">\((context(w),w)\)</span>中的每一个词向量<span
class="math inline">\(w_i\)</span>（共<span
class="math inline">\(2c\)</span>个）进行更新：
<ul>
<li><span class="math inline">\(w_i=w_i+e\)</span></li>
</ul></li>
<li>如果梯度收敛，则结束梯度迭代，否则回到（3）继续迭代。</li>
</ul>
<p><strong>基于Hierarchical
Softmax的Skip-Gram</strong>模型算法流程，梯度迭代使用了随机梯度上升法：<br />
输入：基于Skip-Gram的语料训练样本，词典大小<span
class="math inline">\(V\)</span>，中心词<span
class="math inline">\(w\)</span>，词向量的维度大小<span
class="math inline">\(N\)</span>，上下文<span
class="math inline">\(C\)</span>大小<span
class="math inline">\(2c\)</span>，步长<span
class="math inline">\(\eta\)</span>。<br />
输出：霍夫曼树的内部节点模型参数<span
class="math inline">\(\theta\)</span>，所有的词向量<span
class="math inline">\(W=w_1,w_2,...,w_V\)</span>。<br />
（1）基于语料训练样本建立霍夫曼树。<br />
（2）随机初始化所有的模型参数<span
class="math inline">\(\theta\)</span>和所有的词向量<span
class="math inline">\(W\)</span>。<br />
（3）进行梯度上升迭代过程，对于训练集中的每一个样本<span
class="math inline">\((context(w),w)\)</span>做如下处理：</p>
<ul>
<li>for <span class="math inline">\(s=context(w)\)</span>，计算：
<ul>
<li>e=0，<span class="math inline">\(h_w=w_i\)</span></li>
<li>for <span class="math inline">\(\text{j=2}\)</span> to <span
class="math inline">\(n^w\)</span>，计算：
<ul>
<li><span
class="math inline">\(f=\sigma(h_w\theta_{j-1}^s)\)</span></li>
<li><span class="math inline">\(g=\eta(1-d_j^s-f)\)</span></li>
<li><span class="math inline">\(e=e+g\cdot\theta_{j-1}^s\)</span></li>
<li><span class="math inline">\(\theta_{j-1}^s=\theta_{j-1}^s+g\cdot
h_w\)</span></li>
</ul></li>
</ul></li>
<li>对于<span
class="math inline">\((context(w),w)\)</span>中的每一个词向量<span
class="math inline">\(w_i\)</span>（共<span
class="math inline">\(2c\)</span>个）进行更新：
<ul>
<li><span class="math inline">\(w_i=w_i+e\)</span></li>
</ul></li>
<li>如果梯度收敛，则结束梯度迭代，否则回到（3）继续迭代。</li>
</ul>
<p><strong>优点</strong>：在前面的<strong>CBOW</strong>和<strong>Skip-gram</strong>模型中，softmax计算时分母时需要对所有词的值进行计算求和，word2vec的<strong>Hierarchical
Softmax</strong>采用了霍夫曼二叉树来替代从隐藏层到输出softmax层的过程，<strong>之前softmax计算量为<span
class="math inline">\(V\)</span>，现在为<span
class="math inline">\(log_2V\)</span></strong>。</p>
<p><strong>为什么word2vec中用<span
class="math inline">\(\text{W}\)</span>作为词向量？</strong><br />
在之前讲的三层网络中我们可以选择词向量是<span
class="math inline">\(\{\text{W},\text{W}^\prime\normalsize\}\)</span>，word2vec这中<span
class="math inline">\(\text{W}^\prime\)</span>替换成<span
class="math inline">\(\large\theta\)</span>了，所以word2vec一般采用<span
class="math inline">\(\text{W}\)</span>作为词向量而不用<span
class="math inline">\(\text{W}^\prime\)</span>作为词向量。除此原因外，输入矩阵<span
class="math inline">\(\text{W}\)</span>和输出矩阵<span
class="math inline">\(\text{W}^\prime\)</span>可以看作<strong>所有词作为中心词</strong>或<strong>所有词作为上下文词</strong>而产生的词向量，它们侧重点不同，在不同算法作用也不同，比如在Skip-gram中<span
class="math inline">\(\text{W}\)</span>可看作所有词作为中心词而产生的词向量，在CBOW中<span
class="math inline">\(\text{W}\)</span>可看作所有词作为上下文词产生的词向量。对于<strong>于Hierarchical
Softmax</strong>和后面的<strong>Negative Sampling</strong>都代替了<span
class="math inline">\(\text{W}^\prime\)</span>，从而都是选择<span
class="math inline">\(\text{W}\)</span>作为词向量，而<span
class="math inline">\(\text{W}\)</span>作为中心词而得到词向量是Skip-gram中实现，所以word2vec中选择Skip-gram效果会更好。</p>
<h3 id="negative-sampling">Negative Sampling</h3>
<p>在讲基于<strong>Negative
Sampling</strong>的word2vec模型前，我们先看看<strong>Hierarchical
Softmax</strong>的的缺点。HS使用了霍夫曼树，不难发现对于词频高的词计算很快，但对于词频低的词计算很慢。如果我们的训练样本里的中心词<span
class="math inline">\(w\)</span>是一个很生僻的词，那么就得在霍夫曼树中辛苦的向下走很久了。能不能不用搞这么复杂的一颗霍夫曼树，将模型变的更加简单呢？这就是<strong>Negative
Sampling</strong>（负采样）。</p>
<p><strong>Negative
Sampling</strong>就是这么一种求解word2vec模型的方法，它<strong>摒弃了霍夫曼树</strong>，采用了Negative
Sampling（负采样）的方法来求解，下面我们就来讲述NS中下预测一个词过程：<br />
（1）已知词<span class="math inline">\(w\)</span>的上下文<span
class="math inline">\(context(w)\)</span>，需要预测<span
class="math inline">\(w\)</span>，那么认为词<span
class="math inline">\(w\)</span>作为中心词就是一个正样本<span
class="math inline">\((context(w),w)\)</span>，其他词作为中心词就是负样本<span
class="math inline">\((context(w),w_i)\)</span>，其中<span
class="math inline">\(i\in[1,\text{neg}]\)</span>。通过<strong>负采样</strong>得到
<span class="math inline">\(\text{neg}\)</span> （自己指定）个负样本 +
一个正样本，用<span
class="math inline">\(u\)</span>表示它们的集合。<br />
（2）利用这一个正例<span class="math inline">\((context(w),w)\)</span>和
<span class="math inline">\(\text{neg}\)</span> 个负例<span
class="math inline">\((context(w),w_i)\)</span>进行逻辑回归二分类，我们希望正样本分类概率最大化，可以通过梯度优化完成。这个就是预测一个词的过程。</p>
<p>整个过程要明白两个核心问题：1）如何利用<span
class="math inline">\(u\)</span>来做逻辑回归二分类？
2）如何进行负采样？</p>
<hr />
<p><strong>我们通过基于Negative
Sampling的CBOW来解答第一个问题。</strong><br />
预测一个词时优化的目标函数<span
class="math inline">\(g(w)\)</span>表示为：<br />
<span class="math display">\[
g(w)=\prod\limits_{u\in\{w\}\cup\{w_1...w_{neg}\}}p(u|context(w))
\]</span><br />
其中：<br />
<span class="math display">\[
\begin{aligned}
p(u|context(w))&amp;=
\begin{cases}
   \sigma(h_w\cdot\theta^{u}), &amp;y_u=1\quad\text{(正类)}\\
   1-\sigma(h_w\cdot\theta^{u}), &amp;y_u=0\quad\text{(负类)}
\end{cases}
~\\
\\&amp;=[\sigma(h_w\cdot\theta^u)]^{y_u}\cdot[1-\sigma(h_w\cdot\theta^u)]^{1-y_u}
\end{aligned}
\]</span><br />
上式，代入<span class="math inline">\(g(w)\)</span>中：<br />
<span class="math display">\[
g(w)=\prod\limits_{u\in\{w\}\cup\{w_1...w_{neg}\}}[\sigma(h_w\cdot\theta^u)]^{y_u}\cdot[1-\sigma(h_w\cdot\theta^u)]^{1-y_u}
\]</span><br />
这就是预测一次的优化函数了。此时引入窗口<span
class="math inline">\(C=2c\)</span>，整体的优化函数就是：<br />
<span class="math display">\[
\begin{aligned}
L=log\prod\limits_{C}g(w)=\sum\limits_{C}log(g(w))&amp;=\sum\limits_{C}log\prod\limits_{u\in\{w\}\cup\{w_1...w_{neg}\}}\{[\sigma(h_w\cdot\theta^u)]^{y_u}\cdot[1-\sigma(h_w\cdot\theta^u)]^{1-y_u}\}\\
&amp;=\sum\limits_{C}\sum\limits_{u\in\{w\}\cup\{w_1...w_{neg}\}}\{y_u\cdot
log[\sigma(h_w\cdot\theta^u)]+(1-y_u)\cdot
log[1-\sigma(h_w\cdot\theta^u)]\}
\end{aligned}
\]</span><br />
之后使用随机梯度上升优化即可。</p>
<p><strong>基于Negative Sampling的CBOW</strong>：<br />
输入：基于CBOW的语料训练样本，词典大小<span
class="math inline">\(V\)</span>，中心词<span
class="math inline">\(w\)</span>，词向量的维度大小<span
class="math inline">\(N\)</span>，上下文<span
class="math inline">\(C\)</span>大小<span
class="math inline">\(2c\)</span>，步长<span
class="math inline">\(\eta\)</span>。<br />
输出：词汇表每个词对应的模型参数<span
class="math inline">\(\theta\)</span>，所有的词向量<span
class="math inline">\(W=x_1,x_2,...,x_V\)</span>（避免和下面负样本混淆）。<br />
（1）随机初始化所有的模型参数<span
class="math inline">\(\theta\)</span>和所有的词向量<span
class="math inline">\(W\)</span>。<br />
（2）对于每个训练样本<span
class="math inline">\((context(w),w)\)</span>，负采样出<span
class="math inline">\(neg\)</span>个负样本<span
class="math inline">\((context(w),w_i)\)</span>，其中<span
class="math inline">\(i\in[1,\text{neg}]\)</span>。<br />
（3）进行梯度上升迭代过程，对于训练集中的每一个样本<span
class="math inline">\((context(w),w,w_1...w_{neg})\)</span>做如下处理：</p>
<ul>
<li>e=0，计算<span
class="math inline">\(h_w=\frac{1}{2c}\sum\limits_{i=-c}^cx_{i}\)</span></li>
<li>for <span
class="math inline">\(u=\{w\}\cup\{w_1...w_{neg}\}\)</span>，计算：
<ul>
<li><span class="math inline">\(f=\sigma(h_w\theta^u)\)</span></li>
<li><span class="math inline">\(g=\eta(y^u-f)\)</span></li>
<li><span class="math inline">\(e=e+g\cdot\theta^u\)</span></li>
<li><span class="math inline">\(\theta^u=\theta^u+g\cdot
h_w\)</span></li>
</ul></li>
<li>对于<span
class="math inline">\(context(w)\)</span>中的每一个词向量<span
class="math inline">\(x_i\)</span>（共<span
class="math inline">\(2c\)</span>个）进行更新：
<ul>
<li><span class="math inline">\(x_i=x_i+e\)</span></li>
</ul></li>
<li>如果梯度收敛，则结束梯度迭代，否则回到（3）继续迭代。</li>
</ul>
<p><strong>基于Negative Sampling的Skip-gram</strong>：<br />
输入：基于CBOW的语料训练样本，词典大小<span
class="math inline">\(V\)</span>，中心词<span
class="math inline">\(w\)</span>，词向量的维度大小<span
class="math inline">\(N\)</span>，上下文<span
class="math inline">\(C\)</span>大小<span
class="math inline">\(2c\)</span>，步长<span
class="math inline">\(\eta\)</span>。<br />
输出：词汇表每个词对应的模型参数<span
class="math inline">\(\theta\)</span>，所有的词向量<span
class="math inline">\(W=x_1,x_2,...,x_V\)</span>（避免和下面负样本混淆）。<br />
（1）随机初始化所有的模型参数<span
class="math inline">\(\theta\)</span>和所有的词向量<span
class="math inline">\(W\)</span>。<br />
（2）对于每个训练样本<span
class="math inline">\((context(w),w)\)</span>，负采样出<span
class="math inline">\(neg\)</span>个负样本<span
class="math inline">\((context(w),w_i)\)</span>，其中<span
class="math inline">\(i\in[1,\text{neg}]\)</span>。<br />
（3）进行梯度上升迭代过程，对于训练集中的每一个样本<span
class="math inline">\((context(w),w,w_1...w_{neg})\)</span>做如下处理：</p>
<ul>
<li>for <span class="math inline">\(s=context(w)\)</span>，计算：
<ul>
<li>e=0，<span class="math inline">\(h_w=x_i\)</span></li>
<li>for <span
class="math inline">\(u=\{w\}\cup\{w_1...w_{neg}\}\)</span>，计算：
<ul>
<li><span class="math inline">\(f=\sigma(h_w^s\theta^u)\)</span></li>
<li><span class="math inline">\(g=\eta(y^u-f)\)</span></li>
<li><span class="math inline">\(e=e+g\cdot\theta^u\)</span></li>
<li><span class="math inline">\(\theta^u=\theta^u+g\cdot
h_w^s\)</span></li>
</ul></li>
</ul></li>
<li>对于<span
class="math inline">\(context(w)\)</span>中的每一个词向量<span
class="math inline">\(x_i\)</span>（共<span
class="math inline">\(2c\)</span>个）进行更新：
<ul>
<li><span class="math inline">\(x_i^s=x_i^s+e\)</span></li>
</ul></li>
<li>如果梯度收敛，则结束梯度迭代，否则回到（3）继续迭代。</li>
</ul>
<hr />
<p><strong>第二个问题：负采样如何做？</strong><br />
对于<strong>Negative
Sampling</strong>模型，负采样是一个很重要的环节，对于一个中心词<span
class="math inline">\(w\)</span>，如何生成<span
class="math inline">\(neg\)</span>个负样本呢？由于词典中的词在预料中出现的频次不同，我们希望那些<strong>高频词被选为负样本的概率大</strong>，<strong>低频词被选为负样本的概率低</strong>，这就是负采样的本质要求————<strong>带权采样问题</strong>。</p>
<p>通过一段通俗的描述来帮助理解带权采样的机理：<br />
设大小为<span class="math inline">\(V\)</span>的词典<span
class="math inline">\(D\)</span>中每一个词对应一个线段<span
class="math inline">\(l(w)\)</span>，长度为：<br />
<span class="math display">\[
len(w)=\frac{count(w)}{\sum\limits_{u\in D}count(u)}
\]</span><br />
其中，分子表示一个词在语料中出现的次数（分母中的求和项用来做归一化）。将这些线段连接起来（共<span
class="math inline">\(V\)</span>个线段），形成一个长度为 1
的单位线段。如果随机的往这个线段上打点，则其中长度越长的线段（对应高频词）被打中的概率越大。</p>
<p>word2vec中词典<span
class="math inline">\(D\)</span>的词设置权值时，不是直接使用<span
class="math inline">\(count(w)\)</span>，而是对其做了<span
class="math inline">\(\alpha\)</span>次幂，其中<span
class="math inline">\(\alpha=\large\frac{3}{4}\)</span>，即上式变为：<br />
<span class="math display">\[
len(w)=\frac{[count(w)]^{\large\frac{3}{4}}}{\normalsize\sum\limits_{u\in
D}[count(u)]^{\large\frac{3}{4}}}
\]</span></p>
<p>word2vec中具体做法是：把上面长度为 1
的单位线段分成<strong>等距离</strong>的M份（<span
class="math inline">\(\text{M&gt;&gt;V}\)</span>），把这M份映射到前面讲的<strong>非等距离</strong>的V份中去。然后每次生成一个<span
class="math inline">\([1,M]\)</span>间的随机整数，代表选择<span
class="math inline">\(M\)</span>份中的一份，然后按映射找到对应<span
class="math inline">\(V\)</span>份中的一份，此时它对应的单词就是我们选择的负样本了。这样重复取<span
class="math inline">\(neg\)</span>次，就得到所有负样本了。word2vec中<span
class="math inline">\(M=10^8\)</span>（对应源码中变量table_size）。</p>
<h3 id="a-good-word-embedding">a good word embedding</h3>
<p><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzE1MDcuMDU1MjMucGRmJTIwaHR0cDovL2llZWV4cGxvcmUuaWVlZS5vcmcvZG9jdW1lbnQvNzQ3ODQxNy8=">How
to Generate a Good Word
Embedding?<i class="fa fa-external-link-alt"></i></span>论文中对比了不同模型，总结了选择word embedding经验。<br />
不同模型之间主要区别有两点：<br />
1、目标词和上下文关系。上下文来预测目标词（这类模型更能够捕获单词之间的可替代关系）、目标词来预测上下文<br />
2、上下文表示方法。<br />
<img src="/images/语言模型和词向量/good_embedding1.png"></p>
<p>不同模型之间上下文表示：<br />
<img src="/images/语言模型和词向量/good_embedding2.png"></p>
<p>据研究估计，<strong>文本含义信息的20%来自于词序，剩下的来自于词的选择</strong>。所以忽略词序信息的模型，将会损失约20%的信息。</p>
<p>本文做了三类对比实验：<br />
1、<strong>研究词向量的语义特性</strong>。该类实验是为了对比词向量的语义特性，包括：WordSim353，TOEFL，analogy
task：semantic和syntactic。<br />
2、<strong>将词向量作为特征</strong>。该类实验是为了对比词向量作为处理其他任务的特征时，对该任务性能的提升。包括：文本分类和命名实体识别。前者将词向量加权平均得到文本向量来分类，权值是词频，数据集用的是IMDB；后者用CoNLL03数据集做NER任务。<br />
3、<strong>用词向量来初始化神经网络模型</strong>。该类实验是为了研究词向量作为神经网络的初始值，对NN模型的提升。包括：CNN文本分类和词性标注。前者用了我们之前提到过的Kim的CNN模型，将句子表示成矩阵作为CNN的输入得到句子的表示，进行情感分类，数据集是Stanford
Sentiment Treebank；后者用Wall Street Journal数据集进行了POS
tagging任务。</p>
<p>该文对比了6种模型，并得到如下结论：<br />
Q：<strong>哪个模型最好？如何选择c和w的关系以及c的表示方法？</strong><br />
A：对于一个小型数据集来说，类似skip-gram这样越简单的模型效果越好，对于一个大型数据集来说，稍微复杂一点的模型，比如cbow和order就可以取得非常好的效果。真实数据中，skip-gram、cbow和order这样的模型足够了。在语义任务中，通过c来预测w的模型要更优于C&amp;W这种将c和w都放在输入层的模型。</p>
<p>Q：<strong>数据集的规模和所属领域对词向量的效果有哪些影响？</strong><br />
A：数据集的<strong>领域远比规模重要</strong>，给定一个任务之后，选择任务相关的领域数据将会提升性能，相反，如果数据并不相关，将会导致更差的性能。当然，如果数据都属于同一领域，规模越大性能越好。</p>
<p>Q：<strong>在训练模型时迭代多少次可以有效地避免过拟合？</strong><br />
A：通常的做法是在测试数据集上观察误差，当误差开始上升时即可停止训练，但经过本文的研究，这种方法并不能得到最好的task结果：因为训练词向量的目标是尽可能精确地预测目标词，这个优化目标和实际任务并不一致。因此最好的做法是：直接用实际任务的验证集来挑选迭代次数，即用task
data作为early
stopping的数据。如果实际任务非常耗时，则可以随机挑选某个简单任务（如：情感分类）及其验证集来挑选迭代次数。</p>
<p>Q：<strong>词向量的维度与效果之间的关系？</strong><br />
A：做词向量语义分析任务时，一般维度越大，效果越好。做具体NLP任务时（用作输入特征、或者网络初始化），50维之后效果提升就比较少了。</p>
<p>本文作者做了大量的工作，针对当前词向量模型的方方面面问题进行了研究，并且给出了许多有意义的结果。对此想了解更多，可以看一下作者的<span class="exturl" data-url="aHR0cDovL2xpY3N0YXIubmV0L2FyY2hpdmVzLzYyMA==">《How to Generate a Good Word
Embedding?》导读<i class="fa fa-external-link-alt"></i></span>。</p>
<hr />
<p><strong>word2vec结果评估</strong>：<br />
1、通过kmeans聚类，查看聚类的簇分布。<br />
2、通过词向量计算单词之间的相似度，查看相似词。<br />
3、通过类比：a之于b等价于c之于d。<br />
4、使用tsne降维可视化查看词的分布。</p>
<p><strong>word2vec输入向量和输出向量</strong>（<span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzE2MDguMDU4NTkucGRm">Using the Output Embedding
to Improve Language Models<i class="fa fa-external-link-alt"></i></span>）：<br />
1、在skip-gram模型中，在常见的衡量词向量的指标上，输出向量略微弱于输入向量。<br />
2、<strong>在基于RNN的语言模型中，输出向量反而强于输入向量</strong>。<br />
3、强制输入向量的转置作为输出向量，这可以使得输入向量等于输出向量。这种方式得到的词向量能够提升语言模型的困惑度perplexity。</p>
<p><strong>word2vec计算句子相似度</strong>（<span class="exturl" data-url="aHR0cDovL25scC50b3duL2Jsb2cvc2VudGVuY2Utc2ltaWxhcml0eS8=">Comparing Sentence
Similarity Methods<i class="fa fa-external-link-alt"></i></span>）：<br />
1、无监督方法：<br />
（1）对句子中所有的词的词向量求平均，获得句子embedding。<br />
（2）对句子中所有的词的词向量加权平均，每个词的权重为tf-idf，获得句子embedding。<br />
（3）对句子中所有的词的词向量加权平均，每个词的权重为smooth inverse
frequency:SIF（<span
class="math inline">\(\frac{a}{a+p(w)}\)</span>，<span
class="math inline">\(a\)</span>为超参数通常取0.001，<span
class="math inline">\(p(w)\)</span>为数据集中单词<span
class="math inline">\(w\)</span>的词频）；然后考虑所有的句子，并执行主成分分析；最后对每个句子的词向量加权平均减去first
principal componet，获得句子embedding。<br />
（4）通过 Word Mover's Distance:WMD
，直接度量句子之间的相似度。WMD：使用两个句子中单词的词向量来衡量一个句子中的单词需要在语义空间中移动到另一个句子中的单词的最小距离。<br />
2、有监督方法：<br />
（5）通过分类任务来训练一个文本分类器，取最后一个hidden
layer的输出作为句子embedding。就是使用文本分类器的前几层作为encoder。<br />
（6）直接训练一对句子的相似性，其优点是可以直接得到句子embeding。<br />
<strong>最终结论是：简单加权的词向量平均已经可以作为一个较好的baseline。</strong></p>
<h3 id="常见问题">常见问题</h3>
<p>问：噪声词在实际中被建议设为中心词的单字概率的3/4次幂，为什么？<br />
答：在保证高频词容易被抽到的大方向下，通过权重3/4次幂的方式，适当提升低频词、罕见词被抽到的概率。如果不这么做，低频词，罕见词很难被抽到，以至于不被更新到对应的Embedding。</p>
<p>问：一些“the”和“a”之类的英文高频词会对结果产生什么影响？如何处理？（可参考<span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzEzMTAuNDU0Ni5wZGY=">word2vec论文<i class="fa fa-external-link-alt"></i></span>第2.3节）<br />
答：噪声词为高频词对词向量训练没有什么效果，因为高频词太普遍了，为了抵消罕见词和高频词之间的不平衡，使用简单的二次抽样：训练集中的每个单词
<span class="math inline">\(w_i\)</span>
将有一定概率被丢弃，丢弃概率为：<br />
<span class="math display">\[
P(w_i)=1-\sqrt{\frac{t}{f(w_i)}}
\]</span><br />
其中 <span class="math inline">\(f(w_i)\)</span> 是单词 <span
class="math inline">\(w_i\)</span> 的频率，<span
class="math inline">\(t\)</span> 是选择的阈值，通常在 <span
class="math inline">\(10^{-5}\)</span>
左右，选择这个二次抽样公式是因为它主动地对频率大于 <span
class="math inline">\(t\)</span>
的词进行二次抽样，同时保持了频率ranking，即随着单词在语料库中出现的词频越来越大，该单词保留的概率越来越低。。虽然这个二次抽样公式是启发式选择的，但我们发现它在实践中运作良好。它加快了训练速度，甚至显着提高了罕见词所学向量的准确性。</p>
<p>问：如何训练包括“new york”在内的词组向量？（可参考<span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzEzMTAuNDU0Ni5wZGY=">word2vec论文<i class="fa fa-external-link-alt"></i></span>第4节）<br />
答：首先要找到经常一同出现但在其他语境中并不常见的单词。 例如，"New York
Times"和"Toronto Maple
Leafs"在训练集中将被独一无二的token所取代，而"this
is"将保持不变。这样，我们可以形成许多合理的短语，而不会大大增加词汇量的大小。理论上，我们可以使用所有的n元文法训练Skip-gram模型，但是这太消耗内存。
许多识别文本中短语的技术之前已经被开发出来了，
然而，比较它们超过了我们的工作范围。
我们决定使用一种简单的数据驱动方法，基于unigram和bigram的计数来形成短语：<br />
<span class="math display">\[
score(w_i,w_j) = \frac{count(w_iw_j)-\sigma}{count(w_i)\times
count(w_j)}
\]</span><br />
<span class="math inline">\(\sigma\)</span>
被用作折扣系数，防止形成太多由非常罕见的单词组成的短语。
得分高于所选阈值的bigram将被用作短语。
通常，我们逐渐减少阈值对训练数据进行2-4次传递，从而允许形成更长的短语(由数个单词组成)。</p>
<h2 id="glove">Glove</h2>
<p>学习词向量的所有无监督方法最终都是基于语料库的单词共现统计，因此这些模型之间存在共性。词向量学习算法有两个主要的模型族：</p>
<ul>
<li>基于全局矩阵分解的方法，如：LSA：latent semantic analysis。
<ul>
<li>优点：能够有效的利用全局的统计信息。</li>
<li>缺点：在单词类比任务（如：国王 vs 王后 类比于男人 vs
女人）中表现相对较差。</li>
</ul></li>
<li>基于局部上下文窗口的方法，如：word2vec。
<ul>
<li>优点：在单词类比任务中表现较好。</li>
<li>缺点：因为word2vec
在独立的局部上下文窗口上训练，因此难以利用单词的全局统计信息。</li>
<li>GloVe:Global Vectors for Word Representation，结合了 LSA 算法和
Word2Vec 算法的优点，既考虑了全局统计信息，又利用了局部上下文。</li>
</ul></li>
</ul>
<hr />
<p>设单词-单词共现矩阵为 <span class="math inline">\(X\)</span>
，其中元素 <span class="math inline">\(x_{ij}\)</span> 为词 <span
class="math inline">\(j\)</span> 出现在词 <span
class="math inline">\(i\)</span>
环境(context)的次数。这里"环境"有多种可能的定义。举个例子，在一段文本序列中，如果词
<span class="math inline">\(j\)</span> 出现在词 <span
class="math inline">\(i\)</span>
左边或者右边不超过10个词的距离，我们就可以认为词 <span
class="math inline">\(j\)</span> 出现在词 <span
class="math inline">\(i\)</span> 的环境一次，令 <span
class="math inline">\(x_i=\sum_k x_{ik}\)</span> 为任意词出现在词 <span
class="math inline">\(i\)</span> 的环境的次数，那么：<br />
<span class="math display">\[
P_{ij}=P(j|i)=\frac{x_{ij}}{x_i}
\]</span><br />
为词 <span class="math inline">\(j\)</span> 出现在词 <span
class="math inline">\(i\)</span> 的环境的概率。这一概率也称词 <span
class="math inline">\(i\)</span> 和词 <span
class="math inline">\(j\)</span> 的共现概率。</p>
<p>Glove论文展示了以下一组词对的共现概率与比值：</p>
<p><img src="/images/word2vec/1.png" width="60%"></p>
<p>我们通过商标可以观察以下现象：</p>
<ul>
<li>对于与“ice”相关但与“gas”无关的单词 <span
class="math inline">\(k\)</span> ，例如 <span
class="math inline">\(k=solid\)</span>
，我们预计会有更大的共现概率比值，例如8.9。</li>
<li>对于与“steam”相关但与“ice”无关的单词 <span
class="math inline">\(k\)</span> ，例如 <span
class="math inline">\(k=gas\)</span>
，我们预计较小的共现概率比值，例如0.085。</li>
<li>对于同时与“ice”和“steam”相关的单词 <span
class="math inline">\(k\)</span> ，例如 <span
class="math inline">\(k=water\)</span>
，我们预计其共现概率的比值接近1，例如1.36.</li>
<li>对于与“ice”和“steam”都不相关的单词 <span
class="math inline">\(k\)</span> ，例如 <span
class="math inline">\(k=fashion\)</span>
，我们预计共现概率的比值接近1，例如0.96.</li>
</ul>
<p>由此可见，共现概率的比值能够直观地表达词与词之间的关系。因此，我们可以设计三个词向量的函数来拟合这个比值。<br />
<span class="math display">\[
f(v_i,v_j,\tilde{v}_k)=\frac{P_{ik}}{P_{jk}}
\]</span><br />
其中，<span class="math inline">\(i\)</span> 和 <span
class="math inline">\(j\)</span>是中心词，<span
class="math inline">\(k\)</span> 是上下文词。<br />
<span class="math inline">\(f\)</span> 可以有多种设计，由于 <span
class="math inline">\(f\)</span>
映射的是向量空间，而向量空间是一个线性空间。因此从右侧的除法可以联想到减法：<br />
<span class="math display">\[
f(v_i,v_j,\tilde{v}_k)=f(v_i-v_j,\tilde{v}_k)
\]</span><br />
又因为共现概率的比值是标量，所以我们要求 <span
class="math inline">\(f\)</span> 是标量函数，即 <span
class="math inline">\(v_i-v_j\)</span> 和 <span
class="math inline">\(\tilde{v}_k\)</span>
均为向量，结果要求是标量，因此可以联想到向量的内积：<br />
<span class="math display">\[
f((v_i-v_j)^T\tilde{v}_k)=f(v_i^T\tilde{v}_k-v_j^T\tilde{v}_k)
\]</span><br />
我们希望左边为差的形式转为右边为商的方式，定义：<br />
<span class="math display">\[
f(v_i^T\tilde{v}_k-v_j^T\tilde{v}_k)=\frac{f(v_i^T\tilde{v}_k)}{f(v_j^T\tilde{v}_k)}
\]</span><br />
其中 <span
class="math inline">\(f(v_i^T\tilde{v}_k)=P_{ik}\)</span>，<span
class="math inline">\(f(v_j^T\tilde{v}_k)=P_{jk}\)</span>。<br />
上式左边为差的形式，右边为商的形式。因此联想到 <span
class="math inline">\(exp\)</span> 函数，即 <span
class="math inline">\(f\)</span> 为 <span
class="math inline">\(exp\)</span> 函数：<br />
<span class="math display">\[
exp(v_i^T\tilde{v}_k)=P_{ik}=\frac{x_{ik}}{x_i}
\]</span><br />
等式两边应用 <span class="math inline">\(log\)</span> 函数：<br />
<span class="math display">\[
\begin{aligned}
v_i^T\tilde{v}_k=log(x_{ik})-log(x_i)
\end{aligned}
\]</span><br />
即：<br />
<span class="math display">\[
log(x_{ik})=v_i^T\tilde{v}_k+log(x_i)
\]</span><br />
由于向量的内积具有对称性，即 <span class="math inline">\(X\)</span>
为对称矩阵，需要满足整个条件，但上面式子不满足，因为 <span
class="math inline">\(log(x_i)\)</span> 和 <span
class="math inline">\(log(x_k)\)</span>
不一定相等的，为了解决这个问题，模型引入两个偏置项：<br />
<span class="math display">\[
log(x_{ik})=v_i^T\tilde{v}_k+b_i+b_k
\]</span><br />
将索引 <span class="math inline">\(i\)</span> 和 <span
class="math inline">\(k\)</span>
互换，我们可以验证对称性得两个性质可以同时被上式满足。</p>
<p>上面的公式仅仅是理想状态，实际上只能要求左右两边尽可能相等。于是设计代价函数为：<br />
<span class="math display">\[
J=\sum\limits_{i,j=1}^Vf(x_{ij})(v_i^T\tilde{v}_j+b_i+b_j-log(x_{ij}))^2
\]</span><br />
其中，<span class="math inline">\(f(x_{ij})\)</span>
代表不同词频的词的重要性（权重）。使用优化算法最小化它即可。</p>
<p>对于权重函数 <span class="math inline">\(f(x_{ij})\)</span>
，一个建议的选择是：当 <span class="math inline">\(x&lt;c\)</span>
（例如 <span class="math inline">\(c=100\)</span>），令 <span
class="math inline">\(f(x)=(x/c)^\alpha\)</span> （例如 <span
class="math inline">\(\alpha=0.75\)</span>），反之令 <span
class="math inline">\(f(x)=1\)</span>
。需要注意的是，损失函数的计算复杂度与共现词频矩阵 <span
class="math inline">\(X\)</span> 中非零元素的数目呈线性关系。我们可以从
<span class="math inline">\(X\)</span>
中随机采样小批量非零元素，使用随机梯度下降迭代词向量和偏移项。当所有词向量学习得到后，Glove使用一个词得中心词向量与上下文词向量之和作为该词最终词向量。</p>
<p>GloVe 模型性能与语料库大小的关系：<br />
-
在语法任务中，模型性能随着语料库大小的增长而单调增长。这是因为语料库越大，则语法的统计结果越可靠。<br />
-
在语义任务中，模型性能与语料库绝对大小无关，而与语料库的有效大小有关。有效大小指的是语料库中，与目标语义相关的内容的大小。</p>
<p>GloVe 模型超参数选择：</p>
<ul>
<li>词向量大小：词向量大小越大，则模型性能越好。但是词向量超过 200
维时，维度增加的收益是递减的。</li>
<li>窗口对称性：计算一个单词的上下文时，上下文窗口可以是对称的，也可以是非对称的。
<ul>
<li>对称窗口：既考虑单词左侧的上下文，又考虑单词右侧的上下文。</li>
<li>非对称窗口：只考虑单词左侧的上下文。因为语言的阅读习惯是从左到右，所以只考虑左侧的上下文，不考虑右侧的上下文。</li>
</ul></li>
<li>窗口大小：
<ul>
<li>语法任务：选择小的、非对称的窗口时，模型性能更好。因为语法是局部的，所以小窗口即可；因为语法是依赖于单词顺序的，所以需要非对称窗口。</li>
<li>语义任务：则需要选择更大的窗口。因为语义是非局部的。</li>
</ul></li>
</ul>
<h2 id="fasttext">fastText</h2>
<p>fastText 是 Facebook AI Research 在 2016
年开源的文本分类器，其提出是在论文 《Bag of Tricks for Efficient Text
Classification》 中。目前 fastText 作为文本分类的基准模型。<br />
fastText 的优点是：在保持分类效果的同时，大大缩短了训练时间。</p>
<p>fastText在使用负采样的skip-gram模型基础上，将每个中心词视为子词(subword)的集合，并学习子词的词向量。<br />
以where这个词为例，设子词为3个字符，它的子词包括“&lt;wh”、“whe”、“her”、“ere”、“re&gt;”
和特殊子词（整词）“&lt;where&gt;”。其中的“&lt;”和“&gt;”是为了将作为前后缀的子词区分出来。而且，这里的子词“her”与整词“&lt;her&gt;”也可被分。给定一个词
<span
class="math inline">\(w\)</span>，我们通常可以把字符长度在3-6之间的所有子词和特殊子词的并集
<span class="math inline">\(\mathcal{G}_w\)</span>
取出。假设词典中任意子词 <span class="math inline">\(g\)</span>
的子词向量为 <span
class="math inline">\(z_g\)</span>，我们可以把使用负采样的skip-gram模型的损失函数（中心词
<span class="math inline">\(w_c\)</span>，上下文词 <span
class="math inline">\(w_o\)</span>，噪声词 <span
class="math inline">\(w_k\)</span> 在词典中的索引为 <span
class="math inline">\(i_k\)</span>）：<br />
<span class="math display">\[
-logP(w_o|w_c)=-log\frac{1}{1+exp(-u_o^Tv_c)}-\sum\limits_{k=1,w_k\sim
P(w)}^K log\frac{1}{1+exp(u_{i_k}^Tv_c)}
\]</span><br />
直接替换成：<br />
<span class="math display">\[
-logP(w_o|w_c)=-log\frac{1}{1+exp(-u_o^T\sum_{g\in \mathcal{G}_w }z_g
)}-\sum\limits_{k=1,w_k\sim P(w)}^K
log\frac{1}{1+exp(u_{i_k}^T\sum_{g\in \mathcal{G}_w }z_g)}
\]</span><br />
我们可以看到，原中心词向量被替换成了中心词的子词向量的和。与整词学习（word2vec和Glove）不同，词典以外的新词的词向量可以使用fastText中相应的子词向量之和。</p>
<p>fastText对于一些语言较重要(一定程度避免OOV)，例如阿拉伯语、德语和俄语。例如，德语中有很多复合词，例如兵乓球（table
tennis）在德语中叫“Tischtennis”。fastText可以通过子词可以表达两个词的相关性，例如“Tischtennis”和“Tennis”。</p>
<p><strong>总结：Glove用词向量表达共现词频的对数。fastText用子词向量之和表达整词。</strong></p>
<p>Fasttext用作文本分类时，预测的是标签值。可看作 字符or单词 +
N-gram方式输入，比如2-gram时<code>[w1,w2,w3]+[w12,w23]</code>。由于N-gram过多，使用hash分桶的方式确定N-gram的向量表示，缺点是桶少的话不同的N-gram会分到同一个桶(同一个向量表示)。Fasttext在做负采样时借点了Word2Vec方式，高词频被选为负样本概率大，同时上一个根号(降低高词频采样概率，提高低频词采样概率)。</p>
<p>Fasttext可以做文本分类：<span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzE2MDcuMDE3NTl2Mi5wZGY=">Bag of Tricks for
Efficient Text Classification<i class="fa fa-external-link-alt"></i></span>。<br />
Fasttext可以训练词向量：<span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzE2MDcuMDQ2MDYucGRm">Enriching word vectors with
subword information<i class="fa fa-external-link-alt"></i></span>。<br />
Fasttext官方：https://fasttext.cc/docs/en/supervised-tutorial.html</p>
<p><strong>补充问答</strong>：<br />
问：如果一个词出现在另一个词的背景窗口中，如何利用它们之间在文本序列的距离重新设计条件概率
<span class="math inline">\(P_{ij}\)</span> 的计算方式？（提示：可参考<span class="exturl" data-url="aHR0cHM6Ly9ubHAuc3RhbmZvcmQuZWR1L3B1YnMvZ2xvdmUucGRm">GloVe论文<i class="fa fa-external-link-alt"></i></span>4.2节）<br />
问：如果丢弃Glove中的偏移项，是否也可以满足任意一对词共现的对称性？<br />
问：在fastText中，子词过多怎么办？（例如，6字英文组合数为<span
class="math inline">\(26^6\)</span>）？（提示：可参考<span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzE2MDcuMDE3NTl2Mi5wZGY=">fastText论文<i class="fa fa-external-link-alt"></i></span>3.2节）</p>
<h1 id="补充">补充</h1>
<h2 id="softmax">softmax</h2>
<p><span class="math display">\[
\begin{aligned}
S_i=\dfrac{e^{V_i}}{\sum_j e^{V_j}}
\end{aligned}
\]</span><br />
其中，一个向量$ V<span class="math inline">\(共有\)</span>j<span
class="math inline">\(个值，\)</span>V_i<span
class="math inline">\(表示第\)</span>i<span class="math inline">\(个值。
首先对所有值进行\)</span>e^x$计算，保证所有值都是大于0的。其次进行归一化，保证所有值的和为1。这些特点非常符合概率的要求，所以经常把softmax处理后的值当成概率。</p>
<h2 id="sigmoid">sigmoid</h2>
<p><span class="math display">\[
\sigma(x)=\dfrac{1}{1+e^{-x}}
\]</span><br />
定义域为<span
class="math inline">\((-\infty,+\infty)\)</span>，值域为<span
class="math inline">\((0,1)\)</span>，下图给出了sigmoid的图像：<br />
<img src="/images/语言模型和词向量/sigmoid.png" width="40%"></p>
<p>sigmoid函数<strong>导函数</strong>具有性质：<br />
<span class="math display">\[
\sigma^\prime(x)=\sigma(x)[1-\sigma(x)]
\]</span><br />
由此可知：<br />
<span class="math display">\[
[log\sigma(x)]^\prime=1-\sigma(x)
\]</span><br />
<span class="math display">\[
[log(1-\sigma(x))]^\prime=-\sigma(x)
\]</span></p>
<p>sigmoid的每一次计算是相互独立的，是对当前事件的一次独立判断。我们把它用作二分类，是因为每一次判断的结果都可以根据阈值划分为两类，比如阈值为t，那么计算结果大于t的为一类，低于t的为另一类。也可以把计算结果看作二分类中一类的概率，比如计算结果为p，那么事件是一类的概率就是p，另一个类概率就是1-p。</p>
<h2 id="gensim">gensim</h2>
<p>Word2Vec：<br />
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 4.0</span></span><br><span class="line">gensim.models.word2vec.Word2Vec(sentences=<span class="literal">None</span>, corpus_file=<span class="literal">None</span>, vector_size=<span class="number">100</span>, </span><br><span class="line">alpha=<span class="number">0.025</span>, window=<span class="number">5</span>, min_count=<span class="number">5</span>, max_vocab_size=<span class="literal">None</span>, sample=<span class="number">0.001</span>, seed=<span class="number">1</span>, workers=<span class="number">3</span>, </span><br><span class="line">min_alpha=<span class="number">0.0001</span>, sg=<span class="number">0</span>, hs=<span class="number">0</span>, negative=<span class="number">5</span>, ns_exponent=<span class="number">0.75</span>, cbow_mean=<span class="number">1</span>, hashfxn=&lt;built-<span class="keyword">in</span> function <span class="built_in">hash</span>&gt;, </span><br><span class="line">epochs=<span class="number">5</span>, null_word=<span class="number">0</span>, trim_rule=<span class="literal">None</span>, sorted_vocab=<span class="number">1</span>, batch_words=<span class="number">10000</span>, </span><br><span class="line">compute_loss=<span class="literal">False</span>, callbacks=(), comment=<span class="literal">None</span>, max_final_vocab=<span class="literal">None</span>) </span><br><span class="line"><span class="comment"># 3.8</span></span><br><span class="line">gensim.models.word2vec.Word2Vec(sentences=<span class="literal">None</span>, corpus_file=<span class="literal">None</span>, size=<span class="number">100</span>, </span><br><span class="line">alpha=<span class="number">0.025</span>, window=<span class="number">5</span>, min_count=<span class="number">5</span>, max_vocab_size=<span class="literal">None</span>, sample=<span class="number">0.001</span>, seed=<span class="number">1</span>, workers=<span class="number">3</span>, </span><br><span class="line">min_alpha=<span class="number">0.0001</span>, sg=<span class="number">0</span>, hs=<span class="number">0</span>, negative=<span class="number">5</span>, ns_exponent=<span class="number">0.75</span>, cbow_mean=<span class="number">1</span>, hashfxn=&lt;built-<span class="keyword">in</span> function <span class="built_in">hash</span>&gt;, </span><br><span class="line"><span class="built_in">iter</span>=<span class="number">5</span>, null_word=<span class="number">0</span>, trim_rule=<span class="literal">None</span>, sorted_vocab=<span class="number">1</span>, batch_words=<span class="number">10000</span>, </span><br><span class="line">compute_loss=<span class="literal">False</span>, callbacks=(), max_final_vocab=<span class="literal">None</span>)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">sentences：(iterable of iterables, optional) 要分析的语料，可以是一个列表，或者从文件中遍历读出。</span></span><br><span class="line"><span class="string">          大语料库，建议直接从磁盘/网络流迭代传输句子。参阅word2vec模块中的BrownCorpus，Text8Corpus或LineSentence。</span></span><br><span class="line"><span class="string">corpus_file：(str, optional。LineSentence) 格式的语料库文件路径。</span></span><br><span class="line"><span class="string">size/vector_size：(int, optional) 词向量的维度，默认值是100。</span></span><br><span class="line"><span class="string">      这个维度的取值一般与我们的语料的大小相关，如果是不大的语料，比如小于100M的文本语料，则使用默认值一般就可以了。</span></span><br><span class="line"><span class="string">      如果是超大的语料，建议增大维度。</span></span><br><span class="line"><span class="string">window：(int, optional) 即词向量上下文最大距离。</span></span><br><span class="line"><span class="string">        这个参数在讲解中标记为c，值越大，则和某一词较远的词也会产生上下文关系。默认值为5。</span></span><br><span class="line"><span class="string">        如果是小语料则这个值可以设的更小。对于一般的语料这个值推荐在[5,10]之间。</span></span><br><span class="line"><span class="string">min_count：(int, optional) 忽略词频小于此值的单词。这个值可以去掉一些很生僻的低频词，默认是5。</span></span><br><span class="line"><span class="string">           如果是小语料，可以调低这个值。</span></span><br><span class="line"><span class="string">workers：(int, optional) 训练模型时使用的线程数。</span></span><br><span class="line"><span class="string">sg：(&#123;0, 1&#125;, optional) word2vec两个模型选择。0：CBOW模型。1：Skip-Gram模。默认是0即CBOW模型。</span></span><br><span class="line"><span class="string">hs：(&#123;0, 1&#125;, optional) word2vec两个解法选择。0：Negative Sampling。1：Hierarchical Softmax。默认是0即Negative Sampling。</span></span><br><span class="line"><span class="string">negative：(int, optional) 即使用Negative Sampling时负采样的个数，默认是5。推荐在[3,10]之间。这个参数在讲解中标记为neg。</span></span><br><span class="line"><span class="string">ns_exponent：(float, optional) 负采样分布指数。1.0样本值与频率成正比，0.0样本所有单词均等，负值更多地采样低频词。</span></span><br><span class="line"><span class="string">cbow_mean：(&#123;0, 1&#125;, optional) 仅用于CBOW在做投影的时候。</span></span><br><span class="line"><span class="string">           为0，则算法中的h为上下文的词向量之和，为1则为上下文的词向量的平均值。</span></span><br><span class="line"><span class="string">           在讲解中是按照词向量的平均值来描述的。</span></span><br><span class="line"><span class="string">alpha：(float, optional) 在随机梯度下降法中迭代的初始步长。讲解中标记为η，默认是0.025。</span></span><br><span class="line"><span class="string">min_alpha：(float, optional) 随着训练的进行，学习率线性下降到min_alpha。</span></span><br><span class="line"><span class="string">           由于算法支持在迭代的过程中逐渐减小步长，min_alpha给出了最小的迭代步长值。</span></span><br><span class="line"><span class="string">           随机梯度下降中每轮的迭代步长可以由iter，alpha， min_alpha一起得出。</span></span><br><span class="line"><span class="string">           对于大语料，需要对alpha, min_alpha,iter一起调参，来选择合适的三个值。</span></span><br><span class="line"><span class="string">seed：(int, optional) 随机数发生器种子。</span></span><br><span class="line"><span class="string">max_vocab_size：(int, optional) 词汇构建期间RAM的限制。</span></span><br><span class="line"><span class="string">                如果有更多的独特单词，则修剪不常见的单词。每1000万个类型的字需要大约1GB的RAM。</span></span><br><span class="line"><span class="string">max_final_vocab：(int, optional) 自动选择匹配的min_count将词汇限制为目标词汇大小。</span></span><br><span class="line"><span class="string">sample：(float, optional) 高频词随机下采样的配置阈值，范围是(0,1e-5)。</span></span><br><span class="line"><span class="string">hashfxn：(function, optional) 哈希函数用于随机初始化权重，以提高训练的可重复性。</span></span><br><span class="line"><span class="string">iter/epochs：随机梯度下降法中迭代的最大次数，默认是5。对于大语料，可以增大这个值。</span></span><br><span class="line"><span class="string">trim_rule：(function, optional) 词汇修剪规则，指定某些词语是否应保留在词汇表中，修剪掉或使用默认值处理。</span></span><br><span class="line"><span class="string">sorted_vocab：(&#123;0, 1&#125;, optional) 如果为1，则在分配单词索引前按降序对词汇表进行排序。</span></span><br><span class="line"><span class="string">batch_words：(int, optional) 每一个batch传递给线程单词的数量。</span></span><br><span class="line"><span class="string">compute_loss：(bool, optional) 如果为True，则计算并存储可使用get_latest_training_loss()检索的损失值。</span></span><br><span class="line"><span class="string">callbacks：(iterable of CallbackAny2Vec, optional) 在训练中特定阶段执行回调序列。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="comment"># 建立模型后</span></span><br><span class="line">build_vocab(sentences)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">遍历一次语料库建立词典</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="comment"># 3.8</span></span><br><span class="line">train(sentences=<span class="literal">None</span>, corpus_file=<span class="literal">None</span>, total_examples=<span class="literal">None</span>, total_words=<span class="literal">None</span>, </span><br><span class="line">epochs=<span class="literal">None</span>, start_alpha=<span class="literal">None</span>, end_alpha=<span class="literal">None</span>, word_count=<span class="number">0</span>, queue_factor=<span class="number">2</span>, </span><br><span class="line">report_delay=<span class="number">1.0</span>, compute_loss=<span class="literal">False</span>, callbacks=())</span><br><span class="line"><span class="comment"># 4.0</span></span><br><span class="line">train(corpus_iterable=<span class="literal">None</span>, corpus_file=<span class="literal">None</span>, total_examples=<span class="literal">None</span>, total_words=<span class="literal">None</span>, </span><br><span class="line">epochs=<span class="literal">None</span>, start_alpha=<span class="literal">None</span>, end_alpha=<span class="literal">None</span>, word_count=<span class="number">0</span>, queue_factor=<span class="number">2</span>, </span><br><span class="line">report_delay=<span class="number">1.0</span>, compute_loss=<span class="literal">False</span>, callbacks=(), **kwargs)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">第二次遍历语料库建立神经网络模型</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="comment"># 方法</span></span><br><span class="line">similar_by_word()</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">某一个词向量最相近的词集合</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">similarity()</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">两个词向量的相近程度</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">doesnt_match()</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">找出不同类的词</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">save_word2vec_format(<span class="string">&#x27;./model.txt&#x27;</span>,binary = <span class="literal">False</span>)</span><br><span class="line">save_word2vec_format(<span class="string">&#x27;./model.bin&#x27;</span>,binary = <span class="literal">True</span>)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">第一种，保存了训练的全部信息，可以在读取后追加训练</span></span><br><span class="line"><span class="string">第二种，保存为word2vec文本格式但是保存时丢失了词汇树等部分信息，不能追加训练</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">KeyedVectors.load_word2vec_format(<span class="string">&#x27;./model.bin&#x27;</span>, binary=<span class="literal">False</span>)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">加载模型</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="comment"># 使用Word2Vec得到model后：</span></span><br><span class="line"><span class="comment"># 3.8</span></span><br><span class="line">model.wv.vocab</span><br><span class="line">model.wv.vocab.keys()</span><br><span class="line"><span class="comment"># 4.0</span></span><br><span class="line">model.wv.key_to_index <span class="comment"># 字典</span></span><br><span class="line">model.wv.index_to_key <span class="comment"># 列表</span></span><br><span class="line">model.wv.vectors</span><br><span class="line"><span class="comment"># 共有</span></span><br><span class="line">model.vector_size</span><br><span class="line">model.wv[<span class="string">&#x27;key&#x27;</span>] <span class="comment"># 取单个词向量</span></span><br></pre></td></tr></table></figure><br />
LdaMulticore：<br />
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 4.0/3.8</span></span><br><span class="line">gensim.models.ldamulticore.LdaMulticore(corpus=<span class="literal">None</span>, num_topics=<span class="number">100</span>, </span><br><span class="line">id2word=<span class="literal">None</span>, workers=<span class="literal">None</span>, chunksize=<span class="number">2000</span>, passes=<span class="number">1</span>, batch=<span class="literal">False</span>, alpha=<span class="string">&#x27;symmetric&#x27;</span>, </span><br><span class="line">eta=<span class="literal">None</span>, decay=<span class="number">0.5</span>, offset=<span class="number">1.0</span>, eval_every=<span class="number">10</span>, iterations=<span class="number">50</span>, gamma_threshold=<span class="number">0.001</span>, </span><br><span class="line">random_state=<span class="literal">None</span>, minimum_probability=<span class="number">0.01</span>, minimum_phi_value=<span class="number">0.01</span>, </span><br><span class="line">per_word_topics=<span class="literal">False</span>)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">corpus: &#123;iterable of list of (int, float), scipy.sparse.csc&#125;，语料库。</span></span><br><span class="line"><span class="string">num_topics: int，主题数量。</span></span><br><span class="line"><span class="string">id2word: &#123;dict of (int, str), gensim.corpora.dictionary.Dictionary&#125;，单词id-&gt;单词 词典。</span></span><br><span class="line"><span class="string">workers: int，线程数。</span></span><br><span class="line"><span class="string">chunksize: int，每个训练模块使用文档数量。</span></span><br><span class="line"><span class="string">passes: int，训练期间通过语料库的次数。</span></span><br><span class="line"><span class="string">alpha: float，文档-主题分布的先验信念。</span></span><br><span class="line"><span class="string">      &#x27;symmetric&#x27;: 使用 1.0 / num_topics 的固定对称先验。</span></span><br><span class="line"><span class="string">      &#x27;asymmetric&#x27;: 使用 1.0 / (topic_index + sqrt(num_topics)) 的固定归一化非对称先验。</span></span><br><span class="line"><span class="string">eta: float，主题-词分布的先验信念。</span></span><br><span class="line"><span class="string">      &#x27;symmetric&#x27;: 使用 1.0 / num_topics 的固定对称先验。</span></span><br><span class="line"><span class="string">      &#x27;auto&#x27;: 从语料库中学习非对称先验。</span></span><br><span class="line"><span class="string">per_word_topics: 如果为 True，该模型还会计算一个主题列表，按每个单词最可能的主题的降序排序，</span></span><br><span class="line"><span class="string">                  以及它们的 phi 值乘以特征长度（即字数）。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><br />
词典：<br />
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line">gensim.corpora.dictionary.Dictionary(documents=<span class="literal">None</span>, prune_at=<span class="number">2000000</span>)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">根据文档生成词典</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="comment"># 属性</span></span><br><span class="line">token2id</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">词典：&#123;token:tokenId&#125;</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">dfs</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">单词出现的频率</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="comment"># 方法</span></span><br><span class="line">doc2bow(document, allow_update=<span class="literal">False</span>, return_missing=<span class="literal">False</span>)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">将文档转成词袋格式：一个列表，列表中每个元素为(token_id, token_count)</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">filter_n_most_frequent(remove_n)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">过滤掉出现频率最高的remove_n个单词</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">filter_extremes(no_below=<span class="number">5</span>, no_above=<span class="number">0.5</span>, keep_n=<span class="number">100000</span>) </span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">去掉出现次数低于no_below的。</span></span><br><span class="line"><span class="string">去掉出现次数高于no_above的（百分数）。</span></span><br><span class="line"><span class="string">在上面基础上，保留出现频率前keep_n的单词。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">filter_tokens(bad_ids=<span class="literal">None</span>, good_ids=<span class="literal">None</span>)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">两种用法:</span></span><br><span class="line"><span class="string">(1)去掉bad_id对应的词。</span></span><br><span class="line"><span class="string">(2)保留good_id对应的词而去掉其他词。</span></span><br><span class="line"><span class="string">注意，这里bad_ids和good_ids都是列表形式。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">compacity() </span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">执行完过滤操作以后，可能会造成单词的序号之间有空隙。</span></span><br><span class="line"><span class="string">可以使用该函数来对词典来进行重新排序，去掉这些空隙。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure></p>
<h1 id="参考文献">参考文献</h1>
<p><span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL2hhbmtjcy9IYW5MUA==">HanLP: Han Language
Processing<i class="fa fa-external-link-alt"></i></span><br />
统计自然语言处理 第二版 (宗成庆著)<br />
<span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzEzMDEuMzc4MS5wZGY=">Efficient Estimation of
Word Representations in Vector Space<i class="fa fa-external-link-alt"></i></span><br />
<span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzEzMTAuNDU0Ni5wZGY=">Distributed
Representations of Words and Phrases and their
Compositionality<i class="fa fa-external-link-alt"></i></span><br />
<span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzE0MTEuMjczOC5wZGY=">word2vec Parameter
Learning Explained<i class="fa fa-external-link-alt"></i></span><br />
<span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzE1MDcuMDU1MjMucGRmJTIwaHR0cDovL2llZWV4cGxvcmUuaWVlZS5vcmcvZG9jdW1lbnQvNzQ3ODQxNy8=">How
to Generate a Good Word Embedding?<i class="fa fa-external-link-alt"></i></span><br />
<span class="exturl" data-url="aHR0cDovL25scC50b3duL2Jsb2cvc2VudGVuY2Utc2ltaWxhcml0eS8=">Comparing Sentence
Similarity Methods<i class="fa fa-external-link-alt"></i></span><br />
<span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzE2MDguMDU4NTkucGRm">Using the Output
Embedding to Improve Language Models<i class="fa fa-external-link-alt"></i></span><br />
<span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzE0MDIuMzcyMi5wZGY=">word2vec Explained:
Deriving Mikolov et al.’sNegative-Sampling Word-Embedding
Method<i class="fa fa-external-link-alt"></i></span><br />
<span class="exturl" data-url="aHR0cHM6Ly9ubHAuc3RhbmZvcmQuZWR1L3B1YnMvZ2xvdmUucGRm">GloVe: Global Vectors
for Word Representation<i class="fa fa-external-link-alt"></i></span><br />
<span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzE2MDcuMDE3NTl2Mi5wZGY=">Bag of Tricks for
Efficient Text Classification<i class="fa fa-external-link-alt"></i></span><br />
<span class="exturl" data-url="aHR0cHM6Ly96aC53aWtpcGVkaWEub3JnL3dpa2kvJUU5JTlDJThEJUU1JUE0JUFCJUU2JTlCJUJDJUU3JUJDJTk2JUU3JUEwJTgx">维基百科：霍夫曼编码<i class="fa fa-external-link-alt"></i></span><br />
<span class="exturl" data-url="aHR0cHM6Ly93d3cuY25ibG9ncy5jb20vcGluYXJkL3AvNzI0MzUxMy5odG1s">word2vec原理(二)
基于Hierarchical Softmax的模型<i class="fa fa-external-link-alt"></i></span><br />
<span class="exturl" data-url="aHR0cHM6Ly93d3cuY25ibG9ncy5jb20vcGluYXJkL3AvNzI0OTkwMy5odG1s">word2vec原理(三)
基于Negative Sampling的模型<i class="fa fa-external-link-alt"></i></span><br />
<span class="exturl" data-url="aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2l0cGx1cy9hcnRpY2xlL2RldGFpbHMvMzc5Njk5Nzk=">word2vec
中的数学原理详解（四）基于 Hierarchical Softmax 的模型<i class="fa fa-external-link-alt"></i></span><br />
<span class="exturl" data-url="aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2l0cGx1cy9hcnRpY2xlL2RldGFpbHMvMzc5OTg3OTc=">word2vec
中的数学原理详解（五）基于 Negative Sampling 的模型<i class="fa fa-external-link-alt"></i></span><br />
<span class="exturl" data-url="aHR0cHM6Ly96aC12Mi5kMmwuYWkvY2hhcHRlcl9uYXR1cmFsLWxhbmd1YWdlLXByb2Nlc3NpbmctcHJldHJhaW5pbmcvd29yZDJ2ZWMuaHRtbA==">词嵌入（Word2vec）<i class="fa fa-external-link-alt"></i></span><br />
<span class="exturl" data-url="aHR0cHM6Ly93d3cuYmlsaWJpbGkuY29tL3ZpZGVvL0JWMTJXNDExdjdHYT9mcm9tPXNlYXJjaCZzZWlkPTE0MTM1NDUwODQ5MzA3Mzg5Njc5JnNwbV9pZF9mcm9tPTMzMy4zMzcuMC4w">[MXNet/Gluon]
动手学深度学习第十六课：词向量（word2vec）<i class="fa fa-external-link-alt"></i></span><br />
<span class="exturl" data-url="aHR0cHM6Ly93d3cuYmlsaWJpbGkuY29tL3ZpZGVvL0JWMWFXNDExZTcyOD9mcm9tPXNlYXJjaCZzZWlkPTU3NTg3NDE1MjA2MTQ0OTEzMzEmc3BtX2lkX2Zyb209MzMzLjMzNy4wLjA=">[MXNet/Gluon]
动手学深度学习第十七课：GloVe、fastText和使用预训练的词向量<i class="fa fa-external-link-alt"></i></span></p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://soundmemories.github.io/2021/06/20/Paper/01.Unsupervised%20Data%20Augmentation%20for%20Consistency%20Training/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avstar.png">
      <meta itemprop="name" content="SoundMemories">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="SoundMemories">
      <meta itemprop="description" content="今日事，今日毕">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | SoundMemories">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2021/06/20/Paper/01.Unsupervised%20Data%20Augmentation%20for%20Consistency%20Training/" class="post-title-link" itemprop="url">Unsupervised Data Augmentation for Consistency Training</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2021-06-20 00:00:00" itemprop="dateCreated datePublished" datetime="2021-06-20T00:00:00+08:00">2021-06-20</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Paper/" itemprop="url" rel="index"><span itemprop="name">Paper</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>4k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>15 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="摘要">摘要</h1>
<p>背景：深度学习的模型训练通常依赖大量的标签数据（如Bert、XLNet），在只有少量数据上通常表现不好。由此产生了数据增强，但以前的研究都是基于监督学习的，并且效果不是特别理想。</p>
<p>贡献：本文提出了一种对无监督（无标签）数据增强方式（半监督学习中无标签数据的增强），简称UDA。UDA方法生成的无监督数据与原始无监督数据具备<strong>分布的一致性</strong>，而以前的方法通常只是应用高斯噪声和dropout噪声（无法保证一致性）。</p>
<p>效果：使用这种数据增强方法，在极少量数据集上，六种语言任务和三种视觉任务都得到了明显的提升。IMDb数据分类任务上，仅仅使用20个带标签数据加UDA方法，就超过了25000个带标签数据的训练模型，错误率达到了4.2%。在CIFAR-10上仅用4000张标签图片就达到了2.7%的错误率。在SVHN任务上，仅仅用250个标签数据就达到了2.85%的错误率，这相当于用全数据集才能达到的正确率，而它们的数量级差别达到了1或2（差10倍或100倍）。在大量标签数据集上，UDA同样表现优秀，在ImageNet任务上，使用10%带标签数据，UDA方法就将Top1和Top5的准确率分别由55.1%提高到77.3%，68.7提高到88.5%。在全数据集上，则分别由78.3%提高到94.4%，79%提高到94.5%。</p>
<h1 id="导读">导读</h1>
<p>深度学习需要大量带标签数据，但是实际工程中很难满足，这就需要数据标注，但数据标注是一项耗时耗力的工作。所以，充分利用未标注数据是一个很有意义的研究方向。而半监督方法，是最有前景的方法之一，当前半监督方法可归结为三类：<br />
（1）基于图卷积和图嵌入的图标签传播方法。<br />
（2）将目标数据作为潜变量进行预测。<br />
（3）强制一致/平滑。这种方法在许多任务中被证明具有较好的效果。</p>
<p>强制平滑方法只是使得模型对应较小的噪声不那么敏感。常用方法就是：对于一个样本，添加一些噪声（例如高斯噪声）然后强制让模型对于加噪和不加噪的数据的输出尽量的相似。直观而言就是一个好的模型，应该能够适应各种小的、不改变样本性质的扰动。通常由于扰动函数的不同会有各种不同的方案。</p>
<p>本文在Sajjadi、
Laine等人的研究的基础上，从有监督数据中学习扰动函数，从而得到最优的数据增强方法。良好的数据增强方法能够大大提高模型的结果，并且数据增强方法能应用于各领域。<strong>本文使用的优化方法是最小化增强数据与真实数据之间的KL散度</strong>。虽然有监督数据的数据增强取得了很多成功，但是大量的无监督数据使得UDA这种无监督数据增强方法拥有更广阔前景。</p>
<p>主要贡献：<br />
（1）提出一种TSA方法，该方法能够在无标签数据大于标签数据的时候防止过拟合。<br />
（2）证明<strong>针对性的数据增强</strong>（如AutoAugment）效果明显优于无针对性的数据增强。<br />
（3）验证了本文方法在NLP任务上（如Bert）上的有效性。<br />
（4）在CV和NLP任务中，本文方法都表现优异。<br />
（5）研究一种能应用于分类数据中有标签数据和无标签数据不匹配情况的方法（数据不平衡处理方法）。</p>
<h1 id="内容">内容</h1>
<h2 id="有监督数据增强">有监督数据增强</h2>
<p>在保持标签相同（同一类别）的情况下，通过某种转换方法扩充出类似于真实数据的训练数据。简单而言就是，有一个样本<span
class="math inline">\(x\)</span>，通过转换函数<span
class="math inline">\(q(x)\)</span>生成新数据<span
class="math inline">\(\hat{x}\)</span>，新旧数据有相同的数据标签<span
class="math inline">\(y(\hat{x})=y(x)\)</span>。通常为了得到的增强数据与原始数据相似，使用的是最大似然估计方法。</p>
<p>数据增强方法可以看成是从有标签数据中扩充出更多的有标签数据，然后用扩充数据进行模型训练。因此，扩充数据相对于原始数据必须是有效的变换（例如图片缩放对图片识别可能有效，图片旋转可能无效）。也因此，如何设计转换函数至关重要。</p>
<p>目前，针对NLP任务的有监督数据增强方法已经取得了很大进展。虽然有成果，但是它通常被比喻成“蛋糕上的樱桃”，只是提高有限的性能，这是由于监督数据通常都是少量的。因此，本文研究了一种基于大量数据的无监督数据增强方法。</p>
<h2 id="无监督数据增强">无监督数据增强</h2>
<p>本文研究了一种利用无监督数据的强制平滑方法（类似VAT）。工作流程如下：<br />
<img src="/images/UDA/UDA.png" width="90%"></p>
<p>（1）监督学习部分，使用交叉熵损失函数，模型是<span
class="math inline">\(p_\theta(y|x)\)</span>。<br />
（2）无监督学习部分，使用强制平滑损失函数，对无标签数据进行数据增强，使增强前和增强后的数据分布越相近越好。增强前模型<span
class="math inline">\(p_{\tilde{\theta}}(y|x)\)</span>，增强后模型<span
class="math inline">\(p_\theta(y|\hat{x})\)</span>。<br />
（3）最后，同时使用有标签和无标签数据，把二者模型结合起来，得到Final
Loss。</p>
<p>本文使用最小化<strong>增强后的无标签数据</strong>和<strong>增强前无标签数据</strong>的KL散度。公式如下：<br />
<img src="/images/UDA/UDA_loss1.png" width="60%"></p>
<p>为了同时使用带标签数据和无标签数据，作者在计算带标签数据时上加上交叉熵损失和权重<span
class="math inline">\(\lambda\)</span>。<br />
<img src="/images/UDA/UDA_loss2.png" width="40%"></p>
<p>其中<span class="math inline">\(\it
q(\hat{x}|x)\)</span>是数据增强变换，<span
class="math inline">\(\tilde{\theta}\)</span>是当前参数<span
class="math inline">\(\theta\)</span>的固定副本，表明梯度像Miyato等人所建议的那样，不是通过<span
class="math inline">\(\tilde{\theta}\)</span>传播的。这里使用的数据增强与监督数据增强中使用的增强方法相同。由于数据增强耗时比较大，所以数据增强是离线生成的，单个原始样本会生成多个增强样本。</p>
<p>在无监督学习时，使用了针对性的数据增强：<br />
（1）<strong>Back-translation</strong>：回译能够在保证语义不变的情况下，生成多样的句式。实验证明，在QANet上，这种策略取得了良好的效果。因此作者在情感分类问题等数据集，如IMDb，Yelp-2，Yelp-5，Amazon-2，Amazon-5上采用了这种策略，同时，他们发现，句式的多样性比较有效性更重要。所以使用了<strong>RandAugument</strong>。<br />
（2）<strong>RandAugument</strong>：随机抽样增强，加入噪声。采用随机抽样代替集束搜索策略（一种贪心策略）。具体而言，作者使用WMT14语料库来训练英语到法语和法语到英语的翻译模型，并对每个句子执行回译，而不是整个段落，因为WMT14中的并行数据是用于句子级翻译，而情感分类语料库中的输入类型是段落。<br />
（3）<strong>TF-IDF word
replacement</strong>：虽然回译能够很好的进行数据扩充，但是它并不能保证扩充的句子包含关键词。而对于某些任务，如DBPedia任务，它的目标是预测某些句子属于维基百科的哪个词条。因此关键字非常重要，本文研究了一种在保留TF-IDF高的关键字，用其他非关键字替代TF-IDF分数低的非关键字扩充方案，详细见论文附录B。<br />
增强结果如图所示：<br />
<img src="/images/UDA/trans.png" width="80%"></p>
<p>当然，对CV任务用了<strong>AutoAugument</strong>：用强化学习来搜索图像增强的“最优”组合，其性能明显优于任何人工设计的优化方法。作者使用已发现的增强策略，在CIFAR-10，
SVHN和ImageNet上进行了实验，并在CIFAR-10，SVHN上组合应用了Cutout技术。<br />
增强结果如图所示：<br />
<img src="/images/UDA/trans2.png" width="80%"></p>
<h2
id="数据增强在多样性和有效性上的平衡">数据增强在多样性和有效性上的平衡</h2>
<p>虽然在一些非常优秀的数据增强方法中，能够得到很好的多样性和有效性。但是，由于多样性是通过改变原始数据得到的，所以，它存在改变数据类别的风险，所以，多样性和有效性是存在一定矛盾的。</p>
<p>对于图像分类，AutoAugment算法在有监督的环境下，根据验证集的性能进行优化，从而自动找到多样性和有效性之间的最佳点。</p>
<p>对于文本分类，作者调整随机抽样的强度。一方面，当强度为0时，随机抽样解码退化为贪婪方法，产生完全有效但完全相同的样本。另一方面，当作者使用1的强度时，随机抽样会产生非常不同但几乎不可读的样本。作者发现，设置Softmax强度为0.7、0.8或0.9的表现最好。</p>
<h2 id="训练技巧">训练技巧</h2>
<p>要介绍一些针对不同问题，不同场景下的训练技巧。</p>
<p><strong>Training Signal
Annealing（TSA）</strong>：针对标签数据与未标签数据不平衡时的场景。由于有大量的未标签数据需要UDA处理，所以需要一个较大模型，但是由于较大模型很容易在少量标签数据下过拟合，所以，提出了本方法用于解决该问题。<br />
TSA原理就是在训练过程中，随着未标签数据的增加，逐步去除带标签数据，从而避免模型过拟合到带标签的训练数据。具体而言，就是在训练的<span
class="math inline">\(t\)</span>时刻，设置一个阈值<span
class="math inline">\(\eta_t\)</span>，当<span
class="math inline">\(\frac{1}{k}\leqslant\eta_t\leqslant
1\)</span>，其中<span
class="math inline">\(k\)</span>是类别数。当某个标签计算的<span
class="math inline">\(p_\theta(y^*|x)\)</span>大于阈值<span
class="math inline">\(\eta_t\)</span>，就将该标签数据移除出计算损失的过程，而只计算miniBatch里面的其余数据。假定miniBatch样本记作B，那么该策略计算损失如下：<br />
<img src="/images/UDA/TSA.png" width="40%"><br />
过滤后的样本集合：<br />
<img src="/images/UDA/TSA2.png" width="40%"></p>
<p>阈值<span
class="math inline">\(\eta_t\)</span>用于防止模型过拟合到标签数据。随着<span
class="math inline">\(\eta_t\)</span>向1靠近，模型只能缓慢地从标注的实例中得到监督，大大缓解了过拟合问题。假设T是总训练步数，t是当前的训练步数。为了考虑未标记数据和标记数据的不同比率，有以下三种<span
class="math inline">\(\eta_t\)</span>更新计算方式：<br />
<img src="/images/UDA/TSA3.png" width="90%"></p>
<p>对于数据量少，容易过拟合的情况，使用指数形式比较好。对于标签数据不容易过拟合的情况，比如标签数据比较多或者使用了有效的正则化手段时，使用对数形式会比较好。使用不同更新方式的效果：<br />
<img src="/images/UDA/TSA4.png" width="50%"></p>
<p><strong>Sharpening Predictions</strong><br />
当标签数据很少时，未标签数据和预测的未标签数据分布会很平坦。因此，在计算KL散度时，主要贡献的部分来自于标签数据。例如在Imagenet任务中，使用10%标签数据下，未标签数据的分布明显比标签数据的分布更加平坦。而比较丰富的数据分布是比较有利于模型训练的，因此，提出以下三种锐化方案：<br />
（1）基于置信度的mask：对模型预测效果不好的，预测的概率小于一定阈值的标签，不计算一致性损失。<br />
（2）最小化熵：最小化熵就是使得预测的增广数据能够拥有一个较低的熵，因此，需要在计算损失时，加上熵的计算。<br />
（3）Softmax控制：通过调整Softmax控制输出， <span
class="math inline">\(p_{\tilde{\theta}}(y|x)\)</span>通过<span
class="math inline">\(Softmax(l(x)/\tau)\)</span>计算，其中<span
class="math inline">\(l(x)\)</span>表示结果逻辑分布概率，<span
class="math inline">\(\tau\)</span>表示强度。<span
class="math inline">\(\tau\)</span>越小，分布越锐化。</p>
<p><strong>Domain-relevance Data Filtering</strong><br />
通常，作者希望能够运用领域外的数据，因为它比较容易获取。但是，一般领域外的数据和领域内的数据不匹配。由于数据分布的不匹配，使用领域外的数据往往对模型是有负面影响的。为了获取与当前任务相关的域数据，本文采用一种通用的检测领域外数据的技术。作者用领域内的数据训练了一个模型，让后用它去评估领域外的数据，然后过滤掉置信度低的数据。具体说就是，对于分类任务，对所有领域外数据进行概率计算，只使用其中分类正确且概率高的数据。</p>
<h2 id="实验结果">实验结果</h2>
<p>本文对文本分类和视觉相关任务，运用UDA进行了实验。包括六项文本分类任务和三项图片分类任务。<br />
### 文本分类<br />
实验是基于Bert进行的，因为它在许多NLP任务中表现都很好。具体实验设置请看原始论文，实验结果如下：<br />
<img src="/images/UDA/01.png" width="80%"></p>
<p>实验结果表明，运用UDA后，基本都取得了较大的提高。同时，作者还实验了<strong>不同数量的标签</strong>对结果的影响，结果如下：<br />
<img src="/images/UDA/02.png" width="80%"></p>
<p>作者实验对比了UDA与半监督方法，结果显示，UDA结果明显更优。<br />
<img src="/images/UDA/03.png" width="90%"></p>
<p>同时，作者还对比实验了不同模型的情况：<br />
<img src="/images/UDA/04.png" width="80%"></p>
<h3 id="图像任务">图像任务</h3>
<p>ImageNet之所以要单独拿出来，是因为它是一个很有挑战性的任务，而且数据量很大。作者使用10%标签数据和全数据分别做了对比（图片尺寸224）。<strong>10%标签数据</strong>，ImageNet对比实验结果：<br />
<img src="/images/UDA/05.png" width="50%"></p>
<p><strong>全数据</strong>，ImageNet对比实验结果：<br />
<img src="/images/UDA/06.png" width="50%"></p>
<p>作者做了<strong>使用不同训练策略</strong>下的情况，TSA对比实验结果：<br />
<img src="/images/UDA/07.png" width="50%"></p>
<p>最后，作者做了<strong>消融实验，对比不同策略的重要性</strong>。不同模块的消融实验结果：<br />
<img src="/images/UDA/08.png" width="50%"></p>
<h1 id="总结">总结</h1>
<p>本文提供了一种无监督数据（无标签）数据增强方式，通过<strong>Back-translation</strong>、<strong>RandAugument</strong>、<strong>TF-IDF
word
replacement</strong>方法对无监督文本数据增强，使用<strong>AutoAugument</strong>对图像数据进行增强，最后使用KL散度使新生成的样本数据和原样本数据分布一致，最后结合有监督数据（有标签）形成最终的损失函数，通过<strong>TSA</strong>处理了无标签数据大于有标签数据的过拟合问题。</p>
<p>本文重要的是使用了针对性的数据增强，并且效果很好，不同于传统的高斯噪声、dropout噪声、或者简单的仿射变换，这种针对性的增强能生成更有效的噪声。并且对扰动的有效性和多样性进行了平衡。</p>
<p>这种针对性的思想值得学习，并且考虑分布影响，结合可以利用的增强方式，比如EDA中提到的同义词替换（synonym
replacement）和随机插入（random Insertion，RI）。</p>
<h1 id="参考文献">参考文献</h1>
<p><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzE5MDQuMTI4NDh2Mi5wZGY=">Unsupervised Data
Augmentation for Consistency Training<i class="fa fa-external-link-alt"></i></span><br />
<span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzE5MDQuMTI4NDgucGRmP3JlZj1oYWNrZXJub29uLmNvbQ==">Unsupervised
Data Augmentation for Consistency Training<i class="fa fa-external-link-alt"></i></span><br />
<span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzE5MDEuMTExOTYucGRm">EDA: Easy Data
Augmentation Techniques for Boosting Performance on Text Classification
Tasks<i class="fa fa-external-link-alt"></i></span><br />
<span class="exturl" data-url="aHR0cHM6Ly9zZXZlbm9sZC5naXRodWIuaW8vMjAyMC8wNi90ZXh0X0VEQS8=">自然语言处理之文本数据增强<i class="fa fa-external-link-alt"></i></span><br />
Github：<span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL2dvb2dsZS1yZXNlYXJjaC91ZGE=">uda<i class="fa fa-external-link-alt"></i></span><br />
Github：<span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL3poYW5sYW9iYW4vRURBX05MUF9mb3JfQ2hpbmVzZQ==">EDA_NLP_for_Chinese<i class="fa fa-external-link-alt"></i></span><br />
<span class="exturl" data-url="aHR0cHM6Ly93d3cuamlhbnNodS5jb20vcC81ZDRlMThiOGRlMDQ=">谷歌惊艳的无监督数据增强方法--Unsupervised
Data Augmentation for Consistency Training<i class="fa fa-external-link-alt"></i></span></p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




  <nav class="pagination">
    <a class="extend prev" rel="prev" title="上一页" aria-label="上一页" href="/"><i class="fa fa-angle-left"></i></a><a class="page-number" href="/">1</a><span class="page-number current">2</span><a class="page-number" href="/page/3/">3</a><span class="space">&hellip;</span><a class="page-number" href="/page/13/">13</a><a class="extend next" rel="next" title="下一页" aria-label="下一页" href="/page/3/"><i class="fa fa-angle-right"></i></a>
  </nav>

</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="copyright">
    &copy; 2019 – 
    <span itemprop="copyrightYear">2024</span>
    <span class="with-love">
      <i class="fa fa-heart"></i>
    </span>
    <span class="author" itemprop="copyrightHolder">SoundMemories</span>
  </div>
  <div class="powered-by">由 <span class="exturl" data-url="aHR0cHM6Ly9oZXhvLmlv">Hexo</span> & <span class="exturl" data-url="aHR0cHM6Ly90aGVtZS1uZXh0LmpzLm9yZy9tdXNlLw==">NexT.Muse</span> 强力驱动
  </div>

    </div>
  </footer>

  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>
  <div class="sidebar-dimmer"></div>
  <div class="back-to-top" role="button" aria-label="返回顶部">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>
  <a role="button" class="book-mark-link book-mark-link-fixed"></a>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/next-theme-pjax/0.6.0/pjax.min.js" integrity="sha256-vxLn1tSKWD4dqbMRyv940UYw4sXgMtYcK6reefzZrao=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.7.0/jquery.min.js" integrity="sha256-2Pmvv0kuTBOenSvLm6bvfBSSHrUJ+3A7x6P5Ebd07/g=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.js" integrity="sha256-yt2kYMy0w8AbtF89WXb2P1rfjcP/HTHLT7097U8Y5b8=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/medium-zoom/1.0.8/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/muse.js"></script><script src="/js/next-boot.js"></script><script src="/js/bookmark.js"></script><script src="/js/pjax.js"></script>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-generator-searchdb/1.4.1/search.js" integrity="sha256-1kfA5uHPf65M5cphT2dvymhkuyHPQp5A53EGZOnOLmc=" crossorigin="anonymous"></script>
<script src="/js/third-party/search/local-search.js"></script>

  <script class="next-config" data-name="pdf" type="application/json">{"object_url":{"url":"https://cdnjs.cloudflare.com/ajax/libs/pdfobject/2.2.12/pdfobject.min.js","integrity":"sha256-g2xji1rlE3KsGVClvuxTbcR0Kn2+wtQADSff2Tbb4zA="},"url":"/lib/pdf/web/viewer.html"}</script>
  <script src="/js/third-party/tags/pdf.js"></script>

  <script class="next-config" data-name="mermaid" type="application/json">{"enable":true,"theme":{"light":"neutral","dark":"neutral"},"js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mermaid/10.2.3/mermaid.min.js","integrity":"sha256-JFptYy4KzJ5OQP+Q9fubNf3cxpPPmZKqUOovyEONKrQ="}}</script>
  <script src="/js/third-party/tags/mermaid.js"></script>


  <script src="/js/third-party/fancybox.js"></script>



  




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"none","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>


  <script src="https://cdnjs.cloudflare.com/ajax/libs/quicklink/2.3.0/quicklink.umd.js" integrity="sha256-yvJQOINiH9fWemHn0vCA5lsHWJaHs6/ZmO+1Ft04SvM=" crossorigin="anonymous"></script>
  <script class="next-config" data-name="quicklink" type="application/json">{"enable":true,"home":true,"archive":true,"delay":true,"timeout":3000,"priority":true,"url":"https://soundmemories.github.io/page/2/"}</script>
  <script src="/js/third-party/quicklink.js"></script>

</body>
</html>
