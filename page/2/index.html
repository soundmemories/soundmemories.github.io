<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.2.0">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="https://fonts.loli.net/css?family=Monda:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">

<link rel="stylesheet" href="//cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.1/css/all.min.css">
  <link rel="stylesheet" href="//cdn.jsdelivr.net/npm/animate.css@3.1.1/animate.min.css">
  <link rel="stylesheet" href="//cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.css">

<script class="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"soundmemories.github.io","root":"/","images":"/images","scheme":"Muse","version":"8.0.2","exturl":true,"sidebar":{"position":"left","display":"always","padding":18,"offset":12},"copycode":true,"bookmark":{"enable":true,"color":"#222","save":"auto"},"fancybox":true,"mediumzoom":true,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"},"path":"/search.xml","localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false}};
  </script>

  <meta name="description" content="今日事，今日毕">
<meta property="og:type" content="website">
<meta property="og:title" content="SoundMemories">
<meta property="og:url" content="https://soundmemories.github.io/page/2/index.html">
<meta property="og:site_name" content="SoundMemories">
<meta property="og:description" content="今日事，今日毕">
<meta property="og:locale" content="zh_CN">
<meta property="article:author" content="SoundMemories">
<meta name="twitter:card" content="summary">


<link rel="canonical" href="https://soundmemories.github.io/page/2/">


<script data-pjax class="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'zh-CN'
  };
</script>

  <title>SoundMemories</title>
  






  <noscript>
  <style>
  body { margin-top: 2rem; }

  .use-motion .menu-item,
  .use-motion .sidebar,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header {
    visibility: visible;
  }

  .use-motion .header,
  .use-motion .site-brand-container .toggle,
  .use-motion .footer { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle,
  .use-motion .custom-logo-image {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line {
    transform: scaleX(1);
  }

  .search-pop-overlay, .sidebar-nav { display: none; }
  .sidebar-panel { display: block; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <h1 class="site-title">SoundMemories</h1>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu">
        <li class="menu-item menu-item-主页">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>主页</a>

  </li>
        <li class="menu-item menu-item-分类">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a>

  </li>
        <li class="menu-item menu-item-标签">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a>

  </li>
        <li class="menu-item menu-item-归档">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
        <li class="menu-item menu-item-关于">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" maxlength="80"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="search-result-container no-result">
  <div class="search-result-icon">
    <i class="fa fa-spinner fa-pulse fa-5x"></i>
  </div>
</div>

    </div>
  </div>

</div>
        
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>

  <aside class="sidebar">

    <div class="sidebar-inner sidebar-overview-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>
	   
		  
      <div class="sidebar-panel-container">
        <!--noindex-->
        <section class="post-toc-wrap sidebar-panel">
        </section>
        <!--/noindex-->

        <section class="site-overview-wrap sidebar-panel">
          <div class="site-author site-overview-item animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="SoundMemories"
      src="https://i.loli.net/2020/11/04/6JhNuwtBe4adylS.png">
  <p class="site-author-name" itemprop="name">SoundMemories</p>
  <div class="site-description" itemprop="description">今日事，今日毕</div>
</div>
<div class="site-state-wrap site-overview-item animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives">
          <span class="site-state-item-count">113</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
        <span class="site-state-item-count">7</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
        <span class="site-state-item-count">7</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author site-overview-item animated">
      <span class="links-of-author-item">
        <span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL3NvdW5kbWVtb3JpZXM=" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;soundmemories"><i class="fab fa-github fa-fw"></i>GitHub</span>
      </span>
      <span class="links-of-author-item">
        <span class="exturl" data-url="bWFpbHRvOnNvdW5kbWVtb3JpZXNAMTYzLmNvbQ==" title="E-Mail → mailto:soundmemories@163.com"><i class="fa fa-envelope fa-fw"></i>E-Mail</span>
      </span>
  </div>
  <div class="cc-license site-overview-item animated" itemprop="license">
    <span class="exturl cc-opacity" data-url="aHR0cHM6Ly9jcmVhdGl2ZWNvbW1vbnMub3JnL2xpY2Vuc2VzL2J5LW5jLXNhLzQuMC8="><img src="/images/cc-by-nc-sa.svg" alt="Creative Commons"></span>
  </div>



        </section>
      </div>
    </div>
  </aside>
  <div class="sidebar-dimmer"></div>


    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>
  <a role="button" class="book-mark-link book-mark-link-fixed"></a>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


    <div class="main-inner index posts-expand">
      

      
    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://soundmemories.github.io/2021/06/20/NLP/01.%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%92%8C%E8%AF%8D%E5%90%91%E9%87%8F/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="https://i.loli.net/2020/11/04/6JhNuwtBe4adylS.png">
      <meta itemprop="name" content="SoundMemories">
      <meta itemprop="description" content="今日事，今日毕">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="SoundMemories">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/06/20/NLP/01.%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%92%8C%E8%AF%8D%E5%90%91%E9%87%8F/" class="post-title-link" itemprop="url">语言模型和词向量</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2021-06-20 00:00:00" itemprop="dateCreated datePublished" datetime="2021-06-20T00:00:00+08:00">2021-06-20</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/NLP/" itemprop="url" rel="index"><span itemprop="name">NLP</span></a>
        </span>
    </span>

  
      </div>
      <div class="post-meta">
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>5.7k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>5 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="语言模型"><a href="#语言模型" class="headerlink" title="语言模型"></a>语言模型</h1><p><strong>模型</strong>指的是对事物的数学抽象，那么<strong>语言模型</strong>指的就是对语言现象的数学抽象。准确的讲，给定一个句子 $w$ ，语言模型就是计算句子的出现概率 $p(w)$ 的模型，而统计的对象就是人工标注而成的语料库。</p>
<p>假设构建如下的小型语料库：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">商品 和 服务</span><br><span class="line">商品 和服 物美价廉</span><br><span class="line">服务 和 货币</span><br></pre></td></tr></table></figure><br>每个句子出现的概率都是 $\dfrac{1}{3}$，因为样本空间为 3 ，这 3 次基数平均分给了 3 个句子，所以它们的概率都为$\dfrac{1}{3}$，既然它们的概率之和为 1 ，那么其他句子的概率自然为 0 了，这就是语言模型。然而 $p(w)$ 的计算非常难：句子数量无穷无尽，无法枚举。即便是大型语料库，也只能“枚举”有限的数百万个句子。实际遇到的句子大部分都在语料库之外，意味着它们的概率都被当作 0，这种现象被称为<strong>数据稀疏</strong>。枚举不可行，我们需要一种可计算的、更合理的概率估计方法。</p>
<p>考虑到句子由单词构成，句子无限，单词有限。于是我们从单词构成句子的角度出发去建模句子，把句子表示为单词列表 $\textbf{w}=w_1w_2…w_k$，每个 $w_t, t\in[1,k]$都是一个单词，然后定义<strong>语言模型</strong>：</p>
<script type="math/tex; mode=display">
\begin{aligned}
p(\textbf{w})&=p(w_1w_2...w_k)\\&=p(w_1|w_0)\times p(w_2|w_0w_1)\times...\times p(w_{k+1}|w_0w_1...w_k)\\&=
\prod\limits _{t=1}^{k+1}p(w_{t}|w_0w_1...w_{t-1})
\end{aligned}</script><p>其中，$w_0$=BOS（begin of sentence，或\<s\>），$w_{k+1}$=EOS（end of sentence，或\&lt;/s>），用来标记句子首尾两个特殊“单词”。<br>也就是说，语言模型模拟说话顺序：给定已经说出口的词语序列，预测下一个词语的后验概率。一个单词一个单词地乘上后验概率，我们就能估计任意一句话的概率。以极大似然估计来计算每个后验概率，即：</p>
<script type="math/tex; mode=display">
\begin{aligned}
p(w_{t}|w_0w_1...w_{t-1})=p_{MLE}(w_{t}|w_0w_1...w_{t-1})=\dfrac{c(w_0...w_t)}{c(w_0...w_{t-1})}
\end{aligned}</script><p>其中，$c(w_0…w_t)$表示$w_0…w_t$的计数。</p>
<p>以上面小型语料库为例，计算$p(商品 和 服务)$出现的概率？<br>（1）$p(商品|BOS) =\frac{2}{3}$（“商品”作为第一个词出现的次数为2，所有单词作为第一个词出现的次数为3）；<br>（2）$p(和|BOS 商品) =\frac{1}{2}$（“BOS 商品 和”出现的次数为1，“BOS 商品”出现的次数为2）；<br>（3）$p(服务|BOS 商品 和) =\frac{1}{1}$（“BOS 商品 和 服务”出现的次数为1，“BOS 商品 和”出现的次数为1）；<br>（4）$p(EOS|BOS 商品 和 服务) =\frac{1}{1}$（“BOS 商品 和 服务 EOS”出现的次数为1，“BOS 商品 和 服务”出现的次数为1）；<br>整个句子的概率是4者乘积：$p(商品 和 服务)=\frac{2}{3}\times \frac{1}{2}\times \frac{1}{1}\times \frac{1}{1}=\frac{1}{3}$。</p>
<p>但是随着句子长度增大，语言模型会遇到如下问题：<br>（1）<strong>数据稀疏</strong>。指长度越大的句子越难出现，语料库中极有可能统计不到长句子的频次，导致$p(w_{t}|w_0w_1…w_{t-1})$为0。<br>（2）<strong>计算代价大</strong>。t越大，需要存储的$p(w_{t}|w_0w_1…w_{t-1})$就越多。</p>
<h2 id="n元语法"><a href="#n元语法" class="headerlink" title="n元语法"></a>n元语法</h2><p>为了解决上面两个问题，使用<strong>马尔可夫假设</strong>（Markov Assumption）来简化语言模型：给定时间线上有一串事件顺序发生，假设每个事件的发生概率只取决于前一个事件，那么这串事件构成的因果链被称作<strong>马尔可夫链</strong>。</p>
<p>在语言模型中，第t个事件指的是$w_t$作为第t个单词出现。也就是说，马尔科夫链假设每个单词出现的概率只取决于前一个单词：</p>
<script type="math/tex; mode=display">
\begin{aligned}
p(w_{t}|w_0w_1...w_{t-1})=p(w_t|w_{t-1})
\end{aligned}</script><p>基于此假设，需要计算的量一下子减少了不少，由于每次计算只涉及连续两个单词的二元接续，所以此语言模型称为<strong>二元语法</strong>（bi-gram），也叫<strong>一阶马尔科夫链</strong>：</p>
<script type="math/tex; mode=display">
\begin{aligned}
p(\textbf{w})&=p(w_1w_2...w_k)\\&=p(w_1|w_0)\times p(w_2|w_1)\times...\times p(w_{k+1}|w_k)\\&=
\prod\limits _{t=1}^{k+1}p(w_{t}|w_{t-1})
\end{aligned}</script><p>那么根据这个思路推广下，可以得到<strong>n元语法</strong>（n-gram）的定义：每个单词出现的概率，仅取决于该单词之前n个单词，即：</p>
<script type="math/tex; mode=display">
\begin{aligned}
p(\textbf{w})=\prod\limits_{t=1}^{k+n-1}p(w_{t}|w_{t-(n-1)}...w_{t-1})
\end{aligned}</script><p>当$\text{n=1}$时（即出现在第t位的词$w_t$独立于历史时），称为<strong>一元语法</strong>（uni-gram）；当$\text{n=2}$时（即出现在第t位的词$w_t$仅与它前面的一个历史词$w_{t-1}$有关），称为<strong>二元语法</strong>（bi-gram），也叫<strong>一阶马尔科夫链</strong>；当$\text{n=3}$时（即出现在第t位的词$w_t$仅与它前面的两个历史词$w_{t-1}w_{t-2}$有关），称为<strong>三元语法</strong>（tri-gram），也叫<strong>二阶马尔科夫链</strong>。当$n\geqslant 4$时数据稀疏和计算代价又变的显著了，实际工程中几乎不使用。另外，深度学习带了一种递归神经网络语言模型（RNN Language Model），理论上可以记忆无限个单词，可以看作“无穷元语法”（∞-gram）。</p>
<p>以<strong>二元语法</strong>（bi-gram）为例，计算$p(商品 和 服务)$出现的概率？<br>（1）$p(商品|BOS) =\frac{2}{3}$；<br>（2）$p(和|商品) =\frac{1}{2}$；<br>（3）$p(服务|和) =\frac{1}{2}$；<br>（4）$p(EOS|服务) =\frac{1}{1}$；<br>整个句子的概率是4者乘积：$p(商品 和 服务)=\frac{2}{3}\times \frac{1}{2}\times \frac{1}{2}\times \frac{1}{1}=\frac{1}{6}$。</p>
<p>这次的概率比上次的$\dfrac{1}{3}$要小一半，剩下的概率到哪里去了呢？来算算语料库之外的新句子$p(商品 和 货币)$就知道了：<br>（1）$p(商品|BOS) =\frac{2}{3}$；<br>（2）$p(和|商品) =\frac{1}{2}$；<br>（3）$p(货币|和) =\frac{1}{2}$；<br>（4）$p(EOS|货币) =\frac{1}{1}$；<br>整个句子的概率是4者乘积：$p(商品 和 货币)=\frac{2}{3}\times \frac{1}{2}\times \frac{1}{2}\times \frac{1}{1}=\frac{1}{6}$。<br>原来剩下的$\dfrac{1}{6}$分配给了语料库之外的句子，它们的概率终于不是0了，这样就缓解了一部分数据稀疏的问题。</p>
<h2 id="模型评估：困惑度"><a href="#模型评估：困惑度" class="headerlink" title="模型评估：困惑度"></a>模型评估：困惑度</h2><p>在理想情况下，对两个语言模型A，B进行评估，选定一个特定的任务比如拼写纠错系统，把两个模型A，B都应用在此任务中，最后比较准确率，从而判断A，B的表现。这种评估方法是以应用为中心的度量方法，通过在下游任务中的性能来进行评估。那有没有更简单的评估方法？不需要放在特定的任务中验证？——————<strong>困惑度</strong>（Perplexity）。</p>
<p><strong>困惑度</strong>（Perplexity）是一种信息论测度，用来测量一个概率模型预测样本的好坏，困惑度越低越好。给定一个包含k个词的文本预料$\textbf{w}=w_1w_2…w_k$，和一个基于历史行为的语言模型，其预测结果为$p(\textbf{w})$，则这个语言模型在这个语料的困惑度是：</p>
<script type="math/tex; mode=display">
\begin{aligned}
pp(\textbf{w})=2^{-\dfrac{1}{k}\log p(\textbf{w})}
\end{aligned}</script><p>以二元语法（bi-gram）为例，使用平均交叉熵，此时困惑度可以表示为：</p>
<script type="math/tex; mode=display">
\begin{aligned}
pp(\textbf{w})&=2^{-\dfrac{1}{k}\log p(\textbf{w})}\\&=2^{-\dfrac{1}{k}\log \prod\limits _{t=1}^{k+1}p(w_{t}|w_{t-1})}\\&=
2^{-\dfrac{1}{k}\sum\limits_{t=1}^{k+1}\log p(w_{t}|w_{t-1})}
\end{aligned}</script><p>模型的困惑度越小越好。</p>
<p>计算”商品 和 服务”的困惑度？<br>（1）$p(商品|BOS) =\frac{2}{3}$；<br>（2）$p(和|商品) =\frac{1}{2}$；<br>（3）$p(服务|和) =\frac{1}{2}$；<br>（4）$p(EOS|服务) =\frac{1}{1}$；<br>整个句子的困惑度：$pp(商品 和 服务)=2^{-\dfrac{1}{3}(log\frac{2}{3}+log\frac{1}{2}+log\frac{1}{2}+log\frac{1}{1})}$。</p>
<h2 id="数据平滑"><a href="#数据平滑" class="headerlink" title="数据平滑"></a>数据平滑</h2><p>n元语法虽然有效，但它有一大不足，以二元语法为例，如果$c(w_tw_{t-1})\text{=0}$，因为计算句子概率时的乘法计算，导致整个语料的0-概率分配。0概率会造成非常大的困惑度，这是一种很糟糕的情况。一种避免0-概率事件的方法是使用平滑技术，确保为每个可能的情况都分配一个概率（可能非常小）。</p>
<h3 id="加法平滑"><a href="#加法平滑" class="headerlink" title="加法平滑"></a>加法平滑</h3><p>最简单的一类方法是<strong>加法平滑</strong>（Additive Smoothing），以下公式都以二元语法（bi-gram）为例。</p>
<p>不加平滑时：</p>
<script type="math/tex; mode=display">
\begin{aligned}
p_{ MLE}(w_tw_{t-1})=\dfrac{c(w_{t-1}w_t)}{c(w_t)}
\end{aligned}</script><p><strong>加一平滑</strong>（Add-one Smoothing/Laplace Smoothing）：</p>
<script type="math/tex; mode=display">
\begin{aligned}
p_{ add-one}(w_t|w_{t-1})=\dfrac{c(w_{t-1}w_t)+\textcolor{red}{1}}{c(w_t)+\textcolor{red}{V}}, \quad \textcolor{red}{V}是词表大小，即语料库中所有单词去重的总数
\end{aligned}</script><p><strong>加K平滑</strong>（Add-K Smoothing/Laplace Smoothing）：</p>
<script type="math/tex; mode=display">
\begin{aligned}
p_{ add-k}(w_t|w_{t-1})=\dfrac{c(w_{t-1}w_t)+\textcolor{red}{k}}{c(w_t)+\textcolor{red}{kV}}, \quad \textcolor{red}{V}是词表大小，即语料库中所有单词去重的总数
\end{aligned}</script><h3 id="插值法"><a href="#插值法" class="headerlink" title="插值法"></a>插值法</h3><p>另外一类方法使用back-off策略，即如果没有观测到n元语法，那么就基于n-1元语法计算，利用低阶n元语法平滑高阶n元语法，这就产生了很多方案，最简单的一种是<strong>线性插值法</strong>（Linear Interpolation）。</p>
<p>三元语法（tri-gram）的线性插值法：</p>
<script type="math/tex; mode=display">
\begin{aligned}
p_{tiny Int}(w_t|w_{t-2}w_{t-1})=\lambda_1 p(w_t|w_{t-2}w_{t-1})+\lambda_2p(w_t|w_{t-1})+\lambda_3p(w_t),\quad \lambda_1+\lambda_2+\lambda_3=1
\end{aligned}</script><p>二元语法（bi-gram）的线性插值法：</p>
<script type="math/tex; mode=display">
\begin{aligned}
p_{ Int}(w_t|w_{t-1})=\lambda_1 p(w_t|w_{t-1})+\lambda_2 p(w_t),\quad \lambda_1+\lambda_2=1
\end{aligned}</script><p>一元语法（uni-gram）的线性插值法：</p>
<script type="math/tex; mode=display">
\begin{aligned}
p_{ Int}(w_t)=\lambda_1 p(w_t)+\lambda_2\frac{1}{V},\quad \lambda_1+\lambda_2=1
\end{aligned}</script><p>其中，V是词表大小，即语料库中所有单词去重的总数。</p>
<h3 id="古德-图灵平滑"><a href="#古德-图灵平滑" class="headerlink" title="古德-图灵平滑"></a>古德-图灵平滑</h3><p><strong>古德-图灵平滑</strong>（Good-Turing Smoothing）：对于任何一个出现 $r$ 次n元语法，都假设它出现了$r^*$次：</p>
<script type="math/tex; mode=display">
\begin{aligned}
r^*=\frac{(r+1)N_{r+1}}{N_r} ,\quad  N_r表示训练预料中出现 r次的n元语法的数目
\end{aligned}</script><p>要把整个统计数转化为概率，只需要进行归一化处理：对于统计数为$r$的n元语法，其概率为：</p>
<script type="math/tex; mode=display">
\begin{aligned}
p_r=\frac{r^*}{N}=\frac{(r+1)N_{r+1}}{N_r*N},\quad  N=\sum\limits_{r=1}^{\infty}r^* N_r
\end{aligned}</script><p>注意到：</p>
<script type="math/tex; mode=display">
\begin{aligned}
N=\sum\limits_{r=1}^{\infty}r^* N_r=\sum\limits_{r=1}^{\infty}(r+1)N_{r+1}=\sum\limits_{r=1}^{\infty}rN_r
\end{aligned}</script><p>也就是说，N等于整个分布中的最初的计数。这样，样本中所有事件的概率之和为：</p>
<script type="math/tex; mode=display">
\begin{aligned}
\sum\limits_{r>0}N_rp_r=1-\frac{N_1}{N}<1
\end{aligned}</script><p>因此，有$ N_1/N$的概率剩余量可以分配给所有未见事件（$r=0$的事件）。</p>
<p>但古德-图灵平滑也有其缺陷，比如某一个 $ N_{r+1}$ 为0，此时就无法计算 $p_{r}$ 了，一般这种情况，我们使用机器学习算法去拟合 $ N_{r}$，这样就可以把缺失的部分补上。</p>
<h1 id="词向量"><a href="#词向量" class="headerlink" title="词向量"></a>词向量</h1><p>主要考虑单词或句子，甚至是文章的表示，一般把它们进行向量化处理。</p>
<h2 id="相似度"><a href="#相似度" class="headerlink" title="相似度"></a>相似度</h2><h3 id="距离"><a href="#距离" class="headerlink" title="距离"></a>距离</h3><p>单词/句子用向量表示后，可以计算它们的<strong>距离</strong>来判断相似度（距离越大，相似度越低），$i$为向量下标：<br>（1）<strong>欧氏距离</strong>：$d=||A-B||_2=\sqrt{\sum \limits_{i=1}^n(A_{i} - B_{i})^2}$，两个点的直线距离。<br>（2）<strong>曼哈顿距离</strong>：$d=||A-B||_1=\sum \limits_{i=1}^{n}|A_{i}-B_{i}|$，各个维度的长度差进行累加。常用计算城市间到达距离计算。<br>（3）<strong>闵科夫斯基距离</strong>：$d=||A-B||_P=\sqrt[p]{\sum \limits_{i=1}^n|A_{i}-B_{i}|^p}$，可以根据p来决定距离，如果p=1就是曼哈顿距离；p=2就是欧氏距离；当p趋近无穷时，就会变为长度差最大那个距离。</p>
<h3 id="方向"><a href="#方向" class="headerlink" title="方向"></a>方向</h3><p>但是向量不光有大小还有<strong>方向</strong>的，两个向量之间是有夹角的，从这个角度发现了<strong>余弦相似度</strong>（值越大，相似度越高），$i$为向量下标：</p>
<script type="math/tex; mode=display">
\begin{aligned}
cos(A,B)=\frac{A \cdot B}{|A||B|}=\frac{\sum \limits_{i=1}^{n}A_iB_i}{\sqrt{\sum \limits_{i=1}^{n}A_i^2}\sqrt{\sum \limits_{i=1}^{n}B_i^2}}
\end{aligned}</script><p>余弦相似度的<strong>取值范围是[-1, 1]，相同的两个向量之间的相似度为1</strong>。</p>
<p>当一对文本相似度的长度差距很大、但内容相近时，如果使用词频/词向量作为特征：<br>（1）如果使用欧氏距离的话，它们在特定空间中的欧氏距离通常很大，因而相似度低；<br>（2）而如果使用余弦相似度的话，它们之间的夹角可能很小，因而相似度高。<br>在高维情况下：<br>（1）余弦相似度依然保持“<strong>相同时为1，正交时为0，相反时为-1</strong>”的性质；<br>（2）而欧式距离的数值则受维度的影响，范围不固定，并且含义也比较模糊。</p>
<p>如果希望得到类似于距离的表示，使用$\textbf{1-cos(A,B)}$即为<strong>余弦距离</strong>，<strong>其取值范围是[0, 2]，相同的两个向量余弦距离为0</strong>。<br>在一些场景，比如word2vec中，其向量的模长是经过归一化的，此时欧式距离与余弦距离有着单调的关系：</p>
<script type="math/tex; mode=display">
\begin{aligned}
||A-B||_2=\sqrt{2(1-cos(A,B))}
\end{aligned}</script><p>其中，$||A-B||_2$表示欧氏距离，$cos(A,B)$表示余弦相似度，$1-cos(A,B)$表示余弦距离。此时，如果选择距离小的（相似度最大）的近邻，那么使用余弦相似度和欧氏距离的结果是相同的。<br>总的来说：<strong>欧氏距离体现数值上的绝对差异，余弦距离体现方向上的相对差异。</strong></p>
<h2 id="One-Hot"><a href="#One-Hot" class="headerlink" title="One-Hot"></a>One-Hot</h2><p><strong>词袋模型</strong>（Bag-of-words model）：将所有词语装进一个袋子里，不考虑其词法和语序的问题，即每个词语都是独立的。最简单的一种就是<strong>独热表示</strong>（One-Hot Representation）。<br>假设，词典：[是，天空，蓝色，的]。<strong>每个单词的表示</strong>：<br>“是”　——&gt;[1, 0, 0, 0]<br>“天空”——&gt;[0, 1, 0, 0]<br>“蓝色”——&gt;[0, 0, 1, 0]<br>“的”　——&gt;[0, 0, 0, 1]<br>向量的维度等于词典的的大小。<br><div class="note info"><p>利用One-Hot表示法无法表达<strong>单词</strong>之间的相似度！不管用欧氏距离（任意两个词的相似度计算结果都相同）还是余弦相似度（任意两个词的相似度计算结果都是0）。</p>
<p>One-Hot表示单词/句子的缺点：<br>（1）<strong>稀疏性</strong>（Sparsity）：如果词典非常大，维度就会很大，而一个句子可能只有很少的词，导致出现很多0，造成稀疏问题。核心问题是维度太大。<br>（2）<strong>弱语义</strong>（Semantically Weak）：无法表达词与词之间（语义）的相似度，因为One-Hot表示的单词向量是正交的。核心问题是每个单词的向量只能有一个有效值（local representation），且取值只能是{0,1}。</p>
</div></p>
<h3 id="boolean"><a href="#boolean" class="headerlink" title="boolean"></a>boolean</h3><p><strong>每个句子的表示（boolean）</strong>，向量对应的下标与字典的下标相匹配，其值为该词语出现为1，没出现为0：<br>“天空 是 蓝色”——&gt;[1, 1, 1, 0]<br>“蓝色 是 蓝色”——&gt;[1, 0, 1, 0]</p>
<h3 id="count"><a href="#count" class="headerlink" title="count"></a>count</h3><p><strong>每个句子的表示（count）</strong>，向量对应的下标与字典的下标相匹配，其值为该词语出现的次数：<br>“天空 是 蓝色”——&gt;[1, 1, 1, 0]<br>“蓝色 是 蓝色”——&gt;[1, 0, 2, 0]</p>
<h3 id="TF-IDF"><a href="#TF-IDF" class="headerlink" title="TF-IDF"></a>TF-IDF</h3><p>一句话中每个词的重要程度是不同的，但boolean（每个单词权重相同）和count（出现次数越多不一定越重要）都不合理。由此考虑到新的计算方式————<strong>TF-IDF</strong>。<br><strong>TF-IDF</strong>（term frequency–inverse document frequency）是一种用于信息检索与文本挖掘的常用加权技术。用以评估一个词，对于一个文件集或一个语料库中的其中一份文件的重要程度。词的重要性随着它在文件中出现的次数成正比增加，但同时会随着它在语料库中出现的频率成反比下降。其公式为：</p>
<script type="math/tex; mode=display">
\begin{aligned}
\text{TF-IDF(t,d)}=\text{TF(t,d)}\times \text{IDF(t)}
\end{aligned}</script><p>在一份给定的文件里，<strong>词频</strong>（term frequency，TF）指的是某一个给定的词语在该文件中出现的频率。这个数字是对<strong>词数</strong>（term count）的归一化，以防止它偏向长的文件。对于某一特定文件 $d_j$ 里的词语 $t_i$ 来说，它的词频（在本文件的重要程度）可表示为：</p>
<script type="math/tex; mode=display">
\begin{aligned}
\text{TF}(t_i,d_j)=\frac{n_{i,j}}{\sum_k n_{k,j}}
\end{aligned}</script><p>其中，$n_{i,j}$ 是该词在文件 $d_j$ 中的出现次数，而分母则是在文件 $d_j$ 中所有字词的出现次数之和。<br>有时 $\text{TF}(t_i,d_j)$ 也可以直接采用词频 $n_{i,j}$ 计算，不进行归一化处理。</p>
<p><strong>逆向文件频率</strong>（inverse document frequency，IDF）是一个词语普遍重要性的度量（在整体文件的重要程度，和文件频率反比关系）。某一特定词语的IDF———总文件数目除以包含该词语的文件数目，再取对数（防止它的值过大）：</p>
<script type="math/tex; mode=display">
\begin{aligned}
\text{IDF}(t_i)=log\frac{|D|}{|1+\{j:t_i\in d_j\}|}
\end{aligned}</script><p>其中，$|D|$ 是语料库中文件总数，$\{j:t_i\in d_j\}$ 是包含词语 $t_i$ 的文件数目（如果词语不存在资料库中，按 1 处理）。</p>
<p>假设，词典：[是，天空，蓝色，的]，语料库：[“天空 是 蓝色”, “蓝色 是 蓝色”]。<br><strong>每个句子的表示（TF-IDF）</strong>，向量对应的下标与字典的下标相匹配，其值为该词语的TF-IDF（TF按词频计算）：<br>“天空 是 蓝色”——&gt;$[1·\log\frac{2}{1}, 1·\log\frac{2}{2}, 1·\log\frac{2}{2}, 0]$<br>“蓝色 是 蓝色”——&gt;$[1\log\frac{2}{1}, 0, 2·\log\frac{2}{2}, 0]$</p>
<h2 id="word2vec"><a href="#word2vec" class="headerlink" title="word2vec"></a>word2vec</h2><p>之前我们说了One-Hot表示方法有<strong>稀疏性</strong>、<strong>弱语义</strong>缺点，那么如何解决这些问题？————分布式表示。</p>
<p><strong>分布式表示</strong>（Distributed Representation）的思路是：通过训练，将每个词用<strong>低维度</strong>的向量表示（解决稀疏性/高维度问题，维度不再依赖字典长度），并且每个单词的向量<strong>有多个有效值</strong>（global representation），每个维度上的有效值不再是{0,1}，而是介于[0,1]的值（解决弱语义问题，可计算相似度）。这种把词映射到低维的向量表示，也叫做<strong>词嵌入</strong>（word embedding）。对于句子表示，可以使用平均策略，即句子中所有词的向量求和，再取均值。</p>
<p><strong><span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL3RtaWtvbG92L3dvcmQydmVj">word2vec<i class="fa fa-external-link-alt"></i></span></strong> 就是分布式表示方法的一种，它将词的语义表示为训练语料库中上下文的向量。它根据输入和输出的不同分为<strong>CBOW</strong>（Continuous Bag-Of-Words）和<strong>Skip-Gram</strong>两种模型。而且word2vec对这两种方法进行了优化，从而得到<strong>Hierarchical Softmax</strong>模型和<strong>Negative Sampling</strong>模型。<br><img src="/images/语言模型和词向量/CBOW和Skip-gram.png" width="80%" height="80%"></p>
<p><strong>CBOW</strong>：基于上下文词（输入）预测中心词（输出）。<br><strong>Skip-Gram</strong>：基于中心词（输入）预测上下文词（输出）。</p>
<h3 id="Skip-Gram"><a href="#Skip-Gram" class="headerlink" title="Skip-Gram"></a>Skip-Gram</h3><p><strong>Skip-Gram</strong>核心思想是：用中心词（输入）预测上下文词（输出）。输入的中心词使用One-Hot向量表示，引入一个大小为 $c$ 的窗口，那么上下文词就是由中心词左 $c$ 个词和右 $c$ 个词构成（一般用$2c$表示上下文）我们希望模型输出的就是这些上下文词，而通过<strong>神经网络输出+softmax</strong>计算得到的是所有词的概率，那么只需要优化上下文词的概率最大即可达到我们的目的。</p>
<p>把这个核心思想转化成数学表示，假设有如下一句话：</p>
<script type="math/tex; mode=display">
[w_1...w_{t-1}w_tw_{t+1}...w_V]</script><p>其中 $w_t$ 代表第 $t$ 个词，总共有$ V$个词。我们要计算的就是：</p>
<script type="math/tex; mode=display">
\prod\limits_{t=1}^Vp(\text{context}(w_t)|w_t)</script><p>每一个 $p(\text{context}(w_t)|w_t)$ 是相互独立的。<br>此时引入窗口参数 $i\in \text{[-c,c]}$，$c$ 为窗口大小。中心词 $w_t$ 的上下文词 $\text{context}(w_t)$ 就是其左 $c$ 个词和右 $c$ 个词构成，上式变为：</p>
<script type="math/tex; mode=display">
\begin{aligned}
\prod\limits_{t=1}^V \prod\limits_{i=-c}^c p(w_{t+i}|w_t)
\end{aligned}</script><p>每一个 $p(\text{context}(w_i)|w_t)$ 是相互独立且同分布的。<br>为了方便计算，转成$log$（其实就是对数损失函数），并且取均值：</p>
<script type="math/tex; mode=display">
\begin{aligned}
\dfrac{1}{\text{V}} \sum\limits_{t=1}^V \sum\limits_{i=-c}^c \log p(w_{t+i}|w_t)
\end{aligned}</script><p>我们的目标函数就是引入参数 $\theta$，使整个式子最大化：</p>
<script type="math/tex; mode=display">
\begin{aligned}
\mathop{argmax}\limits_{\theta}\dfrac{1}{\text{V}} \sum\limits_{t=1}^V \sum\limits_{i=-c}^c \log p(w_{t+i}|w_t;\theta)
=\mathop{argmin}\limits_{\theta}-\dfrac{1}{\text{V}} \sum\limits_{t=1}^V \sum\limits_{i=-c}^c \log p(w_{t+i}|w_t;\theta)
\end{aligned}</script><p>上式其实就是我们的优化函数$J(\theta)$，这里为了方便优化计算，最大化转成了最小化，而引入的参数 $\theta$ 其实就是我们要找的<strong>词向量</strong>。</p>
<hr>
<p>那么如何计算呢？一般采用的方法是一个三层的神经网络结构，分为输入层，隐藏层和输出层（softmax层）。<br>这个神经网络计算的是<strong>一个词</strong>（输入）预测<strong>所有词</strong>（输出）的情况：<br><img src="/images/语言模型和词向量/one-word.png" width="60%" height="60%"></p>
<p>$\text{V}$：词汇表的长度;<br>$\text{N}$：隐层神经元个数（词向量维度，需要我们自己指定）;<br>$\text{W}$：输入层到隐层的权重矩阵（词向量矩阵，每一行代表一个词的词向量），维度是$\small [V,N]$;<br>$\text{W}^\prime$：隐层到输出层的权重矩阵（词向量矩阵，每一列代表一个词的词向量），维度是$\small [N,V]$;</p>
<p>我们需要做的是用输入的词去预测输出的词（方便书写这里用行向量表示一个词）：<br>（1）输入层的一个单词 $w_t$ 使用One-Hot表示：</p>
<script type="math/tex; mode=display">
w_t=[x_1...x_t...x_V]</script><p>其中，只有 $x_t$ 为1，其余为0，其中t是输入单词在词汇表中的索引下标，它的维度是$\small [1,V]$。<br>（2）输入的词 $w_t$ 和词向量矩阵 $ W$ 相乘，得到一个维度为$\small [1,N]$的隐层向量 $h$。此过程可看作从词向量矩阵 $ W$取对应的词向量。</p>
<script type="math/tex; mode=display">
\begin{aligned}
h=w_t\cdot\text{W}
\end{aligned}</script><p>（3）隐层向量 $h$ 和 词向量矩阵 $ W^\prime$ 相乘，得到一个维度为$\small [1,V]$的输出向量 $y$。此过程可看作计算当前词向量和所有词向量的相似度。从这个过程可看出<strong>word2vec中隐藏层没有用激活函数</strong>。</p>
<script type="math/tex; mode=display">
\begin{aligned}
y=h\cdot\text{W}^\prime
\end{aligned}</script><p>（4）输出向量 $y$ 再通过softmax计算，从而得到概率。此过程可看作把相似度转成概率，即向量 $y$ 的每个值是当前词向量和另一个词向量相似的概率。下面公式是预测一个词的概率：</p>
<script type="math/tex; mode=display">
\normalsize p(w_{i,i\ne k}|w_t)=p(w_{i,i\ne k}|y)=\dfrac{e^{y_i}}{\sum\limits_{j=1}^V e^{y_j}}</script><p>其中，$w_i$ 代表词表中第 $i$ 个词且$i\mathbb{\ne} t$（非中心词）。$y_i$为在原始输出向量$y$中，与单词$w_i$所对应的维度取值。$y$向量通过softamx计算出来就是当前词$w_t$和所有词的相似概率。<br>为什么是softmax？因为其值域是$[0,1]$，且所有结果的和为$1$。符合我们想要得到概率的目的。</p>
<hr>
<p><img src="/images/语言模型和词向量/skip-gram.png" width="40%"></p>
<p>那么<strong>Skip-Gram</strong>是怎么计算的呢？回到核心思想：用<strong>中心词</strong>（输入）预测<strong>上下文词</strong>（输出）。<br>它引入了窗口 $c$ ，上下文词就是$2c$（左 $c$ 个词和右 $c$ 个词构成），目标是给定一个词$w_t$预测上下文词的概率最大化。以下是一个词$w_t$的优化公式：</p>
<script type="math/tex; mode=display">
\normalsize p(w_{t+i,i\in[-c,c]}|w_t)=\dfrac{e^{y_i}}{\sum\limits_{j=1}^V e^{y_j}}</script><script type="math/tex; mode=display">
\normalsize \mathop{argmax}\limits_{W,W^{\prime}} \prod\limits_{i=-c}^{c} p(w_{t+i}|w_t;\scriptsize{W,W^{\prime}})\normalsize
=\mathop{min}\limits_{W,W^{\prime}} \sum\limits_{i=-c}^{c} -logp(w_{t+i}|w_t;\scriptsize{W,W^{\prime}})</script><p>把所有中心词都训练一遍，就是以下公式：</p>
<script type="math/tex; mode=display">
\begin{aligned}
\mathop{argmax}\limits_{W,W^{\prime}}\dfrac{1}{\text{V}} \sum\limits_{t=1}^V \sum\limits_{i=-c}^c \log p(w_{t+i}|w_t;\scriptsize{W,W^{\prime}})\normalsize
=\mathop{argmin}\limits_{W,W^{\prime}}-\dfrac{1}{\text{V}} \sum\limits_{t=1}^V \sum\limits_{i=-c}^c \log p(w_{t+i}|w_t;\scriptsize{W,W^{\prime}})
\end{aligned}</script><p>其中，损失函数选择对数损失函数（$log$），全局损失定义为所有训练样本上的平均损失。<br>上式损失函数优化过程就可以通过反向传播方法优化（基于梯度的优化），这里不再赘述。所有的中心词都训练一遍后，得到的$\text{W}$和$\text{W}^\prime$（论文中是$v$和$u$，代表中心词向量和上下文词向量）就是我们需要的词向量，可选其中一个作为V个词的N维向量表示（word2vec中一般选择$\text{W}$作为词向量）。</p>
<h3 id="CBOW"><a href="#CBOW" class="headerlink" title="CBOW"></a>CBOW</h3><p><img src="/images/语言模型和词向量/cbow.png" width="40%"></p>
<p><strong>CBOW</strong>核心思想：用上下文词（输入）预测中心词（输出）。<br>（1）输入是多个词（上下文词）的One-Hot表示。在计算隐层的向量前，对输入向量和取均值即可。</p>
<script type="math/tex; mode=display">
\begin{aligned}
h=\frac{\sum\limits_{i=-c}^c w_{t+i}\cdot W}{2c}=\frac{\sum\limits_{i=-c}^c h_{t+i}}{2c}
\end{aligned}</script><p>（2）输出是上下文词向量和某一个词向量相似的概率，使中心词概率最大即可。以下是一个词$w_t$的优化公式：</p>
<script type="math/tex; mode=display">
\normalsize \mathop{argmax}\limits_{W,W^{\prime}} p(w_t|w_{t+i,i\in[-c,c]};\scriptsize{W,W^{\prime}})\normalsize
=\mathop{argmin}\limits_{W,W^{\prime}} -logp(w_t|w_{t+i,i\in[-c,c]};\scriptsize{W,W^{\prime}})</script><p>把所有中心词的上下文词都训练一遍，就是以下公式：</p>
<script type="math/tex; mode=display">
\begin{aligned}
\mathop{argmax}\limits_{W,W^{\prime}}\dfrac{1}{\text{V}} \sum\limits_{t=1}^V \sum\limits_{i=-c}^c \log p(w_t|w_{t+i};\scriptsize{W,W^{\prime}})\normalsize
=\mathop{argmin}\limits_{W,W^{\prime}}-\dfrac{1}{\text{V}} \sum\limits_{t=1}^V \sum\limits_{i=-c}^c \log p(w_t|w_{t+i};\scriptsize{W,W^{\prime}})
\end{aligned}</script><h3 id="Hierarchical-Softmax"><a href="#Hierarchical-Softmax" class="headerlink" title="Hierarchical Softmax"></a>Hierarchical Softmax</h3><p>word2vec也是用了CBOW与Skip-Gram来训练模型与得到词向量，但没有使用神经网络结构，而是使用<strong>霍夫曼树</strong>（Huffman）来替代隐藏层到输出层的过程。</p>
<hr>
<p>我们先来复习下<strong>霍夫曼树</strong>，其特点是<strong>带权路径最短</strong>。首先明确一些概念：<br>（1）<strong>路径</strong>：指从树种一个结点到另一个结点的分支所构成的路线。<br>（2）<strong>路径长度</strong>：指路径上的分支数目。<br>（3）<strong>树的路径长度</strong>：指从根到每个结点的路径长度之和。<br>（4）<strong>带权路径长度</strong>：结点具有权值，从该结点到根之间的路径长度乘以结点的权值，就是该结点的带权路径长度。<br>（5）<strong>树的带权路径长度</strong>（WPL）：指树中所有叶子结点的带权路径长度之和。</p>
<p><strong>霍夫曼树的构造方法</strong>（霍夫曼树可以是n叉树，我们主要以二叉树为例）<br>给定$n$个权值，用这$n$个权值构造霍夫曼树的算法如下：<br>（1）将这个$n$个权值分别看作只有根节点的n棵二叉树，这些二叉树构成的集合记为$\small F$。<br>（2）从$\small F$中选出两棵根节点的权值最小的数（假设为$a$、$b$），作为左、右子树，构造一棵新的二叉树（假设为$c$），新的二叉树的根节点权值为左、右子树根节点权值之和。<br>（3）从F中删除$a$、$b$，加入新构造的树$c$。<br>（4）重复（2）（3）两步，直到$\small F$中只剩下一棵树为止，这棵树就是霍夫曼树。</p>
<p>一个简单的例子：“this is an example of a huffman tree” 中得到的字母频率（权重）来建构霍夫曼树。<br><img src="/images/语言模型和词向量/huffman.png" width="60%"></p>
<p><strong>霍夫曼树特点</strong>：<br>（1）权重（频率）越大的结点，距离根结点越近。<br>（2）树的带权路径长度最短。</p>
<p><strong>霍夫曼编码</strong>：<br>一般得到霍夫曼树后我们会对叶子节点进行霍夫曼编码，由于权重高的叶子节点越靠近根节点，而权重低的叶子节点会远离根节点，这样我们的高权重节点编码值较短，而低权重值编码值较长。这保证的树的带权路径最短，也符合我们的信息论，即我们希望越常用的词拥有更短的编码。如何编码呢？一般对于一个霍夫曼树的节点（根节点除外），可以约定<strong>左子树编码为0</strong>，<strong>右子树编码为1</strong>。如下图所示：<br><img src="/images/语言模型和词向量/huffman1.png" width="60%"></p>
<hr>
<p><strong>word2vec中，霍夫曼编码方式和正常的相反，即约定沿着左子树走编码为1（负类），沿着右子树走编码为0（正类），同时约定左子树的权重不小于右子树的权重。</strong></p>
<p><strong>Hierarchical Softmax</strong>的<strong>隐藏层</strong>到<strong>输出概率</strong>的计算过程（CBOW）：<br>首先按照<strong>词频</strong>建立一棵霍夫曼树，叶子结点就是词典中的每个单词，但顺序和词典中不一定相同。假设预测的词（叶子节点）为$w$，定义一些符号：<br>（1）$p^w$：从根节点到$w$对应叶子节点的路径。<br>（2）$n^w$：路径$p^w$中包含结点个数。<br>（3）$p_1^w,p_2^w,…,p_{n^w}^w$：路径$p^w$中的$n^w$个结点，$p_1^w$表示根节点，$p_{n^w}^w$表示词$w$对应的叶子结点。<br>（4）$d_2^w,d_3^w,…,d_{n^w}^w$：路径$p^w$中的$n^w$个结点的编码，总共有$n^w\text{-1}$个，根结点不对应编码，每个编码值为$\{0,1\}$。<br>（5）$\theta_1^w,\theta_2^w,…,\theta_{n^w-1}^w$：路径$p^w$中的$n^w\text{-1}$个结点的向量（维度为$N,1$），不包括叶子结点。</p>
<p>对于词典中任意的词$w$，霍夫曼树中必存在一条从根结点到词$w$叶子节点的路径$p^w$（路径唯一）。路径$p^w$上存在$n^w\text{-1}$个分支，每个分支看做一次二分类，每一次分类就产生一个概率，将这些概率乘起来就是预测词$w$的概率，即$p(w|context(w))$。<br>由此可以给出$w$的条件概率：</p>
<script type="math/tex; mode=display">
p(w|context(w))=\prod\limits_{j=2}^{n^w}p(d_j^w|h_w;\theta_{j-1}^w)</script><p>从根节点到叶节点经过了$n^w\text{-1}$个节点，编码从下标2开始（根节点无编码），对应的参数向量下标从1开始（根节点为1）。<br>其中<strong>每个</strong>$\small p(d_j^w|h_w;\theta_{j-1}^w)$都是一个逻辑回归二分类：</p>
<script type="math/tex; mode=display">
p(d_j^w|h_w;\theta_{j-1}^w)=
\begin{cases}
   \sigma(h_w\cdot\theta_{j-1}^w), &d_j^w=0\quad\text{(正类)}\\
   1-\sigma(h_w\cdot\theta_{j-1}^w), &d_j^w=1\quad\text{(负类)}
\end{cases}</script><p>其中$h_w$是隐藏层向量，$\sigma$是sigmoid函数。<br>考虑到$d$只有0和1两种取值，我们可以用指数形式方便地将其写到一起：</p>
<script type="math/tex; mode=display">
p(d_j^w|h_w;\theta_{j-1}^w)=[\sigma(h_w\cdot\theta_{j-1}^w)]^{1-d_j^w}\cdot[1-\sigma(h_w\cdot\theta_{j-1}^w)]^{d_j^w}</script><p>所以对于$p(w|context(w))$，目标函数取对数似然，引入窗口$C$代表$context(w)$：</p>
<script type="math/tex; mode=display">
\begin{aligned}
p(w|context(w))&=\sum\limits_{w\in C}log\prod\limits_{j=2}^{n^w}\{[\sigma(h_w\cdot\theta_{j-1}^w)]^{1-d_j^w}\cdot[1-\sigma(h_w\cdot\theta_{j-1}^w)]^{d_j^w}\}\\
&=\sum\limits_{w\in C}\sum\limits_{j=2}^{n^w}\{(1-d_j^w)\cdot log[\sigma(h_w\cdot\theta_{j-1}^w)]+d_j^w\cdot log[1-\sigma(h_w\cdot\theta_{j-1}^w)]\}
\end{aligned}</script><p>其中$C$是上下文单词。接下来只需要对$h_w$和$\theta_{j-1}^w$求梯度，然后用随机梯度上升法优化即可，这个过程和逻辑回归梯度优化类似。</p>
<p>以 <strong>CBOW：上下文词（输入）预测中心词（输出）</strong> 为例，从输入层到隐藏层计算方式不变，最后得到维度为$[1,N]$的隐藏层向量$h$：<br><img src="/images/语言模型和词向量/hs.png"></p>
<p>上图中$w_2$不一定就是词典中的$w_2$。<br>使用$w$表示$w_2$，计算过程如下：<br>第1次：$p(d_2^w|h_w;\theta_1^w)=1-\sigma(h_w\cdot\theta_1^w)$<br>第2次：$p(d_3^w|h_w;\theta_2^w)=1-\sigma(h_w\cdot\theta_2^w)$<br>第3次：$p(d_4^w|h_w;\theta_3^w)=\sigma(h_w\cdot\theta_3^w)$</p>
<p><strong>基于Hierarchical Softmax的CBOW</strong>模型算法流程，梯度迭代使用了随机梯度上升法：<br>输入：基于CBOW的语料训练样本，词典大小$V$，中心词$w$，词向量的维度大小$N$，上下文$C$大小$2c$，步长$\eta$。<br>输出：霍夫曼树的内部节点模型参数$\theta$，所有的词向量$W=w_1,w_2,…,w_V$。<br>（1）基于语料训练样本建立霍夫曼树。<br>（2）随机初始化所有的模型参数$\theta$和所有的词向量$W$。<br>（3）进行梯度上升迭代过程，对于训练集中的每一个样本$(context(w),w)$做如下处理：</p>
<ul>
<li>e=0，计算$h_w=\frac{1}{2c}\sum\limits_{i=-c}^cw_{i}$</li>
<li>for $\text{j=2}$ to $n^w$，计算：<ul>
<li>$f=\sigma(h_w\theta_{j-1}^w)$</li>
<li>$g=\eta(1-d_j^w-f)$</li>
<li>$e=e+g\cdot\theta_{j-1}^w$</li>
<li>$\theta_{j-1}^w=\theta_{j-1}^w+g\cdot h_w$</li>
</ul>
</li>
<li>对于$(context(w),w)$中的每一个词向量$w_i$（共$2c$个）进行更新：<ul>
<li>$w_i=w_i+e$</li>
</ul>
</li>
<li>如果梯度收敛，则结束梯度迭代，否则回到（3）继续迭代。</li>
</ul>
<p><strong>基于Hierarchical Softmax的Skip-Gram</strong>模型算法流程，梯度迭代使用了随机梯度上升法：<br>输入：基于Skip-Gram的语料训练样本，词典大小$V$，中心词$w$，词向量的维度大小$N$，上下文$C$大小$2c$，步长$\eta$。<br>输出：霍夫曼树的内部节点模型参数$\theta$，所有的词向量$W=w_1,w_2,…,w_V$。<br>（1）基于语料训练样本建立霍夫曼树。<br>（2）随机初始化所有的模型参数$\theta$和所有的词向量$W$。<br>（3）进行梯度上升迭代过程，对于训练集中的每一个样本$(context(w),w)$做如下处理：</p>
<ul>
<li>for $s=context(w)$，计算：<ul>
<li>e=0，$h_w=w_i$</li>
<li>for $\text{j=2}$ to $n^w$，计算：<ul>
<li>$f=\sigma(h_w\theta_{j-1}^s)$</li>
<li>$g=\eta(1-d_j^s-f)$</li>
<li>$e=e+g\cdot\theta_{j-1}^s$</li>
<li>$\theta_{j-1}^s=\theta_{j-1}^s+g\cdot h_w$</li>
</ul>
</li>
</ul>
</li>
<li>对于$(context(w),w)$中的每一个词向量$w_i$（共$2c$个）进行更新：<ul>
<li>$w_i=w_i+e$</li>
</ul>
</li>
<li>如果梯度收敛，则结束梯度迭代，否则回到（3）继续迭代。</li>
</ul>
<p><strong>优点</strong>：在前面的<strong>CBOW</strong>和<strong>Skip-gram</strong>模型中，softmax计算时分母时需要对所有词的值进行计算求和，word2vec的<strong>Hierarchical Softmax</strong>采用了霍夫曼二叉树来替代从隐藏层到输出softmax层的过程，<strong>之前softmax计算量为$V$，现在为$log_2V$</strong>。</p>
<p><strong>为什么word2vec中用$\text{W}$作为词向量？</strong><br>在之前讲的三层网络中我们可以选择词向量是$\{\text{W},\text{W}^\prime\normalsize\}$，word2vec这中$\text{W}^\prime$替换成$\large\theta$了，所以word2vec一般采用$\text{W}$作为词向量而不用$\text{W}^\prime$作为词向量。除此原因外，输入矩阵$\text{W}$和输出矩阵$\text{W}^\prime$可以看作<strong>所有词作为中心词</strong>或<strong>所有词作为上下文词</strong>而产生的词向量，它们侧重点不同，在不同算法作用也不同，比如在Skip-gram中$\text{W}$可看作所有词作为中心词而产生的词向量，在CBOW中$\text{W}$可看作所有词作为上下文词产生的词向量。对于<strong>于Hierarchical Softmax</strong>和后面的<strong>Negative Sampling</strong>都代替了$\text{W}^\prime$，从而都是选择$\text{W}$作为词向量，而$\text{W}$作为中心词而得到词向量是Skip-gram中实现，所以word2vec中选择Skip-gram效果会更好。</p>
<h3 id="Negative-Sampling"><a href="#Negative-Sampling" class="headerlink" title="Negative Sampling"></a>Negative Sampling</h3><p>在讲基于<strong>Negative Sampling</strong>的word2vec模型前，我们先看看<strong>Hierarchical Softmax</strong>的的缺点。HS使用了霍夫曼树，不难发现对于词频高的词计算很快，但对于词频低的词计算很慢。如果我们的训练样本里的中心词$w$是一个很生僻的词，那么就得在霍夫曼树中辛苦的向下走很久了。能不能不用搞这么复杂的一颗霍夫曼树，将模型变的更加简单呢？这就是<strong>Negative Sampling</strong>（负采样）。</p>
<p><strong>Negative Sampling</strong>就是这么一种求解word2vec模型的方法，它<strong>摒弃了霍夫曼树</strong>，采用了Negative Sampling（负采样）的方法来求解，下面我们就来讲述NS中下预测一个词过程：<br>（1）已知词$w$的上下文$context(w)$，需要预测$w$，那么认为词$w$作为中心词就是一个正样本$(context(w),w)$，其他词作为中心词就是负样本$(context(w),w_i)$，其中$i\in[1,\text{neg}]$。通过<strong>负采样</strong>得到 $\text{neg}$ （自己指定）个负样本 + 一个正样本，用$u$表示它们的集合。<br>（2）利用这一个正例$(context(w),w)$和 $\text{neg}$ 个负例$(context(w),w_i)$进行逻辑回归二分类，我们希望正样本分类概率最大化，可以通过梯度优化完成。这个就是预测一个词的过程。</p>
<p>整个过程要明白两个核心问题：1）如何利用$u$来做逻辑回归二分类？ 2）如何进行负采样？</p>
<hr>
<p><strong>我们通过基于Negative Sampling的CBOW来解答第一个问题。</strong><br>预测一个词时优化的目标函数$g(w)$表示为：</p>
<script type="math/tex; mode=display">
g(w)=\prod\limits_{u\in\{w\}\cup\{w_1...w_{neg}\}}p(u|context(w))</script><p>其中：</p>
<script type="math/tex; mode=display">
\begin{aligned}
p(u|context(w))&=
\begin{cases}
   \sigma(h_w\cdot\theta^{u}), &y_u=1\quad\text{(正类)}\\
   1-\sigma(h_w\cdot\theta^{u}), &y_u=0\quad\text{(负类)}
\end{cases}
~\\
\\&=[\sigma(h_w\cdot\theta^u)]^{y_u}\cdot[1-\sigma(h_w\cdot\theta^u)]^{1-y_u}
\end{aligned}</script><p>上式，代入$g(w)$中：</p>
<script type="math/tex; mode=display">
g(w)=\prod\limits_{u\in\{w\}\cup\{w_1...w_{neg}\}}[\sigma(h_w\cdot\theta^u)]^{y_u}\cdot[1-\sigma(h_w\cdot\theta^u)]^{1-y_u}</script><p>这就是预测一次的优化函数了。此时引入窗口$C=2c$，整体的优化函数就是：</p>
<script type="math/tex; mode=display">
\begin{aligned}
L=log\prod\limits_{C}g(w)=\sum\limits_{C}log(g(w))&=\sum\limits_{C}log\prod\limits_{u\in\{w\}\cup\{w_1...w_{neg}\}}\{[\sigma(h_w\cdot\theta^u)]^{y_u}\cdot[1-\sigma(h_w\cdot\theta^u)]^{1-y_u}\}\\
&=\sum\limits_{C}\sum\limits_{u\in\{w\}\cup\{w_1...w_{neg}\}}\{y_u\cdot log[\sigma(h_w\cdot\theta^u)]+(1-y_u)\cdot log[1-\sigma(h_w\cdot\theta^u)]\}
\end{aligned}</script><p>之后使用随机梯度上升优化即可。</p>
<p><strong>基于Negative Sampling的CBOW</strong>：<br>输入：基于CBOW的语料训练样本，词典大小$V$，中心词$w$，词向量的维度大小$N$，上下文$C$大小$2c$，步长$\eta$。<br>输出：词汇表每个词对应的模型参数$\theta$，所有的词向量$W=x_1,x_2,…,x_V$（避免和下面负样本混淆）。<br>（1）随机初始化所有的模型参数$\theta$和所有的词向量$W$。<br>（2）对于每个训练样本$(context(w),w)$，负采样出$neg$个负样本$(context(w),w_i)$，其中$i\in[1,\text{neg}]$。<br>（3）进行梯度上升迭代过程，对于训练集中的每一个样本$(context(w),w,w_1…w_{neg})$做如下处理：</p>
<ul>
<li>e=0，计算$h_w=\frac{1}{2c}\sum\limits_{i=-c}^cx_{i}$</li>
<li>for $u=\{w\}\cup\{w_1…w_{neg}\}$，计算：<ul>
<li>$f=\sigma(h_w\theta^u)$</li>
<li>$g=\eta(y^u-f)$</li>
<li>$e=e+g\cdot\theta^u$</li>
<li>$\theta^u=\theta^u+g\cdot h_w$</li>
</ul>
</li>
<li>对于$context(w)$中的每一个词向量$x_i$（共$2c$个）进行更新：<ul>
<li>$x_i=x_i+e$</li>
</ul>
</li>
<li>如果梯度收敛，则结束梯度迭代，否则回到（3）继续迭代。</li>
</ul>
<p><strong>基于Negative Sampling的Skip-gram</strong>：<br>输入：基于CBOW的语料训练样本，词典大小$V$，中心词$w$，词向量的维度大小$N$，上下文$C$大小$2c$，步长$\eta$。<br>输出：词汇表每个词对应的模型参数$\theta$，所有的词向量$W=x_1,x_2,…,x_V$（避免和下面负样本混淆）。<br>（1）随机初始化所有的模型参数$\theta$和所有的词向量$W$。<br>（2）对于每个训练样本$(context(w),w)$，负采样出$neg$个负样本$(context(w),w_i)$，其中$i\in[1,\text{neg}]$。<br>（3）进行梯度上升迭代过程，对于训练集中的每一个样本$(context(w),w,w_1…w_{neg})$做如下处理：</p>
<ul>
<li>for $s=context(w)$，计算：<ul>
<li>e=0，$h_w=x_i$</li>
<li>for $u=\{w\}\cup\{w_1…w_{neg}\}$，计算：<ul>
<li>$f=\sigma(h_w^s\theta^u)$</li>
<li>$g=\eta(y^u-f)$</li>
<li>$e=e+g\cdot\theta^u$</li>
<li>$\theta^u=\theta^u+g\cdot h_w^s$</li>
</ul>
</li>
</ul>
</li>
<li>对于$context(w)$中的每一个词向量$x_i$（共$2c$个）进行更新：<ul>
<li>$x_i^s=x_i^s+e$</li>
</ul>
</li>
<li>如果梯度收敛，则结束梯度迭代，否则回到（3）继续迭代。</li>
</ul>
<hr>
<p><strong>第二个问题：负采样如何做？</strong><br>对于<strong>Negative Sampling</strong>模型，负采样是一个很重要的环节，对于一个中心词$w$，如何生成$neg$个负样本呢？由于词典中的词在预料中出现的频次不同，我们希望那些<strong>高频词被选为负样本的概率大</strong>，<strong>低频词被选为负样本的概率低</strong>，这就是负采样的本质要求————<strong>带权采样问题</strong>。</p>
<p>通过一段通俗的描述来帮助理解带权采样的机理：<br>设大小为$V$的词典$D$中每一个词对应一个线段$l(w)$，长度为：</p>
<script type="math/tex; mode=display">
len(w)=\frac{count(w)}{\sum\limits_{u\in D}count(u)}</script><p>其中，分子表示一个词在语料中出现的次数（分母中的求和项用来做归一化）。将这些线段连接起来（共$V$个线段），形成一个长度为 1 的单位线段。如果随机的往这个线段上打点，则其中长度越长的线段（对应高频词）被打中的概率越大。</p>
<p>word2vec中词典$D$的词设置权值时，不是直接使用$count(w)$，而是对其做了$\alpha$次幂，其中$\alpha=\large\frac{3}{4}$，即上式变为：</p>
<script type="math/tex; mode=display">
len(w)=\frac{[count(w)]^{\large\frac{3}{4}}}{\normalsize\sum\limits_{u\in D}[count(u)]^{\large\frac{3}{4}}}</script><p>word2vec中具体做法是：把上面长度为 1 的单位线段分成<strong>等距离</strong>的M份（$ \text{M&gt;&gt;V}$），把这M份映射到前面讲的<strong>非等距离</strong>的V份中去。然后每次生成一个$[1,M]$间的随机整数，代表选择$M$份中的一份，然后按映射找到对应$V$份中的一份，此时它对应的单词就是我们选择的负样本了。这样重复取$neg$次，就得到所有负样本了。word2vec中$M=10^8$（对应源码中变量table_size）。</p>
<h3 id="a-good-word-embedding"><a href="#a-good-word-embedding" class="headerlink" title="a good word embedding"></a>a good word embedding</h3><p><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzE1MDcuMDU1MjMucGRmJTIwaHR0cDovL2llZWV4cGxvcmUuaWVlZS5vcmcvZG9jdW1lbnQvNzQ3ODQxNy8=">How to Generate a Good Word Embedding?<i class="fa fa-external-link-alt"></i></span>论文中对比了不同模型，总结了选择word embedding经验。<br>不同模型之间主要区别有两点：<br>1、目标词和上下文关系。上下文来预测目标词（这类模型更能够捕获单词之间的可替代关系）、目标词来预测上下文<br>2、上下文表示方法。<br><img src="/images/语言模型和词向量/good_embedding1.png"></p>
<p>不同模型之间上下文表示：<br><img src="/images/语言模型和词向量/good_embedding2.png"></p>
<p>据研究估计，<strong>文本含义信息的20%来自于词序，剩下的来自于词的选择</strong>。所以忽略词序信息的模型，将会损失约20%的信息。</p>
<p>本文做了三类对比实验：<br>1、<strong>研究词向量的语义特性</strong>。该类实验是为了对比词向量的语义特性，包括：WordSim353，TOEFL，analogy task：semantic和syntactic。<br>2、<strong>将词向量作为特征</strong>。该类实验是为了对比词向量作为处理其他任务的特征时，对该任务性能的提升。包括：文本分类和命名实体识别。前者将词向量加权平均得到文本向量来分类，权值是词频，数据集用的是IMDB；后者用CoNLL03数据集做NER任务。<br>3、<strong>用词向量来初始化神经网络模型</strong>。该类实验是为了研究词向量作为神经网络的初始值，对NN模型的提升。包括：CNN文本分类和词性标注。前者用了我们之前提到过的Kim的CNN模型，将句子表示成矩阵作为CNN的输入得到句子的表示，进行情感分类，数据集是Stanford Sentiment Treebank；后者用Wall Street Journal数据集进行了POS tagging任务。</p>
<p>该文对比了6种模型，并得到如下结论：<br>Q：<strong>哪个模型最好？如何选择c和w的关系以及c的表示方法？</strong><br>A：对于一个小型数据集来说，类似skip-gram这样越简单的模型效果越好，对于一个大型数据集来说，稍微复杂一点的模型，比如cbow和order就可以取得非常好的效果。真实数据中，skip-gram、cbow和order这样的模型足够了。在语义任务中，通过c来预测w的模型要更优于C&amp;W这种将c和w都放在输入层的模型。</p>
<p>Q：<strong>数据集的规模和所属领域对词向量的效果有哪些影响？</strong><br>A：数据集的<strong>领域远比规模重要</strong>，给定一个任务之后，选择任务相关的领域数据将会提升性能，相反，如果数据并不相关，将会导致更差的性能。当然，如果数据都属于同一领域，规模越大性能越好。</p>
<p>Q：<strong>在训练模型时迭代多少次可以有效地避免过拟合？</strong><br>A：通常的做法是在测试数据集上观察误差，当误差开始上升时即可停止训练，但经过本文的研究，这种方法并不能得到最好的task结果：因为训练词向量的目标是尽可能精确地预测目标词，这个优化目标和实际任务并不一致。因此最好的做法是：直接用实际任务的验证集来挑选迭代次数，即用task data作为early stopping的数据。如果实际任务非常耗时，则可以随机挑选某个简单任务（如：情感分类）及其验证集来挑选迭代次数。</p>
<p>Q：<strong>词向量的维度与效果之间的关系？</strong><br>A：做词向量语义分析任务时，一般维度越大，效果越好。做具体NLP任务时（用作输入特征、或者网络初始化），50维之后效果提升就比较少了。</p>
<p>本文作者做了大量的工作，针对当前词向量模型的方方面面问题进行了研究，并且给出了许多有意义的结果。对此想了解更多，可以看一下作者的<span class="exturl" data-url="aHR0cDovL2xpY3N0YXIubmV0L2FyY2hpdmVzLzYyMA==">《How to Generate a Good Word Embedding?》导读<i class="fa fa-external-link-alt"></i></span>。</p>
<hr>
<p><strong>word2vec结果评估</strong>：<br>1、通过kmeans聚类，查看聚类的簇分布。<br>2、通过词向量计算单词之间的相似度，查看相似词。<br>3、通过类比：a之于b等价于c之于d。<br>4、使用tsne降维可视化查看词的分布。</p>
<p><strong>word2vec输入向量和输出向量</strong>（<span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzE2MDguMDU4NTkucGRm">Using the Output Embedding to Improve Language Models<i class="fa fa-external-link-alt"></i></span>）：<br>1、在skip-gram模型中，在常见的衡量词向量的指标上，输出向量略微弱于输入向量。<br>2、<strong>在基于RNN的语言模型中，输出向量反而强于输入向量</strong>。<br>3、强制输入向量的转置作为输出向量，这可以使得输入向量等于输出向量。这种方式得到的词向量能够提升语言模型的困惑度perplexity。</p>
<p><strong>word2vec计算句子相似度</strong>（<span class="exturl" data-url="aHR0cDovL25scC50b3duL2Jsb2cvc2VudGVuY2Utc2ltaWxhcml0eS8=">Comparing Sentence Similarity Methods<i class="fa fa-external-link-alt"></i></span>）：<br>1、无监督方法：<br>（1）对句子中所有的词的词向量求平均，获得句子embedding。<br>（2）对句子中所有的词的词向量加权平均，每个词的权重为tf-idf，获得句子embedding。<br>（3）对句子中所有的词的词向量加权平均，每个词的权重为smooth inverse frequency:SIF（$\frac{a}{a+p(w)}$，$a$为超参数通常取0.001，$p(w)$为数据集中单词$w$的词频）；然后考虑所有的句子，并执行主成分分析；最后对每个句子的词向量加权平均减去first principal componet，获得句子embedding。<br>（4）通过 Word Mover’s Distance:WMD ，直接度量句子之间的相似度。WMD：使用两个句子中单词的词向量来衡量一个句子中的单词需要在语义空间中移动到另一个句子中的单词的最小距离。<br>2、有监督方法：<br>（5）通过分类任务来训练一个文本分类器，取最后一个hidden layer的输出作为句子embedding。就是使用文本分类器的前几层作为encoder。<br>（6）直接训练一对句子的相似性，其优点是可以直接得到句子embeding。<br><strong>最终结论是：简单加权的词向量平均已经可以作为一个较好的baseline。</strong></p>
<h3 id="常见问题"><a href="#常见问题" class="headerlink" title="常见问题"></a>常见问题</h3><p>问：噪声词在实际中被建议设为中心词的单字概率的3/4次幂，为什么？<br>答：在保证高频词容易被抽到的大方向下，通过权重3/4次幂的方式，适当提升低频词、罕见词被抽到的概率。如果不这么做，低频词，罕见词很难被抽到，以至于不被更新到对应的Embedding。</p>
<p>问：一些“the”和“a”之类的英文高频词会对结果产生什么影响？如何处理？（可参考<span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzEzMTAuNDU0Ni5wZGY=">word2vec论文<i class="fa fa-external-link-alt"></i></span>第2.3节）<br>答：噪声词为高频词对词向量训练没有什么效果，因为高频词太普遍了，为了抵消罕见词和高频词之间的不平衡，使用简单的二次抽样：训练集中的每个单词 $w_i$ 将有一定概率被丢弃，丢弃概率为：</p>
<script type="math/tex; mode=display">
P(w_i)=1-\sqrt{\frac{t}{f(w_i)}}</script><p>其中 $f(w_i)$ 是单词 $w_i$ 的频率，$t$ 是选择的阈值，通常在 $10^{-5}$ 左右，选择这个二次抽样公式是因为它主动地对频率大于 $t$ 的词进行二次抽样，同时保持了频率ranking，即随着单词在语料库中出现的词频越来越大，该单词保留的概率越来越低。。虽然这个二次抽样公式是启发式选择的，但我们发现它在实践中运作良好。它加快了训练速度，甚至显着提高了罕见词所学向量的准确性。</p>
<p>问：如何训练包括“new york”在内的词组向量？（可参考<span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzEzMTAuNDU0Ni5wZGY=">word2vec论文<i class="fa fa-external-link-alt"></i></span>第4节）<br>答：首先要找到经常一同出现但在其他语境中并不常见的单词。 例如，”New York Times”和”Toronto Maple Leafs”在训练集中将被独一无二的token所取代，而”this is”将保持不变。这样，我们可以形成许多合理的短语，而不会大大增加词汇量的大小。理论上，我们可以使用所有的n元文法训练Skip-gram模型，但是这太消耗内存。 许多识别文本中短语的技术之前已经被开发出来了， 然而，比较它们超过了我们的工作范围。 我们决定使用一种简单的数据驱动方法，基于unigram和bigram的计数来形成短语：</p>
<script type="math/tex; mode=display">
score(w_i,w_j) = \frac{count(w_iw_j)-\sigma}{count(w_i)\times count(w_j)}</script><p>$\sigma$ 被用作折扣系数，防止形成太多由非常罕见的单词组成的短语。 得分高于所选阈值的bigram将被用作短语。 通常，我们逐渐减少阈值对训练数据进行2-4次传递，从而允许形成更长的短语(由数个单词组成)。</p>
<h2 id="Glove"><a href="#Glove" class="headerlink" title="Glove"></a>Glove</h2><p>学习词向量的所有无监督方法最终都是基于语料库的单词共现统计，因此这些模型之间存在共性。词向量学习算法有两个主要的模型族：</p>
<ul>
<li>基于全局矩阵分解的方法，如：LSA：latent semantic analysis。<ul>
<li>优点：能够有效的利用全局的统计信息。</li>
<li>缺点：在单词类比任务（如：国王 vs 王后 类比于男人 vs 女人）中表现相对较差。</li>
</ul>
</li>
<li>基于局部上下文窗口的方法，如：word2vec。<ul>
<li>优点：在单词类比任务中表现较好。</li>
<li>缺点：因为word2vec 在独立的局部上下文窗口上训练，因此难以利用单词的全局统计信息。</li>
<li>GloVe:Global Vectors for Word Representation，结合了 LSA 算法和 Word2Vec 算法的优点，既考虑了全局统计信息，又利用了局部上下文。</li>
</ul>
</li>
</ul>
<hr>
<p>设单词-单词共现矩阵为 $X$ ，其中元素 $x_{ij}$ 为词 $j$ 出现在词 $i$ 环境(context)的次数。这里”环境”有多种可能的定义。举个例子，在一段文本序列中，如果词 $j$ 出现在词 $i$ 左边或者右边不超过10个词的距离，我们就可以认为词 $j$ 出现在词 $i$ 的环境一次，令 $x_i=\sum_k x_{ik}$ 为任意词出现在词 $i$ 的环境的次数，那么：</p>
<script type="math/tex; mode=display">
P_{ij}=P(j|i)=\frac{x_{ij}}{x_i}</script><p>为词 $j$ 出现在词 $i$ 的环境的概率。这一概率也称词 $i$ 和词 $j$ 的共现概率。</p>
<p>Glove论文展示了以下一组词对的共现概率与比值：</p>
<p><img src="/images/word2vec/1.png" width="60%"></p>
<p>我们通过商标可以观察以下现象：</p>
<ul>
<li>对于与“ice”相关但与“gas”无关的单词 $k$ ，例如 $k=solid$ ，我们预计会有更大的共现概率比值，例如8.9。</li>
<li>对于与“steam”相关但与“ice”无关的单词 $k$ ，例如 $k=gas$ ，我们预计较小的共现概率比值，例如0.085。</li>
<li>对于同时与“ice”和“steam”相关的单词 $k$ ，例如 $k=water$ ，我们预计其共现概率的比值接近1，例如1.36.</li>
<li>对于与“ice”和“steam”都不相关的单词 $k$ ，例如 $k=fashion$ ，我们预计共现概率的比值接近1，例如0.96.</li>
</ul>
<p>由此可见，共现概率的比值能够直观地表达词与词之间的关系。因此，我们可以设计三个词向量的函数来拟合这个比值。</p>
<script type="math/tex; mode=display">
f(v_i,v_j,\tilde{v}_k)=\frac{P_{ik}}{P_{jk}}</script><p>其中，$i$ 是中心词，$j$ 和 $k$ 是上下文词。<br>$f$ 可以有多种设计，由于 $f$ 映射的是向量空间，而向量空间是一个线性空间。因此从右侧的除法可以联想到减法：</p>
<script type="math/tex; mode=display">
f(v_i,v_j,\tilde{v}_k)=f(v_i-v_j,\tilde{v}_k)</script><p>又因为共现概率的比值是标量，所以我们要求 $f$ 是标量函数，即 $v_i-v_j$ 和 $\tilde{v}_k$ 均为向量，结果要求是标量，因此可以联想到向量的内积：</p>
<script type="math/tex; mode=display">
f((v_i-v_j)^T\tilde{v}_k)=f(v_i^T\tilde{v}_k-v_j^T\tilde{v}_k)</script><p>我们希望左边为差的形式转为右边为商的方式，定义：</p>
<script type="math/tex; mode=display">
f(v_i^T\tilde{v}_k-v_j^T\tilde{v}_k)=\frac{f(v_i^T\tilde{v}_k)}{f(v_j^T\tilde{v}_k)}</script><p>其中 $f(v_i^T\tilde{v}_k)=P_{ik}$，$f(v_j^T\tilde{v}_k)=P_{jk}$。<br>上式左边为差的形式，右边为商的形式。因此联想到 $exp$ 函数，即 $f$ 为 $exp$ 函数：</p>
<script type="math/tex; mode=display">
exp(v_i^T\tilde{v}_k)=P_{ik}=\frac{x_{ik}}{x_i}</script><p>等式两边应用 $log$ 函数：</p>
<script type="math/tex; mode=display">
\begin{aligned}
v_i^T\tilde{v}_k=log(x_{ik})-log(x_i)
\end{aligned}</script><p>即：</p>
<script type="math/tex; mode=display">
log(x_{ik})=v_i^T\tilde{v}_k+log(x_i)</script><p>由于向量的内积具有对称性，即 $X$ 为对称矩阵，需要满足整个条件，但上面式子不满足，因为 $log(x_i)$ 和 $log(x_k)$ 不一定相等的，为了解决这个问题，模型引入两个偏置项：</p>
<script type="math/tex; mode=display">
log(x_{ik})=v_i^T\tilde{v}_k+b_i+b_k</script><p>将索引 $i$ 和 $k$ 互换，我们可以验证对称性得两个性质可以同时被上式满足。</p>
<p>上面的公式仅仅是理想状态，实际上只能要求左右两边尽可能相等。于是设计代价函数为：</p>
<script type="math/tex; mode=display">
J=\sum\limits_{i,j=1}^Vf(x_{ij})(v_i^T\tilde{v}_j+b_i+b_j-log(x_{ij}))^2</script><p>其中，$f(x_{ij})$ 代表不同词频的词的重要性（权重）。使用优化算法最小化它即可。</p>
<p>对于权重函数 $f(x_{ij})$ ，一个建议的选择是：当 $x&lt;c$ （例如 $c=100$），令 $f(x)=(x/c)^\alpha$ （例如 $\alpha=0.75$），反之令 $f(x)=1$ 。需要注意的是，损失函数的计算复杂度与共现词频矩阵 $X$ 中非零元素的数目呈线性关系。我们可以从 $X$ 中随机采样小批量非零元素，使用随机梯度下降迭代词向量和偏移项。当所有词向量学习得到后，Glove使用一个词得中心词向量与上下文词向量之和作为该词最终词向量。</p>
<p>GloVe 模型性能与语料库大小的关系：</p>
<ul>
<li>在语法任务中，模型性能随着语料库大小的增长而单调增长。这是因为语料库越大，则语法的统计结果越可靠。</li>
<li>在语义任务中，模型性能与语料库绝对大小无关，而与语料库的有效大小有关。有效大小指的是语料库中，与目标语义相关的内容的大小。</li>
</ul>
<p>GloVe 模型超参数选择：</p>
<ul>
<li>词向量大小：词向量大小越大，则模型性能越好。但是词向量超过 200 维时，维度增加的收益是递减的。</li>
<li>窗口对称性：计算一个单词的上下文时，上下文窗口可以是对称的，也可以是非对称的。<ul>
<li>对称窗口：既考虑单词左侧的上下文，又考虑单词右侧的上下文。</li>
<li>非对称窗口：只考虑单词左侧的上下文。因为语言的阅读习惯是从左到右，所以只考虑左侧的上下文，不考虑右侧的上下文。</li>
</ul>
</li>
<li>窗口大小：<ul>
<li>语法任务：选择小的、非对称的窗口时，模型性能更好。因为语法是局部的，所以小窗口即可；因为语法是依赖于单词顺序的，所以需要非对称窗口。</li>
<li>语义任务：则需要选择更大的窗口。因为语义是非局部的。</li>
</ul>
</li>
</ul>
<h2 id="fastText"><a href="#fastText" class="headerlink" title="fastText"></a>fastText</h2><p>fastText 是 Facebook AI Research 在 2016 年开源的文本分类器，其提出是在论文 《Bag of Tricks for Efficient Text Classification》 中。目前 fastText 作为文本分类的基准模型。<br>fastText 的优点是：在保持分类效果的同时，大大缩短了训练时间。</p>
<p>fastText在使用负采样的skip-gram模型基础上，将每个中心词视为子词(subword)的集合，并学习子词的词向量。<br>以where这个词为例，设子词为3个字符，它的子词包括“&#60;wh”、“whe”、“her”、“ere”、“re&#62;” 和特殊子词（整词）“&#60;where&#62;”。其中的“&#60;”和“&#62;”是为了将作为前后缀的子词区分出来。而且，这里的子词“her”与整词“&#60;her&#62;”也可被分。给定一个词 $w$，我们通常可以把字符长度在3-6之间的所有子词和特殊子词的并集 $\mathcal{G}_w$ 取出。假设词典中任意子词 $g$ 的子词向量为 $z_g$，我们可以把使用负采样的skip-gram模型的损失函数（中心词 $w_c$，上下文词 $w_o$，噪声词 $w_k$ 在词典中的索引为 $i_k$）：</p>
<script type="math/tex; mode=display">
-logP(w_o|w_c)=-log\frac{1}{1+exp(-u_o^Tv_c)}-\sum\limits_{k=1,w_k\sim P(w)}^K log\frac{1}{1+exp(u_{i_k}^Tv_c)}</script><p>直接替换成：</p>
<script type="math/tex; mode=display">
-logP(w_o|w_c)=-log\frac{1}{1+exp(-u_o^T\sum_{g\in \mathcal{G}_w }z_g )}-\sum\limits_{k=1,w_k\sim P(w)}^K log\frac{1}{1+exp(u_{i_k}^T\sum_{g\in \mathcal{G}_w }z_g)}</script><p>我们可以看到，原中心词向量被替换成了中心词的子词向量的和。与整词学习（word2vec和Glove）不同，词典以外的新词的词向量可以使用fastText中相应的子词向量之和。</p>
<p>fastText对于一些语言较重要，例如阿拉伯语、德语和俄语。例如，德语中有很多复合词，例如兵乓球（table tennis）在德语中叫“Tischtennis”。fastText可以通过子词可以表达两个词的相关性，例如“Tischtennis”和“Tennis”。</p>
<p><strong>总结：Glove用词向量表达共现词频的对数。fastText用子词向量之和表达整词。</strong></p>
<p>Fasttext可以做文本分类：<span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzE2MDcuMDE3NTl2Mi5wZGY=">Bag of Tricks for Efficient Text Classification<i class="fa fa-external-link-alt"></i></span>。<br>Fasttext可以训练词向量：<span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzE2MDcuMDQ2MDYucGRm">Enriching word vectors with subword information<i class="fa fa-external-link-alt"></i></span>。<br>Fasttext官方：<span class="exturl" data-url="aHR0cHM6Ly9mYXN0dGV4dC5jYy9kb2NzL2VuL3N1cGVydmlzZWQtdHV0b3JpYWwuaHRtbOOAgg==">https://fasttext.cc/docs/en/supervised-tutorial.html。<i class="fa fa-external-link-alt"></i></span></p>
<p><strong>补充问答</strong>：<br>问：如果一个词出现在另一个词的背景窗口中，如何利用它们之间在文本序列的距离重新设计条件概率 $P_{ij}$ 的计算方式？（提示：可参考<span class="exturl" data-url="aHR0cHM6Ly9ubHAuc3RhbmZvcmQuZWR1L3B1YnMvZ2xvdmUucGRm">GloVe论文<i class="fa fa-external-link-alt"></i></span>4.2节）<br>问：如果丢弃Glove中的偏移项，是否也可以满足任意一对词共现的对称性？<br>问：在fastText中，子词过多怎么办？（例如，6字英文组合数为$26^6$）？（提示：可参考<span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzE2MDcuMDE3NTl2Mi5wZGY=">fastText论文<i class="fa fa-external-link-alt"></i></span>3.2节）</p>
<h1 id="补充"><a href="#补充" class="headerlink" title="补充"></a>补充</h1><h2 id="softmax"><a href="#softmax" class="headerlink" title="softmax"></a>softmax</h2><script type="math/tex; mode=display">
\begin{aligned}
S_i=\dfrac{e^{V_i}}{\sum_j e^{V_j}}
\end{aligned}</script><p>其中，一个向量$ V$共有$j$个值，$V_i$表示第$i$个值。<br>首先对所有值进行$e^x$计算，保证所有值都是大于0的。其次进行归一化，保证所有值的和为1。这些特点非常符合概率的要求，所以经常把softmax处理后的值当成概率。</p>
<h2 id="sigmoid"><a href="#sigmoid" class="headerlink" title="sigmoid"></a>sigmoid</h2><script type="math/tex; mode=display">
\sigma(x)=\dfrac{1}{1+e^{-x}}</script><p>定义域为$(-\infty,+\infty)$，值域为$(0,1)$，下图给出了sigmoid的图像：<br><img src="/images/语言模型和词向量/sigmoid.png" width="40%"></p>
<p>sigmoid函数<strong>导函数</strong>具有性质：</p>
<script type="math/tex; mode=display">
\sigma^\prime(x)=\sigma(x)[1-\sigma(x)]</script><p>由此可知：</p>
<script type="math/tex; mode=display">
[log\sigma(x)]^\prime=1-\sigma(x)</script><script type="math/tex; mode=display">
[log(1-\sigma(x))]^\prime=-\sigma(x)</script><p>sigmoid的每一次计算是相互独立的，是对当前事件的一次独立判断。我们把它用作二分类，是因为每一次判断的结果都可以根据阈值划分为两类，比如阈值为t，那么计算结果大于t的为一类，低于t的为另一类。也可以把计算结果看作二分类中一类的概率，比如计算结果为p，那么事件是一类的概率就是p，另一个类概率就是1-p。</p>
<h2 id="gensim"><a href="#gensim" class="headerlink" title="gensim"></a>gensim</h2><p>Word2Vec：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 4.0</span></span><br><span class="line">gensim.models.word2vec.Word2Vec(sentences=<span class="literal">None</span>, corpus_file=<span class="literal">None</span>, vector_size=<span class="number">100</span>, </span><br><span class="line">alpha=<span class="number">0.025</span>, window=<span class="number">5</span>, min_count=<span class="number">5</span>, max_vocab_size=<span class="literal">None</span>, sample=<span class="number">0.001</span>, seed=<span class="number">1</span>, workers=<span class="number">3</span>, </span><br><span class="line">min_alpha=<span class="number">0.0001</span>, sg=<span class="number">0</span>, hs=<span class="number">0</span>, negative=<span class="number">5</span>, ns_exponent=<span class="number">0.75</span>, cbow_mean=<span class="number">1</span>, hashfxn=&lt;built-<span class="keyword">in</span> function <span class="built_in">hash</span>&gt;, </span><br><span class="line">epochs=<span class="number">5</span>, null_word=<span class="number">0</span>, trim_rule=<span class="literal">None</span>, sorted_vocab=<span class="number">1</span>, batch_words=<span class="number">10000</span>, </span><br><span class="line">compute_loss=<span class="literal">False</span>, callbacks=(), comment=<span class="literal">None</span>, max_final_vocab=<span class="literal">None</span>) </span><br><span class="line"><span class="comment"># 3.8</span></span><br><span class="line">gensim.models.word2vec.Word2Vec(sentences=<span class="literal">None</span>, corpus_file=<span class="literal">None</span>, size=<span class="number">100</span>, </span><br><span class="line">alpha=<span class="number">0.025</span>, window=<span class="number">5</span>, min_count=<span class="number">5</span>, max_vocab_size=<span class="literal">None</span>, sample=<span class="number">0.001</span>, seed=<span class="number">1</span>, workers=<span class="number">3</span>, </span><br><span class="line">min_alpha=<span class="number">0.0001</span>, sg=<span class="number">0</span>, hs=<span class="number">0</span>, negative=<span class="number">5</span>, ns_exponent=<span class="number">0.75</span>, cbow_mean=<span class="number">1</span>, hashfxn=&lt;built-<span class="keyword">in</span> function <span class="built_in">hash</span>&gt;, </span><br><span class="line"><span class="built_in">iter</span>=<span class="number">5</span>, null_word=<span class="number">0</span>, trim_rule=<span class="literal">None</span>, sorted_vocab=<span class="number">1</span>, batch_words=<span class="number">10000</span>, </span><br><span class="line">compute_loss=<span class="literal">False</span>, callbacks=(), max_final_vocab=<span class="literal">None</span>)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">sentences：(iterable of iterables, optional) 要分析的语料，可以是一个列表，或者从文件中遍历读出。</span></span><br><span class="line"><span class="string">          大语料库，建议直接从磁盘/网络流迭代传输句子。参阅word2vec模块中的BrownCorpus，Text8Corpus或LineSentence。</span></span><br><span class="line"><span class="string">corpus_file：(str, optional。LineSentence) 格式的语料库文件路径。</span></span><br><span class="line"><span class="string">size/vector_size：(int, optional) 词向量的维度，默认值是100。</span></span><br><span class="line"><span class="string">      这个维度的取值一般与我们的语料的大小相关，如果是不大的语料，比如小于100M的文本语料，则使用默认值一般就可以了。</span></span><br><span class="line"><span class="string">      如果是超大的语料，建议增大维度。</span></span><br><span class="line"><span class="string">window：(int, optional) 即词向量上下文最大距离。</span></span><br><span class="line"><span class="string">        这个参数在讲解中标记为c，值越大，则和某一词较远的词也会产生上下文关系。默认值为5。</span></span><br><span class="line"><span class="string">        如果是小语料则这个值可以设的更小。对于一般的语料这个值推荐在[5,10]之间。</span></span><br><span class="line"><span class="string">min_count：(int, optional) 忽略词频小于此值的单词。这个值可以去掉一些很生僻的低频词，默认是5。</span></span><br><span class="line"><span class="string">           如果是小语料，可以调低这个值。</span></span><br><span class="line"><span class="string">workers：(int, optional) 训练模型时使用的线程数。</span></span><br><span class="line"><span class="string">sg：(&#123;0, 1&#125;, optional) word2vec两个模型选择。0：CBOW模型。1：Skip-Gram模。默认是0即CBOW模型。</span></span><br><span class="line"><span class="string">hs：(&#123;0, 1&#125;, optional) word2vec两个解法选择。0：Negative Sampling。1：Hierarchical Softmax。默认是0即Negative Sampling。</span></span><br><span class="line"><span class="string">negative：(int, optional) 即使用Negative Sampling时负采样的个数，默认是5。推荐在[3,10]之间。这个参数在讲解中标记为neg。</span></span><br><span class="line"><span class="string">ns_exponent：(float, optional) 负采样分布指数。1.0样本值与频率成正比，0.0样本所有单词均等，负值更多地采样低频词。</span></span><br><span class="line"><span class="string">cbow_mean：(&#123;0, 1&#125;, optional) 仅用于CBOW在做投影的时候。</span></span><br><span class="line"><span class="string">           为0，则算法中的h为上下文的词向量之和，为1则为上下文的词向量的平均值。</span></span><br><span class="line"><span class="string">           在讲解中是按照词向量的平均值来描述的。</span></span><br><span class="line"><span class="string">alpha：(float, optional) 在随机梯度下降法中迭代的初始步长。讲解中标记为η，默认是0.025。</span></span><br><span class="line"><span class="string">min_alpha：(float, optional) 随着训练的进行，学习率线性下降到min_alpha。</span></span><br><span class="line"><span class="string">           由于算法支持在迭代的过程中逐渐减小步长，min_alpha给出了最小的迭代步长值。</span></span><br><span class="line"><span class="string">           随机梯度下降中每轮的迭代步长可以由iter，alpha， min_alpha一起得出。</span></span><br><span class="line"><span class="string">           对于大语料，需要对alpha, min_alpha,iter一起调参，来选择合适的三个值。</span></span><br><span class="line"><span class="string">seed：(int, optional) 随机数发生器种子。</span></span><br><span class="line"><span class="string">max_vocab_size：(int, optional) 词汇构建期间RAM的限制。</span></span><br><span class="line"><span class="string">                如果有更多的独特单词，则修剪不常见的单词。每1000万个类型的字需要大约1GB的RAM。</span></span><br><span class="line"><span class="string">max_final_vocab：(int, optional) 自动选择匹配的min_count将词汇限制为目标词汇大小。</span></span><br><span class="line"><span class="string">sample：(float, optional) 高频词随机下采样的配置阈值，范围是(0,1e-5)。</span></span><br><span class="line"><span class="string">hashfxn：(function, optional) 哈希函数用于随机初始化权重，以提高训练的可重复性。</span></span><br><span class="line"><span class="string">iter/epochs：随机梯度下降法中迭代的最大次数，默认是5。对于大语料，可以增大这个值。</span></span><br><span class="line"><span class="string">trim_rule：(function, optional) 词汇修剪规则，指定某些词语是否应保留在词汇表中，修剪掉或使用默认值处理。</span></span><br><span class="line"><span class="string">sorted_vocab：(&#123;0, 1&#125;, optional) 如果为1，则在分配单词索引前按降序对词汇表进行排序。</span></span><br><span class="line"><span class="string">batch_words：(int, optional) 每一个batch传递给线程单词的数量。</span></span><br><span class="line"><span class="string">compute_loss：(bool, optional) 如果为True，则计算并存储可使用get_latest_training_loss()检索的损失值。</span></span><br><span class="line"><span class="string">callbacks：(iterable of CallbackAny2Vec, optional) 在训练中特定阶段执行回调序列。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="comment"># 建立模型后</span></span><br><span class="line">build_vocab(sentences)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">遍历一次语料库建立词典</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="comment"># 3.8</span></span><br><span class="line">train(sentences=<span class="literal">None</span>, corpus_file=<span class="literal">None</span>, total_examples=<span class="literal">None</span>, total_words=<span class="literal">None</span>, </span><br><span class="line">epochs=<span class="literal">None</span>, start_alpha=<span class="literal">None</span>, end_alpha=<span class="literal">None</span>, word_count=<span class="number">0</span>, queue_factor=<span class="number">2</span>, </span><br><span class="line">report_delay=<span class="number">1.0</span>, compute_loss=<span class="literal">False</span>, callbacks=())</span><br><span class="line"><span class="comment"># 4.0</span></span><br><span class="line">train(corpus_iterable=<span class="literal">None</span>, corpus_file=<span class="literal">None</span>, total_examples=<span class="literal">None</span>, total_words=<span class="literal">None</span>, </span><br><span class="line">epochs=<span class="literal">None</span>, start_alpha=<span class="literal">None</span>, end_alpha=<span class="literal">None</span>, word_count=<span class="number">0</span>, queue_factor=<span class="number">2</span>, </span><br><span class="line">report_delay=<span class="number">1.0</span>, compute_loss=<span class="literal">False</span>, callbacks=(), **kwargs)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">第二次遍历语料库建立神经网络模型</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="comment"># 方法</span></span><br><span class="line">similar_by_word()</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">某一个词向量最相近的词集合</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">similarity()</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">两个词向量的相近程度</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">doesnt_match()</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">找出不同类的词</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">save_word2vec_format(<span class="string">&#x27;./model.txt&#x27;</span>,binary = <span class="literal">False</span>)</span><br><span class="line">save_word2vec_format(<span class="string">&#x27;./model.bin&#x27;</span>,binary = <span class="literal">True</span>)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">第一种，保存了训练的全部信息，可以在读取后追加训练</span></span><br><span class="line"><span class="string">第二种，保存为word2vec文本格式但是保存时丢失了词汇树等部分信息，不能追加训练</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">KeyedVectors.load_word2vec_format(<span class="string">&#x27;./model.bin&#x27;</span>, binary=<span class="literal">False</span>)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">加载模型</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><br>LdaMulticore：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 4.0/3.8</span></span><br><span class="line">gensim.models.ldamulticore.LdaMulticore(corpus=<span class="literal">None</span>, num_topics=<span class="number">100</span>, </span><br><span class="line">id2word=<span class="literal">None</span>, workers=<span class="literal">None</span>, chunksize=<span class="number">2000</span>, passes=<span class="number">1</span>, batch=<span class="literal">False</span>, alpha=<span class="string">&#x27;symmetric&#x27;</span>, </span><br><span class="line">eta=<span class="literal">None</span>, decay=<span class="number">0.5</span>, offset=<span class="number">1.0</span>, eval_every=<span class="number">10</span>, iterations=<span class="number">50</span>, gamma_threshold=<span class="number">0.001</span>, </span><br><span class="line">random_state=<span class="literal">None</span>, minimum_probability=<span class="number">0.01</span>, minimum_phi_value=<span class="number">0.01</span>, </span><br><span class="line">per_word_topics=<span class="literal">False</span>)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">corpus: &#123;iterable of list of (int, float), scipy.sparse.csc&#125;，语料库。</span></span><br><span class="line"><span class="string">num_topics: int，主题数量。</span></span><br><span class="line"><span class="string">id2word: &#123;dict of (int, str), gensim.corpora.dictionary.Dictionary&#125;，单词id-&gt;单词 词典。</span></span><br><span class="line"><span class="string">workers: int，线程数。</span></span><br><span class="line"><span class="string">chunksize: int，每个训练模块使用文档数量。</span></span><br><span class="line"><span class="string">passes: int，训练期间通过语料库的次数。</span></span><br><span class="line"><span class="string">alpha: float，文档-主题分布的先验信念。</span></span><br><span class="line"><span class="string">      &#x27;symmetric&#x27;: 使用 1.0 / num_topics 的固定对称先验。</span></span><br><span class="line"><span class="string">      &#x27;asymmetric&#x27;: 使用 1.0 / (topic_index + sqrt(num_topics)) 的固定归一化非对称先验。</span></span><br><span class="line"><span class="string">eta: float，主题-词分布的先验信念。</span></span><br><span class="line"><span class="string">      &#x27;symmetric&#x27;: 使用 1.0 / num_topics 的固定对称先验。</span></span><br><span class="line"><span class="string">      &#x27;auto&#x27;: 从语料库中学习非对称先验。</span></span><br><span class="line"><span class="string">per_word_topics: 如果为 True，该模型还会计算一个主题列表，按每个单词最可能的主题的降序排序，</span></span><br><span class="line"><span class="string">                  以及它们的 phi 值乘以特征长度（即字数）。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><br>词典：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line">gensim.corpora.dictionary.Dictionary(documents=<span class="literal">None</span>, prune_at=<span class="number">2000000</span>)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">根据文档生成词典</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="comment"># 属性</span></span><br><span class="line">token2id</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">词典：&#123;token:tokenId&#125;</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">dfs</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">单词出现的频率</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="comment"># 方法</span></span><br><span class="line">doc2bow(document, allow_update=<span class="literal">False</span>, return_missing=<span class="literal">False</span>)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">将文档转成词袋格式：一个列表，列表中每个元素为(token_id, token_count)</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">filter_n_most_frequent(remove_n)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">过滤掉出现频率最高的remove_n个单词</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">filter_extremes(no_below=<span class="number">5</span>, no_above=<span class="number">0.5</span>, keep_n=<span class="number">100000</span>) </span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">去掉出现次数低于no_below的。</span></span><br><span class="line"><span class="string">去掉出现次数高于no_above的（百分数）。</span></span><br><span class="line"><span class="string">在上面基础上，保留出现频率前keep_n的单词。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">filter_tokens(bad_ids=<span class="literal">None</span>, good_ids=<span class="literal">None</span>)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">两种用法:</span></span><br><span class="line"><span class="string">(1)去掉bad_id对应的词。</span></span><br><span class="line"><span class="string">(2)保留good_id对应的词而去掉其他词。</span></span><br><span class="line"><span class="string">注意，这里bad_ids和good_ids都是列表形式。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">compacity() </span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">执行完过滤操作以后，可能会造成单词的序号之间有空隙。</span></span><br><span class="line"><span class="string">可以使用该函数来对词典来进行重新排序，去掉这些空隙。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure></p>
<h1 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h1><p><span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL2hhbmtjcy9IYW5MUA==">HanLP: Han Language Processing<i class="fa fa-external-link-alt"></i></span><br>统计自然语言处理 第二版 (宗成庆著)<br><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzEzMDEuMzc4MS5wZGY=">Efficient Estimation of Word Representations in Vector Space<i class="fa fa-external-link-alt"></i></span><br><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzEzMTAuNDU0Ni5wZGY=">Distributed Representations of Words and Phrases and their Compositionality<i class="fa fa-external-link-alt"></i></span><br><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzE0MTEuMjczOC5wZGY=">word2vec Parameter Learning Explained<i class="fa fa-external-link-alt"></i></span><br><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzE1MDcuMDU1MjMucGRmJTIwaHR0cDovL2llZWV4cGxvcmUuaWVlZS5vcmcvZG9jdW1lbnQvNzQ3ODQxNy8=">How to Generate a Good Word Embedding?<i class="fa fa-external-link-alt"></i></span><br><span class="exturl" data-url="aHR0cDovL25scC50b3duL2Jsb2cvc2VudGVuY2Utc2ltaWxhcml0eS8=">Comparing Sentence Similarity Methods<i class="fa fa-external-link-alt"></i></span><br><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzE2MDguMDU4NTkucGRm">Using the Output Embedding to Improve Language Models<i class="fa fa-external-link-alt"></i></span><br><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzE0MDIuMzcyMi5wZGY=">word2vec Explained: Deriving Mikolov et al.’sNegative-Sampling Word-Embedding Method<i class="fa fa-external-link-alt"></i></span><br><span class="exturl" data-url="aHR0cHM6Ly9ubHAuc3RhbmZvcmQuZWR1L3B1YnMvZ2xvdmUucGRm">GloVe: Global Vectors for Word Representation<i class="fa fa-external-link-alt"></i></span><br><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzE2MDcuMDE3NTl2Mi5wZGY=">Bag of Tricks for Efficient Text Classification<i class="fa fa-external-link-alt"></i></span><br><span class="exturl" data-url="aHR0cHM6Ly96aC53aWtpcGVkaWEub3JnL3dpa2kvJUU5JTlDJThEJUU1JUE0JUFCJUU2JTlCJUJDJUU3JUJDJTk2JUU3JUEwJTgx">维基百科：霍夫曼编码<i class="fa fa-external-link-alt"></i></span><br><span class="exturl" data-url="aHR0cHM6Ly93d3cuY25ibG9ncy5jb20vcGluYXJkL3AvNzI0MzUxMy5odG1s">word2vec原理(二) 基于Hierarchical Softmax的模型<i class="fa fa-external-link-alt"></i></span><br><span class="exturl" data-url="aHR0cHM6Ly93d3cuY25ibG9ncy5jb20vcGluYXJkL3AvNzI0OTkwMy5odG1s">word2vec原理(三) 基于Negative Sampling的模型<i class="fa fa-external-link-alt"></i></span><br><span class="exturl" data-url="aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2l0cGx1cy9hcnRpY2xlL2RldGFpbHMvMzc5Njk5Nzk=">word2vec 中的数学原理详解（四）基于 Hierarchical Softmax 的模型<i class="fa fa-external-link-alt"></i></span><br><span class="exturl" data-url="aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2l0cGx1cy9hcnRpY2xlL2RldGFpbHMvMzc5OTg3OTc=">word2vec 中的数学原理详解（五）基于 Negative Sampling 的模型<i class="fa fa-external-link-alt"></i></span><br><span class="exturl" data-url="aHR0cHM6Ly96aC12Mi5kMmwuYWkvY2hhcHRlcl9uYXR1cmFsLWxhbmd1YWdlLXByb2Nlc3NpbmctcHJldHJhaW5pbmcvd29yZDJ2ZWMuaHRtbA==">词嵌入（Word2vec）<i class="fa fa-external-link-alt"></i></span><br><span class="exturl" data-url="aHR0cHM6Ly93d3cuYmlsaWJpbGkuY29tL3ZpZGVvL0JWMTJXNDExdjdHYT9mcm9tPXNlYXJjaCZhbXA7c2VpZD0xNDEzNTQ1MDg0OTMwNzM4OTY3OSZhbXA7c3BtX2lkX2Zyb209MzMzLjMzNy4wLjA=">[MXNet/Gluon] 动手学深度学习第十六课：词向量（word2vec）<i class="fa fa-external-link-alt"></i></span><br><span class="exturl" data-url="aHR0cHM6Ly93d3cuYmlsaWJpbGkuY29tL3ZpZGVvL0JWMWFXNDExZTcyOD9mcm9tPXNlYXJjaCZhbXA7c2VpZD01NzU4NzQxNTIwNjE0NDkxMzMxJmFtcDtzcG1faWRfZnJvbT0zMzMuMzM3LjAuMA==">[MXNet/Gluon] 动手学深度学习第十七课：GloVe、fastText和使用预训练的词向量<i class="fa fa-external-link-alt"></i></span></p>

      
    </div>

    
    
    
      


    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://soundmemories.github.io/2021/06/19/Paper/00.Paper%E9%98%85%E8%AF%BB%E6%8A%80%E5%B7%A7/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="https://i.loli.net/2020/11/04/6JhNuwtBe4adylS.png">
      <meta itemprop="name" content="SoundMemories">
      <meta itemprop="description" content="今日事，今日毕">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="SoundMemories">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/06/19/Paper/00.Paper%E9%98%85%E8%AF%BB%E6%8A%80%E5%B7%A7/" class="post-title-link" itemprop="url">Paper阅读技巧</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2021-06-19 00:00:00" itemprop="dateCreated datePublished" datetime="2021-06-19T00:00:00+08:00">2021-06-19</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Paper/" itemprop="url" rel="index"><span itemprop="name">Paper</span></a>
        </span>
    </span>

  
      </div>
      <div class="post-meta">
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>5k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>5 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="Paper"><a href="#Paper" class="headerlink" title="Paper"></a>Paper</h1><p>Paper一般会发表在期刊和顶会中，在期刊上发表Paper需要花费大量时间和精力不断修改完善论文，所以周期很长，尤其是顶级期刊。而在顶会中发表论文周期会短些，所以很多重要的成果都先在顶会中出现。一般工作后，重点关注顶会即可。<br>
          <!--noindex-->
            <div class="post-button">
              <a class="btn" href="/2021/06/19/Paper/00.Paper%E9%98%85%E8%AF%BB%E6%8A%80%E5%B7%A7/#more" rel="contents">
                阅读全文 &raquo;
              </a>
            </div>
          <!--/noindex-->
        
      
    </div>

    
    
    
      


    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://soundmemories.github.io/2021/06/19/NLP/00.NLP%E7%AE%80%E4%BB%8B/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="https://i.loli.net/2020/11/04/6JhNuwtBe4adylS.png">
      <meta itemprop="name" content="SoundMemories">
      <meta itemprop="description" content="今日事，今日毕">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="SoundMemories">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/06/19/NLP/00.NLP%E7%AE%80%E4%BB%8B/" class="post-title-link" itemprop="url">NLP简介</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2021-06-19 00:00:00" itemprop="dateCreated datePublished" datetime="2021-06-19T00:00:00+08:00">2021-06-19</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/NLP/" itemprop="url" rel="index"><span itemprop="name">NLP</span></a>
        </span>
    </span>

  
      </div>
      <div class="post-meta">
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>10k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>10 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="什么是NLP？"><a href="#什么是NLP？" class="headerlink" title="什么是NLP？"></a>什么是NLP？</h1><p><strong>NLP（Natural Language Processing，自然语言处理）</strong>，是融合了计算机科学、人工智能、语言学的交叉学科。其主要目的是让计算机学会处理人类的语言，甚至实现终极目标——————理解人类语言。</p>
<p>NLP可以概括为<strong>NLP=NLU+NLG</strong>：<br><strong>NLU（Natural Language Understand，自然语言理解）</strong>：语音或文本 ——&gt; 结构化的语义。<br><strong>NLG（Natural Language Generation，自然语言生成）</strong>：把结构化的语义 ——&gt; 文本或语音。<br>
          <!--noindex-->
            <div class="post-button">
              <a class="btn" href="/2021/06/19/NLP/00.NLP%E7%AE%80%E4%BB%8B/#more" rel="contents">
                阅读全文 &raquo;
              </a>
            </div>
          <!--/noindex-->
        
      
    </div>

    
    
    
      


    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://soundmemories.github.io/2021/02/12/Graph/03.GraphSAGE/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="https://i.loli.net/2020/11/04/6JhNuwtBe4adylS.png">
      <meta itemprop="name" content="SoundMemories">
      <meta itemprop="description" content="今日事，今日毕">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="SoundMemories">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/02/12/Graph/03.GraphSAGE/" class="post-title-link" itemprop="url">GraphSAGE</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2021-02-12 00:00:00" itemprop="dateCreated datePublished" datetime="2021-02-12T00:00:00+08:00">2021-02-12</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Graph/" itemprop="url" rel="index"><span itemprop="name">Graph</span></a>
        </span>
    </span>

  
      </div>
      <div class="post-meta">
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>14k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>13 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>大型图中节点的低维embedding已被证明在各种预测任务中非常有用，从内容推荐到识别蛋白质功能。然而，大多数现有的方法都要求在embedding训练期间图中的所有节点都存在；这些以前的方法本质上是<strong>transductive</strong>（直推式），不能自然地推广到看不见的节点。这里我们介绍GraphSAGE，这是一个通用<strong>inductive</strong>（归纳式）框架，它利用节点特征信息（例如，文本属性）高效地为以前看不见的数据生成节点embedding。我们学习的不是为每个节点训练单个embedding，而是通过从节点的局部邻域采样和聚合特征来生成嵌入的函数。</p>
<p>GraphSAGE是一个经典的基于空域的算法，它从两个方面对传统的GCN做了改进：<br>（1）<strong>在训练时的，采样方式将GCN的全图采样优化到部分以节点为中心的邻居抽样，这使得大规模图数据的分布式训练成为可能，并且使得网络可以学习没有见过的节点，这也使得GraphSAGE可以做归纳学习（Inductive Learning）</strong>。<br>（2）GraphSAGE研究了若干种<strong>邻居聚合</strong>的方式，并通过实验和理论分析对比了不同聚合方式的优缺点。</p>
<p><strong>归纳学习（Inductive Learning）</strong>：指可以对训练过程中见不到的数据直接计算而不需要重新对整个图进行学习。<br><strong>直推学习（Transductive Learning）</strong>：指所有的数据都可以在训练的时候拿到，学习的过程是在这个固定的图上进行学习，一旦图中的某些节点发生变化，则需要对整个图进行重新训练和学习。</p>
<h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p><img src="/images/GraphSAGE/1.png" width="80%"></p>
<p>GraphSAGE的算法核心是<strong>将整张图的采样优化到当前邻居节点的采样</strong>，因此从邻居<strong>采样</strong>（Sample）和邻居<strong>聚合</strong>（aggregate）两个方面来对GraphSAGE进行解释。具体流程如上图所示，可分为三个步骤：</p>
<ol>
<li>对图中每个顶点邻居顶点进行采样，因为每个节点的度是不一致的，为了计算高效， 为每个节点采样固定数量的邻居。</li>
<li>根据聚合函数聚合邻居顶点蕴含的信息。</li>
<li>得到图中各顶点的向量表示供下游任务使用。</li>
</ol>
<h1 id="Embedding-generation"><a href="#Embedding-generation" class="headerlink" title="Embedding generation"></a>Embedding generation</h1><p>GraphSAGE的前向传播算法如下，前向传播描述了如何使用聚合函数对节点的邻居信息进行聚合，从而生成节点embedding：</p>
<ol>
<li>首先确定深度K，代表聚合深度，对每一层 $k=1…K$ 进行遍历；</li>
<li>在当前层$k$，<strong>采样</strong>固定size大小的邻居节点进行<strong>聚合</strong>，得到$h_{N(v)}^k$，和自己进行CONCAT，做一个线性变换经过激活得到$h_{v}^k$，再归一化。</li>
<li>每层都按（1）（2）处理，最后得到顶点的embedding。</li>
</ol>
<p>即在每次迭代(或搜索深度)，顶点从它们的局部邻居聚合信息，并且随着这个过程的迭代，顶点会从越来越远的地方获得信息。</p>
<p><strong>采样</strong>：出于对计算效率的考虑，对每个顶点采样一定数量的邻居顶点作为待聚合信息的顶点。设采样数量为$m$，若顶点邻居数少于$m$,则采用有放回的抽样方法，直到采样出$m$个顶点。若顶点邻居数大于$m$，则采用无放回的抽样。作者给出了建议：在二层 $K=2$ 时，第一次采样数量 $S_1$ 和第二次采样数量 $S_2$ 满足公式：$S_1 · S_2 \leq 500$。<br><strong>聚合</strong>：之后会单独介绍。<br><img src="/images/GraphSAGE/2.png" width="80%"></p>
<p>举个例子：<br><img src="/images/GraphSAGE/2.1.png" width="80%"></p>
<p>在GraphSAGE之前的GCN模型中，都是采用的全图的训练方式，也就是说每一轮的迭代都要对全图的节点进行更新，当图的规模很大时，这种训练方式无疑是很耗时甚至无法更新的。mini-batch的训练时深度学习一个非常重要的特点，那么能否将mini-batch的思想用到GraphSAGE中呢，GraphSAGE提出了一个解决方案：<strong>只需记录当前mini-batch用到的所有节点（包括邻居节点），存储下来，之后再进行训练，这样每次训练就不需要保存整个图的信息</strong>。<br><img src="/images/GraphSAGE/3.png" width="80%"><br>注意，mini-batch采样是从中心点$K$层到最外层$1$，在聚合时是从最外层$1$到中心层$K$的。</p>
<p>举个例子：<br><img src="/images/GraphSAGE/3.1.png" width="80%"><br><img src="/images/GraphSAGE/3.2.png" width="80%"></p>
<h1 id="Aggregator-Architectures"><a href="#Aggregator-Architectures" class="headerlink" title="Aggregator Architectures"></a>Aggregator Architectures</h1><p>在图中顶点的邻居是无序的，所以希望构造出的聚合函数是<strong>对称的</strong>（即也就是对它输入的各种排列，函数的输出结果不变）和<strong>可导的</strong>。 聚合函数的对称性（symmetry property）确保了神经网络模型可以被训练且可以应用于任意顺序的顶点邻居特征集合上。因为要进行反向传播求梯度，需要聚合函数可导。</p>
<p><strong>Mean aggregator</strong><br>mean aggregator将目标顶点和邻居顶点的第$k-1$层向量拼接起来，然后对向量的每个维度进行求均值的操作，将得到的结果做一次非线性变换产生目标顶点的第$k$层表示向量。<br>文中用下面的式子替换算法1中的4行AGGREGATE和5行CONCAT得到GCN的inductive变形：<br><img src="/images/GraphSAGE/4.png" width="40%"></p>
<p><strong>LSTM aggregator</strong><br>LSTM本身是有顺序的，但是通过将输入节点随机排列（对顶点的邻居进行一次乱序操作），使得LSTM可以适用于无序的集合。</p>
<p><strong>Pooling aggregator</strong><br>pooling聚合器，它既是对称的，又是可训练的。Pooling aggregator先对目标顶点的邻居顶点的embedding向量进行一次非线性变换，之后进行一次pooling操作(max pooling or mean pooling)，将得到结果与目标顶点的表示向量拼接，最后再经过一次非线性变换得到目标顶点的第k层表示向量。<br><img src="/images/GraphSAGE/5.png" width="50%"></p>
<h1 id="Learning-the-parameters-of-GraphSAGE"><a href="#Learning-the-parameters-of-GraphSAGE" class="headerlink" title="Learning the parameters of GraphSAGE"></a>Learning the parameters of GraphSAGE</h1><p>GraphSAGE支持无监督训练和有监督训练两种方式。</p>
<p>GraphSAGE的<strong>无监督学习</strong>的理论基于假设：节点 $u$ 与其邻居 $v$ 相似，那么其表征的距离也近，而与没有交集的节点 $v_n$ 不相似，那么其表征的距离也远，损失函数为：<br><img src="/images/GraphSAGE/6.png" width="50%"><br>其中 $z_u$ 为节点 $u$ 通过GraphSAGE得到的embedding，$v$ 是节点 $u$ 通过随机游走得到的邻居，$v_n \sim P_n(v)$ 表示负采样，$Q$为样本数。embedding之间相似度通过向量点积计算得到。<br>损失函数的目的是拉近正样本距离，拉远负样本距离。正样本是通过随机游走得到的。</p>
<p><strong>有监督学习</strong>比较简单，使用满足预测目标的任务作为损失函数，例如交叉熵等。</p>
<h1 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h1><p>实验使用baselines：</p>
<ol>
<li>Random，随机分类器</li>
<li>Raw features，手工特征（非图特征）</li>
<li>deepwalk（图拓扑特征）</li>
<li>DeepWalk + features， deepwalk + 手工特征</li>
</ol>
<p><img src="/images/GraphSAGE/7.png" width="80%"></p>
<p><strong>实验结果1：分类准确率（micro-averaged F1 scores）</strong></p>
<ol>
<li>可以看到GraphSAGE的性能显著优于baseline方法。</li>
<li>三个数据集上的实验结果表明，一般是LSTM或pooling效果比较好，有监督都比无监督好。</li>
<li>无监督版本的GraphSAGE-pool对引文数据和Reddit数据的连接（concatenation）性能分别比DeepWalk embeddings和raw features的连接性能好13.8%和29.1%，而有监督版本的连接性能分别提高了19.7%和37.2%。</li>
<li>尽管LSTM是为有序数据而不是无序集设计的，但是基于LSTM的聚合器显示了强大的性能。</li>
<li>最后，可以看到无监督GraphSAGE的性能与完全监督的版本相比具有相当的竞争力，这表明文中的框架可以在不进行特定于任务的微调（task-specific fine-tuning）的情况下实现强大的性能</li>
</ol>
<p><strong>实验结果2：运行时间和参数敏感性</strong><br><strong>计算时间</strong>：GraphSAGE中LSTM训练速度最慢，但相比DeepWalk，GraphSAGE在预测时间减少100-500倍（因为对于未知节点，DeepWalk要重新进行随机游走以及通过SGD学习embedding）。<br><strong>邻居采样数量</strong>：图B中邻居采样数量递增，F1也增大，但计算时间也变大。 为了平衡F1和计算时间，将S1设为25<br><strong>聚合K跳内信息</strong>：在GraphSAGE， K=2 相比K=1 有10-15%的提升；但将K设置超过2，效果上只有0-5%的提升，但是计算时间却变大了10-100倍。</p>
<p><strong>实验结果3：不同聚合器之间的比较</strong></p>
<ol>
<li>LSTM和pool的效果较好。</li>
<li>为了更定量地了解这些趋势，实验中将设置六种不同的实验，即(3个数据集)×(非监督、监督)。</li>
<li>GraphSAGE-LSTM比GraphSAGE-pool慢得多(≈2×)，这可能使基于pooling的聚合器在总体上略占优势。</li>
<li>LSTM方法和pooling方法之间没有显著差异。</li>
<li>文中使用非参数Wilcoxon Signed-Rank检验来量化实验中不同聚合器之间的差异，在适用的情况下报告T-statistic和p-value。</li>
</ol>
<h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>GraphSAGE的一个强大之处是它在一个子集学到的模型也可以应用到其它模型上，原因是因为GraphSAGE的参数是共享的（每层计算时的参数W是共享的）。当有一个新的图或者有一个节点加入到已训练的图中时，我们只需要知道这个新图或者新节点的结构信息，通过共享的参数，便可以得到它们的特征向量。</p>
<p>它最大的贡献在于给图模型赋予了归纳学习的能力，从而大范围的扩大了GNN的落地场景。另外GraphSAGE的预测速度非常之快，因为它只需要选择若干跳的若干个邻居即可，而这两个可以选择的参数往往也比较小，往往取两跳邻居就能得到不错的学习效果。GraphSAGE的第三个优点是它非常好理解，不需要复杂的图理论基础，对于学习图神经网络也是非常好的入门读物。</p>
<h1 id="Code"><a href="#Code" class="headerlink" title="Code"></a>Code</h1><p>Pytorch（最基础的实现）：<span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL3dpbGxpYW1sZWlmL2dyYXBoc2FnZS1zaW1wbGUv">https://github.com/williamleif/graphsage-simple/<i class="fa fa-external-link-alt"></i></span><br>Tensorflow（全版本）：<span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL3dpbGxpYW1sZWlmL0dyYXBoU0FHRQ==">https://github.com/williamleif/GraphSAGE<i class="fa fa-external-link-alt"></i></span><br>Pytorch（全版本）：<span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL3R3amlhbmcvZ3JhcGhTQUdFLXB5dG9yY2g=">https://github.com/twjiang/graphSAGE-pytorch<i class="fa fa-external-link-alt"></i></span></p>
<p>以Pytorch基础版本为例，节点分类任务：<br><figure class="highlight python"><figcaption><span>model.py</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> init</span><br><span class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> f1_score</span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> defaultdict</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> graphsage.encoders <span class="keyword">import</span> Encoder</span><br><span class="line"><span class="keyword">from</span> graphsage.aggregators <span class="keyword">import</span> MeanAggregator</span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">Simple supervised GraphSAGE model as well as examples running the model</span></span><br><span class="line"><span class="string">on the Cora and Pubmed datasets.</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="comment"># 有监督学习下的GraphSage</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SupervisedGraphSage</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># num_classes类别数，这里是7</span></span><br><span class="line">    <span class="comment"># enc传入的是enc2，看作递归调用，从外层开始计算</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, num_classes, enc</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(SupervisedGraphSage, self).__init__()</span><br><span class="line">        self.enc = enc</span><br><span class="line">        self.xent = nn.CrossEntropyLoss() <span class="comment"># 交叉熵损失</span></span><br><span class="line"></span><br><span class="line">        self.weight = nn.Parameter(torch.FloatTensor(num_classes, enc.embed_dim)) <span class="comment"># 初始权重</span></span><br><span class="line">        init.xavier_uniform(self.weight)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, nodes</span>):</span></span><br><span class="line">        embeds = self.enc(nodes)  <span class="comment"># 通过enc拿到nodes的embeddings</span></span><br><span class="line">        scores = self.weight.mm(embeds) <span class="comment"># 通过W得到score</span></span><br><span class="line">        <span class="keyword">return</span> scores.t() <span class="comment"># score转置</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">loss</span>(<span class="params">self, nodes, labels</span>):</span></span><br><span class="line">        scores = self.forward(nodes)</span><br><span class="line">        <span class="keyword">return</span> self.xent(scores, labels.squeeze()) <span class="comment"># score和labels交叉熵</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_cora</span>():</span></span><br><span class="line">    num_nodes = <span class="number">2708</span></span><br><span class="line">    num_feats = <span class="number">1433</span></span><br><span class="line">    feat_data = np.zeros((num_nodes, num_feats))</span><br><span class="line">    labels = np.empty((num_nodes,<span class="number">1</span>), dtype=np.int64)</span><br><span class="line">    node_map = &#123;&#125;</span><br><span class="line">    label_map = &#123;&#125;</span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&quot;cora/cora.content&quot;</span>) <span class="keyword">as</span> fp:</span><br><span class="line">        <span class="keyword">for</span> i,line <span class="keyword">in</span> <span class="built_in">enumerate</span>(fp):</span><br><span class="line">            info = line.strip().split()</span><br><span class="line">            feat_data[i,:] = <span class="built_in">map</span>(<span class="built_in">float</span>, info[<span class="number">1</span>:<span class="number">-1</span>])</span><br><span class="line">            node_map[info[<span class="number">0</span>]] = i</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> info[<span class="number">-1</span>] <span class="keyword">in</span> label_map:</span><br><span class="line">                label_map[info[<span class="number">-1</span>]] = <span class="built_in">len</span>(label_map)</span><br><span class="line">            labels[i] = label_map[info[<span class="number">-1</span>]]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 设定字典中默认value为set类型，当key不存在时也不会报错</span></span><br><span class="line">    adj_lists = defaultdict(<span class="built_in">set</span>) </span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&quot;cora/cora.cites&quot;</span>) <span class="keyword">as</span> fp:</span><br><span class="line">        <span class="keyword">for</span> i,line <span class="keyword">in</span> <span class="built_in">enumerate</span>(fp):</span><br><span class="line">            info = line.strip().split()</span><br><span class="line">            paper1 = node_map[info[<span class="number">0</span>]] <span class="comment"># paper1的id</span></span><br><span class="line">            paper2 = node_map[info[<span class="number">1</span>]] <span class="comment"># paper2的id</span></span><br><span class="line">            adj_lists[paper1].add(paper2) <span class="comment"># paper1连接paper2</span></span><br><span class="line">            adj_lists[paper2].add(paper1) <span class="comment"># paper2连接paper1</span></span><br><span class="line">    <span class="comment"># feat_data：(2708,1433)</span></span><br><span class="line">    <span class="comment"># labels：(2708,1)</span></span><br><span class="line">    <span class="comment"># adj_lists：存储边的连接 defaultdict(&lt;class &#x27;set&#x27;&gt;,&#123;163:&#123;1536,2563...&#125;,...&#125;)</span></span><br><span class="line">    <span class="keyword">return</span> feat_data, labels, adj_lists</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">run_cora</span>():</span></span><br><span class="line">    np.random.seed(<span class="number">1</span>)</span><br><span class="line">    random.seed(<span class="number">1</span>)</span><br><span class="line">    num_nodes = <span class="number">2708</span></span><br><span class="line">    feat_data, labels, adj_lists = load_cora()</span><br><span class="line">    features = nn.Embedding(<span class="number">2708</span>, <span class="number">1433</span>)</span><br><span class="line">    <span class="comment"># 权重矩阵</span></span><br><span class="line">    features.weight = nn.Parameter(torch.FloatTensor(feat_data), requires_grad=<span class="literal">False</span>)</span><br><span class="line">   <span class="comment"># features.cuda()</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 使用MeanAggregator和Encoder</span></span><br><span class="line">    agg1 = MeanAggregator(features, cuda=<span class="literal">True</span>)</span><br><span class="line">    enc1 = Encoder(features, <span class="number">1433</span>, <span class="number">128</span>, adj_lists, agg1, gcn=<span class="literal">True</span>, cuda=<span class="literal">False</span>)</span><br><span class="line">    agg2 = MeanAggregator(<span class="keyword">lambda</span> nodes : enc1(nodes).t(), cuda=<span class="literal">False</span>)</span><br><span class="line">    enc2 = Encoder(<span class="keyword">lambda</span> nodes : enc1(nodes).t(), enc1.embed_dim, <span class="number">128</span>, adj_lists, agg2,</span><br><span class="line">            base_model=enc1, gcn=<span class="literal">True</span>, cuda=<span class="literal">False</span>)</span><br><span class="line">    enc1.num_samples = <span class="number">5</span> <span class="comment"># 1层搜索size</span></span><br><span class="line">    enc2.num_samples = <span class="number">5</span> <span class="comment"># 2层搜索size</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># num_classes类别数，这里是7</span></span><br><span class="line">    <span class="comment"># 传入的是enc2，看作递归调用</span></span><br><span class="line">    graphsage = SupervisedGraphSage(<span class="number">7</span>, enc2)</span><br><span class="line"><span class="comment">#    graphsage.cuda()</span></span><br><span class="line">    <span class="comment"># permutation随机排列序列，对2708个节点随机排列</span></span><br><span class="line">    rand_indices = np.random.permutation(num_nodes)</span><br><span class="line">    test = rand_indices[:<span class="number">1000</span>] <span class="comment"># 测试集</span></span><br><span class="line">    val = rand_indices[<span class="number">1000</span>:<span class="number">1500</span>] <span class="comment"># 验证集</span></span><br><span class="line">    train = <span class="built_in">list</span>(rand_indices[<span class="number">1500</span>:]) <span class="comment"># 训练集</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 初始化优化器，并传递相关参数</span></span><br><span class="line">    optimizer = torch.optim.SGD(<span class="built_in">filter</span>(<span class="keyword">lambda</span> p : p.requires_grad, graphsage.parameters()), lr=<span class="number">0.7</span>)</span><br><span class="line">    times = []</span><br><span class="line">    <span class="keyword">for</span> batch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">        batch_nodes = train[:<span class="number">256</span>] <span class="comment"># batch取前256个节点</span></span><br><span class="line">        random.shuffle(train) <span class="comment"># 随即打乱</span></span><br><span class="line">        start_time = time.time() <span class="comment"># 记录开始时间</span></span><br><span class="line">        optimizer.zero_grad() <span class="comment"># 初始化梯度为0</span></span><br><span class="line">        <span class="comment"># 前向传播计算损失</span></span><br><span class="line">        loss = graphsage.loss(batch_nodes, </span><br><span class="line">                Variable(torch.LongTensor(labels[np.array(batch_nodes)])))</span><br><span class="line">        loss.backward() <span class="comment"># 反向传播</span></span><br><span class="line">        optimizer.step() <span class="comment"># 更新梯度</span></span><br><span class="line">        end_time = time.time() <span class="comment"># 记录结束时间</span></span><br><span class="line">        times.append(end_time-start_time) <span class="comment"># 得到时间差</span></span><br><span class="line">        <span class="built_in">print</span> batch, loss.data[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">    val_output = graphsage.forward(val) </span><br><span class="line">    <span class="built_in">print</span> <span class="string">&quot;Validation F1:&quot;</span>, f1_score(labels[val], val_output.data.numpy().argmax(axis=<span class="number">1</span>), average=<span class="string">&quot;micro&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span> <span class="string">&quot;Average batch time:&quot;</span>, np.mean(times)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_pubmed</span>():</span></span><br><span class="line">    <span class="comment">#hardcoded for simplicity...</span></span><br><span class="line">    num_nodes = <span class="number">19717</span></span><br><span class="line">    num_feats = <span class="number">500</span></span><br><span class="line">    feat_data = np.zeros((num_nodes, num_feats))</span><br><span class="line">    labels = np.empty((num_nodes, <span class="number">1</span>), dtype=np.int64)</span><br><span class="line">    node_map = &#123;&#125;</span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&quot;pubmed-data/Pubmed-Diabetes.NODE.paper.tab&quot;</span>) <span class="keyword">as</span> fp:</span><br><span class="line">        fp.readline()</span><br><span class="line">        feat_map = &#123;entry.split(<span class="string">&quot;:&quot;</span>)[<span class="number">1</span>]:i<span class="number">-1</span> <span class="keyword">for</span> i,entry <span class="keyword">in</span> <span class="built_in">enumerate</span>(fp.readline().split(<span class="string">&quot;\t&quot;</span>))&#125;</span><br><span class="line">        <span class="keyword">for</span> i, line <span class="keyword">in</span> <span class="built_in">enumerate</span>(fp):</span><br><span class="line">            info = line.split(<span class="string">&quot;\t&quot;</span>)</span><br><span class="line">            node_map[info[<span class="number">0</span>]] = i</span><br><span class="line">            labels[i] = <span class="built_in">int</span>(info[<span class="number">1</span>].split(<span class="string">&quot;=&quot;</span>)[<span class="number">1</span>])<span class="number">-1</span></span><br><span class="line">            <span class="keyword">for</span> word_info <span class="keyword">in</span> info[<span class="number">2</span>:<span class="number">-1</span>]:</span><br><span class="line">                word_info = word_info.split(<span class="string">&quot;=&quot;</span>)</span><br><span class="line">                feat_data[i][feat_map[word_info[<span class="number">0</span>]]] = <span class="built_in">float</span>(word_info[<span class="number">1</span>])</span><br><span class="line">    adj_lists = defaultdict(<span class="built_in">set</span>)</span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&quot;pubmed-data/Pubmed-Diabetes.DIRECTED.cites.tab&quot;</span>) <span class="keyword">as</span> fp:</span><br><span class="line">        fp.readline()</span><br><span class="line">        fp.readline()</span><br><span class="line">        <span class="keyword">for</span> line <span class="keyword">in</span> fp:</span><br><span class="line">            info = line.strip().split(<span class="string">&quot;\t&quot;</span>)</span><br><span class="line">            paper1 = node_map[info[<span class="number">1</span>].split(<span class="string">&quot;:&quot;</span>)[<span class="number">1</span>]]</span><br><span class="line">            paper2 = node_map[info[<span class="number">-1</span>].split(<span class="string">&quot;:&quot;</span>)[<span class="number">1</span>]]</span><br><span class="line">            adj_lists[paper1].add(paper2)</span><br><span class="line">            adj_lists[paper2].add(paper1)</span><br><span class="line">    <span class="keyword">return</span> feat_data, labels, adj_lists</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">run_pubmed</span>():</span></span><br><span class="line">    np.random.seed(<span class="number">1</span>)</span><br><span class="line">    random.seed(<span class="number">1</span>)</span><br><span class="line">    num_nodes = <span class="number">19717</span></span><br><span class="line">    feat_data, labels, adj_lists = load_pubmed()</span><br><span class="line">    features = nn.Embedding(<span class="number">19717</span>, <span class="number">500</span>)</span><br><span class="line">    features.weight = nn.Parameter(torch.FloatTensor(feat_data), requires_grad=<span class="literal">False</span>)</span><br><span class="line">   <span class="comment"># features.cuda()</span></span><br><span class="line"></span><br><span class="line">    agg1 = MeanAggregator(features, cuda=<span class="literal">True</span>)</span><br><span class="line">    enc1 = Encoder(features, <span class="number">500</span>, <span class="number">128</span>, adj_lists, agg1, gcn=<span class="literal">True</span>, cuda=<span class="literal">False</span>)</span><br><span class="line">    agg2 = MeanAggregator(<span class="keyword">lambda</span> nodes : enc1(nodes).t(), cuda=<span class="literal">False</span>)</span><br><span class="line">    enc2 = Encoder(<span class="keyword">lambda</span> nodes : enc1(nodes).t(), enc1.embed_dim, <span class="number">128</span>, adj_lists, agg2,</span><br><span class="line">            base_model=enc1, gcn=<span class="literal">True</span>, cuda=<span class="literal">False</span>)</span><br><span class="line">    enc1.num_samples = <span class="number">10</span></span><br><span class="line">    enc2.num_samples = <span class="number">25</span></span><br><span class="line"></span><br><span class="line">    graphsage = SupervisedGraphSage(<span class="number">3</span>, enc2)</span><br><span class="line"><span class="comment">#    graphsage.cuda()</span></span><br><span class="line">    rand_indices = np.random.permutation(num_nodes)</span><br><span class="line">    test = rand_indices[:<span class="number">1000</span>]</span><br><span class="line">    val = rand_indices[<span class="number">1000</span>:<span class="number">1500</span>]</span><br><span class="line">    train = <span class="built_in">list</span>(rand_indices[<span class="number">1500</span>:])</span><br><span class="line"></span><br><span class="line">    optimizer = torch.optim.SGD(<span class="built_in">filter</span>(<span class="keyword">lambda</span> p : p.requires_grad, graphsage.parameters()), lr=<span class="number">0.7</span>)</span><br><span class="line">    times = []</span><br><span class="line">    <span class="keyword">for</span> batch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">200</span>):</span><br><span class="line">        batch_nodes = train[:<span class="number">1024</span>]</span><br><span class="line">        random.shuffle(train)</span><br><span class="line">        start_time = time.time()</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        loss = graphsage.loss(batch_nodes, </span><br><span class="line">                Variable(torch.LongTensor(labels[np.array(batch_nodes)])))</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line">        end_time = time.time()</span><br><span class="line">        times.append(end_time-start_time)</span><br><span class="line">        <span class="built_in">print</span> batch, loss.data[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">    val_output = graphsage.forward(val) </span><br><span class="line">    <span class="built_in">print</span> <span class="string">&quot;Validation F1:&quot;</span>, f1_score(labels[val], val_output.data.numpy().argmax(axis=<span class="number">1</span>), average=<span class="string">&quot;micro&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span> <span class="string">&quot;Average batch time:&quot;</span>, np.mean(times)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    run_cora()</span><br></pre></td></tr></table></figure></p>
<figure class="highlight python"><figcaption><span>encoders.py</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> init</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Encoder</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Encodes a node&#x27;s using &#x27;convolutional&#x27; GraphSage approach</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, features, feature_dim, </span></span></span><br><span class="line"><span class="function"><span class="params">            embed_dim, adj_lists, aggregator,</span></span></span><br><span class="line"><span class="function"><span class="params">            num_sample=<span class="number">10</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">            base_model=<span class="literal">None</span>, gcn=<span class="literal">False</span>, cuda=<span class="literal">False</span>, </span></span></span><br><span class="line"><span class="function"><span class="params">            feature_transform=<span class="literal">False</span></span>):</span> </span><br><span class="line">        <span class="built_in">super</span>(Encoder, self).__init__()</span><br><span class="line"></span><br><span class="line">        self.features = features <span class="comment"># (2708, 1433)</span></span><br><span class="line">        self.feat_dim = feature_dim <span class="comment"># 输入维度1433</span></span><br><span class="line">        self.adj_lists = adj_lists  <span class="comment"># 边的连接defaultdict(&lt;class &#x27;set&#x27;&gt;,&#123;163:&#123;1536,2563...&#125;,...&#125;)</span></span><br><span class="line">        self.aggregator = aggregator <span class="comment"># MeanAggregator</span></span><br><span class="line">        self.num_sample = num_sample <span class="comment"># 采样邻居节点size</span></span><br><span class="line">        <span class="keyword">if</span> base_model != <span class="literal">None</span>:</span><br><span class="line">            self.base_model = base_model</span><br><span class="line"></span><br><span class="line">        self.gcn = gcn</span><br><span class="line">        self.embed_dim = embed_dim <span class="comment"># 输出维度128</span></span><br><span class="line">        self.cuda = cuda</span><br><span class="line">        self.aggregator.cuda = cuda</span><br><span class="line">        <span class="comment"># 注意concat之后维度是2倍</span></span><br><span class="line">        self.weight = nn.Parameter(</span><br><span class="line">                torch.FloatTensor(embed_dim, self.feat_dim <span class="keyword">if</span> self.gcn <span class="keyword">else</span> <span class="number">2</span> * self.feat_dim))</span><br><span class="line">        init.xavier_uniform(self.weight)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, nodes</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Generates embeddings for a batch of nodes.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        nodes     -- list of nodes</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment"># 使用传递过来的aggregator计算（这里是mean），得到所有邻居节点的aggregator信息</span></span><br><span class="line">        neigh_feats = self.aggregator.forward(nodes, [self.adj_lists[<span class="built_in">int</span>(node)] <span class="keyword">for</span> node <span class="keyword">in</span> nodes], </span><br><span class="line">                self.num_sample)</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> self.gcn:</span><br><span class="line">            <span class="keyword">if</span> self.cuda:</span><br><span class="line">                self_feats = self.features(torch.LongTensor(nodes).cuda())</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                self_feats = self.features(torch.LongTensor(nodes))</span><br><span class="line">            <span class="comment"># CONCAT当前节点和邻居节点</span></span><br><span class="line">            combined = torch.cat([self_feats, neigh_feats], dim=<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            combined = neigh_feats</span><br><span class="line">        <span class="comment"># W*CONCAT</span></span><br><span class="line">        combined = F.relu(self.weight.mm(combined.t()))</span><br><span class="line">        <span class="keyword">return</span> combined</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><figcaption><span>aggregators.py</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">Set of modules for aggregating embeddings of neighbors.</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MeanAggregator</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Aggregates a node&#x27;s embeddings using mean of neighbors&#x27; embeddings</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, features, cuda=<span class="literal">False</span>, gcn=<span class="literal">False</span></span>):</span> </span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Initializes the aggregator for a specific graph.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        features -- function mapping LongTensor of node ids to FloatTensor of feature values.</span></span><br><span class="line"><span class="string">        cuda -- whether to use GPU</span></span><br><span class="line"><span class="string">        gcn --- whether to perform concatenation GraphSAGE-style, or add self-loops GCN-style</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">        <span class="built_in">super</span>(MeanAggregator, self).__init__()</span><br><span class="line"></span><br><span class="line">        self.features = features</span><br><span class="line">        self.cuda = cuda</span><br><span class="line">        self.gcn = gcn</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, nodes, to_neighs, num_sample=<span class="number">10</span></span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        nodes --- list of nodes in a batch</span></span><br><span class="line"><span class="string">        to_neighs --- list of sets, each set is the set of neighbors for node in batch</span></span><br><span class="line"><span class="string">        num_sample --- number of neighbors to sample. No sampling if None.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        nodes：batch节点列表</span></span><br><span class="line"><span class="string">        to_neighs：集合列表，每个集合是batch中一个节点的邻居集合</span></span><br><span class="line"><span class="string">        num_sample：邻居采样size</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment"># Local pointers to functions (speed hack)</span></span><br><span class="line">        _set = <span class="built_in">set</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> num_sample <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            _sample = random.sample</span><br><span class="line">            <span class="comment"># sample：截取列表的指定长度的随机数，但是不会改变列表本身的排序</span></span><br><span class="line">            <span class="comment"># 从to_neigh中采样num_sample个（当邻居节点数量to_neigh大于num_sample）</span></span><br><span class="line">            samp_neighs = [_set(_sample(to_neigh, </span><br><span class="line">                            num_sample,</span><br><span class="line">                            )) <span class="keyword">if</span> <span class="built_in">len</span>(to_neigh) &gt;= num_sample <span class="keyword">else</span> to_neigh <span class="keyword">for</span> to_neigh <span class="keyword">in</span> to_neighs]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            samp_neighs = to_neighs <span class="comment"># num_sample为None，每个节点都对其所有邻居节点全部计算</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.gcn:</span><br><span class="line">            samp_neighs = [samp_neigh + <span class="built_in">set</span>([nodes[i]]) <span class="keyword">for</span> i, samp_neigh <span class="keyword">in</span> <span class="built_in">enumerate</span>(samp_neighs)]</span><br><span class="line">        <span class="comment"># set.union():返回多个集合的并集，即包含了所有集合的元素，重复的元素只会出现一次</span></span><br><span class="line">        unique_nodes_list = <span class="built_in">list</span>(<span class="built_in">set</span>.union(*samp_neighs))</span><br><span class="line">        <span class="comment"># &#123;节点:编号&#125;</span></span><br><span class="line">        unique_nodes = &#123;n:i <span class="keyword">for</span> i,n <span class="keyword">in</span> <span class="built_in">enumerate</span>(unique_nodes_list)&#125;</span><br><span class="line">        <span class="comment"># (采样节点（包括了batch节点）,独一无二节点)</span></span><br><span class="line">        mask = Variable(torch.zeros(<span class="built_in">len</span>(samp_neighs), <span class="built_in">len</span>(unique_nodes)))</span><br><span class="line">        <span class="comment"># 得到采样节点对应的编号</span></span><br><span class="line">        column_indices = [unique_nodes[n] <span class="keyword">for</span> samp_neigh <span class="keyword">in</span> samp_neighs <span class="keyword">for</span> n <span class="keyword">in</span> samp_neigh]  </span><br><span class="line">        <span class="comment"># 每个节点对应采样数量个i作为行坐标 </span></span><br><span class="line">        row_indices = [i <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(samp_neighs)) <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(samp_neighs[i]))]</span><br><span class="line">        <span class="comment"># 得到邻接矩阵</span></span><br><span class="line">        mask[row_indices, column_indices] = <span class="number">1</span></span><br><span class="line">        <span class="keyword">if</span> self.cuda:</span><br><span class="line">            mask = mask.cuda()</span><br><span class="line">        <span class="comment"># 统计每个节点的邻居数量</span></span><br><span class="line">        num_neigh = mask.<span class="built_in">sum</span>(<span class="number">1</span>, keepdim=<span class="literal">True</span>) </span><br><span class="line">        <span class="comment"># 每个邻居节点占比</span></span><br><span class="line">        mask = mask.div(num_neigh)</span><br><span class="line">        <span class="comment"># 得到每个邻居节点的特征</span></span><br><span class="line">        <span class="keyword">if</span> self.cuda:</span><br><span class="line">            embed_matrix = self.features(torch.LongTensor(unique_nodes_list).cuda())</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            embed_matrix = self.features(torch.LongTensor(unique_nodes_list))</span><br><span class="line">        <span class="comment"># mean aggregators 计算每个邻居的节点特征mean</span></span><br><span class="line">        to_feats = mask.mm(embed_matrix)</span><br><span class="line">        <span class="keyword">return</span> to_feats</span><br></pre></td></tr></table></figure>
<h1 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h1><p><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzE3MDYuMDIyMTYucGRm">Inductive Representation Learning on Large Graphs<i class="fa fa-external-link-alt"></i></span><br><span class="exturl" data-url="aHR0cHM6Ly96aHVhbmxhbi56aGlodS5jb20vcC8zMzYxOTU4NjI=">GraphSAGE详解<i class="fa fa-external-link-alt"></i></span><br><span class="exturl" data-url="aHR0cHM6Ly96aHVhbmxhbi56aGlodS5jb20vcC83OTYzNzc4Nw==">【Graph Neural Network】GraphSAGE: 算法原理，实现和应用<i class="fa fa-external-link-alt"></i></span><br><span class="exturl" data-url="aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3l5bDQyNDUyNS9hcnRpY2xlL2RldGFpbHMvMTAwNTMyODQ5">[论文笔记]：GraphSAGE：Inductive Representation Learning on Large Graphs 论文详解 NIPS 2017<i class="fa fa-external-link-alt"></i></span></p>

      
    </div>

    
    
    
      


    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://soundmemories.github.io/2021/02/10/Graph/02.GAT/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="https://i.loli.net/2020/11/04/6JhNuwtBe4adylS.png">
      <meta itemprop="name" content="SoundMemories">
      <meta itemprop="description" content="今日事，今日毕">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="SoundMemories">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/02/10/Graph/02.GAT/" class="post-title-link" itemprop="url">GAT</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2021-02-10 00:00:00" itemprop="dateCreated datePublished" datetime="2021-02-10T00:00:00+08:00">2021-02-10</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Graph/" itemprop="url" rel="index"><span itemprop="name">Graph</span></a>
        </span>
    </span>

  
      </div>
      <div class="post-meta">
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>20k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>18 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="GCN缺点"><a href="#GCN缺点" class="headerlink" title="GCN缺点"></a>GCN缺点</h1><p><strong>GCN假设图是无向的</strong>，因为利用了对称的拉普拉斯矩阵 (只有邻接矩阵 A 是对称的，拉普拉斯矩阵才可以正交分解)，不能直接用于有向图。</p>
<p><strong>GCN不能处理动态图</strong>，GCN在训练时依赖于具体的图结构，测试的时候也要在相同的图上进行。因此只能处理transductive任务，不能处理inductive任务。transductive指训练和测试的时候基于相同的图结构，例如在一个社交网络上，知道一部分人的类别，预测另一部分人的类别。inductive指训练和测试使用不同的图结构，例如在一个社交网络上训练，在另一个社交网络上预测。</p>
<p><strong>GCN不能为每个邻居分配不同的权重</strong>，GCN 在卷积时对所有邻居节点均一视同仁，不能根据节点重要性分配不同的权重。</p>
<p>2018年图注意力网络GAT被提出，用于解决GCN的上述问题：GAT 采用了 Attention 机制，对邻近节点特征加权求和。邻近节点特征的权重完全取决于节点特征，而不依赖具体的网络结构，可以用于 inductive 任务。</p>
<h1 id="GAT"><a href="#GAT" class="headerlink" title="GAT"></a>GAT</h1><p>本文作者提出GATs方法，利用一个隐藏的self-attention层，来处理一些图卷积中的问题。不需要复杂的矩阵运算或者对图结构的事先了解，通过叠加self-attention层，在卷积过程中将不同的重要性分配给邻域内的不同节点，同时处理不同大小的邻域。作者分别设计了inductive setting和transductive setting的任务实验，GATs模型在基线数据集Cora、Citeseer、Pubmed citation和PPI数据集上取得了state-of-the-art的结果。</p>
<p>和所有的attention mechanism一样，GAT的计算也分为两步：<strong>计算注意力系数</strong>（attention coefficient）和<strong>加权求和</strong>（aggregate）。</p>
<h2 id="计算注意力系数"><a href="#计算注意力系数" class="headerlink" title="计算注意力系数"></a>计算注意力系数</h2><p>这里假设层的输入（Garaph）包含 $N$ 个节点，每个节点的特征向量为$h_i$，维度是 $F$，输出包含 $N$ 个节点，每个节点的特征向量为$h_i^{\prime}$，维度是 $F^{\prime}$。为了$h_i^{\prime}$获得足够的表达能力，将输入特征转换为更高层次的特征，至少需要一个可学习的线性变换，为此，使用共享权重矩阵$W$作用到每个节点。然后我们在节点上使用<strong>self-attention</strong>来计算一个attention系数，即通过 $a$ 得到 $e_{ij}$，节点 $j$ 是节点 $i$ 的邻居。<br><img src="/images/GAT/1.png" width="80%"></p>
<p>论文中说，self-attention是一种<strong>Global graph attention</strong>，会将注意力分配到图中所有的节点上，这种做法显然会丢失结构信息。通过self-attention注意力机制可以计算任意两个样本的关系，使一个样本用其他所有样本表示，但是第一，基于空间相似假设，一个样本与一定范围内的样本关系较为密切，第二，样本较多的时候，计算量非常大。为了解决这一问题，作者使用了一种 <strong>masked attention</strong> 的方法，<strong>对于一个样本来说只利用邻域内的样本计算注意力系数和新的表示，即仅将注意力分配到节点的一阶邻居节点集上</strong>。</p>
<p>针对每个节点执行self-attention机制 </p>
<script type="math/tex; mode=display">a ： R^{F^{\prime}}×R^{F^{\prime}} \to R</script><p>计算注意力互相关系数attention coefficients：</p>
<script type="math/tex; mode=display">e_{ij}=a(Wh_i,Wh_j)</script><p>其中：</p>
<ul>
<li>注意力系数 $e_{ij}$ 表示的节点j对于节点i的重要性。</li>
<li>向量 $h$ 就是特征向量。</li>
<li>$a$ 是一个 $R^{F^{\prime}}×R^{F^{\prime}} \to R$ 的映射。</li>
<li>$W \in R^{F^{\prime}×F}$，使 $W$ 将每个特征转换为可用的表达性更强的特征。</li>
</ul>
<p>为了使注意力系数更容易计算和比较，引入softmax对所有的 $i$ 的相邻节点 $j$ 进行正则化：<br><img src="/images/GAT/2.png" width="40%"></p>
<p>其中：</p>
<ul>
<li>$N_i$ 表示节点 $i$ 的邻居节点集合。</li>
<li>这个系数 $\alpha$ 就是每次卷积时，用来进行加权求和的系数。</li>
</ul>
<p>实验中，注意力机制 $a$ 是一个单层的前馈神经网络，通过权值向量来确定 $a\in R^{2F^{\prime}}$，并且加入了LeakyRelu的非线性激活，这里小于零斜率为0.2。（Relu:小于0就是0，大于0斜率为1；LRelu:小于0斜率固定一个值，大于0斜率为1；PRelu:小于0斜率可变，大于0斜率为1； 还有CRelu，Elu，SELU）。<br><img src="/images/GAT/3.png" width="60%"></p>
<p>本文中使用的是：<br><img src="/images/GAT/4.png" width="50%"><br>其中，$||$ 表示concat操作（串联）。下面左图就是表示 $Wh_i$和 $Wh_j$经过串联以后，再和权值向量 $a\in R^{2F^{\prime}}$ 相乘后，最后进行一个softmax归一化处理后的示意图。<br><img src="/images/GAT/5.png" width="80%"></p>
<h2 id="加权求和"><a href="#加权求和" class="headerlink" title="加权求和"></a>加权求和</h2><p>得到归一化的注意力系数后，使用归一化的值计算对应特征的线性组合，作为每个顶点最后的输出特征（最后可以加一个非线性层$\sigma$）。<br><img src="/images/GAT/6.png" width="25%"></p>
<p>$h_i^{\prime}$ 就是GAT输出的节点 $i$ 融合了邻域信息的新特征。</p>
<h2 id="多头注意力机制"><a href="#多头注意力机制" class="headerlink" title="多头注意力机制"></a>多头注意力机制</h2><p>为了使 self-attention 的学习过程更稳定，发现使用 多头注意力（multi-head attention）来扩展注意力机制是很有效的。<br>使用 $K$ 个独立的 attention 机制执行加权求和这样的变换，然后他们的特征连接（concatednated）在一起，就可以得到如下的输出：<br><img src="/images/GAT/7.png" width="30%"></p>
<p>其中，最后的返回输出 $h^{\prime}$，每个顶点都会有 $KF^{\prime}$ 维的特征（不是$F^{\prime}$）。</p>
<p>下面右图表示 $K=3$ 时 多头注意力机制示意图。例如此图，节点1在邻域中具有多端注意机制，不同的箭头样式表示独立的注意力计算，通过连接或平均每个head获得$h_1$。<br><img src="/images/GAT/5.png" width="80%"></p>
<p>对于最后一个卷积层，如果还是使用多头注意力机制，那么就不用采取连接的方式合并不同的attention机制的结果了，而是采用求平均的方式进行处理，即：<br><img src="/images/GAT/8.png" width="30%"><br>这里指的是直接连接softmax的方式，如果接一个全连接层，是无所谓的。</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>GAT 不依赖于完整的图结构，只依赖于边，因此可以用于 inductive 任务。<br>GAT 可用于有向图。<br>采用 Attention 机制，可以为不同的邻居节点分配不同的权重。<br>尽管 multi-head attention 使得参数数量和空间复杂度变成 $K$ 倍，但是每个 head 可以并行计算。</p>
<p>注意力机制以共享的方式应用于图的所有边，因此它不需要预先得到整个图结构或者所有顶点。这带来几个影响：<br>（1）图可以是有向图，也可以是无向图。如果边 $j \to i$  不存在，则我们不需要计算系数 $a_{i,j}$。<br>（2）GAT 可以直接应用到归纳学习 inductinve learning ：模型可以预测那些在训练集中从未出现的图。</p>
<p>GraphSage 归纳学习模型对每个顶点采样固定大小的邻域，从而保持计算过程的一致性。这使得模型无法在测试期间访问整个邻域。注意：由于训练期间训练多个 epoch，则可能访问到顶点的整个邻域。也可以使用LSTM 技术来聚合邻域顶点，但是这需要假设邻域中存在一个一致的顶点顺序。GAT 没有这两个问题：GAT 在作用在完整的邻域上，并且不关心邻域内顶点的顺序。</p>
<h2 id="Code"><a href="#Code" class="headerlink" title="Code"></a>Code</h2><p>Pytorch：<span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL0RpZWdvOTk5L3B5R0FU">https://github.com/Diego999/pyGAT<i class="fa fa-external-link-alt"></i></span><br>Tensorflow：<span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL1BldGFyVi0vR0FU">https://github.com/PetarV-/GAT<i class="fa fa-external-link-alt"></i></span><br>博客：<span class="exturl" data-url="aHR0cHM6Ly9wZXRhci12LmNvbS9HQVQv">https://petar-v.com/GAT/<i class="fa fa-external-link-alt"></i></span></p>
<p>以Pytorch版本为例：<br><figure class="highlight python"><figcaption><span>train.py</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> division</span><br><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> print_function</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> glob</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> utils <span class="keyword">import</span> load_data, accuracy</span><br><span class="line"><span class="keyword">from</span> models <span class="keyword">import</span> GAT, SpGAT</span><br><span class="line"></span><br><span class="line"><span class="comment"># Training settings</span></span><br><span class="line">parser = argparse.ArgumentParser()</span><br><span class="line">parser.add_argument(<span class="string">&#x27;--no-cuda&#x27;</span>, action=<span class="string">&#x27;store_true&#x27;</span>, default=<span class="literal">False</span>, <span class="built_in">help</span>=<span class="string">&#x27;Disables CUDA training.&#x27;</span>)</span><br><span class="line">parser.add_argument(<span class="string">&#x27;--fastmode&#x27;</span>, action=<span class="string">&#x27;store_true&#x27;</span>, default=<span class="literal">False</span>, <span class="built_in">help</span>=<span class="string">&#x27;Validate during training pass.&#x27;</span>)</span><br><span class="line">parser.add_argument(<span class="string">&#x27;--sparse&#x27;</span>, action=<span class="string">&#x27;store_true&#x27;</span>, default=<span class="literal">False</span>, <span class="built_in">help</span>=<span class="string">&#x27;GAT with sparse version or not.&#x27;</span>)</span><br><span class="line">parser.add_argument(<span class="string">&#x27;--seed&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">int</span>, default=<span class="number">72</span>, <span class="built_in">help</span>=<span class="string">&#x27;Random seed.&#x27;</span>)</span><br><span class="line">parser.add_argument(<span class="string">&#x27;--epochs&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">int</span>, default=<span class="number">10000</span>, <span class="built_in">help</span>=<span class="string">&#x27;Number of epochs to train.&#x27;</span>)</span><br><span class="line">parser.add_argument(<span class="string">&#x27;--lr&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">float</span>, default=<span class="number">0.005</span>, <span class="built_in">help</span>=<span class="string">&#x27;Initial learning rate.&#x27;</span>)</span><br><span class="line">parser.add_argument(<span class="string">&#x27;--weight_decay&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">float</span>, default=<span class="number">5e-4</span>, <span class="built_in">help</span>=<span class="string">&#x27;Weight decay (L2 loss on parameters).&#x27;</span>)</span><br><span class="line">parser.add_argument(<span class="string">&#x27;--hidden&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">int</span>, default=<span class="number">8</span>, <span class="built_in">help</span>=<span class="string">&#x27;Number of hidden units.&#x27;</span>)</span><br><span class="line">parser.add_argument(<span class="string">&#x27;--nb_heads&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">int</span>, default=<span class="number">8</span>, <span class="built_in">help</span>=<span class="string">&#x27;Number of head attentions.&#x27;</span>)</span><br><span class="line">parser.add_argument(<span class="string">&#x27;--dropout&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">float</span>, default=<span class="number">0.6</span>, <span class="built_in">help</span>=<span class="string">&#x27;Dropout rate (1 - keep probability).&#x27;</span>)</span><br><span class="line">parser.add_argument(<span class="string">&#x27;--alpha&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">float</span>, default=<span class="number">0.2</span>, <span class="built_in">help</span>=<span class="string">&#x27;Alpha for the leaky_relu.&#x27;</span>)</span><br><span class="line">parser.add_argument(<span class="string">&#x27;--patience&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">int</span>, default=<span class="number">100</span>, <span class="built_in">help</span>=<span class="string">&#x27;Patience&#x27;</span>)</span><br><span class="line"></span><br><span class="line">args = parser.parse_args()</span><br><span class="line">args.cuda = <span class="keyword">not</span> args.no_cuda <span class="keyword">and</span> torch.cuda.is_available()</span><br><span class="line"></span><br><span class="line">random.seed(args.seed)</span><br><span class="line">np.random.seed(args.seed)</span><br><span class="line">torch.manual_seed(args.seed)</span><br><span class="line"><span class="keyword">if</span> args.cuda:</span><br><span class="line">    torch.cuda.manual_seed(args.seed)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Load data</span></span><br><span class="line">adj, features, labels, idx_train, idx_val, idx_test = load_data()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Model and optimizer</span></span><br><span class="line"><span class="keyword">if</span> args.sparse:</span><br><span class="line">    model = SpGAT(nfeat=features.shape[<span class="number">1</span>], </span><br><span class="line">                nhid=args.hidden, </span><br><span class="line">                nclass=<span class="built_in">int</span>(labels.<span class="built_in">max</span>()) + <span class="number">1</span>, </span><br><span class="line">                dropout=args.dropout, </span><br><span class="line">                nheads=args.nb_heads, </span><br><span class="line">                alpha=args.alpha)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    model = GAT(nfeat=features.shape[<span class="number">1</span>], </span><br><span class="line">                nhid=args.hidden, </span><br><span class="line">                nclass=<span class="built_in">int</span>(labels.<span class="built_in">max</span>()) + <span class="number">1</span>, </span><br><span class="line">                dropout=args.dropout, </span><br><span class="line">                nheads=args.nb_heads, </span><br><span class="line">                alpha=args.alpha)</span><br><span class="line">optimizer = optim.Adam(model.parameters(), </span><br><span class="line">                       lr=args.lr, </span><br><span class="line">                       weight_decay=args.weight_decay)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> args.cuda:</span><br><span class="line">    model.cuda()</span><br><span class="line">    features = features.cuda()</span><br><span class="line">    adj = adj.cuda()</span><br><span class="line">    labels = labels.cuda()</span><br><span class="line">    idx_train = idx_train.cuda()</span><br><span class="line">    idx_val = idx_val.cuda()</span><br><span class="line">    idx_test = idx_test.cuda()</span><br><span class="line"></span><br><span class="line">features, adj, labels = Variable(features), Variable(adj), Variable(labels)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span>(<span class="params">epoch</span>):</span></span><br><span class="line">    t = time.time()</span><br><span class="line">    model.train()</span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    output = model(features, adj) <span class="comment"># GAT模块 features：(2708,1433) output：(2708,7)</span></span><br><span class="line">    loss_train = F.nll_loss(output[idx_train], labels[idx_train])</span><br><span class="line">    acc_train = accuracy(output[idx_train], labels[idx_train])</span><br><span class="line">    loss_train.backward()</span><br><span class="line">    optimizer.step()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> args.fastmode:</span><br><span class="line">        <span class="comment"># Evaluate validation set performance separately,</span></span><br><span class="line">        <span class="comment"># deactivates dropout during validation run.</span></span><br><span class="line">        model.<span class="built_in">eval</span>()</span><br><span class="line">        output = model(features, adj)</span><br><span class="line"></span><br><span class="line">    loss_val = F.nll_loss(output[idx_val], labels[idx_val])</span><br><span class="line">    acc_val = accuracy(output[idx_val], labels[idx_val])</span><br><span class="line">    print(<span class="string">&#x27;Epoch: &#123;:04d&#125;&#x27;</span>.<span class="built_in">format</span>(epoch+<span class="number">1</span>),</span><br><span class="line">          <span class="string">&#x27;loss_train: &#123;:.4f&#125;&#x27;</span>.<span class="built_in">format</span>(loss_train.data.item()),</span><br><span class="line">          <span class="string">&#x27;acc_train: &#123;:.4f&#125;&#x27;</span>.<span class="built_in">format</span>(acc_train.data.item()),</span><br><span class="line">          <span class="string">&#x27;loss_val: &#123;:.4f&#125;&#x27;</span>.<span class="built_in">format</span>(loss_val.data.item()),</span><br><span class="line">          <span class="string">&#x27;acc_val: &#123;:.4f&#125;&#x27;</span>.<span class="built_in">format</span>(acc_val.data.item()),</span><br><span class="line">          <span class="string">&#x27;time: &#123;:.4f&#125;s&#x27;</span>.<span class="built_in">format</span>(time.time() - t))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> loss_val.data.item()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_test</span>():</span></span><br><span class="line">    model.<span class="built_in">eval</span>()</span><br><span class="line">    output = model(features, adj)</span><br><span class="line">    loss_test = F.nll_loss(output[idx_test], labels[idx_test])</span><br><span class="line">    acc_test = accuracy(output[idx_test], labels[idx_test])</span><br><span class="line">    print(<span class="string">&quot;Test set results:&quot;</span>,</span><br><span class="line">          <span class="string">&quot;loss= &#123;:.4f&#125;&quot;</span>.<span class="built_in">format</span>(loss_test.data[<span class="number">0</span>]),</span><br><span class="line">          <span class="string">&quot;accuracy= &#123;:.4f&#125;&quot;</span>.<span class="built_in">format</span>(acc_test.data[<span class="number">0</span>]))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Train model</span></span><br><span class="line">t_total = time.time()</span><br><span class="line">loss_values = []</span><br><span class="line">bad_counter = <span class="number">0</span></span><br><span class="line">best = args.epochs + <span class="number">1</span></span><br><span class="line">best_epoch = <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(args.epochs):</span><br><span class="line">    loss_values.append(train(epoch))</span><br><span class="line"></span><br><span class="line">    torch.save(model.state_dict(), <span class="string">&#x27;&#123;&#125;.pkl&#x27;</span>.<span class="built_in">format</span>(epoch))</span><br><span class="line">    <span class="keyword">if</span> loss_values[<span class="number">-1</span>] &lt; best:</span><br><span class="line">        best = loss_values[<span class="number">-1</span>]</span><br><span class="line">        best_epoch = epoch</span><br><span class="line">        bad_counter = <span class="number">0</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        bad_counter += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> bad_counter == args.patience:</span><br><span class="line">        <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">    files = glob.glob(<span class="string">&#x27;*.pkl&#x27;</span>)</span><br><span class="line">    <span class="keyword">for</span> file <span class="keyword">in</span> files:</span><br><span class="line">        epoch_nb = <span class="built_in">int</span>(file.split(<span class="string">&#x27;.&#x27;</span>)[<span class="number">0</span>])</span><br><span class="line">        <span class="keyword">if</span> epoch_nb &lt; best_epoch:</span><br><span class="line">            os.remove(file)</span><br><span class="line"></span><br><span class="line">files = glob.glob(<span class="string">&#x27;*.pkl&#x27;</span>)</span><br><span class="line"><span class="keyword">for</span> file <span class="keyword">in</span> files:</span><br><span class="line">    epoch_nb = <span class="built_in">int</span>(file.split(<span class="string">&#x27;.&#x27;</span>)[<span class="number">0</span>])</span><br><span class="line">    <span class="keyword">if</span> epoch_nb &gt; best_epoch:</span><br><span class="line">        os.remove(file)</span><br><span class="line"></span><br><span class="line">print(<span class="string">&quot;Optimization Finished!&quot;</span>)</span><br><span class="line">print(<span class="string">&quot;Total time elapsed: &#123;:.4f&#125;s&quot;</span>.<span class="built_in">format</span>(time.time() - t_total))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Restore best model</span></span><br><span class="line">print(<span class="string">&#x27;Loading &#123;&#125;th epoch&#x27;</span>.<span class="built_in">format</span>(best_epoch))</span><br><span class="line">model.load_state_dict(torch.load(<span class="string">&#x27;&#123;&#125;.pkl&#x27;</span>.<span class="built_in">format</span>(best_epoch)))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Testing</span></span><br><span class="line">compute_test()</span><br></pre></td></tr></table></figure></p>
<figure class="highlight python"><figcaption><span>model.py</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">from</span> layers <span class="keyword">import</span> GraphAttentionLayer, SpGraphAttentionLayer</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">GAT</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, nfeat, nhid, nclass, dropout, alpha, nheads</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;Dense version of GAT.&quot;&quot;&quot;</span></span><br><span class="line">        <span class="built_in">super</span>(GAT, self).__init__()</span><br><span class="line">        self.dropout = dropout</span><br><span class="line"></span><br><span class="line">        <span class="comment"># nfeat：输入特征1433，nhid：隐层特征8，nheads：多头数量8</span></span><br><span class="line">        <span class="comment"># 因为用了多头，结果包含nheads个layer</span></span><br><span class="line">        self.attentions = [GraphAttentionLayer(nfeat, nhid, dropout=dropout, alpha=alpha, concat=<span class="literal">True</span>) <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(nheads)]</span><br><span class="line">        <span class="keyword">for</span> i, attention <span class="keyword">in</span> <span class="built_in">enumerate</span>(self.attentions):</span><br><span class="line">            self.add_module(<span class="string">&#x27;attention_&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(i), attention)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 第二层（最后一层）的attention layer</span></span><br><span class="line">        self.out_att = GraphAttentionLayer(nhid * nheads, nclass, dropout=dropout, alpha=alpha, concat=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x, adj</span>):</span></span><br><span class="line">        x = F.dropout(x, self.dropout, training=self.training)</span><br><span class="line">        <span class="comment"># 将每层attention拼接，8次，x：(2708,64)</span></span><br><span class="line">        x = torch.cat([att(x, adj) <span class="keyword">for</span> att <span class="keyword">in</span> self.attentions], dim=<span class="number">1</span>) </span><br><span class="line">        x = F.dropout(x, self.dropout, training=self.training)</span><br><span class="line">        x = F.elu(self.out_att(x, adj)) <span class="comment"># 第二层的attention layer </span></span><br><span class="line">        <span class="keyword">return</span> F.log_softmax(x, dim=<span class="number">1</span>) <span class="comment"># 输出(2708,7)</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SpGAT</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, nfeat, nhid, nclass, dropout, alpha, nheads</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;Sparse version of GAT.&quot;&quot;&quot;</span></span><br><span class="line">        <span class="built_in">super</span>(SpGAT, self).__init__()</span><br><span class="line">        self.dropout = dropout</span><br><span class="line"></span><br><span class="line">        self.attentions = [SpGraphAttentionLayer(nfeat, </span><br><span class="line">                                                 nhid, </span><br><span class="line">                                                 dropout=dropout, </span><br><span class="line">                                                 alpha=alpha, </span><br><span class="line">                                                 concat=<span class="literal">True</span>) <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(nheads)]</span><br><span class="line">        <span class="keyword">for</span> i, attention <span class="keyword">in</span> <span class="built_in">enumerate</span>(self.attentions):</span><br><span class="line">            self.add_module(<span class="string">&#x27;attention_&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(i), attention)</span><br><span class="line"></span><br><span class="line">        self.out_att = SpGraphAttentionLayer(nhid * nheads, </span><br><span class="line">                                             nclass, </span><br><span class="line">                                             dropout=dropout, </span><br><span class="line">                                             alpha=alpha, </span><br><span class="line">                                             concat=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x, adj</span>):</span></span><br><span class="line">        x = F.dropout(x, self.dropout, training=self.training)</span><br><span class="line">        x = torch.cat([att(x, adj) <span class="keyword">for</span> att <span class="keyword">in</span> self.attentions], dim=<span class="number">1</span>)</span><br><span class="line">        x = F.dropout(x, self.dropout, training=self.training)</span><br><span class="line">        x = F.elu(self.out_att(x, adj))</span><br><span class="line">        <span class="keyword">return</span> F.log_softmax(x, dim=<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><figcaption><span>layers.py</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">GraphAttentionLayer</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Simple GAT layer, similar to https://arxiv.org/abs/1710.10903</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, in_features, out_features, dropout, alpha, concat=<span class="literal">True</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>(GraphAttentionLayer, self).__init__()</span><br><span class="line">        self.dropout = dropout</span><br><span class="line">        self.in_features = in_features <span class="comment"># 1433</span></span><br><span class="line">        self.out_features = out_features <span class="comment"># 8</span></span><br><span class="line">        self.alpha = alpha</span><br><span class="line">        self.concat = concat</span><br><span class="line"></span><br><span class="line">        self.W = nn.Parameter(torch.empty(size=(in_features, out_features))) <span class="comment"># (1433,8)</span></span><br><span class="line">        nn.init.xavier_uniform_(self.W.data, gain=<span class="number">1.414</span>)</span><br><span class="line">        self.a = nn.Parameter(torch.empty(size=(<span class="number">2</span>*out_features, <span class="number">1</span>))) <span class="comment"># (16,1) 公式中的a</span></span><br><span class="line">        nn.init.xavier_uniform_(self.a.data, gain=<span class="number">1.414</span>)</span><br><span class="line"></span><br><span class="line">        self.leakyrelu = nn.LeakyReLU(self.alpha)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, h, adj</span>):</span></span><br><span class="line">        Wh = torch.mm(h, self.W) <span class="comment"># h.shape: (N, in_features), Wh.shape: (N, out_features)</span></span><br><span class="line">        <span class="comment"># a_input：(2708,2708,16) 每一个节点和所有节点特征拼接。</span></span><br><span class="line">        a_input = self._prepare_attentional_mechanism_input(Wh) </span><br><span class="line">        e = self.leakyrelu(torch.matmul(a_input, self.a).squeeze(<span class="number">2</span>))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 之前计算的是一个节点和所有节点的attention，其实需要的是连接的节点的attention系数</span></span><br><span class="line">        zero_vec = <span class="number">-9e15</span>*torch.ones_like(e)</span><br><span class="line">        <span class="comment"># adj &gt; 0 表示相邻的</span></span><br><span class="line">        attention = torch.where(adj &gt; <span class="number">0</span>, e, zero_vec) <span class="comment"># 将邻接矩阵中小于0的变成负无穷</span></span><br><span class="line">        attention = F.softmax(attention, dim=<span class="number">1</span>) <span class="comment"># 按行求softmax，sum(axis=1)==1</span></span><br><span class="line">        attention = F.dropout(attention, self.dropout, training=self.training)</span><br><span class="line">        h_prime = torch.matmul(attention, Wh) <span class="comment"># 聚合邻居函数 h_prime：(2708,8)</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.concat:</span><br><span class="line">            <span class="keyword">return</span> F.elu(h_prime) <span class="comment"># elu-激活函数</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> h_prime</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_prepare_attentional_mechanism_input</span>(<span class="params">self, Wh</span>):</span></span><br><span class="line">        N = Wh.size()[<span class="number">0</span>] <span class="comment"># number of nodes</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Below, two matrices are created that contain embeddings in their rows in different orders.</span></span><br><span class="line">        <span class="comment"># (e stands for embedding)</span></span><br><span class="line">        <span class="comment"># These are the rows of the first matrix (Wh_repeated_in_chunks): </span></span><br><span class="line">        <span class="comment"># e1, e1, ..., e1,            e2, e2, ..., e2,            ..., eN, eN, ..., eN</span></span><br><span class="line">        <span class="comment"># &#x27;-------------&#x27; -&gt; N times  &#x27;-------------&#x27; -&gt; N times       &#x27;-------------&#x27; -&gt; N times</span></span><br><span class="line">        <span class="comment"># </span></span><br><span class="line">        <span class="comment"># These are the rows of the second matrix (Wh_repeated_alternating): </span></span><br><span class="line">        <span class="comment"># e1, e2, ..., eN, e1, e2, ..., eN, ..., e1, e2, ..., eN </span></span><br><span class="line">        <span class="comment"># &#x27;----------------------------------------------------&#x27; -&gt; N times</span></span><br><span class="line">        <span class="comment"># </span></span><br><span class="line">        </span><br><span class="line">        Wh_repeated_in_chunks = Wh.repeat_interleave(N, dim=<span class="number">0</span>) <span class="comment"># 复制</span></span><br><span class="line">        Wh_repeated_alternating = Wh.repeat(N, <span class="number">1</span>)</span><br><span class="line">        <span class="comment"># Wh_repeated_in_chunks.shape == Wh_repeated_alternating.shape == (N * N, out_features)</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># The all_combination_matrix, created below, will look like this (|| denotes concatenation):</span></span><br><span class="line">        <span class="comment"># e1 || e1</span></span><br><span class="line">        <span class="comment"># e1 || e2</span></span><br><span class="line">        <span class="comment"># e1 || e3</span></span><br><span class="line">        <span class="comment"># ...</span></span><br><span class="line">        <span class="comment"># e1 || eN</span></span><br><span class="line">        <span class="comment"># e2 || e1</span></span><br><span class="line">        <span class="comment"># e2 || e2</span></span><br><span class="line">        <span class="comment"># e2 || e3</span></span><br><span class="line">        <span class="comment"># ...</span></span><br><span class="line">        <span class="comment"># e2 || eN</span></span><br><span class="line">        <span class="comment"># ...</span></span><br><span class="line">        <span class="comment"># eN || e1</span></span><br><span class="line">        <span class="comment"># eN || e2</span></span><br><span class="line">        <span class="comment"># eN || e3</span></span><br><span class="line">        <span class="comment"># ...</span></span><br><span class="line">        <span class="comment"># eN || eN</span></span><br><span class="line"></span><br><span class="line">        all_combinations_matrix = torch.cat([Wh_repeated_in_chunks, Wh_repeated_alternating], dim=<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># all_combinations_matrix.shape == (N * N, 2 * out_features)</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> all_combinations_matrix.view(N, N, <span class="number">2</span> * self.out_features)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__repr__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">return</span> self.__class__.__name__ + <span class="string">&#x27; (&#x27;</span> + <span class="built_in">str</span>(self.in_features) + <span class="string">&#x27; -&gt; &#x27;</span> + <span class="built_in">str</span>(self.out_features) + <span class="string">&#x27;)&#x27;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SpecialSpmmFunction</span>(<span class="params">torch.autograd.Function</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Special function for only sparse region backpropataion layer.&quot;&quot;&quot;</span></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">ctx, indices, values, shape, b</span>):</span></span><br><span class="line">        <span class="keyword">assert</span> indices.requires_grad == <span class="literal">False</span></span><br><span class="line">        a = torch.sparse_coo_tensor(indices, values, shape)</span><br><span class="line">        ctx.save_for_backward(a, b)</span><br><span class="line">        ctx.N = shape[<span class="number">0</span>]</span><br><span class="line">        <span class="keyword">return</span> torch.matmul(a, b)</span><br><span class="line"></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">backward</span>(<span class="params">ctx, grad_output</span>):</span></span><br><span class="line">        a, b = ctx.saved_tensors</span><br><span class="line">        grad_values = grad_b = <span class="literal">None</span></span><br><span class="line">        <span class="keyword">if</span> ctx.needs_input_grad[<span class="number">1</span>]:</span><br><span class="line">            grad_a_dense = grad_output.matmul(b.t())</span><br><span class="line">            edge_idx = a._indices()[<span class="number">0</span>, :] * ctx.N + a._indices()[<span class="number">1</span>, :]</span><br><span class="line">            grad_values = grad_a_dense.view(<span class="number">-1</span>)[edge_idx]</span><br><span class="line">        <span class="keyword">if</span> ctx.needs_input_grad[<span class="number">3</span>]:</span><br><span class="line">            grad_b = a.t().matmul(grad_output)</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">None</span>, grad_values, <span class="literal">None</span>, grad_b</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SpecialSpmm</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, indices, values, shape, b</span>):</span></span><br><span class="line">        <span class="keyword">return</span> SpecialSpmmFunction.apply(indices, values, shape, b)</span><br><span class="line"></span><br><span class="line">    </span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SpGraphAttentionLayer</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Sparse version GAT layer, similar to https://arxiv.org/abs/1710.10903</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, in_features, out_features, dropout, alpha, concat=<span class="literal">True</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>(SpGraphAttentionLayer, self).__init__()</span><br><span class="line">        self.in_features = in_features</span><br><span class="line">        self.out_features = out_features</span><br><span class="line">        self.alpha = alpha</span><br><span class="line">        self.concat = concat</span><br><span class="line"></span><br><span class="line">        self.W = nn.Parameter(torch.zeros(size=(in_features, out_features)))</span><br><span class="line">        nn.init.xavier_normal_(self.W.data, gain=<span class="number">1.414</span>)</span><br><span class="line">                </span><br><span class="line">        self.a = nn.Parameter(torch.zeros(size=(<span class="number">1</span>, <span class="number">2</span>*out_features)))</span><br><span class="line">        nn.init.xavier_normal_(self.a.data, gain=<span class="number">1.414</span>)</span><br><span class="line"></span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line">        self.leakyrelu = nn.LeakyReLU(self.alpha)</span><br><span class="line">        self.special_spmm = SpecialSpmm()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, <span class="built_in">input</span>, adj</span>):</span></span><br><span class="line">        dv = <span class="string">&#x27;cuda&#x27;</span> <span class="keyword">if</span> <span class="built_in">input</span>.is_cuda <span class="keyword">else</span> <span class="string">&#x27;cpu&#x27;</span></span><br><span class="line"></span><br><span class="line">        N = <span class="built_in">input</span>.size()[<span class="number">0</span>]</span><br><span class="line">        edge = adj.nonzero().t()</span><br><span class="line"></span><br><span class="line">        h = torch.mm(<span class="built_in">input</span>, self.W)</span><br><span class="line">        <span class="comment"># h: N x out</span></span><br><span class="line">        <span class="keyword">assert</span> <span class="keyword">not</span> torch.isnan(h).<span class="built_in">any</span>()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Self-attention on the nodes - Shared attention mechanism</span></span><br><span class="line">        edge_h = torch.cat((h[edge[<span class="number">0</span>, :], :], h[edge[<span class="number">1</span>, :], :]), dim=<span class="number">1</span>).t()</span><br><span class="line">        <span class="comment"># edge: 2*D x E</span></span><br><span class="line"></span><br><span class="line">        edge_e = torch.exp(-self.leakyrelu(self.a.mm(edge_h).squeeze()))</span><br><span class="line">        <span class="keyword">assert</span> <span class="keyword">not</span> torch.isnan(edge_e).<span class="built_in">any</span>()</span><br><span class="line">        <span class="comment"># edge_e: E</span></span><br><span class="line"></span><br><span class="line">        e_rowsum = self.special_spmm(edge, edge_e, torch.Size([N, N]), torch.ones(size=(N,<span class="number">1</span>), device=dv))</span><br><span class="line">        <span class="comment"># e_rowsum: N x 1</span></span><br><span class="line"></span><br><span class="line">        edge_e = self.dropout(edge_e)</span><br><span class="line">        <span class="comment"># edge_e: E</span></span><br><span class="line"></span><br><span class="line">        h_prime = self.special_spmm(edge, edge_e, torch.Size([N, N]), h)</span><br><span class="line">        <span class="keyword">assert</span> <span class="keyword">not</span> torch.isnan(h_prime).<span class="built_in">any</span>()</span><br><span class="line">        <span class="comment"># h_prime: N x out</span></span><br><span class="line">        </span><br><span class="line">        h_prime = h_prime.div(e_rowsum)</span><br><span class="line">        <span class="comment"># h_prime: N x out</span></span><br><span class="line">        <span class="keyword">assert</span> <span class="keyword">not</span> torch.isnan(h_prime).<span class="built_in">any</span>()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.concat:</span><br><span class="line">            <span class="comment"># if this layer is not last layer,</span></span><br><span class="line">            <span class="keyword">return</span> F.elu(h_prime)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># if this layer is last layer,</span></span><br><span class="line">            <span class="keyword">return</span> h_prime</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__repr__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">return</span> self.__class__.__name__ + <span class="string">&#x27; (&#x27;</span> + <span class="built_in">str</span>(self.in_features) + <span class="string">&#x27; -&gt; &#x27;</span> + <span class="built_in">str</span>(self.out_features) + <span class="string">&#x27;)&#x27;</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><figcaption><span>utils.py</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> scipy.sparse <span class="keyword">as</span> sp</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">encode_onehot</span>(<span class="params">labels</span>):</span></span><br><span class="line">    <span class="comment"># The classes must be sorted before encoding to enable static class encoding.</span></span><br><span class="line">    <span class="comment"># In other words, make sure the first class always maps to index 0.</span></span><br><span class="line">    classes = <span class="built_in">sorted</span>(<span class="built_in">list</span>(<span class="built_in">set</span>(labels)))</span><br><span class="line">    classes_dict = &#123;c: np.identity(<span class="built_in">len</span>(classes))[i, :] <span class="keyword">for</span> i, c <span class="keyword">in</span> <span class="built_in">enumerate</span>(classes)&#125;</span><br><span class="line">    labels_onehot = np.array(<span class="built_in">list</span>(<span class="built_in">map</span>(classes_dict.get, labels)), dtype=np.int32)</span><br><span class="line">    <span class="keyword">return</span> labels_onehot</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_data</span>(<span class="params">path=<span class="string">&quot;./data/cora/&quot;</span>, dataset=<span class="string">&quot;cora&quot;</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Load citation network dataset (cora only for now)&quot;&quot;&quot;</span></span><br><span class="line">    print(<span class="string">&#x27;Loading &#123;&#125; dataset...&#x27;</span>.<span class="built_in">format</span>(dataset))</span><br><span class="line"></span><br><span class="line">    idx_features_labels = np.genfromtxt(<span class="string">&quot;&#123;&#125;&#123;&#125;.content&quot;</span>.<span class="built_in">format</span>(path, dataset), dtype=np.dtype(<span class="built_in">str</span>))</span><br><span class="line">    features = sp.csr_matrix(idx_features_labels[:, <span class="number">1</span>:<span class="number">-1</span>], dtype=np.float32)</span><br><span class="line">    labels = encode_onehot(idx_features_labels[:, <span class="number">-1</span>])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># build graph</span></span><br><span class="line">    idx = np.array(idx_features_labels[:, <span class="number">0</span>], dtype=np.int32)</span><br><span class="line">    idx_map = &#123;j: i <span class="keyword">for</span> i, j <span class="keyword">in</span> <span class="built_in">enumerate</span>(idx)&#125;</span><br><span class="line">    <span class="comment"># 将边导入</span></span><br><span class="line">    edges_unordered = np.genfromtxt(<span class="string">&quot;&#123;&#125;&#123;&#125;.cites&quot;</span>.<span class="built_in">format</span>(path, dataset), dtype=np.int32)</span><br><span class="line">    <span class="comment"># 将点对应到dictionary中</span></span><br><span class="line">    edges = np.array(<span class="built_in">list</span>(<span class="built_in">map</span>(idx_map.get, edges_unordered.flatten())), dtype=np.int32).reshape(edges_unordered.shape)</span><br><span class="line">    <span class="comment"># 建立边的邻接矩阵</span></span><br><span class="line">    adj = sp.coo_matrix((np.ones(edges.shape[<span class="number">0</span>]), (edges[:, <span class="number">0</span>], edges[:, <span class="number">1</span>])), shape=(labels.shape[<span class="number">0</span>], labels.shape[<span class="number">0</span>]), dtype=np.float32)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># build symmetric adjacency matrix 构建对称矩阵</span></span><br><span class="line">    adj = adj + adj.T.multiply(adj.T &gt; adj) - adj.multiply(adj.T &gt; adj)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 归一化</span></span><br><span class="line">    features = normalize_features(features)</span><br><span class="line">    adj = normalize_adj(adj + sp.eye(adj.shape[<span class="number">0</span>]))</span><br><span class="line"></span><br><span class="line">    idx_train = <span class="built_in">range</span>(<span class="number">140</span>)</span><br><span class="line">    idx_val = <span class="built_in">range</span>(<span class="number">200</span>, <span class="number">500</span>)</span><br><span class="line">    idx_test = <span class="built_in">range</span>(<span class="number">500</span>, <span class="number">1500</span>)</span><br><span class="line"></span><br><span class="line">    adj = torch.FloatTensor(np.array(adj.todense()))</span><br><span class="line">    features = torch.FloatTensor(np.array(features.todense()))</span><br><span class="line">    labels = torch.LongTensor(np.where(labels)[<span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">    idx_train = torch.LongTensor(idx_train)</span><br><span class="line">    idx_val = torch.LongTensor(idx_val)</span><br><span class="line">    idx_test = torch.LongTensor(idx_test)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> adj, features, labels, idx_train, idx_val, idx_test</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">normalize_adj</span>(<span class="params">mx</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Row-normalize sparse matrix&quot;&quot;&quot;</span></span><br><span class="line">    rowsum = np.array(mx.<span class="built_in">sum</span>(<span class="number">1</span>))</span><br><span class="line">    r_inv_sqrt = np.power(rowsum, <span class="number">-0.5</span>).flatten()</span><br><span class="line">    r_inv_sqrt[np.isinf(r_inv_sqrt)] = <span class="number">0.</span></span><br><span class="line">    r_mat_inv_sqrt = sp.diags(r_inv_sqrt)</span><br><span class="line">    <span class="keyword">return</span> mx.dot(r_mat_inv_sqrt).transpose().dot(r_mat_inv_sqrt)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">normalize_features</span>(<span class="params">mx</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Row-normalize sparse matrix&quot;&quot;&quot;</span></span><br><span class="line">    rowsum = np.array(mx.<span class="built_in">sum</span>(<span class="number">1</span>))</span><br><span class="line">    r_inv = np.power(rowsum, <span class="number">-1</span>).flatten()</span><br><span class="line">    r_inv[np.isinf(r_inv)] = <span class="number">0.</span></span><br><span class="line">    r_mat_inv = sp.diags(r_inv)</span><br><span class="line">    mx = r_mat_inv.dot(mx)</span><br><span class="line">    <span class="keyword">return</span> mx</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">accuracy</span>(<span class="params">output, labels</span>):</span></span><br><span class="line">    preds = output.<span class="built_in">max</span>(<span class="number">1</span>)[<span class="number">1</span>].type_as(labels)</span><br><span class="line">    correct = preds.eq(labels).double()</span><br><span class="line">    correct = correct.<span class="built_in">sum</span>()</span><br><span class="line">    <span class="keyword">return</span> correct / <span class="built_in">len</span>(labels)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><figcaption><span>visualize_graph.py</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> graphviz <span class="keyword">import</span> Digraph</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> models</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">make_dot</span>(<span class="params">var, params</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot; Produces Graphviz representation of PyTorch autograd graph</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Blue nodes are the Variables that require grad, orange are Tensors</span></span><br><span class="line"><span class="string">    saved for backward in torch.autograd.Function</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        var: output Variable</span></span><br><span class="line"><span class="string">        params: dict of (name, Variable) to add names to node that</span></span><br><span class="line"><span class="string">            require grad (TODO: make optional)</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    param_map = &#123;<span class="built_in">id</span>(v): k <span class="keyword">for</span> k, v <span class="keyword">in</span> params.items()&#125;</span><br><span class="line">    print(param_map)</span><br><span class="line">    </span><br><span class="line">    node_attr = <span class="built_in">dict</span>(style=<span class="string">&#x27;filled&#x27;</span>,</span><br><span class="line">                     shape=<span class="string">&#x27;box&#x27;</span>,</span><br><span class="line">                     align=<span class="string">&#x27;left&#x27;</span>,</span><br><span class="line">                     fontsize=<span class="string">&#x27;12&#x27;</span>,</span><br><span class="line">                     ranksep=<span class="string">&#x27;0.1&#x27;</span>,</span><br><span class="line">                     height=<span class="string">&#x27;0.2&#x27;</span>)</span><br><span class="line">    dot = Digraph(node_attr=node_attr, graph_attr=<span class="built_in">dict</span>(size=<span class="string">&quot;12,12&quot;</span>))</span><br><span class="line">    seen = <span class="built_in">set</span>()</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">size_to_str</span>(<span class="params">size</span>):</span></span><br><span class="line">        <span class="keyword">return</span> <span class="string">&#x27;(&#x27;</span>+(<span class="string">&#x27;, &#x27;</span>).join([<span class="string">&#x27;%d&#x27;</span>% v <span class="keyword">for</span> v <span class="keyword">in</span> size])+<span class="string">&#x27;)&#x27;</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">add_nodes</span>(<span class="params">var</span>):</span></span><br><span class="line">        <span class="keyword">if</span> var <span class="keyword">not</span> <span class="keyword">in</span> seen:</span><br><span class="line">            <span class="keyword">if</span> torch.is_tensor(var):</span><br><span class="line">                dot.node(<span class="built_in">str</span>(<span class="built_in">id</span>(var)), size_to_str(var.size()), fillcolor=<span class="string">&#x27;orange&#x27;</span>)</span><br><span class="line">            <span class="keyword">elif</span> <span class="built_in">hasattr</span>(var, <span class="string">&#x27;variable&#x27;</span>):</span><br><span class="line">                u = var.variable</span><br><span class="line">                node_name = <span class="string">&#x27;%s\n %s&#x27;</span> % (param_map.get(<span class="built_in">id</span>(u)), size_to_str(u.size()))</span><br><span class="line">                dot.node(<span class="built_in">str</span>(<span class="built_in">id</span>(var)), node_name, fillcolor=<span class="string">&#x27;lightblue&#x27;</span>)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                dot.node(<span class="built_in">str</span>(<span class="built_in">id</span>(var)), <span class="built_in">str</span>(<span class="built_in">type</span>(var).__name__))</span><br><span class="line">            seen.add(var)</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">hasattr</span>(var, <span class="string">&#x27;next_functions&#x27;</span>):</span><br><span class="line">                <span class="keyword">for</span> u <span class="keyword">in</span> var.next_functions:</span><br><span class="line">                    <span class="keyword">if</span> u[<span class="number">0</span>] <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                        dot.edge(<span class="built_in">str</span>(<span class="built_in">id</span>(u[<span class="number">0</span>])), <span class="built_in">str</span>(<span class="built_in">id</span>(var)))</span><br><span class="line">                        add_nodes(u[<span class="number">0</span>])</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">hasattr</span>(var, <span class="string">&#x27;saved_tensors&#x27;</span>):</span><br><span class="line">                <span class="keyword">for</span> t <span class="keyword">in</span> var.saved_tensors:</span><br><span class="line">                    dot.edge(<span class="built_in">str</span>(<span class="built_in">id</span>(t)), <span class="built_in">str</span>(<span class="built_in">id</span>(var)))</span><br><span class="line">                    add_nodes(t)</span><br><span class="line">    add_nodes(var.grad_fn)</span><br><span class="line">    <span class="keyword">return</span> dot</span><br><span class="line"></span><br><span class="line">inputs = torch.randn(<span class="number">100</span>, <span class="number">50</span>).cuda()</span><br><span class="line">adj = torch.randn(<span class="number">100</span>, <span class="number">100</span>).cuda()</span><br><span class="line">model = models.SpGAT(<span class="number">50</span>, <span class="number">8</span>, <span class="number">7</span>, <span class="number">0.5</span>, <span class="number">0.01</span>, <span class="number">3</span>)</span><br><span class="line">model = model.cuda()</span><br><span class="line">y = model(inputs, adj)</span><br><span class="line"></span><br><span class="line">g = make_dot(y, model.state_dict())</span><br><span class="line">g.view()</span><br></pre></td></tr></table></figure>
<h1 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h1><p><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzE3MTAuMTA5MDMucGRm">GRAPH ATTENTION NETWORKS<i class="fa fa-external-link-alt"></i></span><br><span class="exturl" data-url="aHR0cHM6Ly9iYWlqaWFoYW8uYmFpZHUuY29tL3M/aWQ9MTY3MTAyODk2NDU0NDg4NDc0OSZhbXA7d2ZyPXNwaWRlciZhbXA7Zm9yPXBj">GAT 图注意力网络 Graph Attention Network<i class="fa fa-external-link-alt"></i></span><br><span class="exturl" data-url="aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQ1MzEyMTQxL2FydGljbGUvZGV0YWlscy8xMDYyOTExOTU=">图注意力网络(GAT)<i class="fa fa-external-link-alt"></i></span><br><span class="exturl" data-url="aHR0cHM6Ly93d3cuamlxaXpoaXhpbi5jb20vYXJ0aWNsZXMvMjAxOS0wMi0xOS03">深入理解图注意力机制<i class="fa fa-external-link-alt"></i></span><br><span class="exturl" data-url="aHR0cHM6Ly96aHVhbmxhbi56aGlodS5jb20vcC8zMzkwODc3MDY=">GAT详解<i class="fa fa-external-link-alt"></i></span></p>

      
    </div>

    
    
    
      


    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://soundmemories.github.io/2021/02/05/Graph/01.GCN/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="https://i.loli.net/2020/11/04/6JhNuwtBe4adylS.png">
      <meta itemprop="name" content="SoundMemories">
      <meta itemprop="description" content="今日事，今日毕">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="SoundMemories">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/02/05/Graph/01.GCN/" class="post-title-link" itemprop="url">GCN</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2021-02-05 00:00:00" itemprop="dateCreated datePublished" datetime="2021-02-05T00:00:00+08:00">2021-02-05</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Graph/" itemprop="url" rel="index"><span itemprop="name">Graph</span></a>
        </span>
    </span>

  
      </div>
      <div class="post-meta">
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>19k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>17 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="图论"><a href="#图论" class="headerlink" title="图论"></a>图论</h1><p>最开始是1707年由 Seven bridges 问题而来的，欧拉把这个问题转成图解决。近代图论1959年 Erdos 和 Renyi 发表的 Random Graph 一系列论文建立起来的全新领域。目前学术界和工业界是基于1999年 brabasi 提出的 Scale-free Network 进行的。</p>
<h2 id="degree-distribution"><a href="#degree-distribution" class="headerlink" title="degree-distribution"></a>degree-distribution</h2><p><strong>度分布，即每个节点按边的数量分类，每类节点占总结点比例</strong>。一般 Random Graph 的度分布是 <strong>泊松分布</strong>（poisson，类似正态分布）。但基于 Scale-free Network 的提出，我们发现很多网络结构是 <strong>幂律分布</strong>（power-law）。</p>
<p><img src="/images/GCN/随机网络.png" width="80%"><br><img src="/images/GCN/无标度网络.png" width="80%"></p>
<h2 id="Distance-Hilbert-Space"><a href="#Distance-Hilbert-Space" class="headerlink" title="Distance(Hilbert Space)"></a>Distance(Hilbert Space)</h2><p>每个节点具有多维features，可以看成多维空间，计算Distance也可看作Similarity。为什么在 Hilbert Space ，因为要方便后面约束优化。</p>
<p>GCN要求：<br>（1）weights $\geq$ 0<br>（2）linear calculation<br>（3）Inner product</p>
<p>Hilbert Space（括号表示内积）：<br>（1）对称性：$(\vec{y}, \vec{x})$ = $(\vec{x}, \vec{y})$<br>（2）线性：$(a\vec{x_1}+b \vec{x_2}, \vec{y})$ = $a(\vec{x_1}, \vec{y})+b(\vec{x_2}, \vec{y})$<br>（3）半正定性：$(\vec{x}, \vec{x}) \geq 0$，if $\vec{x}=\vec{0}$，$(\vec{x}, \vec{x}) = 0$</p>
<p>Distance(Hilbert Space)：$d(\vec{x}, \vec{y})=||\vec{x}-\vec{y}||=\sqrt{(\vec{x}-\vec{y}, \vec{x}-\vec{y})}$</p>
<h2 id="adjacency-matrix"><a href="#adjacency-matrix" class="headerlink" title="adjacency matrix"></a>adjacency matrix</h2><p>邻接矩阵。阶为$n$的图$G$的邻接矩阵$A$是$n\times n$的。将$G$的顶点标签为$v_{1},v_{2},…,v_{n}$。若$(v_{i},v_{j})\in E(G)$，$A_{ij}=1$，否则$A_{ij}=0$。也可以用大于0的值表示边的权值，例如可以用边权值表示一个点到另一个点的距离。</p>
<p>无向图的邻接矩阵计算方法是每条边为对应的单元加上1，而每个自环加上2。这样让某一节点的度数可以通过邻接矩阵的对应行或者列求和得到。<br><img src="/images/GCN/邻接矩阵.png" width="50%"></p>
<p>有向图的邻接矩阵可以是不对称的。我们可以定义有向图的邻接矩阵中的某个元素 $A_{ij}$ 代表：<br>（1）从 $i$ 指向 $j$ 的边数目。<br>（2）从 $j$ 指向 $i$ 的边数目。<br>在第一种定义下，有向图的某个节点的入度可以通过对应的列（column）求和而得，出度可以通过对应的行（row）求和而得。在第二种定义下，入度可以通过对应的行（row）求和而得，出度可以通过对应的列（column）求和而得。<br><img src="/images/GCN/邻接矩阵2.png" width="50%"></p>
<h2 id="Clustering-coefficient"><a href="#Clustering-coefficient" class="headerlink" title="Clustering coefficient"></a>Clustering coefficient</h2><p>聚类系数，一个图中的顶点之间结集成团的程度的系数。集聚系数分为整体与局部两种。整体集聚系数可以给出一个图中整体的集聚程度的评估，而局部集聚系数则可以测量图中每一个结点附近的集聚程度。</p>
<p><strong>分子</strong>：闭三点组（邻近三点组成“三角形”）数量。<br><strong>分母</strong>：闭三点组（邻近三点组成“三角形”）数量 + 开三点组（邻近三点组成“缺一条边的三角形”）数量。</p>
<p>整体聚类系数：对每个节点的聚类系数求和取均值。<strong>如果该值很大，表示图是很稠密的，节点和节点之间联系紧密</strong>。</p>
<h2 id="Betweenness"><a href="#Betweenness" class="headerlink" title="Betweenness"></a>Betweenness</h2><p>也叫Betweenness centrality，分node和edge两种计算方法：<br>（1）<strong>node Betweenness</strong>：计算经过一个点的最短路径的数量占所有最短路径数量比例。两点一组，遍历所有组，计算每组中经过一个点的最短路径的数量占该组最短路径数量比例，最后求和。<br>（2）<strong>edge Betweenness</strong>：node Betweenness 换成边即可。</p>
<p><strong>这个值很大，表示此node或edge很重要，因为很多最短路径都要经过它，表现流通性</strong>。</p>
<p>缺点：需要遍历所有点找到所有最短路径，一般的算法有 迪杰斯特拉算法（Dijkstra） 和 弗洛伊德算法（Floyd），时间复杂度都为 $O(n^3)$。</p>
<h1 id="GCN"><a href="#GCN" class="headerlink" title="GCN"></a>GCN</h1><p>GCN（Graph Convolutional Networks，图卷积神经网络），实际上跟CNN的作用一样，就是一个特征提取器，只不过它的对象是图数据。<br>GCN精妙地设计了一种从图数据中提取特征的方法，从而让我们可以使用这些特征去对图数据进行<strong>节点分类</strong>（node classification）、<strong>图分类</strong>（graph classification）、<strong>边预测</strong>（link prediction），还可以顺便得到<strong>图的嵌入表示</strong>（graph embedding）。</p>
<p>GCN发展历史，那么肯定绕不过下面三篇论文：<br><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzEzMTIuNjIwMy5wZGY=">Spectral Networks and Deep Locally Connected Networks on Graphs<i class="fa fa-external-link-alt"></i></span> 2014年<br><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzE2MDYuMDkzNzUucGRm">Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering<i class="fa fa-external-link-alt"></i></span> 2016年<br><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzE2MDkuMDI5MDcucGRm">Semi-Supervised Classification with Graph Convolutional Networks<i class="fa fa-external-link-alt"></i></span> 2017年</p>
<p>在计算机科学领域、理论物理复杂网络领域的研究者在图（Graph）的空间域（spatial domain）和频谱域（spectral domain）分别提出了不同形式的图神经网络，并最终在2017年实现了空间域模型和频谱域模型的融合，即目前我们使用的第三代GCN。</p>
<p>对于其中理论和公式非常感兴趣的参考<span class="exturl" data-url="aHR0cDovL3h0ZjYxNS5jb20vMjAxOS8wMi8yNC9nY24v">图卷积神经网络理论基础<i class="fa fa-external-link-alt"></i></span>。</p>
<h2 id="Fourier-transform"><a href="#Fourier-transform" class="headerlink" title="Fourier transform"></a>Fourier transform</h2><script type="math/tex; mode=display">
F(w)=\frac{1}{2\pi}\int_{-\infty}^{+\infty} f(t)e^{-j\omega t} {\rm d}t</script><p>$F(\omega)$ 就是<strong>傅里叶变换</strong>，得到的就是<strong>频域曲线</strong>。每个频率$\omega$下都有对应的振幅$F(\omega)$。从几何上来看，$f(t)$ 以 $e^{-j\omega t}$ 为基函数投影，$F(w)$ 就是以频率 $\omega$ 对应基上的投影的坐标。</p>
<p>从数学角度来看，$f(x)$ 是函数 $f$ 在 $t$ 处的取值，所有基都对该处取值有贡献，即把每个$F(w)$ 投影到 $e^{-j\omega t}$ 基方向上分量累加起来，得到的就是该点处的函数值。</p>
<script type="math/tex; mode=display">
f(t) = \int_{-\infty}^{+\infty}F(w)e^{-j\omega t}\, {\rm d}\omega=\sum_{\omega}F(w)e^{-j\omega t}</script><p>上面简化了一下，用 $w$ 代表频率。这个公式也叫做<strong>逆傅里叶变换</strong>。</p>
<h2 id="Laplacian-operater"><a href="#Laplacian-operater" class="headerlink" title="Laplacian operater"></a>Laplacian operater</h2><script type="math/tex; mode=display">
\Delta f = \Delta^2 f = \sum_{i=1}^{n}\frac{\partial^2 f}{\partial x_i^2}</script><p>$f$ 是拉普拉斯算子作用的函数，求函数各向二阶导数再求和，定义为 $f$ 上的拉普拉斯算子。<br>可以理解为：<strong>二阶导数等于其在所有自由度上微扰之后获得的增益</strong>。<br>更形象的理解：<strong>拉普拉斯算子就是在所有自由度上进行微小变化后获得的增益</strong>。</p>
<p>求 $e^{-j\omega t}$ 上的 Laplacian operater：</p>
<script type="math/tex; mode=display">
\Delta f = \Delta e^{-j\omega t} = \sum_{i=1}^{n}\frac{\partial^2 e^{-j\omega t}}{\partial t^2}=-\omega^2 e^{-j\omega t}</script><p>由此可知，$e^{-j\omega t}$ 是 <strong>Laplacian operater 的特征向量</strong>（满足特征方程 $A\vec{x}=\lambda \vec{x}$）。</p>
<h2 id="Graph-Laplacian-operater"><a href="#Graph-Laplacian-operater" class="headerlink" title="Graph Laplacian operater"></a>Graph Laplacian operater</h2><p>Laplacian operater 推广到 Graph：假设<strong>图是一个完全图，即任意两个节点之间都有一条边，那么对一个节点进行微扰，它可能变成任意一个节点</strong>。即：</p>
<script type="math/tex; mode=display">
f=(f_1,f_2...f_N)</script><p>是函数 $f$ 在节点 $1..N$ 上的函数值，代表<strong>跟节点相关的信息</strong>，如节点属性等，此时可看作每一个节点是一个向量。</p>
<p>假设一个节点 $f_i$ ，其一阶邻域节点集合为 $N_i$ ，$f_j$为 $N_i$ 集合的一个节点，对于任意节点 $f_i$ ，对 $f_i$ 节点进行微扰，它可能变为任意一个与他相邻的节点 $f_j \in N_i$。前面提到，拉普拉斯算子就是在所有自由度上进行微小变化后获得的增益。对于 Graph 而言，从节点 $i$ 变化到节点 $j$ 增益是 $f_i−f_j$，即节点 $f_i$ 的 <strong>Graph Laplacian operater</strong>：</p>
<script type="math/tex; mode=display">
\Delta f_i =\sum_{j \in N_i} (f_i - f_j)</script><p>通俗理解，当前节点的 Graph Laplacian operater 就是 <strong>当前节点和所有邻接节点的差值</strong>。</p>
<h2 id="Laplacian-Matrix"><a href="#Laplacian-Matrix" class="headerlink" title="Laplacian Matrix"></a>Laplacian Matrix</h2><p>把 Graph Laplacian operater 公式变换一下，<strong>考虑权重</strong>：$w_{ij}=0$ 表示  $i,j$ 不相邻，$w_{ij}=1$ 表示  $i,j$ 相邻，那么上面公式可以转换为：</p>
<script type="math/tex; mode=display">
\begin{aligned}
\Delta f_i &=\sum_{j \in N} w_{ij}(f_i - f_j)\\
&=\sum_{j \in N} w_{ij}f_i - \sum_{j \in N} w_{ij}f_j\\
&=d_if_i-w_if
\end{aligned}</script><p>其中：令 $d_i=\sum_{j \in N}w_{ij}$ ，表示节点 $i$ 的度。令 $w_i=[w_{i1},…,w_{iN}]$ 行。令 $f=[f_1,…f_N]^T$ 列。<br>对于所有节点：</p>
<script type="math/tex; mode=display">
\begin{aligned}
\Delta f&=
\begin{bmatrix}
   d_1f_1-w_1f \\
   d_2f_2-w_2f \\
   \cdots \\
   d_Nf_N-w_Nf
\end{bmatrix}\\
&=
\begin{equation*}
    \begin{bmatrix} 
    d_1 &0&\cdots&0 \\
    0&d_2&\cdots&0\\
    \vdots&\vdots& \ddots&\vdots \\
    0&0&\cdots&d_N 
    \end{bmatrix}
    \end{equation*}f-\begin{bmatrix}
   w_1\\
   w_2\\
   \cdots \\
   w_N
\end{bmatrix}f\\ 
&=(D-W)f
\end{aligned}</script><p>$D$ 就是<strong>度矩阵</strong>（dgree matrix），$W$ 就是<strong>邻接矩阵</strong>（adjacency matrix），$L=D-W$ 就是<strong>拉普拉斯矩阵</strong>（Laplacian Matrix）。</p>
<p><img src="/images/GCN/LM.png" width="80%"></p>
<p>根据$\Delta f = Lf$，那么可以看作 <strong>Laplacian operater 等于 Laplacian Matrix</strong> ，即$\Delta=L$。这样<strong>求 Graph Laplacian operater 等价于求 Laplacian Matrix</strong>。</p>
<p>Laplacian Matrix 是<strong>半正定对称矩阵</strong>，因此拥有诸多优秀性质：</p>
<ul>
<li>对称矩阵一定n个线性无关的特征向量</li>
<li>半正定矩阵的特征值一定非负</li>
<li>对阵矩阵的特征向量相互正交，即所有特征向量构成的矩阵为正交矩阵</li>
</ul>
<p>对 Laplacian Matrix 进行特征分解：</p>
<script type="math/tex; mode=display">
\Delta=L=U\Lambda U^T</script><p>其中，$U$的每一列为$L$的<strong>特征向量</strong>，$\Lambda$ 是$L$的<strong>特征值矩阵</strong>，$U^T$的每一行为$L$的<strong>特征向量</strong>。</p>
<h2 id="Graph-Fourier-transform"><a href="#Graph-Fourier-transform" class="headerlink" title="Graph Fourier transform"></a>Graph Fourier transform</h2><p>前面提到，$e^{-j\omega t}$ 是 $\Delta$ 的<strong>特征向量</strong>，而后推导出：$\Delta=L=U\Lambda U^T$ ，$U^T$的每一行为$L$的<strong>特征向量</strong>$\phi_w$，因此我们可得到：</p>
<ul>
<li>频率$w$ $\to$ 特征值$\lambda_w$</li>
<li>正弦函数 $e^{-j\omega t}$ $\to$ 特征向量$\phi_w$</li>
<li>振幅$F(w)$ $\to$ 振幅$F(\lambda_w)$</li>
</ul>
<p>这样就把传统傅里叶变换推广到了图傅里叶变换。推广到矩阵形式：</p>
<script type="math/tex; mode=display">
\hat{f} = U^Tf</script><p>逆变换：</p>
<script type="math/tex; mode=display">
f = U\hat{f}</script><h2 id="Graph-Convolution"><a href="#Graph-Convolution" class="headerlink" title="Graph Convolution"></a>Graph Convolution</h2><p><strong>卷积定理：函数卷积的傅里叶变换是函数傅立叶变换的乘积，即对于函数 $f$ 与 $g$ 两者的卷积是其函数傅立叶变换乘积的逆变换</strong>。时域上的卷积-&gt;频域上的相乘后逆变换。从而方便计算，可以看作一种Mapping方式，把时域信号转成频域信号处理。</p>
<script type="math/tex; mode=display">
f*g = \mathcal{F}^{-1}\{\mathcal{F}\{f\} \cdot \mathcal{F}\{g\}\}</script><p>其中，$f$ 是图信号，$g$ 是卷积核。通过 Graph Fourier transform：</p>
<script type="math/tex; mode=display">
f*g=U(U^Tg\cdot U^Tf)</script><p>由于对 $g$ 和 $f$ 进行傅里叶变换的结果为 $U^Tg$ 和 $U^Tf$ 都是一个列向量，所以也可以写成：</p>
<script type="math/tex; mode=display">
f*g=U(U^Tg\odot U^Tf)</script><p>$\odot$表示哈达马积，对于两个向量，就是进行内积运算；对于维度相同的两个矩阵，就是对应元素的乘积运算。</p>
<p>通常把 $U^Tg$ 整体看作可学习的卷积核，这里把它写作 $g_{\theta}$（由参数 $\theta$ 构成的对角矩阵 $diag(\theta)$）。最终图上的卷积公式：</p>
<script type="math/tex; mode=display">
f*g=Ug_{\theta}U^Tf</script><p>由于参数 $\theta$ 的确定与 $L$ 的特征值有关，可把 $g_{\theta}$ 看作是特征值 $\Lambda$ 的一个函数，那么可把 $g_{\theta}$ 看成是拉普拉斯矩阵 $L$ 的一系列特征值组成的对角矩阵的形式，即定义$g_{\theta}=diag(U^Tg)=g_{\theta}(\Lambda)$：</p>
<script type="math/tex; mode=display">
f*g=Ug_{\theta}(\Lambda)U^Tf=U
\begin{equation*}
    \begin{bmatrix} 
    \hat{g}(\lambda_1) &   &\\
    & \ddots & \\
    & & \hat{g}(\lambda_N)
    \end{bmatrix}
    \end{equation*}
U^Tf</script><h2 id="Graph-Convolution-Networks"><a href="#Graph-Convolution-Networks" class="headerlink" title="Graph Convolution Networks"></a>Graph Convolution Networks</h2><p><strong>第一代GCN</strong>（Spectral CNN）：简单的把 $g_{\theta}$（由参数 $\theta$ 构成的对角矩阵 $diag(\theta)$）看作是一个可学习参数的集合，其中 $x$ 是节点特征向量：</p>
<script type="math/tex; mode=display">
f*g=x*g_{\theta}=Ug_{\theta}U^Tx</script><p>第一代GCN缺点：<br>（1）计算复杂度高$O(n^2)$，每次计算都需要特征分解求U；每一次前向传播，都要计算$U,g_{\theta},U^T$ 三者的乘积。<br>（2）没有正则化（no normalization）。<br>（3）没有考虑自身权重（no self-weight）。</p>
<hr>
<p><strong>第二代GCN</strong>（ChebNet）：定义特征向量对角矩阵的切比雪夫多项式为滤波器：</p>
<script type="math/tex; mode=display">
g_{\theta'}(\Lambda) \approx \sum_{k=0}^{K}\theta_{k}^{'}\Lambda^k=\sum_{k=0}^{K}\theta_{k}^{'}T_{k}(\tilde{\Lambda})</script><p>其中：</p>
<ul>
<li>$\tilde{\Lambda}=\frac{2}{\lambda_{max}}\Lambda-I_N$，$\lambda_{max}$是L的最大特征值。</li>
<li>$\theta \in \mathbb{R}^K$ 是切比雪夫系数的向量。</li>
<li>切比雪夫多项式（类似泰勒展开）定义为：$T_{k}(x)=2xT_{k-1}(x)-T_{k-2}(x)$，其中 $T_0(x)=1,T_1(x)=x$。</li>
</ul>
<p>就是利用Chebyshev多项式拟合卷积核的方法，来降低计算复杂度。但首先提出Chebyshev多项式K阶截断展开来拟合，并对 $\Lambda$ 进行归一化使其元素位于[-1,1]之间的是<span class="exturl" data-url="aHR0cHM6Ly9oYWwuaW5yaWEuZnIvaW5yaWEtMDA1NDE4NTUvZG9jdW1lbnQ=">Hammond et al.(2011) ：Wavelets on graphs via spectral graph theory<i class="fa fa-external-link-alt"></i></span>，二代GCN借鉴了这一方法。</p>
<p>回到 $g_{\theta}$ 和输入 $x$ 的卷积：</p>
<script type="math/tex; mode=display">
\begin{aligned}
g_{\theta}*x &= U \sum_{k=0}^{K}\theta_{k}^{'}\Lambda^k U^Tx\\
&=\sum_{k=0}^{K}\theta_{k}^{'}(U \Lambda^kU^T) x\\
&=\sum_{k=0}^{K}\theta_{k}^{'}(U \Lambda U^T)^k x\\
&=\sum_{k=0}^{K}\theta_{k}^{'}L^{k}x
\end{aligned}</script><p>这里面就用到拉普拉斯矩阵 $L$。计算复杂度为 $O(kn^2)$。使用切比雪夫展开，其中 $\tilde{L}=\frac{2}{\lambda_{max}}L-I_N$：</p>
<script type="math/tex; mode=display">
g_{\theta^{'}}*x=\sum_{k=0}^{K}\theta_{k}^{'}T_{x}(\tilde{L})x</script><hr>
<p><strong>第三代GCN</strong>（一阶ChebNet）：只对切比雪夫展开到一阶，即 $K=1,\lambda_{max}=2$，那么 $\tilde{L}=L-I_N$，且 $T_0(\tilde{L})=1,T_1(\tilde{L})=\tilde{L}$ ，第二代公式可简化为：</p>
<script type="math/tex; mode=display">
\begin{aligned}
g_{\theta^{'}}*x &= \theta_0^{'}T_0(\tilde{L})x+\theta_1^{'}T_1(\tilde{L})x\\
&=\theta_0^{'}x+\theta_1^{'}(L-I_N)x\\
\end{aligned}</script><p>对$L$做归一化处理：</p>
<script type="math/tex; mode=display">
\begin{aligned}
\hat{L}&=D^{-\frac{1}{2}}(L)D^{-\frac{1}{2}}\\
&=D^{-\frac{1}{2}}(D-W)D^{-\frac{1}{2}}\\
&=I_N-D^{-\frac{1}{2}}WD^{-\frac{1}{2}}
\end{aligned}</script><p>代入到前式中得到：</p>
<script type="math/tex; mode=display">
\theta_0^{\prime}x+\theta_1^{\prime}(L-I_N)x=\theta_0^{'}x+(-\theta_1^{'}(D^{-\frac{1}{2}}WD^{-\frac{1}{2}})x)</script><p>由于不希望 $\theta_0^{\prime}$ 和 $\theta_1^{\prime}$ 出现，所以假设 $\theta_0^{\prime}=-\theta_1^{\prime}=\theta$：</p>
<script type="math/tex; mode=display">
g_{\theta^{'}}*x=\theta(I_N+D^{-\frac{1}{2}}WD^{-\frac{1}{2}})x</script><p>注意 $I_N+D^{-\frac{1}{2}}WD^{-\frac{1}{2}}$ 的特征值被限制在了[0,2]中。由于这一步输出可能作为下一层的输入，会再次与 $I_N+D^{-\frac{1}{2}}WD^{-\frac{1}{2}}$ 相乘重复这样的操作将会导致数值不稳定、梯度消失/爆炸等问题。</p>
<p>为了解决该问题，引入renormalization（就是加了自环）：令 $\tilde{W}=W+I_N, \tilde{D}_i=\sum_j \tilde{W}_{ij}$：</p>
<script type="math/tex; mode=display">
I_N+D^{-\frac{1}{2}}WD^{-\frac{1}{2}} \approx \tilde{D}^{-\frac{1}{2}}\tilde{W}\tilde{D}^{-\frac{1}{2}}</script><p>那么，带入之前的公式得到：</p>
<script type="math/tex; mode=display">
\underbrace{\boldsymbol{g}_{\boldsymbol{\theta^{\prime}}} * \boldsymbol{x}}_{\mathbb{R}^{n \times n}} = \theta(\underbrace{\tilde{\boldsymbol{D}}^{-1/2}\tilde{\boldsymbol{W}} \tilde{\boldsymbol{D}}^{-1/2}}_{\mathbb{R}^{n \times n}}) \underbrace{\boldsymbol{x}}_{\mathbb{R}^{n \times 1}}</script><p>推广到多通道和多卷积，则卷积结果写作矩阵形式如下：</p>
<script type="math/tex; mode=display">
\underbrace{\boldsymbol{Z}}_{\mathbb{R}^{N \times F}} = \underbrace{\tilde{\boldsymbol{D}}^{-1/2}\tilde{\boldsymbol{W}} \tilde{\boldsymbol{D}}^{-1/2}}_{\mathbb{R}^{N \times N}} \underbrace{\boldsymbol{X}}_{\mathbb{R}^{N \times C}} \ \ \underbrace{\boldsymbol{\Theta}}_{\mathbb{R}^{C \times F}}</script><p>其中，$N$ 是<strong>节点数量</strong>，$C$ 是通道数或者称作节点的<strong>特征维度</strong>，$F$ 为<strong>卷积核数量</strong>。$D$ 就是<strong>度矩阵</strong>，$W$ 就是<strong>邻接矩阵</strong>，$X$ 是节点的<strong>特征矩阵</strong>，$\Theta$ 是<strong>卷积核参数矩阵</strong>，最终得到的卷积结果 $\boldsymbol{Z} \in \mathbb{R}^{N \times F}$，即每个节点的卷积结果的维数等于卷积核数量。上述操作可以叠加多层，对 $Z$ 激活一下，然后将激活后的 $Z$ 作为下一层的节点的特征矩阵。</p>
<p>第三代GCN特点总结：</p>
<ul>
<li>解决了计算复杂度高的问题：复杂度为$O(E)$ (稀疏矩阵优化的话)，$E$ 是图中边的几何。</li>
<li>只考虑1-hop，若要建模多hop，通过叠加层数，获得更大的感受野。（联想NLP中使用卷积操作语句序列时，也是通过叠加多层来达到获取长依赖的目的）。</li>
</ul>
<h1 id="Code"><a href="#Code" class="headerlink" title="Code"></a>Code</h1><p>作者给出了源码，分两个版本：</p>
<ul>
<li>tensorflow：<span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL3RraXBmL2djbg==">gcn<i class="fa fa-external-link-alt"></i></span></li>
<li>pytorch：<span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL3RraXBmL3B5Z2Nu">pygcn<i class="fa fa-external-link-alt"></i></span></li>
<li>数据集地址：<span class="exturl" data-url="aHR0cHM6Ly9saW5xcy1kYXRhLnNvZS51Y3NjLmVkdS9wdWJsaWMvbGJjL2NvcmEudGd6">cora<i class="fa fa-external-link-alt"></i></span></li>
</ul>
<p>cora数据集有2708个样本，每个样本由1433维特征表示，每个样本是一篇科学论文，每篇论文可能为7个类别，样本和样本之间包括了5429个连接。</p>
<p>模型输入：<br><strong>X</strong>：N×D的特征矩阵，N表示节点数量（cora数据集就是2708），D表示输入特征（cora数据集就是1433）。<br><strong>A</strong>：邻接矩阵。<br>模型输出：<br><strong>Z</strong>：N×F的特征矩阵，F是每个输出节点的特征维度（这个维度自己设置）。</p>
<p>使用的公式：<br>$H^{(l+1)}=f(H^{(l)},A),\qquad H^{(0)}=X, H^{(L)}=Z$<br>$f(H^{(l)},A)=\sigma(AH^{(l)}W^{(l)}),\qquad f(H^{(l)},A)=\sigma(\hat{D}^{-\frac{1}{2}}\hat{A}\hat{D}^{-\frac{1}{2}}H^{(l)}W^{(l)})$</p>
<p>其中，$\hat{A}=A+I$，$I$是对角矩阵(自环)，$\hat{A}$ 是加上自环(节点本身信息)后的邻接矩阵。如果一个节点有非常多的邻居，那么函数$f$就会越来越大，所以加上一个归一化$\hat{D}$是$\hat{A}$的度矩阵，有两种方法$\hat{D}^{-1}A$和$\hat{D}^{-\frac{1}{2}}\hat{A}\hat{D}^{-\frac{1}{2}}$。</p>
<p>以pytorch版本为例：<br>文件结构：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">.</span><br><span class="line">├── data      &#x2F;&#x2F; 图数据</span><br><span class="line">├── pygcn</span><br><span class="line">    ├── inits    &#x2F;&#x2F; 初始化的一些公用函数</span><br><span class="line">    ├── layers     &#x2F;&#x2F; GCN层的定义</span><br><span class="line">        ├── class GraphConvolution</span><br><span class="line">        ├── reset parameters</span><br><span class="line">        ├── forward</span><br><span class="line">    ├── models     &#x2F;&#x2F; 模型结构定义</span><br><span class="line">        ├── class GCN</span><br><span class="line">        ├── forward</span><br><span class="line">    ├── train    &#x2F;&#x2F; 训练</span><br><span class="line">        ├── def train</span><br><span class="line">        ├── def test </span><br><span class="line">    └── utils    &#x2F;&#x2F;  工具函数的定义</span><br><span class="line">        ├── encode_onehot</span><br><span class="line">        ├── load_data</span><br><span class="line">        ├── normazlize</span><br><span class="line">        ├── accuracy</span><br><span class="line">        ├── sparse mx to torch sparse tensor</span><br><span class="line">├── setup.py &#x2F;&#x2F;启动函数</span><br></pre></td></tr></table></figure></p>
<figure class="highlight python"><figcaption><span>model.py</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">from</span> pygcn.layers <span class="keyword">import</span> GraphConvolution</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">GCN</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, nfeat, nhid, nclass, dropout</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(GCN, self).__init__()</span><br><span class="line"></span><br><span class="line">        self.gc1 = GraphConvolution(nfeat, nhid)  <span class="comment"># nfeat：N×D的D</span></span><br><span class="line">        self.gc2 = GraphConvolution(nhid, nclass) <span class="comment"># nclass：类别，这里是7类</span></span><br><span class="line">        self.dropout = dropout</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x, adj</span>):</span></span><br><span class="line">        x = F.relu(self.gc1(x, adj)) <span class="comment"># 第一层输出+relu</span></span><br><span class="line">        x = F.dropout(x, self.dropout, training=self.training)</span><br><span class="line">        x = self.gc2(x, adj)  <span class="comment"># 第二层输出</span></span><br><span class="line">        <span class="keyword">return</span> F.log_softmax(x, dim=<span class="number">1</span>) <span class="comment"># 第二层输出+log_softmax</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><figcaption><span>layers.py</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> math</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> torch.nn.parameter <span class="keyword">import</span> Parameter</span><br><span class="line"><span class="keyword">from</span> torch.nn.modules.module <span class="keyword">import</span> Module</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">GraphConvolution</span>(<span class="params">Module</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Simple GCN layer, similar to https://arxiv.org/abs/1609.02907</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, in_features, out_features, bias=<span class="literal">True</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>(GraphConvolution, self).__init__()</span><br><span class="line">        self.in_features = in_features  <span class="comment"># 每层的输入维度</span></span><br><span class="line">        self.out_features = out_features   <span class="comment"># 每层的输出维度</span></span><br><span class="line">        self.weight = Parameter(torch.FloatTensor(in_features, out_features))</span><br><span class="line">        <span class="keyword">if</span> bias:</span><br><span class="line">            self.bias = Parameter(torch.FloatTensor(out_features))</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.register_parameter(<span class="string">&#x27;bias&#x27;</span>, <span class="literal">None</span>)</span><br><span class="line">        self.reset_parameters()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">reset_parameters</span>(<span class="params">self</span>):</span> <span class="comment"># 参数初始化方法</span></span><br><span class="line">        stdv = <span class="number">1.</span> / math.sqrt(self.weight.size(<span class="number">1</span>))</span><br><span class="line">        self.weight.data.uniform_(-stdv, stdv)</span><br><span class="line">        <span class="keyword">if</span> self.bias <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            self.bias.data.uniform_(-stdv, stdv)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, <span class="built_in">input</span>, adj</span>):</span> <span class="comment"># 实现AHW，第一次时H是X</span></span><br><span class="line">        support = torch.mm(<span class="built_in">input</span>, self.weight) <span class="comment"># 实现XW</span></span><br><span class="line">        <span class="comment"># Sparse matrix multiplication, https://github.com/tkipf/pygcn/issues/19</span></span><br><span class="line">        <span class="comment"># output = torch.spmm(adj, support) # spmm后续版本被移除了，使用sparse.mm替代</span></span><br><span class="line">        output = torch.sparse.mm(adj, support) <span class="comment"># 实现AXW</span></span><br><span class="line">        <span class="keyword">if</span> self.bias <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">return</span> output + self.bias</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> output</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__repr__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">return</span> self.__class__.__name__ + <span class="string">&#x27; (&#x27;</span> \</span><br><span class="line">               + <span class="built_in">str</span>(self.in_features) + <span class="string">&#x27; -&gt; &#x27;</span> \</span><br><span class="line">               + <span class="built_in">str</span>(self.out_features) + <span class="string">&#x27;)&#x27;</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><figcaption><span>util.py</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> scipy.sparse <span class="keyword">as</span> sp</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="comment"># focus</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">encode_onehot</span>(<span class="params">labels</span>):</span></span><br><span class="line">    classes = <span class="built_in">set</span>(labels)</span><br><span class="line">    classes_dict = &#123;c: np.identity(<span class="built_in">len</span>(classes))[i, :] <span class="keyword">for</span> i, c <span class="keyword">in</span></span><br><span class="line">                    <span class="built_in">enumerate</span>(classes)&#125;</span><br><span class="line">    labels_onehot = np.array(<span class="built_in">list</span>(<span class="built_in">map</span>(classes_dict.get, labels)),</span><br><span class="line">                             dtype=np.int32)</span><br><span class="line">    <span class="keyword">return</span> labels_onehot</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_data</span>(<span class="params">path=<span class="string">&quot;../data/cora/&quot;</span>, dataset=<span class="string">&quot;cora&quot;</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Load citation network dataset (cora only for now)&quot;&quot;&quot;</span></span><br><span class="line">    print(<span class="string">&#x27;Loading &#123;&#125; dataset...&#x27;</span>.<span class="built_in">format</span>(dataset))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 读进来的数据为：idx，features，labels</span></span><br><span class="line">    idx_features_labels = np.genfromtxt(<span class="string">&quot;&#123;&#125;&#123;&#125;.content&quot;</span>.<span class="built_in">format</span>(path, dataset),</span><br><span class="line">                                        dtype=np.dtype(<span class="built_in">str</span>))</span><br><span class="line">    <span class="comment"># csr_matrix数据存储成稀疏方式，格式为csr</span></span><br><span class="line">    features = sp.csr_matrix(idx_features_labels[:, <span class="number">1</span>:<span class="number">-1</span>], dtype=np.float32)</span><br><span class="line">    labels = encode_onehot(idx_features_labels[:, <span class="number">-1</span>]) <span class="comment"># 使用onehot编码类别 (2708, 7)</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># build graph</span></span><br><span class="line">    idx = np.array(idx_features_labels[:, <span class="number">0</span>], dtype=np.int32)</span><br><span class="line">    idx_map = &#123;j: i <span class="keyword">for</span> i, j <span class="keyword">in</span> <span class="built_in">enumerate</span>(idx)&#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 读进来的数据为：edges，unordered</span></span><br><span class="line">    <span class="comment"># [[     35,    1033],</span></span><br><span class="line">    <span class="comment">#  [     35,  103482],...]</span></span><br><span class="line">    edges_unordered = np.genfromtxt(<span class="string">&quot;&#123;&#125;&#123;&#125;.cites&quot;</span>.<span class="built_in">format</span>(path, dataset),</span><br><span class="line">                                    dtype=np.int32)</span><br><span class="line">    <span class="comment"># 转成对应map编号</span></span><br><span class="line">    <span class="comment"># [[ 163,  402],</span></span><br><span class="line">    <span class="comment">#  [ 163,  659],...]</span></span><br><span class="line">    edges = np.array(<span class="built_in">list</span>(<span class="built_in">map</span>(idx_map.get, edges_unordered.flatten())),</span><br><span class="line">                     dtype=np.int32).reshape(edges_unordered.shape)</span><br><span class="line">    <span class="comment"># (edges[:, 0], edges[:, 1])坐标点，(np.ones(edges.shape[0])每个坐标位置的值为1</span></span><br><span class="line">    <span class="comment"># 此步得到的是有向图邻接矩阵</span></span><br><span class="line">    adj = sp.coo_matrix((np.ones(edges.shape[<span class="number">0</span>]), (edges[:, <span class="number">0</span>], edges[:, <span class="number">1</span>])),</span><br><span class="line">                        shape=(labels.shape[<span class="number">0</span>], labels.shape[<span class="number">0</span>]),</span><br><span class="line">                        dtype=np.float32)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># build symmetric adjacency matrix !</span></span><br><span class="line">    <span class="comment"># 无向图，邻接矩阵是对称的，https://zhuanlan.zhihu.com/p/78191258</span></span><br><span class="line">    <span class="comment"># see https://github.com/tkipf/pygcn/issues/3</span></span><br><span class="line">    adj = adj + adj.T.multiply(adj.T &gt; adj) - adj.multiply(adj.T &gt; adj)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># see https://github.com/tkipf/pygcn/issues/47</span></span><br><span class="line">    <span class="comment"># 归一化防止梯度消失</span></span><br><span class="line">    features = normalize(features)</span><br><span class="line">    adj = normalize(adj + sp.eye(adj.shape[<span class="number">0</span>])) <span class="comment"># 加对角矩阵I，即A+I=\hat&#123;A&#125;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 切分数据</span></span><br><span class="line">    idx_train = <span class="built_in">range</span>(<span class="number">140</span>)</span><br><span class="line">    idx_val = <span class="built_in">range</span>(<span class="number">200</span>, <span class="number">500</span>)</span><br><span class="line">    idx_test = <span class="built_in">range</span>(<span class="number">500</span>, <span class="number">1500</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 转成Tensor</span></span><br><span class="line">    features = torch.FloatTensor(np.array(features.todense()))</span><br><span class="line">    labels = torch.LongTensor(np.where(labels)[<span class="number">1</span>])</span><br><span class="line">    adj = sparse_mx_to_torch_sparse_tensor(adj)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 转成Tensor</span></span><br><span class="line">    idx_train = torch.LongTensor(idx_train)</span><br><span class="line">    idx_val = torch.LongTensor(idx_val)</span><br><span class="line">    idx_test = torch.LongTensor(idx_test)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> adj, features, labels, idx_train, idx_val, idx_test</span><br><span class="line"></span><br><span class="line"><span class="comment"># focus</span></span><br><span class="line"><span class="comment"># 对特征矩阵features和邻接矩阵adj做标准化，防止梯度消失</span></span><br><span class="line"><span class="comment"># 每个值除以它所在行的和</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">normalize</span>(<span class="params">mx</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Row-normalize sparse matrix&quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># see https://github.com/tkipf/gcn/blob/master/gcn/utils.py#L122</span></span><br><span class="line">    <span class="comment"># 对每行求和得到rowsum</span></span><br><span class="line">    rowsum = np.array(mx.<span class="built_in">sum</span>(<span class="number">1</span>))</span><br><span class="line">    <span class="comment"># 求逆得到r_inv</span></span><br><span class="line">    r_inv = np.power(rowsum, <span class="number">-1</span>).flatten()</span><br><span class="line">    <span class="comment"># 如果某一行全为0，则r_inv算出来会等于无穷大，将这些行的r_inv置为0</span></span><br><span class="line">    r_inv[np.isinf(r_inv)] = <span class="number">0.</span></span><br><span class="line">    r_mat_inv = sp.diags(r_inv)</span><br><span class="line">    mx = r_mat_inv.dot(mx)</span><br><span class="line">    <span class="keyword">return</span> mx</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">accuracy</span>(<span class="params">output, labels</span>):</span></span><br><span class="line">    preds = output.<span class="built_in">max</span>(<span class="number">1</span>)[<span class="number">1</span>].type_as(labels)</span><br><span class="line">    correct = preds.eq(labels).double()</span><br><span class="line">    correct = correct.<span class="built_in">sum</span>()</span><br><span class="line">    <span class="keyword">return</span> correct / <span class="built_in">len</span>(labels)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sparse_mx_to_torch_sparse_tensor</span>(<span class="params">sparse_mx</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Convert a scipy sparse matrix to a torch sparse tensor.&quot;&quot;&quot;</span></span><br><span class="line">    sparse_mx = sparse_mx.tocoo().astype(np.float32)</span><br><span class="line">    indices = torch.from_numpy(</span><br><span class="line">        np.vstack((sparse_mx.row, sparse_mx.col)).astype(np.int64))</span><br><span class="line">    values = torch.from_numpy(sparse_mx.data)</span><br><span class="line">    shape = torch.Size(sparse_mx.shape)</span><br><span class="line">    <span class="keyword">return</span> torch.sparse.FloatTensor(indices, values, shape)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><figcaption><span>train.py</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> division</span><br><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> print_function</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> pygcn.utils <span class="keyword">import</span> load_data, accuracy</span><br><span class="line"><span class="keyword">from</span> pygcn.models <span class="keyword">import</span> GCN</span><br><span class="line"></span><br><span class="line"><span class="comment"># Training settings</span></span><br><span class="line">parser = argparse.ArgumentParser()</span><br><span class="line">parser.add_argument(<span class="string">&#x27;--no-cuda&#x27;</span>, action=<span class="string">&#x27;store_true&#x27;</span>, default=<span class="literal">False</span>,</span><br><span class="line">                    <span class="built_in">help</span>=<span class="string">&#x27;Disables CUDA training.&#x27;</span>)</span><br><span class="line">parser.add_argument(<span class="string">&#x27;--fastmode&#x27;</span>, action=<span class="string">&#x27;store_true&#x27;</span>, default=<span class="literal">False</span>,</span><br><span class="line">                    <span class="built_in">help</span>=<span class="string">&#x27;Validate during training pass.&#x27;</span>)</span><br><span class="line">parser.add_argument(<span class="string">&#x27;--seed&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">int</span>, default=<span class="number">42</span>, <span class="built_in">help</span>=<span class="string">&#x27;Random seed.&#x27;</span>)</span><br><span class="line">parser.add_argument(<span class="string">&#x27;--epochs&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">int</span>, default=<span class="number">200</span>,</span><br><span class="line">                    <span class="built_in">help</span>=<span class="string">&#x27;Number of epochs to train.&#x27;</span>)</span><br><span class="line">parser.add_argument(<span class="string">&#x27;--lr&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">float</span>, default=<span class="number">0.01</span>,</span><br><span class="line">                    <span class="built_in">help</span>=<span class="string">&#x27;Initial learning rate.&#x27;</span>)</span><br><span class="line">parser.add_argument(<span class="string">&#x27;--weight_decay&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">float</span>, default=<span class="number">5e-4</span>,</span><br><span class="line">                    <span class="built_in">help</span>=<span class="string">&#x27;Weight decay (L2 loss on parameters).&#x27;</span>)</span><br><span class="line">parser.add_argument(<span class="string">&#x27;--hidden&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">int</span>, default=<span class="number">16</span>,</span><br><span class="line">                    <span class="built_in">help</span>=<span class="string">&#x27;Number of hidden units.&#x27;</span>)</span><br><span class="line">parser.add_argument(<span class="string">&#x27;--dropout&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">float</span>, default=<span class="number">0.5</span>,</span><br><span class="line">                    <span class="built_in">help</span>=<span class="string">&#x27;Dropout rate (1 - keep probability).&#x27;</span>)</span><br><span class="line"></span><br><span class="line">args = parser.parse_args()</span><br><span class="line">args.cuda = <span class="keyword">not</span> args.no_cuda <span class="keyword">and</span> torch.cuda.is_available()</span><br><span class="line"></span><br><span class="line">np.random.seed(args.seed)</span><br><span class="line">torch.manual_seed(args.seed)</span><br><span class="line"><span class="keyword">if</span> args.cuda:</span><br><span class="line">    torch.cuda.manual_seed(args.seed)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Load data</span></span><br><span class="line">adj, features, labels, idx_train, idx_val, idx_test = load_data()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Model and optimizer</span></span><br><span class="line">model = GCN(nfeat=features.shape[<span class="number">1</span>],</span><br><span class="line">            nhid=args.hidden,</span><br><span class="line">            nclass=labels.<span class="built_in">max</span>().item() + <span class="number">1</span>,</span><br><span class="line">            dropout=args.dropout)</span><br><span class="line">optimizer = optim.Adam(model.parameters(),</span><br><span class="line">                       lr=args.lr, weight_decay=args.weight_decay)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> args.cuda:</span><br><span class="line">    model.cuda()</span><br><span class="line">    features = features.cuda()</span><br><span class="line">    adj = adj.cuda()</span><br><span class="line">    labels = labels.cuda()</span><br><span class="line">    idx_train = idx_train.cuda()</span><br><span class="line">    idx_val = idx_val.cuda()</span><br><span class="line">    idx_test = idx_test.cuda()</span><br><span class="line"></span><br><span class="line"><span class="comment"># focus</span></span><br><span class="line"><span class="comment"># 这里训练时给140个带标签，输入的是全部数据特征，整体是个半监督的任务</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span>(<span class="params">epoch</span>):</span></span><br><span class="line">    t = time.time()</span><br><span class="line">    model.train()</span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    output = model(features, adj)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># classify each node</span></span><br><span class="line">    <span class="comment"># 只考虑 train ids 计算 loss</span></span><br><span class="line">    <span class="comment"># see https://github.com/tkipf/pygcn/issues/50</span></span><br><span class="line">    <span class="comment"># 如果输出用softmax，这里就用交叉熵损失cross_entropy</span></span><br><span class="line">    <span class="comment"># 这里使用负对数似然损失nll_loss，因为前面输出用的是log_softmax</span></span><br><span class="line">    <span class="comment"># torch.nn.CrossEntropyLoss、cross_entropy都是上面两个函数的组合nll_loss(log_softmax(input))</span></span><br><span class="line">    loss_train = F.nll_loss(output[idx_train], labels[idx_train])</span><br><span class="line">    acc_train = accuracy(output[idx_train], labels[idx_train])</span><br><span class="line">    loss_train.backward()</span><br><span class="line">    optimizer.step()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> args.fastmode:</span><br><span class="line">        <span class="comment"># Evaluate validation set performance separately,</span></span><br><span class="line">        <span class="comment"># deactivates dropout during validation run.</span></span><br><span class="line">        model.<span class="built_in">eval</span>()</span><br><span class="line">        output = model(features, adj)</span><br><span class="line"></span><br><span class="line">    loss_val = F.nll_loss(output[idx_val], labels[idx_val])</span><br><span class="line">    acc_val = accuracy(output[idx_val], labels[idx_val])</span><br><span class="line">    print(<span class="string">&#x27;Epoch: &#123;:04d&#125;&#x27;</span>.<span class="built_in">format</span>(epoch+<span class="number">1</span>),</span><br><span class="line">          <span class="string">&#x27;loss_train: &#123;:.4f&#125;&#x27;</span>.<span class="built_in">format</span>(loss_train.item()),</span><br><span class="line">          <span class="string">&#x27;acc_train: &#123;:.4f&#125;&#x27;</span>.<span class="built_in">format</span>(acc_train.item()),</span><br><span class="line">          <span class="string">&#x27;loss_val: &#123;:.4f&#125;&#x27;</span>.<span class="built_in">format</span>(loss_val.item()),</span><br><span class="line">          <span class="string">&#x27;acc_val: &#123;:.4f&#125;&#x27;</span>.<span class="built_in">format</span>(acc_val.item()),</span><br><span class="line">          <span class="string">&#x27;time: &#123;:.4f&#125;s&#x27;</span>.<span class="built_in">format</span>(time.time() - t))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test</span>():</span></span><br><span class="line">    model.<span class="built_in">eval</span>()</span><br><span class="line">    output = model(features, adj)</span><br><span class="line">    loss_test = F.nll_loss(output[idx_test], labels[idx_test])</span><br><span class="line">    acc_test = accuracy(output[idx_test], labels[idx_test])</span><br><span class="line">    print(<span class="string">&quot;Test set results:&quot;</span>,</span><br><span class="line">          <span class="string">&quot;loss= &#123;:.4f&#125;&quot;</span>.<span class="built_in">format</span>(loss_test.item()),</span><br><span class="line">          <span class="string">&quot;accuracy= &#123;:.4f&#125;&quot;</span>.<span class="built_in">format</span>(acc_test.item()))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Train model</span></span><br><span class="line">t_total = time.time()</span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(args.epochs):</span><br><span class="line">    train(epoch)</span><br><span class="line">print(<span class="string">&quot;Optimization Finished!&quot;</span>)</span><br><span class="line">print(<span class="string">&quot;Total time elapsed: &#123;:.4f&#125;s&quot;</span>.<span class="built_in">format</span>(time.time() - t_total))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Testing</span></span><br><span class="line">test()</span><br></pre></td></tr></table></figure>
<h1 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h1><p><span class="exturl" data-url="aHR0cHM6Ly96aC53aWtpcGVkaWEub3JnL3dpa2kvJUU5JTlBJThGJUU2JTlDJUJBJUU1JTlCJUJF">随机图<i class="fa fa-external-link-alt"></i></span><br><span class="exturl" data-url="aHR0cHM6Ly96aC53aWtpcGVkaWEub3JnL3dpa2kvJUU2JTk3JUEwJUU1JUIwJUJBJUU1JUJBJUE2JUU3JUJEJTkxJUU3JUJCJTlD">无尺度网络<i class="fa fa-external-link-alt"></i></span>，<span class="exturl" data-url="aHR0cHM6Ly93d3cuY25ibG9ncy5jb20vbGVlengvcC85NDM2ODIwLmh0bWw=">Scale Free Network<i class="fa fa-external-link-alt"></i></span></p>
<p><span class="exturl" data-url="aHR0cHM6Ly96aC53aWtpcGVkaWEub3JnL3dpa2kvJUU1JUJBJUE2JUU1JTg4JTg2JUU1JUI4JTgz">度分布<i class="fa fa-external-link-alt"></i></span><br><span class="exturl" data-url="aHR0cHM6Ly96aC53aWtpcGVkaWEub3JnL3dpa2kvJUU5JTgyJUJCJUU2JThFJUE1JUU3JTlGJUE5JUU5JTk4JUI1">邻接矩阵<i class="fa fa-external-link-alt"></i></span><br><span class="exturl" data-url="aHR0cHM6Ly9lbi53aWtpcGVkaWEub3JnL3dpa2kvQ2x1c3RlcmluZ19jb2VmZmljaWVudA==">Clustering coefficient<i class="fa fa-external-link-alt"></i></span><br><span class="exturl" data-url="aHR0cHM6Ly9lbi53aWtpcGVkaWEub3JnL3dpa2kvQmV0d2Vlbm5lc3M=">Betweenness<i class="fa fa-external-link-alt"></i></span><br><span class="exturl" data-url="aHR0cHM6Ly93d3cuemhpaHUuY29tL3F1ZXN0aW9uLzE5OTY3Nzc4">如何理解希尔伯特空间？<i class="fa fa-external-link-alt"></i></span>，<span class="exturl" data-url="aHR0cHM6Ly96aC53aWtpcGVkaWEub3JnL3dpa2kvJUU1JUI4JThDJUU1JUIwJTk0JUU0JUJDJUFGJUU3JTg5JUI5JUU3JUE5JUJBJUU5JTk3JUI0">希尔伯特空间<i class="fa fa-external-link-alt"></i></span><br><span class="exturl" data-url="aHR0cHM6Ly9lbi53aWtpcGVkaWEub3JnL3dpa2kvRm91cmllcl90cmFuc2Zvcm0=">Fourier transform<i class="fa fa-external-link-alt"></i></span>，<span class="exturl" data-url="aHR0cHM6Ly96aC53aWtpcGVkaWEub3JnL3dpa2kvJUU1JTgyJTg1JUU5JTg3JThDJUU1JThGJUI2JUU1JThGJTk4JUU2JThEJUEy">傅里叶变换<i class="fa fa-external-link-alt"></i></span></p>
<p><span class="exturl" data-url="aHR0cDovL3RraXBmLmdpdGh1Yi5pby9ncmFwaC1jb252b2x1dGlvbmFsLW5ldHdvcmtzLw==">GRAPH CONVOLUTIONAL NETWORKS<i class="fa fa-external-link-alt"></i></span><br><span class="exturl" data-url="aHR0cHM6Ly93d3cuc29odS5jb20vYS8zNDI2MzQyOTFfNjUxODkz">跳出公式，看清全局，图神经网络（GCN）原理详解<i class="fa fa-external-link-alt"></i></span><br><span class="exturl" data-url="aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3l5bDQyNDUyNS9hcnRpY2xlL2RldGFpbHMvMTAwMDU4MjY0I0dDTl84Mjg=">图卷积网络 GCN Graph Convolutional Network（谱域GCN）的理解和详细推导<i class="fa fa-external-link-alt"></i></span><br><span class="exturl" data-url="aHR0cDovL3h0ZjYxNS5jb20vMjAxOS8wMi8yNC9nY24v">图卷积神经网络理论基础<i class="fa fa-external-link-alt"></i></span><br><span class="exturl" data-url="aHR0cHM6Ly96aHVhbmxhbi56aGlodS5jb20vcC83NjAwMTA4MA==">GNN综述——从入门到入门<i class="fa fa-external-link-alt"></i></span><br><span class="exturl" data-url="aHR0cHM6Ly93d3cuY25ibG9ncy5jb20vbnhmLXJhYmJpdDc1L3AvMTEzMDYxOTguaHRtbA==">GCN总结<i class="fa fa-external-link-alt"></i></span></p>

      
    </div>

    
    
    
      


    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://soundmemories.github.io/2021/02/03/Graph/00.%E5%9B%BE%E6%B7%B1%E5%BA%A6%E8%A1%A8%E7%A4%BA/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="https://i.loli.net/2020/11/04/6JhNuwtBe4adylS.png">
      <meta itemprop="name" content="SoundMemories">
      <meta itemprop="description" content="今日事，今日毕">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="SoundMemories">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/02/03/Graph/00.%E5%9B%BE%E6%B7%B1%E5%BA%A6%E8%A1%A8%E7%A4%BA/" class="post-title-link" itemprop="url">图深度表示</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2021-02-03 00:00:00" itemprop="dateCreated datePublished" datetime="2021-02-03T00:00:00+08:00">2021-02-03</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Graph/" itemprop="url" rel="index"><span itemprop="name">Graph</span></a>
        </span>
    </span>

  
      </div>
      <div class="post-meta">
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>4.7k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>4 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="图结构"><a href="#图结构" class="headerlink" title="图结构"></a>图结构</h1><p>在现实中，很多情景的构成是不规则的图结构，比如社交网络、金融网络、化学分子结构等等。</p>
<h1 id="什么是图表示学习？"><a href="#什么是图表示学习？" class="headerlink" title="什么是图表示学习？"></a>什么是图表示学习？</h1><p>简单讲就是把图结构映射到向量空间（也叫graph embedding），或者由向量空间映射到图结构（也叫 graph generate）。</p>
<p>我们为什么这么做呢？或者说将图映射到向量空间的优势是什么？<br>（1）向量表示相对传统图表示（邻接矩阵、邻接表）对现有机器学习算法更友好。<br>（2）可以更好的将拓扑信息和节点本身特征结合。</p>
<h1 id="基于图结构的表示学习"><a href="#基于图结构的表示学习" class="headerlink" title="基于图结构的表示学习"></a>基于图结构的表示学习</h1><p>图论、数据挖掘角度：如何在学习到向量的表示中保留尽可能多的图拓扑结构的信息。</p>
<p>节点的向量表示只来源于图的拓扑结构（nxn 的邻接矩阵表达的图结构），只是对图结构的单一表示，缺乏对图节点特征消息的表示。下图d远远小于n。</p>
<p><img src="/images/图深度表示/图表示1.png" width="100%"></p>
<h2 id="传统方法"><a href="#传统方法" class="headerlink" title="传统方法"></a>传统方法</h2><p>第一个想法就是<strong>降维</strong>（nxn -&gt; nxd），利用现有的降维方法实现，比如PCA、LDA等等。致命缺点：时间复杂度高；<strong>不能保留图中节点和节点之间的拓扑信息</strong>。</p>
<ul>
<li>这里有个问题：什么叫做<strong>保留拓扑信息</strong>？</li>
<li><p>目标：<strong>在拓扑域里面‘邻近’&lt;==&gt; 在向量域‘邻近’</strong>。</p>
</li>
<li><p>问题：<strong>如何建模节点之间的邻近信息？ -&gt; 如何定义’邻近’？</strong></p>
</li>
<li>定义邻近的方法很多：<strong>共同出现、高阶邻近（n-hop邻居）、团体邻近（属于某一个团体）</strong>。</li>
</ul>
<p>所以现有算法都是围绕着 <strong>定义‘邻近’</strong> 和 <strong>求解‘邻近’</strong> 这两个点展开。</p>
<h2 id="Deepwalk"><a href="#Deepwalk" class="headerlink" title="Deepwalk"></a>Deepwalk</h2><ul>
<li><strong>动机</strong>：如何动态的建模邻居信息？</li>
<li><strong>1-hop建模</strong>：对于某个节点只看其邻居节点。两个相邻的结点就可以定义为邻近。<ul>
<li>太局部，忽略了图上的一些全局信息。</li>
</ul>
</li>
<li><strong>n-hop建模</strong>：对于某个节点考虑其n步邻居节点。两个n阶临近的结点也可以定义为邻近。<ul>
<li>组合爆炸，复杂度高，且没必要。</li>
</ul>
</li>
<li>解决方法：<ul>
<li>考虑有限步的情况，例如只考虑1，2 hop，即 LINE 2015。</li>
<li>使用采样的方式————随机游走（Random Walk）思想的 Deepwalk 2014。</li>
</ul>
</li>
</ul>
<p><strong>Deepwalk</strong>：<strong>利用随机游走采样生成的序列去定义节点间的邻近关系</strong>。<strong>在足够多的采样情况下，可以很好的刻画节点之间的邻近信息</strong>。<strong>这样就把图信息，转成了序列信息，通过Word2Vec把序列向量化即可（每个点看成词）</strong>。<br>总结：<br>（1）使用定长的随机游走去采样图中节点的邻近关系。<br>（2）节点-&gt;词语，随机游走序列-&gt;句子。<br>（3）使用自然语言处理相关模型（例如word2vec）对随机游走得到的序列进行表示学习。</p>
<p>基于Random Walk的思路出现了很多 XXX2vec 的论文，基本套路都一样。</p>
<h2 id="Node2vec"><a href="#Node2vec" class="headerlink" title="Node2vec"></a>Node2vec</h2><p><strong>动机</strong>：简单的随机游走采样不够好（不能体现出BFS/DFS性质）。<br><strong>核心思想</strong>：等概率跳 -&gt; 人工设计概率来跳。</p>
<p>当从结点 t 跳跃到结点 v 之后，算法下一步从结点 v 向邻居结点跳跃的概率是不同的。<br><img src="/images/图深度表示/node2vec.png" width="50%"></p>
<p>从结点 v 回跳到上一个结点 t 的 $\alpha$ 为 $\frac{1}{p}$，从结点 v 跳到 t、v 的公共邻居结点的 $\alpha$ 为 1，从结点 v 跳到其他邻居的 $\alpha$ 为 $\frac{1}{q}$。<br><img src="/images/图深度表示/node2vec2.png" width="50%"></p>
<p>我们发现，当 p 比较小的时候，结点间的跳转类似于 BFS，结点间的“接近”就可以理解为结点在<strong>邻接关系</strong>上“接近”；当 q 比较小的时候，结点间的跳转类似于 DFS，节点间的“接近”就可以视作是<strong>结构上相似</strong>。<br><img src="/images/图深度表示/node2vec3.png" width="50%"></p>
<h2 id="Struc2vec"><a href="#Struc2vec" class="headerlink" title="Struc2vec"></a>Struc2vec</h2><p><strong>动机</strong>：保留局部结构一致性。<br><strong>核心思想</strong>：在原来的图上构建一个新图。</p>
<h2 id="Metapath2vec"><a href="#Metapath2vec" class="headerlink" title="Metapath2vec"></a>Metapath2vec</h2><p><strong>动机</strong>：异构图上存在不同类型的节点，这些节点不能等同看待，其间关系可能存在一些固定模式。<br><strong>核心思路</strong>：使用预定义的Meta-Path来进行Random Walk。</p>
<h1 id="基于图特征的学习（图神经网络）"><a href="#基于图特征的学习（图神经网络）" class="headerlink" title="基于图特征的学习（图神经网络）"></a>基于图特征的学习（图神经网络）</h1><p>节点的向量表示既包含了图的拓扑信息（nxn 的邻接矩阵表达的图结构）也包含了节点的特征向量集合（nxf 的特征向量）。<br><img src="/images/图深度表示/图表示2.png" width="100%"></p>
<p>机器学习、特征工程角度：如何通过有效利用图拓扑结构信息结合现有的特征向量得到新的特征。<br>比如：图像-&gt;向量，视频-&gt;向量…。可以不严谨的说<strong>所有深度学习问题都可以归结为表示学习的问题</strong>。<br><strong>挑战</strong>：如何利用我们在图片/视频上取得的成功经验来应对图特征的表示学习问题？</p>
<p><strong>卷积神经网络</strong>（Convolutional Neural Network）：表示学习利器。<br>从图的角度看图像上的CNN：在欧式空间上的格点图（平移不变性、多尺度结构）。<br><strong>目标</strong>：将在欧式空间上的CNN扩展到拓扑空间————<strong>图卷积</strong>。</p>
<h2 id="GCN"><a href="#GCN" class="headerlink" title="GCN"></a>GCN</h2><p>GCN（Graph Convolutional Networks，图卷积神经网络）：</p>
<ul>
<li><strong>输入</strong>：邻接矩阵（节点数×节点数），特征矩阵（节点数×输入特征数）。</li>
<li><strong>输出</strong>：新的特征矩阵（节点数×输出特征数）。</li>
<li>网络层面：多层网络可以叠加。</li>
<li>节点层面：节点<strong>自身特征</strong>和其<strong>邻域特征</strong>的聚合。<br><img src="/images/图深度表示/GCN1.png" width="100%"></li>
</ul>
<p>公式如下：<br><img src="/images/图深度表示/GCN2.png" width="40%"></p>
<p>$\tilde{A}=A+I_N$：带自环的邻接矩阵。<br>$\tilde{D}=\sum_j \tilde{A}_{ij}$：度矩阵。<br>$H$：特征矩阵。<br>$W$：模型参数。<br>$\sigma(.)$：激活函数。</p>
<p><strong>两层GCN构造&amp;损失函数</strong>：<br><img src="/images/图深度表示/GCN3.png" width="50%"><br><img src="/images/图深度表示/GCN4.png" width="25%"></p>
<p><strong>GCN的推导思路</strong>：在图的拓扑空间近似在谱空间中的图滤波的操作，减少可学习参数。</p>
<p><strong>从另一个角度理解GCN</strong>：对<strong>邻居节点</strong>特征的<strong>带权重</strong>（$\tilde{D}^{-\frac{1}{2}}$）的<strong>聚合</strong>（$\tilde{A}H^{(l)}$）。</p>
<h2 id="GraphSAGE"><a href="#GraphSAGE" class="headerlink" title="GraphSAGE"></a>GraphSAGE</h2><p>对<strong>聚合</strong>和<strong>邻居节点</strong>进行了扩展定义：<br>（1）<strong>聚合</strong>：Mean Pooling/Max Pooling/LSTM，etc。<br>（2）<strong>邻居节点</strong>：Fix-length sample -&gt; 可以用来加速GCN计算。</p>
<p><img src="/images/图深度表示/GraphSAGE.png" width="100%"></p>
<h2 id="GAT"><a href="#GAT" class="headerlink" title="GAT"></a>GAT</h2><p>GAT（GRAPH ATTENTION NETWORKS，图注意力网络）：对<strong>权重</strong>（$\tilde{D}^{-\frac{1}{2}}$）进行了扩展。<br>（1）GCN中使用的邻接矩阵权重是提前给定的$\tilde{D}^{-\frac{1}{2}}$。<br>（2）图注意力网络引入了<strong>自注意力机制</strong>，利用当前节点的特征以及其邻居节点的特征计算邻居节点的重要性，把该重要性作为新的邻接矩阵进行卷积计算。<br>（3）有势：利用节点特征的相似性更能反映邻接信息。<br><img src="/images/图深度表示/GAT1.png" width="80%"></p>
<p><img src="/images/图深度表示/GAT2.png" width="50%"><br><img src="/images/图深度表示/GAT3.png" width="40%"></p>
<h1 id="图学习面临的挑战"><a href="#图学习面临的挑战" class="headerlink" title="图学习面临的挑战"></a>图学习面临的挑战</h1><h2 id="如何将图神经网络模型做到更大的图上（如何做大）？"><a href="#如何将图神经网络模型做到更大的图上（如何做大）？" class="headerlink" title="如何将图神经网络模型做到更大的图上（如何做大）？"></a>如何将图神经网络模型做到更大的图上（如何做大）？</h2><p>因为$\tilde{D}^{-\frac{1}{2}}\tilde{A}\tilde{D}^{-\frac{1}{2}}$中邻接矩阵用到所有节点，难以处理超大图。即<strong>对所有邻接节点进行聚合并不高效</strong>。</p>
<p><strong>思路</strong>：采样，采用一部分点/边来进行运算。<br><strong>FastGCN</strong>：<br>（1）把图节点特征看作有一个隐含概率分布产生，利用该分布对每一层的所有节点<strong>整体采样</strong>，避免了采样点个数的指数增加。<br>（2）采样的目标是尽量减少采样的方差-&gt;基于节点degree的采样。<br>（3）<strong>缺点</strong>：没有考虑层间点和点的关系。</p>
<p>为了克服这个缺点出现了层间采样的方法：<br><strong>ASGCN</strong>：FastGCN采样方式并不合理，在图极大而采样比例极少时，层间连接会急剧减少。<br>（1）自顶向下Layer-dependent的采样方式。<br>（2）在控制每层采样个数的同时，确保上下两层之间的连接是密集。<br>（3）通过公式证明了可以保证采样无偏和减小采样方差。<br>（4）扩展：加入了残差连接，能考虑二阶邻居的信息传播。在采样设置下，实现了注意力机制。</p>
<h2 id="如何有效训练更复杂的图神经网络模型（如何做深）？"><a href="#如何有效训练更复杂的图神经网络模型（如何做深）？" class="headerlink" title="如何有效训练更复杂的图神经网络模型（如何做深）？"></a>如何有效训练更复杂的图神经网络模型（如何做深）？</h2><p>为什么不能做深？<br>（1）过拟合（Overfitting）：参数数量过多造成的泛化性降低。<br>（2）<strong>过平滑</strong>（Over-Smoothing）：<strong>多层的邻居聚合造成的特征均化</strong>。</p>
<p>Over-Smoothing的定义：经过L层特征聚合后特征收敛到一个和输入特征无关的子空间M的现象。<br><img src="/images/图深度表示/Dropedge2.png" width="20%"></p>
<p><strong>挑战</strong>：如何减弱Over-Smoothing？<br><strong>DropEdge</strong>：<strong>在每个epoch训练前，随机丢掉一定比例的边</strong>。<br><img src="/images/图深度表示/Dropedge.png" width="20%"></p>
<p>为什么DropEdge可以减弱Over-Smoothing？<br>（1）<strong>DropEdge可以减缓收敛到子空间M的速度</strong>。<img src="/images/图深度表示/Dropedge3.png" width="20%"><br>（2）<strong>DropEdge可以减少收敛过程中的信息损失</strong>。<img src="/images/图深度表示/Dropedge4.png" width="30%"></p>
<p>由此通过减弱Over-Smoothing的影响，可以使我们可以成功在更复杂更深层的图神经网络上进行训练，并且提升精度。</p>
<h1 id="应用"><a href="#应用" class="headerlink" title="应用"></a>应用</h1><p>药物属性预测和可解释性问题：DualMPNN。所有属性预测数据上大幅超越SOTA算法并提供了模型的可解释性。<br>复杂层次图学习问题：SEAL算法。基于复杂层次图结构的GNN模型。应用于安全场景中的群分类任务。<br>社交网络谣言检测：Bi-GCN。首创双向GCN结构并将其应用于谣言检测问题。<br>统一黑盒攻击框架：GF-Attacker。首个可以对于多种图模型进行黑盒攻击的攻击框架。</p>
<h2 id="SEAL"><a href="#SEAL" class="headerlink" title="SEAL"></a>SEAL</h2><p><strong>背景</strong>：在实际数据中，图相互之间的关系可以建模成图，即层级图结构。比如QQ群和QQ群的关系、学术论文引用（不同领域间的引用构成层次图，领域内的文章引用构成实例图）。<br><img src="/images/图深度表示/SEAL.png" width="70%"></p>
<p><strong>问题</strong>：如何预测实例图的分类标签？<br><strong>挑战</strong>：<br>（1）<strong>如何利用统一长度的向量来表示具有不同大小的实例图</strong>？</p>
<ul>
<li>在不同层级下学习图的表示：<ul>
<li>节点层级：$G(V,E) -&gt; H^{n×v}$</li>
<li>层图级：$G(V,E) -&gt; \it e^{v}$</li>
</ul>
</li>
<li>自注意力图表示学习（Self-Attentive Graph Embedding）<ul>
<li>图大小不变性————自注意力机制（$\it e \in R^{r×v}$）</li>
<li>节点重要性——————自注意力机制</li>
<li>排列不变性——————GCN Smoothing</li>
</ul>
</li>
</ul>
<p><img src="/images/图深度表示/SEAL2.png" width="80%"></p>
<p>（2）<strong>如何在不同层级去融合实例图和层次图的信息</strong>？</p>
<ul>
<li><strong>实例图层次</strong>（Instance Classifier）：Graph Level Learning （SEGA）。</li>
<li><strong>层次图层次</strong>（Hierarchical Classifier）：Node Level Learning（GCN）。</li>
<li><strong>特征共享</strong>：将实例图的输出作为层次图模型的输入。<br><img src="/images/图深度表示/SEAL3.png" width="90%"></li>
</ul>
<h1 id="时间线"><a href="#时间线" class="headerlink" title="时间线"></a>时间线</h1><p><img src="/images/图深度表示/GNN.png" width="70%"></p>
<h1 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h1><p><span class="exturl" data-url="aHR0cHM6Ly96aC53aWtpcGVkaWEub3JnL3dpa2kvJUU1JTlCJUJFXyglRTYlOTUlQjAlRTUlQUQlQTY=">图 (数学)<i class="fa fa-external-link-alt"></i></span>)<br><span class="exturl" data-url="aHR0cHM6Ly96aC53aWtpcGVkaWEub3JnL3dpa2kvJUU1JTlCJUJFJUU4JUFFJUJB">图论<i class="fa fa-external-link-alt"></i></span><br><span class="exturl" data-url="aHR0cHM6Ly96aC53aWtpcGVkaWEub3JnL3dpa2kvJUU5JTgyJUJCJUU2JThFJUE1JUU3JTlGJUE5JUU5JTk4JUI1">邻接矩阵<i class="fa fa-external-link-alt"></i></span><br><span class="exturl" data-url="aHR0cHM6Ly93d3cuYmlsaWJpbGkuY29tL3ZpZGVvL2F2ODM1MTk3NjU/ZnJvbT1zZWFyY2gmYW1wO3NlaWQ9NDIxMTQxNDU5NzU0ODIzOTY3Ng==">图深度表示（GNN）的基础和前沿进展<i class="fa fa-external-link-alt"></i></span><br><span class="exturl" data-url="aHR0cHM6Ly93d3cuY25ibG9ncy5jb20vbnhmLXJhYmJpdDc1L3AvMTEzMDYxOTguaHRtbA==">GCN总结<i class="fa fa-external-link-alt"></i></span></p>
<p><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzE0MDMuNjY1Mi5wZGYlQzMlQUYlQzIlQkMlRTIlODAlQkE=">DeepWalk: Online Learning of Social Representations<i class="fa fa-external-link-alt"></i></span> DeepWalk 2014年<br><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzE1MDMuMDM1NzgucGRmJUMyJUEwJUUzJTgwJTkwV1dX">LINE: Large-scale Information Network Embedding<i class="fa fa-external-link-alt"></i></span>  LINE 2015年<br><span class="exturl" data-url="aHR0cHM6Ly93d3ctY3MtZmFjdWx0eS5zdGFuZm9yZC5lZHUvcGVvcGxlL2p1cmUvcHVicy9ub2RlMnZlYy1rZGQxNi5wZGY=">node2vec: Scalable Feature Learning for Networks<i class="fa fa-external-link-alt"></i></span>  node2vec 2016年<br><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzE3MDQuMDMxNjUucGRm">struc2vec: Learning Node Representations from Structural Identity<i class="fa fa-external-link-alt"></i></span>  struc2vec 2017年<br><span class="exturl" data-url="aHR0cHM6Ly9kbC5hY20ub3JnL2RvaS9wZGYvMTAuMTE0NS8zMDk3OTgzLjMwOTgwMzY=">metapath2vec: Scalable Representation Learning for Heterogeneous Networks<i class="fa fa-external-link-alt"></i></span>  metapath2vec 2017年</p>
<p><span class="exturl" data-url="aHR0cHM6Ly9vcGVucmV2aWV3Lm5ldC9wZGY/aWQ9U0pVNGF5WWds">SEMI-SUPERVISED CLASSIFICATION WITH GRAPH CONVOLUTIONAL NETWORKS<i class="fa fa-external-link-alt"></i></span> GCN 2017年<br><span class="exturl" data-url="aHR0cHM6Ly9wcm9jZWVkaW5ncy5uZXVyaXBzLmNjL3BhcGVyLzIwMTcvZmlsZS81ZGQ5ZGI1ZTAzM2RhOWM2ZmI1YmE4M2M3YTdlYmVhOS1QYXBlci5wZGY=">Inductive Representation Learning on Large Graphs<i class="fa fa-external-link-alt"></i></span> GraphSAGE 2017年<br><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzE3MTAuMTA5MDMucGRm">GRAPH ATTENTION NETWORKS<i class="fa fa-external-link-alt"></i></span> GAT 2018年</p>
<p><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzE4MDEuMTAyNDcucGRm">FASTGCN: FAST LEARNING WITH GRAPH CONVOLU TIONAL NETWORKS VIA IMPORTANCE SAMPLING<i class="fa fa-external-link-alt"></i></span> FASTGCN 2018年<br><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzE4MDkuMDUzNDMucGRm">Adaptive Sampling Towards Fast Graph Representation Learning<i class="fa fa-external-link-alt"></i></span>  ASGCN 2018年</p>
<p><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzE5MDcuMTA5MDMucGRm">DROPEDGE: TOWARDS DEEP GRAPH CONVOLU TIONAL NETWORKS ON NODE CLASSIFICATION<i class="fa fa-external-link-alt"></i></span> DROPEDGE 2020年<br><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzE5MDQuMDUwMDMucGRm">Semi-Supervised Graph Classification: A Hierarchical Graph Perspective<i class="fa fa-external-link-alt"></i></span> SEAL 2019年</p>

      
    </div>

    
    
    
      


    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://soundmemories.github.io/2020/10/17/Machine%20Learning/40.cheatsheet/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="https://i.loli.net/2020/11/04/6JhNuwtBe4adylS.png">
      <meta itemprop="name" content="SoundMemories">
      <meta itemprop="description" content="今日事，今日毕">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="SoundMemories">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/10/17/Machine%20Learning/40.cheatsheet/" class="post-title-link" itemprop="url">cheatsheet</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2020-10-17 00:00:00" itemprop="dateCreated datePublished" datetime="2020-10-17T00:00:00+08:00">2020-10-17</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Machine-Learning/" itemprop="url" rel="index"><span itemprop="name">Machine Learning</span></a>
        </span>
    </span>

  
      </div>
      <div class="post-meta">
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>11k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>10 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><h2 id="Math"><a href="#Math" class="headerlink" title="Math"></a>Math</h2><ol>
<li>MLE<script type="math/tex; mode=display">
\theta_{MLE}=\mathop{argmax}\limits _{\theta}\log p(X|\theta)\mathop{=}\limits _{iid}\mathop{argmax}\limits _{\theta}\sum\limits _{i=1}^{N}\log p(x_{i}|\theta)</script></li>
</ol>
<ol>
<li><p>MAP</p>
<script type="math/tex; mode=display">
\theta_{MAP}=\mathop{argmax}\limits _{\theta}p(\theta|X)=\mathop{argmax}\limits _{\theta}p(X|\theta)\cdot p(\theta)</script></li>
<li><p>Gaussian Distribution</p>
<script type="math/tex; mode=display">
\begin{align}&p(x|\mu,\Sigma)=\frac{1}{(2\pi)^{p/2}|\Sigma|^{1/2}}e^{-\frac{1}{2}(x-\mu)^{T}\Sigma^{-1}(x-\mu)}\\
&\Delta=(x-\mu)^{T}\Sigma^{-1}(x-\mu)=\sum\limits _{i=1}^{p}(x-\mu)^{T}u_{i}\frac{1}{\lambda_{i}}u_{i}^{T}(x-\mu)=\sum\limits _{i=1}^{p}\frac{y_{i}^{2}}{\lambda_{i}}
\end{align}</script></li>
<li><p>已知 $x\sim\mathcal{N}(\mu,\Sigma), y\sim Ax+b$，有：</p>
<script type="math/tex; mode=display">
\begin{align}y\sim\mathcal{N}(A\mu+b, A\Sigma A^T)
\end{align}</script></li>
<li><p>记 $x=(x_1, x_2,\cdots,x_p)^T=(x_{a,m\times 1}, x_{b,n\times1})^T,\mu=(\mu_{a,m\times1}, \mu_{b,n\times1}),\Sigma=\begin{pmatrix}\Sigma_{aa}&amp;\Sigma_{ab}\\\Sigma_{ba}&amp;\Sigma_{bb}\end{pmatrix}$，已知 $x\sim\mathcal{N}(\mu,\Sigma)$，则：</p>
<script type="math/tex; mode=display">
\begin{align}&x_a\sim\mathcal{N}(\mu_a,\Sigma_{aa})\\
&x_b|x_a\sim\mathcal{N}(\mu_{b|a},\Sigma_{b|a})\\
&\mu_{b|a}=\Sigma_{ba}\Sigma_{aa}^{-1}(x_a-\mu_a)+\mu_b\\
&\Sigma_{b|a}=\Sigma_{bb}-\Sigma_{ba}\Sigma_{aa}^{-1}\Sigma_{ab}
\end{align}</script></li>
</ol>
<h2 id="Linear-Regression"><a href="#Linear-Regression" class="headerlink" title="Linear Regression"></a>Linear Regression</h2><h3 id="Model"><a href="#Model" class="headerlink" title="Model"></a>Model</h3><ol>
<li><p>Dataset: </p>
<script type="math/tex; mode=display">
\mathcal{D}=\{(x_1, y_1),(x_2, y_2),\cdots,(x_N, y_N)\}</script></li>
<li><p>Notation:</p>
<script type="math/tex; mode=display">
X=(x_1,x_2,\cdots,x_N)^T,Y=(y_1,y_2,\cdots,y_N)^T</script></li>
<li><p>Model:</p>
<script type="math/tex; mode=display">
f(w)=w^Tx</script></li>
</ol>
<h3 id="Loss-Function"><a href="#Loss-Function" class="headerlink" title="Loss Function"></a>Loss Function</h3><ol>
<li>最小二乘误差/高斯噪声的MLE<script type="math/tex; mode=display">
L(w)=\sum\limits_{i=1}^N||w^Tx_i-y_i||^2_2</script></li>
</ol>
<h3 id="闭式解"><a href="#闭式解" class="headerlink" title="闭式解"></a>闭式解</h3><script type="math/tex; mode=display">
\begin{align}\hat{w}=(X^TX)^{-1}X^TY=X^+Y\\
X=U\Sigma V^T\\
X^+=V\Sigma^{-1}U^T
\end{align}</script><h3 id="正则化"><a href="#正则化" class="headerlink" title="正则化"></a>正则化</h3><script type="math/tex; mode=display">
\begin{align}
L1-Gaussian \ priori&:\mathop{argmin}\limits_wL(w)+\lambda||w||_1,\lambda\gt0\\
L2-Laplasian\ priori-Sparsity&:\mathop{argmin}\limits_wL(w)+\lambda||w||^2_2,\lambda \gt 0
\end{align}</script><h2 id="Linear-Classification"><a href="#Linear-Classification" class="headerlink" title="Linear Classification"></a>Linear Classification</h2><h3 id="Hard"><a href="#Hard" class="headerlink" title="Hard"></a>Hard</h3><h4 id="PCA"><a href="#PCA" class="headerlink" title="PCA"></a>PCA</h4><ol>
<li><p>Idea: 在线性模型上加入激活函数</p>
</li>
<li><p>Loss Function:</p>
</li>
</ol>
<script type="math/tex; mode=display">
L(w)=\sum\limits_{x_i\in\mathcal{D}_{wrong}}-y_iw^Tx_i</script><ol>
<li>Parameters:</li>
</ol>
<script type="math/tex; mode=display">
w^{t+1}\leftarrow w^{t}+\lambda y_ix_i</script><h4 id="Fisher"><a href="#Fisher" class="headerlink" title="Fisher"></a>Fisher</h4><ol>
<li><p>Idea: 投影，类内小，类间大。</p>
</li>
<li><p>Loss Function:</p>
<script type="math/tex; mode=display">
\begin{align}&J(w)=\frac{w^TS_bw}{w^TS_ww}\\
&S_b=(\overline{x_{c1}}-\overline{x_{c2}})(\overline{x_{c1}}-\overline{x_{c2}})^T\\
&S_w=S_1+S_2
\end{align}</script></li>
<li><p>闭式解，投影方向:</p>
<script type="math/tex; mode=display">
S_w^{-1}(\overline{x_{c1}}-\overline{x_{c2}})</script></li>
</ol>
<h3 id="Soft"><a href="#Soft" class="headerlink" title="Soft"></a>Soft</h3><h4 id="判别模型"><a href="#判别模型" class="headerlink" title="判别模型"></a>判别模型</h4><h5 id="Logistic-Regression"><a href="#Logistic-Regression" class="headerlink" title="Logistic Regression"></a>Logistic Regression</h5><ol>
<li><p>Idea，激活函数:</p>
<script type="math/tex; mode=display">
\begin{align}p(C_1|x)&=\frac{1}{1+\exp(-a)}\\
a&=w^Tx
\end{align}</script></li>
<li><p>Loss Function(交叉熵):</p>
<script type="math/tex; mode=display">
\hat{w}=\mathop{argmax}_wJ(w)=\mathop{argmax}_w\sum\limits_{i=1}^N(y_i\log p_1+(1-y_i)\log p_0)</script></li>
<li><p>解法，SGD</p>
<script type="math/tex; mode=display">
J'(w)=\sum\limits_{i=1}^N(y_i-p_1)x_i</script></li>
</ol>
<h4 id="生成模型"><a href="#生成模型" class="headerlink" title="生成模型"></a>生成模型</h4><h5 id="GDA"><a href="#GDA" class="headerlink" title="GDA"></a>GDA</h5><ol>
<li><p>Model</p>
<ol>
<li>$y\sim Bernoulli(\phi)$</li>
<li>$x|y=1\sim\mathcal{N}(\mu_1,\Sigma)$</li>
<li>$x|y=0\sim\mathcal{N}(\mu_0,\Sigma)$</li>
</ol>
</li>
<li><p>MAP</p>
<script type="math/tex; mode=display">
\begin{align}
&\mathop{argmax}_{\phi,\mu_0,\mu_1,\Sigma}\log p(X|Y)p(Y)\nonumber\\
&=\mathop{argmax}_{\phi,\mu_0,\mu_1,\Sigma}\sum\limits_{i=1}^N((1-y_i)\log\mathcal{N}(\mu_0,\Sigma)+y_i\log \mathcal{N}(\mu_1,\Sigma)+y_i\log\phi+(1-y_i)\log(1-\phi))
\end{align}</script></li>
<li><p>解</p>
<script type="math/tex; mode=display">
\begin{align}\phi&=\frac{N_1}{N}\\
\mu_1&=\frac{\sum\limits_{i=1}^Ny_ix_i}{N_1}\\
\mu_0&=\frac{\sum\limits_{i=1}^N(1-y_i)x_i}{N_0}\\
\Sigma&=\frac{N_1S_1+N_2S_2}{N}
\end{align}</script></li>
</ol>
<h5 id="Naive-Bayesian"><a href="#Naive-Bayesian" class="headerlink" title="Naive Bayesian"></a>Naive Bayesian</h5><ol>
<li><p>Model, 对单个数据点的各个维度作出限制</p>
<script type="math/tex; mode=display">
x_i\perp x_j|y,\forall\  i\ne j</script><ol>
<li>$x_i$ 为连续变量：$p(x_i|y)=\mathcal{N}(\mu_i,\sigma_i^2)$</li>
<li>$x_i$ 为离散变量：类别分布（Categorical）：$p(x_i=i|y)=\theta_i,\sum\limits_{i=1}^K\theta_i=1$</li>
<li>$p(y)=\phi^y(1-\phi)^{1-y}$</li>
</ol>
</li>
<li><p>解：和GDA相同</p>
</li>
</ol>
<h2 id="Dimension-Reduction"><a href="#Dimension-Reduction" class="headerlink" title="Dimension Reduction"></a>Dimension Reduction</h2><p>中心化：</p>
<script type="math/tex; mode=display">
\begin{align}S
&=\frac{1}{N}X^T(E_N-\frac{1}{N}\mathbb{I}_{N1}\mathbb{I}_{1N})(E_N-\frac{1}{N}\mathbb{I}_{N1}\mathbb{I}_{1N})^TX\nonumber\\
&=\frac{1}{N}X^TH^2X=\frac{1}{N}X^THX
\end{align}</script><h3 id="PCA-1"><a href="#PCA-1" class="headerlink" title="PCA"></a>PCA</h3><ol>
<li><p>Idea: 坐标变换，寻找线性无关的新基矢，取信息损失最小的前几个维度</p>
</li>
<li><p>Loss Function:</p>
<script type="math/tex; mode=display">
\begin{align}J
&=\sum\limits_{j=1}^qu_j^TSu_j\ ,\ s.t.\ u_j^Tu_j=1
\end{align}</script></li>
<li><p>解：</p>
<ol>
<li><p>特征分解法</p>
<script type="math/tex; mode=display">
S=U\Lambda U^T</script></li>
<li><p>SVD for X/S</p>
<script type="math/tex; mode=display">
\begin{align}HX=U\Sigma V^T\\
S=\frac{1}{N}V\Sigma^T\Sigma V^T
\\new\ co=HX\cdot V\end{align}</script></li>
<li><p>SVD for T</p>
<script type="math/tex; mode=display">
\begin{align}T=HXX^TH=U\Sigma\Sigma^TU^T\\
new\ co=U\Sigma
\end{align}</script></li>
</ol>
</li>
</ol>
<h3 id="p-PCA"><a href="#p-PCA" class="headerlink" title="p-PCA"></a>p-PCA</h3><ol>
<li><p>Model:</p>
<script type="math/tex; mode=display">
\begin{align}
z&\sim\mathcal{N}(\mathbb{O}_{q1},\mathbb{I}_{qq})\\
x&=Wz+\mu+\varepsilon\\
\varepsilon&\sim\mathcal{N}(0,\sigma^2\mathbb{I}_{pp})
\end{align}</script></li>
<li><p>Learning: E-M</p>
</li>
<li><p>Inference:</p>
<script type="math/tex; mode=display">
p(z|x)=\mathcal{N}(W^T(WW^T+\sigma^2\mathbb{I})^{-1}(x-\mu),\mathbb{I}-W^T(WW^T+\sigma^2\mathbb{I})^{-1}W)</script></li>
</ol>
<h2 id="SVM"><a href="#SVM" class="headerlink" title="SVM"></a>SVM</h2><ol>
<li>强对偶关系：凸优化+（松弛）Slater 条件-&gt;强对偶。</li>
<li>参数求解：KKT条件<ol>
<li>可行域</li>
<li>互补松弛+梯度为0</li>
</ol>
</li>
</ol>
<h3 id="Hard-margin"><a href="#Hard-margin" class="headerlink" title="Hard-margin"></a>Hard-margin</h3><ol>
<li><p>Idea: 最大化间隔</p>
</li>
<li><p>Model:</p>
<script type="math/tex; mode=display">
\mathop{argmin}_{w,b}\frac{1}{2}w^Tw\ s.t.\ y_i(w^Tx_i+b)\ge1,i=1,2,\cdots,N</script></li>
<li><p>对偶问题</p>
<script type="math/tex; mode=display">
\max_{\lambda}-\frac{1}{2}\sum\limits_{i=1}^N\sum\limits_{j=1}^N\lambda_i\lambda_jy_iy_jx_i^Tx_j+\sum\limits_{i=1}^N\lambda_i,\ s.t.\ \lambda_i\ge0</script></li>
<li><p>模型参数</p>
<script type="math/tex; mode=display">
\hat{w}=\sum\limits_{i=1}^N\lambda_iy_ix_i\\
\hat{b}=y_k-w^Tx_k=y_k-\sum\limits_{i=1}^N\lambda_iy_ix_i^Tx_k,\exists k,1-y_k(w^Tx_k+b)=0</script></li>
</ol>
<h3 id="Soft-margin"><a href="#Soft-margin" class="headerlink" title="Soft-margin"></a>Soft-margin</h3><ol>
<li><p>Idea:允许少量错误</p>
</li>
<li><p>Model:</p>
<script type="math/tex; mode=display">
error=\sum\limits_{i=1}^N\max\{0,1-y_i(w^Tx_i+b)\}\\
\mathop{argmin}_{w,b}\frac{1}{2}w^Tw+C\sum\limits_{i=1}^N\xi_i\ s.t.\ y_i(w^Tx_i+b)\ge1-\xi_i,\xi_i\ge0,i=1,2,\cdots,N</script></li>
</ol>
<h3 id="Kernel"><a href="#Kernel" class="headerlink" title="Kernel"></a>Kernel</h3><p>对称的正定函数都可以作为正定核。</p>
<h2 id="Exp-Family"><a href="#Exp-Family" class="headerlink" title="Exp Family"></a>Exp Family</h2><ol>
<li><p>表达式</p>
<script type="math/tex; mode=display">
p(x|\eta)=h(x)\exp(\eta^T\phi(x)-A(\eta))=\frac{1}{\exp(A(\eta))}h(x)\exp(\eta^T\phi(x))</script></li>
<li><p>对数配分函数</p>
<script type="math/tex; mode=display">
\begin{align} 
A'(\eta)=\mathbb{E}_{p(x|\eta)}[\phi(x)]\\
A''(\eta)=Var_{p(x|\eta)}[\phi(x)]
\end{align}</script></li>
<li><p>指数族分布满足最大熵定理</p>
</li>
</ol>
<h2 id="PGM"><a href="#PGM" class="headerlink" title="PGM"></a>PGM</h2><h3 id="Representation"><a href="#Representation" class="headerlink" title="Representation"></a>Representation</h3><ol>
<li>有向图<script type="math/tex; mode=display">
p(x_1,x_2,\cdots,x_p)=\prod\limits_{i=1}^pp(x_i|x_{parent(i)})</script>D-separation<script type="math/tex; mode=display">
p(x_i|x_{-i})=\frac{p(x)}{\int p(x)dx_{i}}=\frac{\prod\limits_{j=1}^pp(x_j|x_{parents(j)})}{\int\prod\limits_{j=1}^pp(x_j|x_{parents(j)})dx_i}=\frac{p(x_i|x_{parents(i)})p(x_{child(i)}|x_i)}{\int p(x_i|x_{parents(i)})p(x_{child(i)}|x_i)dx_i}</script></li>
</ol>
<ol>
<li><p>无向图</p>
<script type="math/tex; mode=display">
\begin{align}p(x)=\frac{1}{Z}\prod\limits_{i=1}^{K}\phi(x_{ci})\\
Z=\sum\limits_{x\in\mathcal{X}}\prod\limits_{i=1}^{K}\phi(x_{ci})\\
\phi(x_{ci})=\exp(-E(x_{ci}))
\end{align}</script></li>
<li><p>有向转无向</p>
<ol>
<li>将每个节点的父节点两两相连</li>
<li>将有向边替换为无向边</li>
</ol>
</li>
</ol>
<h3 id="Learning"><a href="#Learning" class="headerlink" title="Learning"></a>Learning</h3><p>参数学习-EM</p>
<ol>
<li><p>目的：解决具有隐变量的混合模型的参数估计（极大似然估计）</p>
</li>
<li><p>参数：</p>
<script type="math/tex; mode=display">
\theta_{MLE}=\mathop{argmax}\limits_\theta\log p(x|\theta)</script></li>
</ol>
<ol>
<li><p>迭代求解：</p>
<script type="math/tex; mode=display">
\theta^{t+1}=\mathop{argmax}\limits_{\theta}\int_z\log [p(x,z|\theta)]p(z|x,\theta^t)dz=\mathbb{E}_{z|x,\theta^t}[\log p(x,z|\theta)]</script></li>
<li><p>原理</p>
<script type="math/tex; mode=display">
\log p(x|\theta^t)\le\log p(x|\theta^{t+1})</script></li>
<li><p>广义EM</p>
<ol>
<li><p>E step：</p>
<script type="math/tex; mode=display">
\hat{q}^{t+1}(z)=\mathop{argmax}_q\int_zq^t(z)\log\frac{p(x,z|\theta)}{q^t(z)}dz,fixed\ \theta</script></li>
<li><p>M step：</p>
<script type="math/tex; mode=display">
\hat{\theta}=\mathop{argmax}_\theta \int_zq^{t+1}(z)\log\frac{p(x,z|\theta)}{q^{t+1}(z)}dz,fixed\ \hat{q}</script></li>
</ol>
</li>
</ol>
<h3 id="Inference"><a href="#Inference" class="headerlink" title="Inference"></a>Inference</h3><ol>
<li><p>精确推断</p>
<ol>
<li><p>VE</p>
</li>
<li><p>BP</p>
<script type="math/tex; mode=display">
m_{j\to i}(i)=\sum\limits_j\phi_j(j)\phi_{ij}(ij)\prod\limits_{k\in Neighbour(j)-i}m_{k\to j}(j)</script></li>
<li><p>MP</p>
<script type="math/tex; mode=display">
m_{j\to i}=\max\limits_{j}\phi_j\phi_{ij}\prod\limits_{k\in Neighbour(j)-i}m_{k\to j}</script></li>
</ol>
</li>
<li><p>近似推断</p>
<ol>
<li><p>确定性近似，VI</p>
<ol>
<li><p>变分表达式</p>
<script type="math/tex; mode=display">
\hat{q}(Z)=\mathop{argmax}_{q(Z)}L(q)</script></li>
<li><p>平均场近似下的 VI-坐标上升</p>
<script type="math/tex; mode=display">
\mathbb{E}_{\prod\limits_{i\ne j}q_i(Z_i)}[\log p(X,Z)]=\log \hat{p}(X,Z_j)\\
q_j(Z_j)=\hat{p}(X,Z_j)</script></li>
<li><p>SGVI-变成优化问题，重参数法</p>
<script type="math/tex; mode=display">
\begin{aligned}
\mathop{argmax}_{q(Z)}L(q)=\mathop{argmax}_{\phi}L(\phi)\\
\nabla_\phi L(\phi)=\mathbb{E}_{q_\phi}[(\nabla_\phi\log q_\phi)(\log p_\theta(x^i,z)-\log q_\phi(z))]\\
=\mathbb{E}_{p(\varepsilon)}[\nabla_z[\log p_\theta(x^i,z)-\log q_\phi(z)]\nabla_\phi g_\phi(\varepsilon,x^i)]\\
z=g_\phi(\varepsilon,x^i),\varepsilon\sim p(\varepsilon)
\end{aligned}</script></li>
</ol>
</li>
<li><p>随机性近似</p>
<ol>
<li><p>蒙特卡洛方法采样</p>
<ol>
<li><p>CDF 采样</p>
</li>
<li><p>拒绝采样， $q(z)$，使得 $\forall z_i,Mq(z_i)\ge p(z_i)$，拒绝因子：$\alpha=\frac{p(z^i)}{Mq(z^i)}\le1$</p>
</li>
<li><p>重要性采样</p>
<script type="math/tex; mode=display">
\mathbb{E}_{p(z)}[f(z)]=\int p(z)f(z)dz=\int \frac{p(z)}{q(z)}f(z)q(z)dz\simeq\frac{1}{N}\sum\limits_{i=1}^Nf(z_i)\frac{p(z_i)}{q(z_i)}</script></li>
<li><p>重要性重采样：重要性采样+重采样</p>
</li>
</ol>
</li>
<li><p>MCMC：构建马尔可夫链概率序列，使其收敛到平稳分布 $p(z)$。</p>
<ol>
<li><p>转移矩阵（提议分布）</p>
<script type="math/tex; mode=display">
p(z)\cdot Q_{z\to z^*}\alpha(z,z^*)=p(z^*)\cdot Q_{z^*\to z}\alpha(z^*,z)\\
\alpha(z,z^*)=\min\{1,\frac{p(z^*)Q_{z^*\to z}}{p(z)Q_{z\to z^*}}\}</script></li>
<li><p>算法（MH）：</p>
<ol>
<li>通过在0，1之间均匀分布取点 $u$</li>
<li>生成 $z^<em>\sim Q(z^</em>|z^{i-1})$</li>
<li>计算 $\alpha$ 值</li>
<li>如果 $\alpha\ge u$，则 $z^i=z^*$，否则 $z^{i}=z^{i-1}$</li>
</ol>
</li>
</ol>
</li>
<li><p>Gibbs 采样：给定初始值 $z_1^0,z_2^0,\cdots$在 $t+1$ 时刻，采样 $z_i^{t+1}\sim p(z_i|z_{-i})$，从第一个维度一个个采样。</p>
</li>
</ol>
</li>
</ol>
</li>
</ol>
<h2 id="GMM"><a href="#GMM" class="headerlink" title="GMM"></a>GMM</h2><ol>
<li><p>Model</p>
<script type="math/tex; mode=display">
p(x)=\sum\limits_{k=1}^Kp_k\mathcal{N}(x|\mu_k,\Sigma_k)</script></li>
<li><p>求解-EM</p>
<script type="math/tex; mode=display">
\begin{align}Q(\theta,\theta^t)&=\sum\limits_z[\log\prod\limits_{i=1}^Np(x_i,z_i|\theta)]\prod \limits_{i=1}^Np(z_i|x_i,\theta^t)\nonumber\\
&=\sum\limits_z[\sum\limits_{i=1}^N\log p(x_i,z_i|\theta)]\prod \limits_{i=1}^Np(z_i|x_i,\theta^t)\nonumber\\
&=\sum\limits_{i=1}^N\sum\limits_{z_i}\log p(x_i,z_i|\theta)p(z_i|x_i,\theta^t)\nonumber\\
&=\sum\limits_{i=1}^N\sum\limits_{z_i}\log p_{z_i}\mathcal{N(x_i|\mu_{z_i},\Sigma_{z_i})}\frac{p_{z_i}^t\mathcal{N}(x_i|\mu_{z_i}^t,\Sigma_{z_i}^t)}{\sum\limits_kp_k^t\mathcal{N}(x_i|\mu_k^t,\Sigma_k^t)}
\end{align}</script><script type="math/tex; mode=display">
p_k^{t+1}=\frac{1}{N}\sum\limits_{i=1}^Np(z_i=k|x_i,\theta^t)</script></li>
</ol>
<h2 id="序列模型-HMM，LDS，Particle"><a href="#序列模型-HMM，LDS，Particle" class="headerlink" title="序列模型-HMM，LDS，Particle"></a>序列模型-HMM，LDS，Particle</h2><ol>
<li><p>假设：</p>
<ol>
<li><p>齐次 Markov 假设（未来只依赖于当前）：</p>
<script type="math/tex; mode=display">
p(i_{t+1}|i_t,i_{t-1},\cdots,i_1,o_t,o_{t-1},\cdots,o_1)=p(i_{t+1}|i_t)</script></li>
<li><p>观测独立假设：</p>
<script type="math/tex; mode=display">
p(o_t|i_t,i_{t-1},\cdots,i_1,o_{t-1},\cdots,o_1)=p(o_t|i_t)</script></li>
</ol>
</li>
<li><p>参数</p>
<script type="math/tex; mode=display">
\lambda=(\pi,A,B)</script></li>
</ol>
<h3 id="离散线性隐变量-HMM"><a href="#离散线性隐变量-HMM" class="headerlink" title="离散线性隐变量-HMM"></a>离散线性隐变量-HMM</h3><ol>
<li><p>Evaluation：$p(O|\lambda)$，Forward-Backward 算法</p>
<script type="math/tex; mode=display">
p(O|\lambda)=\sum\limits_{i=1}^Np(O,i_T=q_i|\lambda)=\sum\limits_{i=1}^N\alpha_T(i)=\sum\limits_{i=1}^Nb_i(o_1)\pi_i\beta_1(i)\\
\alpha_{t+1}(j)=\sum\limits_{i=1}^Nb_{j}(o_t)a_{ij}\alpha_t(i)\\
\beta_t(i)=\sum\limits_{j=1}^Nb_j(o_{t+1})a_{ij}\beta_{t+1}(j)</script></li>
<li><p>Learning：$\lambda=\mathop{argmax}\limits_{\lambda}p(O|\lambda)$，EM 算法（Baum-Welch）</p>
<script type="math/tex; mode=display">
\lambda^{t+1}=\mathop{argmax}_\lambda\sum\limits_I\log p(O,I|\lambda)p(O,I|\lambda^t)\\=\sum\limits_I[\log \pi_{i_1}+\sum\limits_{t=2}^T\log a_{i_{t-1},i_t}+\sum\limits_{t=1}^T\log b_{i_t}(o_t)]p(O,I|\lambda^t)</script></li>
<li><p>Decoding：$I=\mathop{argmax}\limits_{I}p(I|O,\lambda)$，Viterbi 算法-动态规划</p>
<script type="math/tex; mode=display">
\delta_{t}(j)=\max\limits_{i_1,\cdots,i_{t-1}}p(o_1,\cdots,o_t,i_1,\cdots,i_{t-1},i_t=q_i)\\\delta_{t+1}(j)=\max\limits_{1\le i\le N}\delta_t(i)a_{ij}b_j(o_{t+1})\\\psi_{t+1}(j)=\mathop{argmax}\limits_{1\le i\le N}\delta_t(i)a_{ij}</script></li>
</ol>
<h3 id="连续线性隐变量-LDS"><a href="#连续线性隐变量-LDS" class="headerlink" title="连续线性隐变量-LDS"></a>连续线性隐变量-LDS</h3><ol>
<li><p>Model</p>
<script type="math/tex; mode=display">
\begin{align}
p(z_t|z_{t-1})&\sim\mathcal{N}(A\cdot z_{t-1}+B,Q)\\
p(x_t|z_t)&\sim\mathcal{N}(C\cdot z_t+D,R)\\
z_1&\sim\mathcal{N}(\mu_1,\Sigma_1)
\end{align}</script></li>
<li><p>滤波</p>
<script type="math/tex; mode=display">
p(z_t|x_{1:t})=p(x_{1:t},z_t)/p(x_{1:t})\propto p(x_{1:t},z_t)\\=p(x_t|z_t)p(z_t|x_{1:t-1})p(x_{1:t-1})\propto p(x_t|z_t)p(z_t|x_{1:t-1})</script></li>
<li><p>递推求解-线性高斯模型</p>
<ol>
<li><p>Prediction</p>
<script type="math/tex; mode=display">
p(z_t|x_{1:t-1})=\int_{z_{t-1}}p(z_t|z_{t-1})p(z_{t-1}|x_{1:t-1})dz_{t-1}=\int_{z_{t-1}}\mathcal{N}(Az_{t-1}+B,Q)\mathcal{N}(\mu_{t-1},\Sigma_{t-1})dz_{t-1}</script></li>
<li><p>Update:</p>
<script type="math/tex; mode=display">
p(z_t|x_{1:t})\propto p(x_t|z_t)p(z_t|x_{1:t-1}</script></li>
</ol>
</li>
</ol>
<h3 id="连续非线性隐变量-粒子滤波"><a href="#连续非线性隐变量-粒子滤波" class="headerlink" title="连续非线性隐变量-粒子滤波"></a>连续非线性隐变量-粒子滤波</h3><p>通过采样(SIR)解决：</p>
<script type="math/tex; mode=display">
\mathbb{E}[f(z)]=\int_zf(z)p(z)dz=\int_zf(z)\frac{p(z)}{q(z)}q(z)dz=\sum\limits_{i=1}^Nf(z_i)\frac{p(z_i)}{q(z_i)}</script><ol>
<li><p>采样</p>
<script type="math/tex; mode=display">
w_t^i\propto\frac{p(x_t|z_t)p(z_t|z_{t-1})}{q(z_t|z_{1:t-1},x_{1:t})}w_{t-1}^i\\
q(z_t|z_{1:t-1},x_{1:t})=p(z_t|z_{t-1})</script></li>
<li><p>重采样</p>
</li>
</ol>
<h2 id="CRF"><a href="#CRF" class="headerlink" title="CRF"></a>CRF</h2><ol>
<li><p>PDF</p>
<script type="math/tex; mode=display">
p(Y=y|X=x)=\frac{1}{Z(x,\theta)}\exp[\theta^TH(y_t,y_{t-1},x)]</script></li>
<li><p>边缘概率</p>
<script type="math/tex; mode=display">
\begin{aligned}
p(y_t=i|x)=\sum\limits_{y_{1:t-1}}\sum\limits_{y_{t+1:T}}\frac{1}{Z}\prod\limits_{t'=1}^T\phi_{t'}(y_{t'-1},y_{t'},x)\\
p(y_t=i|x)=\frac{1}{Z}\Delta_l\Delta_r\\
\Delta_l=\sum\limits_{y_{1:t-1}}\phi_{1}(y_0,y_1,x)\phi_2(y_1,y_2,x)\cdots\phi_{t-1}(y_{t-2},y_{t-1},x)\phi_t(y_{t-1},y_t=i,x)\\
\Delta_r=\sum\limits_{y_{t+1:T}}\phi_{t+1}(y_t=i,y_{t+1},x)\phi_{t+2}(y_{t+1},y_{t+2},x)\cdots\phi_T(y_{T-1},y_T,x)
\end{aligned}</script><script type="math/tex; mode=display">
\begin{aligned}
\alpha_t(i)=\Delta_l=\sum\limits_{j\in S}\phi_t(y_{t-1}=j,y_t=i,x)\alpha_{t-1}(j)\\
\Delta_r=\beta_t(i)=\sum\limits_{j\in S}\phi_{t+1}(y_t=i,y_{t+1}=j,x)\beta_{t+1}(j)
\end{aligned}</script></li>
<li><p>学习</p>
<script type="math/tex; mode=display">
\nabla_\lambda L=\sum\limits_{i=1}^N\sum\limits_{t=1}^T[f(y_{t-1},y_t,x^i)-\sum\limits_{y_{t-1}}\sum\limits_{y_t}p(y_{t-1},y_t|x^i)f(y_{t-1},y_t,x^i)]</script></li>
</ol>
<div class="note info"><p>文章转载自<span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL3NodWh1YWkwMDc=">Jie Zhou<i class="fa fa-external-link-alt"></i></span>的<span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL3NodWh1YWkwMDcvTWFjaGluZS1MZWFybmluZy1TZXNzaW9u">Machine-Learning-Session<i class="fa fa-external-link-alt"></i></span>。</p>
</div>
      
    </div>

    
    
    
      


    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://soundmemories.github.io/2020/10/16/Machine%20Learning/39.ApproInference/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="https://i.loli.net/2020/11/04/6JhNuwtBe4adylS.png">
      <meta itemprop="name" content="SoundMemories">
      <meta itemprop="description" content="今日事，今日毕">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="SoundMemories">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/10/16/Machine%20Learning/39.ApproInference/" class="post-title-link" itemprop="url">ApproInference</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2020-10-16 00:00:00" itemprop="dateCreated datePublished" datetime="2020-10-16T00:00:00+08:00">2020-10-16</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Machine-Learning/" itemprop="url" rel="index"><span itemprop="name">Machine Learning</span></a>
        </span>
    </span>

  
      </div>
      <div class="post-meta">
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>646</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>1 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="近似推断"><a href="#近似推断" class="headerlink" title="近似推断"></a>近似推断</h1><p>这一讲中的近似推断具体描述在深度生成模型中的近似推断。推断的目的有下面几个部分：</p>
<ol>
<li>推断本身，根据结果（观测）得到原因（隐变量）。</li>
<li>为参数的学习提供帮助。</li>
</ol>
<p>但是推断本身是一个困难的额任务，计算复杂度往往很高，对于无向图，由于节点之间的联系过多，那么因子分解很难进行，并且相互之间都有耦合，于是很难求解，仅仅在某些情况如 RBM 中可解，在有向图中，常常由于条件独立性问题，如两个节点之间条件相关（explain away），于是求解这些节点的条件概率就很困难，仅仅在某些概率假设情况下可解如高斯模型，于是需要近似推断。</p>
<p>事实上，我们常常讲推断问题变为优化问题，即：</p>
<script type="math/tex; mode=display">
Log-likehood:\sum\limits_{v\in V}\log p(v)</script><p>对上面这个问题，由于：</p>
<script type="math/tex; mode=display">
\log p(v)=\log\frac{p(v,h)}{p(h|v)}=\log\frac{p(v,h)}{q(h|v)}+\log\frac{q(h|v)}{p(h|v)}</script><p>左右两边对 $h$ 积分：</p>
<script type="math/tex; mode=display">
\int_h\log p(v)\cdot q(h|v)dh=\log p(v)</script><p>右边积分有：</p>
<script type="math/tex; mode=display">
\mathbb{E}_{q(h|v)}[\log\frac{p(v,h)}{q(h|v)}]+KL(q(h|v)||p(h|v))=\mathbb{E}_{q(h|v)}[\log p(v,h)]+H(q)+KL(q||p)</script><p>其中前两项是 ELBO，于是这就变成一个优化 ELBO 的问题。</p>
<div class="note info"><p>文章转载自<span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL3NodWh1YWkwMDc=">Jie Zhou<i class="fa fa-external-link-alt"></i></span>的<span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL3NodWh1YWkwMDcvTWFjaGluZS1MZWFybmluZy1TZXNzaW9u">Machine-Learning-Session<i class="fa fa-external-link-alt"></i></span>。</p>
</div>
      
    </div>

    
    
    
      


    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://soundmemories.github.io/2020/10/15/Machine%20Learning/37.NN/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="https://i.loli.net/2020/11/04/6JhNuwtBe4adylS.png">
      <meta itemprop="name" content="SoundMemories">
      <meta itemprop="description" content="今日事，今日毕">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="SoundMemories">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/10/15/Machine%20Learning/37.NN/" class="post-title-link" itemprop="url">NN</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2020-10-15 00:00:00" itemprop="dateCreated datePublished" datetime="2020-10-15T00:00:00+08:00">2020-10-15</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Machine-Learning/" itemprop="url" rel="index"><span itemprop="name">Machine Learning</span></a>
        </span>
    </span>

  
      </div>
      <div class="post-meta">
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>915</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>1 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="前馈神经网络"><a href="#前馈神经网络" class="headerlink" title="前馈神经网络"></a>前馈神经网络</h1><p>机器学习我们已经知道可以分为两大流派：</p>
<ol>
<li><p>频率派，这个流派的方法叫做统计学习，根据具体问题有下面的算法：</p>
<ol>
<li><p>正则化，L1，L2 等</p>
</li>
<li><p>核化，如核支撑向量机</p>
</li>
<li><p>集成化，AdaBoost，RandomForest</p>
</li>
<li><p>层次化，神经网络，神经网络有各种不同的模型，有代表性的有：</p>
<ol>
<li>多层感知机</li>
<li>Autoencoder</li>
<li>CNN</li>
<li>RNN</li>
</ol>
<p>这几种模型又叫做深度神经网络。</p>
</li>
</ol>
</li>
<li><p>贝叶斯派，这个流派的方法叫概率图模型，根据图特点分为：</p>
<ol>
<li>有向图-贝叶斯网络，加入层次化后有深度有向网络，包括<ol>
<li>Sigmoid Belief Network</li>
<li>Variational Autoencoder</li>
<li>GAN</li>
</ol>
</li>
<li>无向图-马尔可夫网络，加入层次化后有深度玻尔兹曼机。</li>
<li>混合，加入层次化后有深度信念网络</li>
</ol>
<p>这几个加入层次化后的模型叫做深度生成网络。</p>
</li>
</ol>
<p>从广义来说，深度学习包括深度生成网络和深度神经网络。</p>
<h2 id="From-PLA-to-DL"><a href="#From-PLA-to-DL" class="headerlink" title="From PLA to DL"></a>From PLA to DL</h2><ul>
<li>1958，PLA</li>
<li>1969，PLA 不能解决 XOR 等非线性数据</li>
<li>1981，MLP，多层感知机的出现解决了上面的问题</li>
<li>1986，BP 算法应用在 MLP 上，RNN</li>
<li>1989，CNN，Univeral Approximation Theorem，但是于此同时，由于深度和宽度的相对效率不知道，并且无法解决 BP 算法的梯度消失问题</li>
<li>1993，1995，SVM + kernel，AdaBoost，RandomForest，这些算法的发展，DL 逐渐没落</li>
<li>1997，LSTM</li>
<li>2006，基于 RBM 的 深度信念网络和深度自编码</li>
<li>2009，GPU的发展</li>
<li>2011，在语音方面的应用</li>
<li>2012，ImageNet</li>
<li>2013，VAE</li>
<li>2014，GAN</li>
<li>2016，AlphaGo</li>
<li>2018，GNN</li>
</ul>
<p>DL 不是一个新的东西，其近年来的大发展主要原因如下：</p>
<ol>
<li>数据量变大</li>
<li>分布式计算的发展</li>
<li>硬件算力的发展</li>
</ol>
<h2 id="非线性问题"><a href="#非线性问题" class="headerlink" title="非线性问题"></a>非线性问题</h2><p>对于非线性的问题，有三种方法：</p>
<ol>
<li>非线性转换，将低维空间转换到高维空间（Cover 定理），从而变为一个线性问题。</li>
<li>核方法，由于非线性转换是变换为高维空间，因此可能导致维度灾难，并且可能很难得到这个变换函数，核方法不直接寻找这个转换，而是寻找一个内积。</li>
<li>神经网络方法，将复合运算变为基本的线性运算的组合。</li>
</ol>
<div class="note info"><p>文章转载自<span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL3NodWh1YWkwMDc=">Jie Zhou<i class="fa fa-external-link-alt"></i></span>的<span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL3NodWh1YWkwMDcvTWFjaGluZS1MZWFybmluZy1TZXNzaW9u">Machine-Learning-Session<i class="fa fa-external-link-alt"></i></span>。</p>
</div>
      
    </div>

    
    
    
      


    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>





  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/"><i class="fa fa-angle-left" aria-label="上一页"></i></a><a class="page-number" href="/">1</a><span class="page-number current">2</span><a class="page-number" href="/page/3/">3</a><span class="space">&hellip;</span><a class="page-number" href="/page/12/">12</a><a class="extend next" rel="next" href="/page/3/"><i class="fa fa-angle-right" aria-label="下一页"></i></a>
  </nav>



      

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      const activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      const commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

    </div>
  </main>

  <footer class="footer">
    <div class="footer-inner">
      

      

<div class="copyright">
  
  &copy; 2019 – 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">SoundMemories</span>
</div>
  <div class="powered-by">由 <span class="exturl theme-link" data-url="aHR0cHM6Ly9oZXhvLmlv">Hexo</span> & <span class="exturl theme-link" data-url="aHR0cHM6Ly90aGVtZS1uZXh0LmpzLm9yZy9tdXNlLw==">NexT.Muse</span> 强力驱动
  </div>

    </div>
  </footer>

  
  <script src="//cdn.jsdelivr.net/npm/animejs@3.2.1/lib/anime.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/@next-theme/pjax@0.4.0/pjax.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/jquery@3.5.1/dist/jquery.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js"></script>
<script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/muse.js"></script><script src="/js/next-boot.js"></script><script src="/js/bookmark.js"></script>
  <script>
var pjax = new Pjax({
  selectors: [
    'head title',
    '.page-configurations',
    '.main-inner',
    '.post-toc-wrap',
    '.languages',
    '.pjax'
  ],
  analytics: false,
  cacheBust: false,
  scrollRestoration: false,
  scrollTo: !CONFIG.bookmark.enable
});

document.addEventListener('pjax:success', () => {
  pjax.executeScripts(document.querySelectorAll('script[data-pjax], .pjax script'));
  NexT.boot.refresh();
  // Define Motion Sequence & Bootstrap Motion.
  if (CONFIG.motion.enable) {
    NexT.motion.integrator
      .init()
      .add(NexT.motion.middleWares.subMenu)
      .add(NexT.motion.middleWares.postList)
      .bootstrap();
  }
  const hasTOC = document.querySelector('.post-toc');
  document.querySelector('.sidebar-inner').classList.toggle('sidebar-nav-active', hasTOC);
  document.querySelector(hasTOC ? '.sidebar-nav-toc' : '.sidebar-nav-overview').click();
  NexT.utils.updateSidebarPosition();
});
</script>


  




  <script src="/js/local-search.js"></script>








<script data-pjax>
document.querySelectorAll('.pdfobject-container').forEach(element => {
  const url = element.dataset.target;
  const pdfOpenParams = {
    navpanes : 0,
    toolbar  : 0,
    statusbar: 0,
    pagemode : 'thumbs',
    view     : 'FitH'
  };
  const pdfOpenFragment = '#' + Object.entries(pdfOpenParams).map(([key, value]) => `${key}=${encodeURIComponent(value)}`).join('&');
  const fullURL = `/lib/pdf/web/viewer.html?file=${encodeURIComponent(url)}${pdfOpenFragment}`;

  if (NexT.utils.supportsPDFs()) {
    element.innerHTML = `<embed class="pdfobject" src="${url + pdfOpenFragment}" type="application/pdf" style="height: ${element.dataset.height};">`;
  } else {
    element.innerHTML = `<iframe src="${fullURL}" style="height: ${element.dataset.height};" frameborder="0"></iframe>`;
  }
});
</script>


<script data-pjax>
if (document.querySelectorAll('.mermaid').length) {
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/mermaid@8.8.2/dist/mermaid.min.js', () => {
    mermaid.init({
      theme    : 'neutral',
      logLevel : 3,
      flowchart: { curve     : 'linear' },
      gantt    : { axisFormat: '%m/%d/%Y' },
      sequence : { actorMargin: 50 }
    }, '.mermaid');
  }, window.mermaid);
}
</script>





  








    <div class="pjax">
  

  
      <script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              const target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    const script = document.createElement('script');
    script.src = '//cdn.jsdelivr.net/npm/mathjax@3.1.2/es5/tex-mml-chtml.js';
    script.defer = true;
    document.head.appendChild(script);
  } else {
    MathJax.startup.document.state(0);
    MathJax.typesetClear();
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  
  <script src="//cdn.jsdelivr.net/npm/quicklink@2.0.0/dist/quicklink.umd.js"></script>
  <script>
      window.addEventListener('load', () => {
      quicklink.listen({
        timeout : 3000,
        priority: true,
        ignores : [uri => uri.includes('#'),uri => uri === 'https://soundmemories.github.io/page/2/',]
      });
      });
  </script>

    </div>
</body>
</html>
