<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222" media="(prefers-color-scheme: light)">
<meta name="theme-color" content="#222" media="(prefers-color-scheme: dark)"><meta name="generator" content="Hexo 6.3.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="https://fonts.loli.net/css?family=Monda:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">

<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css" integrity="sha256-HtsXJanqjKTc8vVQjO4YMhiqFoXkfBsjBWcX91T1jr8=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.css" integrity="sha256-Vzbj7sDDS/woiFS3uNKo8eIuni59rjyNGtXfstRzStA=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"soundmemories.github.io","root":"/","images":"/images","scheme":"Muse","darkmode":true,"version":"8.17.1","exturl":true,"sidebar":{"position":"left","display":"always","padding":18,"offset":12},"copycode":{"enable":true,"style":null},"bookmark":{"enable":true,"color":"#222","save":"auto"},"mediumzoom":true,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"},"path":"/search.xml","localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false}}</script><script src="/js/config.js"></script>

    <meta name="description" content="今日事，今日毕">
<meta property="og:type" content="website">
<meta property="og:title" content="SoundMemories">
<meta property="og:url" content="https://soundmemories.github.io/page/3/index.html">
<meta property="og:site_name" content="SoundMemories">
<meta property="og:description" content="今日事，今日毕">
<meta property="og:locale" content="zh_CN">
<meta property="article:author" content="SoundMemories">
<meta name="twitter:card" content="summary">


<link rel="canonical" href="https://soundmemories.github.io/page/3/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":true,"isPost":false,"lang":"zh-CN","comments":"","permalink":"","path":"page/3/index.html","title":""}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>SoundMemories</title>
  








  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <h1 class="site-title">SoundMemories</h1>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="搜索" role="button">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签<span class="badge">8</span></a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类<span class="badge">8</span></a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档<span class="badge">122</span></a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup"><div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" maxlength="80"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close" role="button">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="search-result-container no-result">
  <div class="search-result-icon">
    <i class="fa fa-spinner fa-pulse fa-5x"></i>
  </div>
</div>

    </div>
  </div>

</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-overview-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="SoundMemories"
      src="/images/avstar.png">
  <p class="site-author-name" itemprop="name">SoundMemories</p>
  <div class="site-description" itemprop="description">今日事，今日毕</div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">122</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">8</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">8</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author animated">
      <span class="links-of-author-item">
        <span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL3NvdW5kbWVtb3JpZXM=" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;soundmemories"><i class="fab fa-github fa-fw"></i>GitHub</span>
      </span>
      <span class="links-of-author-item">
        <span class="exturl" data-url="bWFpbHRvOnNvdW5kbWVtb3JpZXNAMTYzLmNvbQ==" title="E-Mail → mailto:soundmemories@163.com"><i class="fa fa-envelope fa-fw"></i>E-Mail</span>
      </span>
  </div>
  <div class="cc-license animated" itemprop="license">
    <span class="exturl cc-opacity" data-url="aHR0cHM6Ly9jcmVhdGl2ZWNvbW1vbnMub3JnL2xpY2Vuc2VzL2J5LW5jLXNhLzQuMC8="><img src="https://cdnjs.cloudflare.com/ajax/libs/creativecommons-vocabulary/2020.11.3/assets/license_badges/small/by_nc_sa.svg" alt="Creative Commons"></span>
  </div>

        </div>
      </div>
    </div>

    
        <div class="pjax">
        </div>
  </aside>


    </div>

    <div class="main-inner index posts-expand">

    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://soundmemories.github.io/2021/02/10/Graph/02.GAT/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avstar.png">
      <meta itemprop="name" content="SoundMemories">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="SoundMemories">
      <meta itemprop="description" content="今日事，今日毕">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | SoundMemories">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2021/02/10/Graph/02.GAT/" class="post-title-link" itemprop="url">GAT</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2021-02-10 00:00:00" itemprop="dateCreated datePublished" datetime="2021-02-10T00:00:00+08:00">2021-02-10</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Graph/" itemprop="url" rel="index"><span itemprop="name">Graph</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>4.1k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>15 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="gcn缺点">GCN缺点</h1>
<p><strong>GCN假设图是无向的</strong>，因为利用了对称的拉普拉斯矩阵
(只有邻接矩阵 A
是对称的，拉普拉斯矩阵才可以正交分解)，不能直接用于有向图。</p>
<p><strong>GCN不能处理动态图</strong>，GCN在训练时依赖于具体的图结构，测试的时候也要在相同的图上进行。因此只能处理transductive任务，不能处理inductive任务。transductive指训练和测试的时候基于相同的图结构，例如在一个社交网络上，知道一部分人的类别，预测另一部分人的类别。inductive指训练和测试使用不同的图结构，例如在一个社交网络上训练，在另一个社交网络上预测。</p>
<p><strong>GCN不能为每个邻居分配不同的权重</strong>，GCN
在卷积时对所有邻居节点均一视同仁，不能根据节点重要性分配不同的权重。</p>
<p>2018年图注意力网络GAT被提出，用于解决GCN的上述问题：GAT 采用了
Attention
机制，对邻近节点特征加权求和。邻近节点特征的权重完全取决于节点特征，而不依赖具体的网络结构，可以用于
inductive 任务。</p>
<h1 id="gat">GAT</h1>
<p>本文作者提出GATs方法，利用一个隐藏的self-attention层，来处理一些图卷积中的问题。不需要复杂的矩阵运算或者对图结构的事先了解，通过叠加self-attention层，在卷积过程中将不同的重要性分配给邻域内的不同节点，同时处理不同大小的邻域。作者分别设计了inductive
setting和transductive
setting的任务实验，GATs模型在基线数据集Cora、Citeseer、Pubmed
citation和PPI数据集上取得了state-of-the-art的结果。</p>
<p>和所有的attention
mechanism一样，GAT的计算也分为两步：<strong>计算注意力系数</strong>（attention
coefficient）和<strong>加权求和</strong>（aggregate）。</p>
<h2 id="计算注意力系数">计算注意力系数</h2>
<p>这里假设层的输入（Garaph）包含 <span class="math inline">\(N\)</span>
个节点，每个节点的特征向量为<span
class="math inline">\(h_i\)</span>，维度是 <span
class="math inline">\(F\)</span>，输出包含 <span
class="math inline">\(N\)</span> 个节点，每个节点的特征向量为<span
class="math inline">\(h_i^{\prime}\)</span>，维度是 <span
class="math inline">\(F^{\prime}\)</span>。为了<span
class="math inline">\(h_i^{\prime}\)</span>获得足够的表达能力，将输入特征转换为更高层次的特征，至少需要一个可学习的线性变换，为此，使用共享权重矩阵<span
class="math inline">\(W\)</span>作用到每个节点。然后我们在节点上使用<strong>self-attention</strong>来计算一个attention系数，即通过
<span class="math inline">\(a\)</span> 得到 <span
class="math inline">\(e_{ij}\)</span>，节点 <span
class="math inline">\(j\)</span> 是节点 <span
class="math inline">\(i\)</span> 的邻居。<br />
<img src="/images/GAT/1.png" width="80%"></p>
<p>论文中说，self-attention是一种<strong>Global graph
attention</strong>，会将注意力分配到图中所有的节点上，这种做法显然会丢失结构信息。通过self-attention注意力机制可以计算任意两个样本的关系，使一个样本用其他所有样本表示，但是第一，基于空间相似假设，一个样本与一定范围内的样本关系较为密切，第二，样本较多的时候，计算量非常大。为了解决这一问题，作者使用了一种
<strong>masked attention</strong>
的方法，<strong>对于一个样本来说只利用邻域内的样本计算注意力系数和新的表示，即仅将注意力分配到节点的一阶邻居节点集上</strong>。</p>
<p>针对每个节点执行self-attention机制<br />
<span class="math display">\[a ： R^{F^{\prime}}×R^{F^{\prime}} \to
R\]</span></p>
<p>计算注意力互相关系数attention coefficients：<br />
<span class="math display">\[e_{ij}=a(Wh_i,Wh_j)\]</span><br />
其中：</p>
<ul>
<li>注意力系数 <span class="math inline">\(e_{ij}\)</span>
表示的节点j对于节点i的重要性。</li>
<li>向量 <span class="math inline">\(h\)</span> 就是特征向量。</li>
<li><span class="math inline">\(a\)</span> 是一个 <span
class="math inline">\(R^{F^{\prime}}×R^{F^{\prime}} \to R\)</span>
的映射。</li>
<li><span class="math inline">\(W \in R^{F^{\prime}×F}\)</span>，使
<span class="math inline">\(W\)</span>
将每个特征转换为可用的表达性更强的特征。</li>
</ul>
<p>为了使注意力系数更容易计算和比较，引入softmax对所有的 <span
class="math inline">\(i\)</span> 的相邻节点 <span
class="math inline">\(j\)</span> 进行正则化：<br />
<img src="/images/GAT/2.png" width="40%"></p>
<p>其中：</p>
<ul>
<li><span class="math inline">\(N_i\)</span> 表示节点 <span
class="math inline">\(i\)</span> 的邻居节点集合。</li>
<li>这个系数 <span class="math inline">\(\alpha\)</span>
就是每次卷积时，用来进行加权求和的系数。</li>
</ul>
<p>实验中，注意力机制 <span class="math inline">\(a\)</span>
是一个单层的前馈神经网络，通过权值向量来确定 <span
class="math inline">\(a\in
R^{2F^{\prime}}\)</span>，并且加入了LeakyRelu的非线性激活，这里小于零斜率为0.2。（Relu:小于0就是0，大于0斜率为1；LRelu:小于0斜率固定一个值，大于0斜率为1；PRelu:小于0斜率可变，大于0斜率为1；
还有CRelu，Elu，SELU）。<br />
<img src="/images/GAT/3.png" width="60%"></p>
<p>本文中使用的是：<br />
<img src="/images/GAT/4.png" width="50%"><br />
其中，<span class="math inline">\(||\)</span>
表示concat操作（串联）。下面左图就是表示 <span
class="math inline">\(Wh_i\)</span>和 <span
class="math inline">\(Wh_j\)</span>经过串联以后，再和权值向量 <span
class="math inline">\(a\in R^{2F^{\prime}}\)</span>
相乘后，最后进行一个softmax归一化处理后的示意图。<br />
<img src="/images/GAT/5.png" width="80%"></p>
<h2 id="加权求和">加权求和</h2>
<p>得到归一化的注意力系数后，使用归一化的值计算对应特征的线性组合，作为每个顶点最后的输出特征（最后可以加一个非线性层<span
class="math inline">\(\sigma\)</span>）。<br />
<img src="/images/GAT/6.png" width="25%"></p>
<p><span class="math inline">\(h_i^{\prime}\)</span> 就是GAT输出的节点
<span class="math inline">\(i\)</span> 融合了邻域信息的新特征。</p>
<h2 id="多头注意力机制">多头注意力机制</h2>
<p>为了使 self-attention 的学习过程更稳定，发现使用
多头注意力（multi-head attention）来扩展注意力机制是很有效的。<br />
使用 <span class="math inline">\(K\)</span> 个独立的 attention
机制执行加权求和这样的变换，然后他们的特征连接（concatednated）在一起，就可以得到如下的输出：<br />
<img src="/images/GAT/7.png" width="30%"></p>
<p>其中，最后的返回输出 <span
class="math inline">\(h^{\prime}\)</span>，每个顶点都会有 <span
class="math inline">\(KF^{\prime}\)</span> 维的特征（不是<span
class="math inline">\(F^{\prime}\)</span>）。</p>
<p>下面右图表示 <span class="math inline">\(K=3\)</span> 时
多头注意力机制示意图。例如此图，节点1在邻域中具有多端注意机制，不同的箭头样式表示独立的注意力计算，通过连接或平均每个head获得<span
class="math inline">\(h_1\)</span>。<br />
<img src="/images/GAT/5.png" width="80%"></p>
<p>对于最后一个卷积层，如果还是使用多头注意力机制，那么就不用采取连接的方式合并不同的attention机制的结果了，而是采用求平均的方式进行处理，即：<br />
<img src="/images/GAT/8.png" width="30%"><br />
这里指的是直接连接softmax的方式，如果接一个全连接层，是无所谓的。</p>
<h2 id="总结">总结</h2>
<p>GAT 不依赖于完整的图结构，只依赖于边，因此可以用于 inductive
任务。<br />
GAT 可用于有向图。<br />
采用 Attention 机制，可以为不同的邻居节点分配不同的权重。<br />
尽管 multi-head attention 使得参数数量和空间复杂度变成 <span
class="math inline">\(K\)</span> 倍，但是每个 head 可以并行计算。</p>
<p>注意力机制以共享的方式应用于图的所有边，因此它不需要预先得到整个图结构或者所有顶点。这带来几个影响：<br />
（1）图可以是有向图，也可以是无向图。如果边 <span
class="math inline">\(j \to i\)</span> 不存在，则我们不需要计算系数
<span class="math inline">\(a_{i,j}\)</span>。<br />
（2）GAT 可以直接应用到归纳学习 inductinve learning
：模型可以预测那些在训练集中从未出现的图。</p>
<p>GraphSage
归纳学习模型对每个顶点采样固定大小的邻域，从而保持计算过程的一致性。这使得模型无法在测试期间访问整个邻域。注意：由于训练期间训练多个
epoch，则可能访问到顶点的整个邻域。也可以使用LSTM
技术来聚合邻域顶点，但是这需要假设邻域中存在一个一致的顶点顺序。GAT
没有这两个问题：GAT
在作用在完整的邻域上，并且不关心邻域内顶点的顺序。</p>
<h2 id="code">Code</h2>
<p>Pytorch：<span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL0RpZWdvOTk5L3B5R0FU">pyGAT<i class="fa fa-external-link-alt"></i></span><br />
Tensorflow：<span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL1BldGFyVi0vR0FU">GAT<i class="fa fa-external-link-alt"></i></span><br />
博客：<span class="exturl" data-url="aHR0cHM6Ly9wZXRhci12LmNvbS9HQVQv">GAT<i class="fa fa-external-link-alt"></i></span></p>
<p>以Pytorch版本为例：<br />
<figure class="highlight python"><figcaption><span>train.py</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> division</span><br><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> print_function</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> glob</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> utils <span class="keyword">import</span> load_data, accuracy</span><br><span class="line"><span class="keyword">from</span> models <span class="keyword">import</span> GAT, SpGAT</span><br><span class="line"></span><br><span class="line"><span class="comment"># Training settings</span></span><br><span class="line">parser = argparse.ArgumentParser()</span><br><span class="line">parser.add_argument(<span class="string">&#x27;--no-cuda&#x27;</span>, action=<span class="string">&#x27;store_true&#x27;</span>, default=<span class="literal">False</span>, <span class="built_in">help</span>=<span class="string">&#x27;Disables CUDA training.&#x27;</span>)</span><br><span class="line">parser.add_argument(<span class="string">&#x27;--fastmode&#x27;</span>, action=<span class="string">&#x27;store_true&#x27;</span>, default=<span class="literal">False</span>, <span class="built_in">help</span>=<span class="string">&#x27;Validate during training pass.&#x27;</span>)</span><br><span class="line">parser.add_argument(<span class="string">&#x27;--sparse&#x27;</span>, action=<span class="string">&#x27;store_true&#x27;</span>, default=<span class="literal">False</span>, <span class="built_in">help</span>=<span class="string">&#x27;GAT with sparse version or not.&#x27;</span>)</span><br><span class="line">parser.add_argument(<span class="string">&#x27;--seed&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">int</span>, default=<span class="number">72</span>, <span class="built_in">help</span>=<span class="string">&#x27;Random seed.&#x27;</span>)</span><br><span class="line">parser.add_argument(<span class="string">&#x27;--epochs&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">int</span>, default=<span class="number">10000</span>, <span class="built_in">help</span>=<span class="string">&#x27;Number of epochs to train.&#x27;</span>)</span><br><span class="line">parser.add_argument(<span class="string">&#x27;--lr&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">float</span>, default=<span class="number">0.005</span>, <span class="built_in">help</span>=<span class="string">&#x27;Initial learning rate.&#x27;</span>)</span><br><span class="line">parser.add_argument(<span class="string">&#x27;--weight_decay&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">float</span>, default=<span class="number">5e-4</span>, <span class="built_in">help</span>=<span class="string">&#x27;Weight decay (L2 loss on parameters).&#x27;</span>)</span><br><span class="line">parser.add_argument(<span class="string">&#x27;--hidden&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">int</span>, default=<span class="number">8</span>, <span class="built_in">help</span>=<span class="string">&#x27;Number of hidden units.&#x27;</span>)</span><br><span class="line">parser.add_argument(<span class="string">&#x27;--nb_heads&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">int</span>, default=<span class="number">8</span>, <span class="built_in">help</span>=<span class="string">&#x27;Number of head attentions.&#x27;</span>)</span><br><span class="line">parser.add_argument(<span class="string">&#x27;--dropout&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">float</span>, default=<span class="number">0.6</span>, <span class="built_in">help</span>=<span class="string">&#x27;Dropout rate (1 - keep probability).&#x27;</span>)</span><br><span class="line">parser.add_argument(<span class="string">&#x27;--alpha&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">float</span>, default=<span class="number">0.2</span>, <span class="built_in">help</span>=<span class="string">&#x27;Alpha for the leaky_relu.&#x27;</span>)</span><br><span class="line">parser.add_argument(<span class="string">&#x27;--patience&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">int</span>, default=<span class="number">100</span>, <span class="built_in">help</span>=<span class="string">&#x27;Patience&#x27;</span>)</span><br><span class="line"></span><br><span class="line">args = parser.parse_args()</span><br><span class="line">args.cuda = <span class="keyword">not</span> args.no_cuda <span class="keyword">and</span> torch.cuda.is_available()</span><br><span class="line"></span><br><span class="line">random.seed(args.seed)</span><br><span class="line">np.random.seed(args.seed)</span><br><span class="line">torch.manual_seed(args.seed)</span><br><span class="line"><span class="keyword">if</span> args.cuda:</span><br><span class="line">    torch.cuda.manual_seed(args.seed)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Load data</span></span><br><span class="line">adj, features, labels, idx_train, idx_val, idx_test = load_data()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Model and optimizer</span></span><br><span class="line"><span class="keyword">if</span> args.sparse:</span><br><span class="line">    model = SpGAT(nfeat=features.shape[<span class="number">1</span>], </span><br><span class="line">                nhid=args.hidden, </span><br><span class="line">                nclass=<span class="built_in">int</span>(labels.<span class="built_in">max</span>()) + <span class="number">1</span>, </span><br><span class="line">                dropout=args.dropout, </span><br><span class="line">                nheads=args.nb_heads, </span><br><span class="line">                alpha=args.alpha)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    model = GAT(nfeat=features.shape[<span class="number">1</span>], </span><br><span class="line">                nhid=args.hidden, </span><br><span class="line">                nclass=<span class="built_in">int</span>(labels.<span class="built_in">max</span>()) + <span class="number">1</span>, </span><br><span class="line">                dropout=args.dropout, </span><br><span class="line">                nheads=args.nb_heads, </span><br><span class="line">                alpha=args.alpha)</span><br><span class="line">optimizer = optim.Adam(model.parameters(), </span><br><span class="line">                       lr=args.lr, </span><br><span class="line">                       weight_decay=args.weight_decay)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> args.cuda:</span><br><span class="line">    model.cuda()</span><br><span class="line">    features = features.cuda()</span><br><span class="line">    adj = adj.cuda()</span><br><span class="line">    labels = labels.cuda()</span><br><span class="line">    idx_train = idx_train.cuda()</span><br><span class="line">    idx_val = idx_val.cuda()</span><br><span class="line">    idx_test = idx_test.cuda()</span><br><span class="line"></span><br><span class="line">features, adj, labels = Variable(features), Variable(adj), Variable(labels)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">train</span>(<span class="params">epoch</span>):</span><br><span class="line">    t = time.time()</span><br><span class="line">    model.train()</span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    output = model(features, adj) <span class="comment"># GAT模块 features：(2708,1433) output：(2708,7)</span></span><br><span class="line">    loss_train = F.nll_loss(output[idx_train], labels[idx_train])</span><br><span class="line">    acc_train = accuracy(output[idx_train], labels[idx_train])</span><br><span class="line">    loss_train.backward()</span><br><span class="line">    optimizer.step()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> args.fastmode:</span><br><span class="line">        <span class="comment"># Evaluate validation set performance separately,</span></span><br><span class="line">        <span class="comment"># deactivates dropout during validation run.</span></span><br><span class="line">        model.<span class="built_in">eval</span>()</span><br><span class="line">        output = model(features, adj)</span><br><span class="line"></span><br><span class="line">    loss_val = F.nll_loss(output[idx_val], labels[idx_val])</span><br><span class="line">    acc_val = accuracy(output[idx_val], labels[idx_val])</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Epoch: &#123;:04d&#125;&#x27;</span>.<span class="built_in">format</span>(epoch+<span class="number">1</span>),</span><br><span class="line">          <span class="string">&#x27;loss_train: &#123;:.4f&#125;&#x27;</span>.<span class="built_in">format</span>(loss_train.data.item()),</span><br><span class="line">          <span class="string">&#x27;acc_train: &#123;:.4f&#125;&#x27;</span>.<span class="built_in">format</span>(acc_train.data.item()),</span><br><span class="line">          <span class="string">&#x27;loss_val: &#123;:.4f&#125;&#x27;</span>.<span class="built_in">format</span>(loss_val.data.item()),</span><br><span class="line">          <span class="string">&#x27;acc_val: &#123;:.4f&#125;&#x27;</span>.<span class="built_in">format</span>(acc_val.data.item()),</span><br><span class="line">          <span class="string">&#x27;time: &#123;:.4f&#125;s&#x27;</span>.<span class="built_in">format</span>(time.time() - t))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> loss_val.data.item()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">compute_test</span>():</span><br><span class="line">    model.<span class="built_in">eval</span>()</span><br><span class="line">    output = model(features, adj)</span><br><span class="line">    loss_test = F.nll_loss(output[idx_test], labels[idx_test])</span><br><span class="line">    acc_test = accuracy(output[idx_test], labels[idx_test])</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Test set results:&quot;</span>,</span><br><span class="line">          <span class="string">&quot;loss= &#123;:.4f&#125;&quot;</span>.<span class="built_in">format</span>(loss_test.data[<span class="number">0</span>]),</span><br><span class="line">          <span class="string">&quot;accuracy= &#123;:.4f&#125;&quot;</span>.<span class="built_in">format</span>(acc_test.data[<span class="number">0</span>]))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Train model</span></span><br><span class="line">t_total = time.time()</span><br><span class="line">loss_values = []</span><br><span class="line">bad_counter = <span class="number">0</span></span><br><span class="line">best = args.epochs + <span class="number">1</span></span><br><span class="line">best_epoch = <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(args.epochs):</span><br><span class="line">    loss_values.append(train(epoch))</span><br><span class="line"></span><br><span class="line">    torch.save(model.state_dict(), <span class="string">&#x27;&#123;&#125;.pkl&#x27;</span>.<span class="built_in">format</span>(epoch))</span><br><span class="line">    <span class="keyword">if</span> loss_values[-<span class="number">1</span>] &lt; best:</span><br><span class="line">        best = loss_values[-<span class="number">1</span>]</span><br><span class="line">        best_epoch = epoch</span><br><span class="line">        bad_counter = <span class="number">0</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        bad_counter += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> bad_counter == args.patience:</span><br><span class="line">        <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">    files = glob.glob(<span class="string">&#x27;*.pkl&#x27;</span>)</span><br><span class="line">    <span class="keyword">for</span> file <span class="keyword">in</span> files:</span><br><span class="line">        epoch_nb = <span class="built_in">int</span>(file.split(<span class="string">&#x27;.&#x27;</span>)[<span class="number">0</span>])</span><br><span class="line">        <span class="keyword">if</span> epoch_nb &lt; best_epoch:</span><br><span class="line">            os.remove(file)</span><br><span class="line"></span><br><span class="line">files = glob.glob(<span class="string">&#x27;*.pkl&#x27;</span>)</span><br><span class="line"><span class="keyword">for</span> file <span class="keyword">in</span> files:</span><br><span class="line">    epoch_nb = <span class="built_in">int</span>(file.split(<span class="string">&#x27;.&#x27;</span>)[<span class="number">0</span>])</span><br><span class="line">    <span class="keyword">if</span> epoch_nb &gt; best_epoch:</span><br><span class="line">        os.remove(file)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Optimization Finished!&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Total time elapsed: &#123;:.4f&#125;s&quot;</span>.<span class="built_in">format</span>(time.time() - t_total))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Restore best model</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Loading &#123;&#125;th epoch&#x27;</span>.<span class="built_in">format</span>(best_epoch))</span><br><span class="line">model.load_state_dict(torch.load(<span class="string">&#x27;&#123;&#125;.pkl&#x27;</span>.<span class="built_in">format</span>(best_epoch)))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Testing</span></span><br><span class="line">compute_test()</span><br></pre></td></tr></table></figure></p>
<figure class="highlight python"><figcaption><span>model.py</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">from</span> layers <span class="keyword">import</span> GraphAttentionLayer, SpGraphAttentionLayer</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">GAT</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, nfeat, nhid, nclass, dropout, alpha, nheads</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;Dense version of GAT.&quot;&quot;&quot;</span></span><br><span class="line">        <span class="built_in">super</span>(GAT, self).__init__()</span><br><span class="line">        self.dropout = dropout</span><br><span class="line"></span><br><span class="line">        <span class="comment"># nfeat：输入特征1433，nhid：隐层特征8，nheads：多头数量8</span></span><br><span class="line">        <span class="comment"># 因为用了多头，结果包含nheads个layer</span></span><br><span class="line">        self.attentions = [GraphAttentionLayer(nfeat, nhid, dropout=dropout, alpha=alpha, concat=<span class="literal">True</span>) <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(nheads)]</span><br><span class="line">        <span class="keyword">for</span> i, attention <span class="keyword">in</span> <span class="built_in">enumerate</span>(self.attentions):</span><br><span class="line">            self.add_module(<span class="string">&#x27;attention_&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(i), attention)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 第二层（最后一层）的attention layer</span></span><br><span class="line">        self.out_att = GraphAttentionLayer(nhid * nheads, nclass, dropout=dropout, alpha=alpha, concat=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, adj</span>):</span><br><span class="line">        x = F.dropout(x, self.dropout, training=self.training)</span><br><span class="line">        <span class="comment"># 将每层attention拼接，8次，x：(2708,64)</span></span><br><span class="line">        x = torch.cat([att(x, adj) <span class="keyword">for</span> att <span class="keyword">in</span> self.attentions], dim=<span class="number">1</span>) </span><br><span class="line">        x = F.dropout(x, self.dropout, training=self.training)</span><br><span class="line">        x = F.elu(self.out_att(x, adj)) <span class="comment"># 第二层的attention layer </span></span><br><span class="line">        <span class="keyword">return</span> F.log_softmax(x, dim=<span class="number">1</span>) <span class="comment"># 输出(2708,7)</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">SpGAT</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, nfeat, nhid, nclass, dropout, alpha, nheads</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;Sparse version of GAT.&quot;&quot;&quot;</span></span><br><span class="line">        <span class="built_in">super</span>(SpGAT, self).__init__()</span><br><span class="line">        self.dropout = dropout</span><br><span class="line"></span><br><span class="line">        self.attentions = [SpGraphAttentionLayer(nfeat, </span><br><span class="line">                                                 nhid, </span><br><span class="line">                                                 dropout=dropout, </span><br><span class="line">                                                 alpha=alpha, </span><br><span class="line">                                                 concat=<span class="literal">True</span>) <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(nheads)]</span><br><span class="line">        <span class="keyword">for</span> i, attention <span class="keyword">in</span> <span class="built_in">enumerate</span>(self.attentions):</span><br><span class="line">            self.add_module(<span class="string">&#x27;attention_&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(i), attention)</span><br><span class="line"></span><br><span class="line">        self.out_att = SpGraphAttentionLayer(nhid * nheads, </span><br><span class="line">                                             nclass, </span><br><span class="line">                                             dropout=dropout, </span><br><span class="line">                                             alpha=alpha, </span><br><span class="line">                                             concat=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, adj</span>):</span><br><span class="line">        x = F.dropout(x, self.dropout, training=self.training)</span><br><span class="line">        x = torch.cat([att(x, adj) <span class="keyword">for</span> att <span class="keyword">in</span> self.attentions], dim=<span class="number">1</span>)</span><br><span class="line">        x = F.dropout(x, self.dropout, training=self.training)</span><br><span class="line">        x = F.elu(self.out_att(x, adj))</span><br><span class="line">        <span class="keyword">return</span> F.log_softmax(x, dim=<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><figcaption><span>layers.py</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">GraphAttentionLayer</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Simple GAT layer, similar to https://arxiv.org/abs/1710.10903</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_features, out_features, dropout, alpha, concat=<span class="literal">True</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(GraphAttentionLayer, self).__init__()</span><br><span class="line">        self.dropout = dropout</span><br><span class="line">        self.in_features = in_features <span class="comment"># 1433</span></span><br><span class="line">        self.out_features = out_features <span class="comment"># 8</span></span><br><span class="line">        self.alpha = alpha</span><br><span class="line">        self.concat = concat</span><br><span class="line"></span><br><span class="line">        self.W = nn.Parameter(torch.empty(size=(in_features, out_features))) <span class="comment"># (1433,8)</span></span><br><span class="line">        nn.init.xavier_uniform_(self.W.data, gain=<span class="number">1.414</span>)</span><br><span class="line">        self.a = nn.Parameter(torch.empty(size=(<span class="number">2</span>*out_features, <span class="number">1</span>))) <span class="comment"># (16,1) 公式中的a</span></span><br><span class="line">        nn.init.xavier_uniform_(self.a.data, gain=<span class="number">1.414</span>)</span><br><span class="line"></span><br><span class="line">        self.leakyrelu = nn.LeakyReLU(self.alpha)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, h, adj</span>):</span><br><span class="line">        Wh = torch.mm(h, self.W) <span class="comment"># h.shape: (N, in_features), Wh.shape: (N, out_features)</span></span><br><span class="line">        <span class="comment"># a_input：(2708,2708,16) 每一个节点和所有节点特征拼接。</span></span><br><span class="line">        a_input = self._prepare_attentional_mechanism_input(Wh) </span><br><span class="line">        e = self.leakyrelu(torch.matmul(a_input, self.a).squeeze(<span class="number">2</span>))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 之前计算的是一个节点和所有节点的attention，其实需要的是连接的节点的attention系数</span></span><br><span class="line">        zero_vec = -<span class="number">9e15</span>*torch.ones_like(e)</span><br><span class="line">        <span class="comment"># adj &gt; 0 表示相邻的</span></span><br><span class="line">        attention = torch.where(adj &gt; <span class="number">0</span>, e, zero_vec) <span class="comment"># 将邻接矩阵中小于0的变成负无穷</span></span><br><span class="line">        attention = F.softmax(attention, dim=<span class="number">1</span>) <span class="comment"># 按行求softmax，sum(axis=1)==1</span></span><br><span class="line">        attention = F.dropout(attention, self.dropout, training=self.training)</span><br><span class="line">        h_prime = torch.matmul(attention, Wh) <span class="comment"># 聚合邻居函数 h_prime：(2708,8)</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.concat:</span><br><span class="line">            <span class="keyword">return</span> F.elu(h_prime) <span class="comment"># elu-激活函数</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> h_prime</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_prepare_attentional_mechanism_input</span>(<span class="params">self, Wh</span>):</span><br><span class="line">        N = Wh.size()[<span class="number">0</span>] <span class="comment"># number of nodes</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Below, two matrices are created that contain embeddings in their rows in different orders.</span></span><br><span class="line">        <span class="comment"># (e stands for embedding)</span></span><br><span class="line">        <span class="comment"># These are the rows of the first matrix (Wh_repeated_in_chunks): </span></span><br><span class="line">        <span class="comment"># e1, e1, ..., e1,            e2, e2, ..., e2,            ..., eN, eN, ..., eN</span></span><br><span class="line">        <span class="comment"># &#x27;-------------&#x27; -&gt; N times  &#x27;-------------&#x27; -&gt; N times       &#x27;-------------&#x27; -&gt; N times</span></span><br><span class="line">        <span class="comment"># </span></span><br><span class="line">        <span class="comment"># These are the rows of the second matrix (Wh_repeated_alternating): </span></span><br><span class="line">        <span class="comment"># e1, e2, ..., eN, e1, e2, ..., eN, ..., e1, e2, ..., eN </span></span><br><span class="line">        <span class="comment"># &#x27;----------------------------------------------------&#x27; -&gt; N times</span></span><br><span class="line">        <span class="comment"># </span></span><br><span class="line">        </span><br><span class="line">        Wh_repeated_in_chunks = Wh.repeat_interleave(N, dim=<span class="number">0</span>) <span class="comment"># 复制</span></span><br><span class="line">        Wh_repeated_alternating = Wh.repeat(N, <span class="number">1</span>)</span><br><span class="line">        <span class="comment"># Wh_repeated_in_chunks.shape == Wh_repeated_alternating.shape == (N * N, out_features)</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># The all_combination_matrix, created below, will look like this (|| denotes concatenation):</span></span><br><span class="line">        <span class="comment"># e1 || e1</span></span><br><span class="line">        <span class="comment"># e1 || e2</span></span><br><span class="line">        <span class="comment"># e1 || e3</span></span><br><span class="line">        <span class="comment"># ...</span></span><br><span class="line">        <span class="comment"># e1 || eN</span></span><br><span class="line">        <span class="comment"># e2 || e1</span></span><br><span class="line">        <span class="comment"># e2 || e2</span></span><br><span class="line">        <span class="comment"># e2 || e3</span></span><br><span class="line">        <span class="comment"># ...</span></span><br><span class="line">        <span class="comment"># e2 || eN</span></span><br><span class="line">        <span class="comment"># ...</span></span><br><span class="line">        <span class="comment"># eN || e1</span></span><br><span class="line">        <span class="comment"># eN || e2</span></span><br><span class="line">        <span class="comment"># eN || e3</span></span><br><span class="line">        <span class="comment"># ...</span></span><br><span class="line">        <span class="comment"># eN || eN</span></span><br><span class="line"></span><br><span class="line">        all_combinations_matrix = torch.cat([Wh_repeated_in_chunks, Wh_repeated_alternating], dim=<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># all_combinations_matrix.shape == (N * N, 2 * out_features)</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> all_combinations_matrix.view(N, N, <span class="number">2</span> * self.out_features)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__repr__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> self.__class__.__name__ + <span class="string">&#x27; (&#x27;</span> + <span class="built_in">str</span>(self.in_features) + <span class="string">&#x27; -&gt; &#x27;</span> + <span class="built_in">str</span>(self.out_features) + <span class="string">&#x27;)&#x27;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">SpecialSpmmFunction</span>(torch.autograd.Function):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Special function for only sparse region backpropataion layer.&quot;&quot;&quot;</span></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">ctx, indices, values, shape, b</span>):</span><br><span class="line">        <span class="keyword">assert</span> indices.requires_grad == <span class="literal">False</span></span><br><span class="line">        a = torch.sparse_coo_tensor(indices, values, shape)</span><br><span class="line">        ctx.save_for_backward(a, b)</span><br><span class="line">        ctx.N = shape[<span class="number">0</span>]</span><br><span class="line">        <span class="keyword">return</span> torch.matmul(a, b)</span><br><span class="line"></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">backward</span>(<span class="params">ctx, grad_output</span>):</span><br><span class="line">        a, b = ctx.saved_tensors</span><br><span class="line">        grad_values = grad_b = <span class="literal">None</span></span><br><span class="line">        <span class="keyword">if</span> ctx.needs_input_grad[<span class="number">1</span>]:</span><br><span class="line">            grad_a_dense = grad_output.matmul(b.t())</span><br><span class="line">            edge_idx = a._indices()[<span class="number">0</span>, :] * ctx.N + a._indices()[<span class="number">1</span>, :]</span><br><span class="line">            grad_values = grad_a_dense.view(-<span class="number">1</span>)[edge_idx]</span><br><span class="line">        <span class="keyword">if</span> ctx.needs_input_grad[<span class="number">3</span>]:</span><br><span class="line">            grad_b = a.t().matmul(grad_output)</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">None</span>, grad_values, <span class="literal">None</span>, grad_b</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">SpecialSpmm</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, indices, values, shape, b</span>):</span><br><span class="line">        <span class="keyword">return</span> SpecialSpmmFunction.apply(indices, values, shape, b)</span><br><span class="line"></span><br><span class="line">    </span><br><span class="line"><span class="keyword">class</span> <span class="title class_">SpGraphAttentionLayer</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Sparse version GAT layer, similar to https://arxiv.org/abs/1710.10903</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_features, out_features, dropout, alpha, concat=<span class="literal">True</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(SpGraphAttentionLayer, self).__init__()</span><br><span class="line">        self.in_features = in_features</span><br><span class="line">        self.out_features = out_features</span><br><span class="line">        self.alpha = alpha</span><br><span class="line">        self.concat = concat</span><br><span class="line"></span><br><span class="line">        self.W = nn.Parameter(torch.zeros(size=(in_features, out_features)))</span><br><span class="line">        nn.init.xavier_normal_(self.W.data, gain=<span class="number">1.414</span>)</span><br><span class="line">                </span><br><span class="line">        self.a = nn.Parameter(torch.zeros(size=(<span class="number">1</span>, <span class="number">2</span>*out_features)))</span><br><span class="line">        nn.init.xavier_normal_(self.a.data, gain=<span class="number">1.414</span>)</span><br><span class="line"></span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line">        self.leakyrelu = nn.LeakyReLU(self.alpha)</span><br><span class="line">        self.special_spmm = SpecialSpmm()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, <span class="built_in">input</span>, adj</span>):</span><br><span class="line">        dv = <span class="string">&#x27;cuda&#x27;</span> <span class="keyword">if</span> <span class="built_in">input</span>.is_cuda <span class="keyword">else</span> <span class="string">&#x27;cpu&#x27;</span></span><br><span class="line"></span><br><span class="line">        N = <span class="built_in">input</span>.size()[<span class="number">0</span>]</span><br><span class="line">        edge = adj.nonzero().t()</span><br><span class="line"></span><br><span class="line">        h = torch.mm(<span class="built_in">input</span>, self.W)</span><br><span class="line">        <span class="comment"># h: N x out</span></span><br><span class="line">        <span class="keyword">assert</span> <span class="keyword">not</span> torch.isnan(h).<span class="built_in">any</span>()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Self-attention on the nodes - Shared attention mechanism</span></span><br><span class="line">        edge_h = torch.cat((h[edge[<span class="number">0</span>, :], :], h[edge[<span class="number">1</span>, :], :]), dim=<span class="number">1</span>).t()</span><br><span class="line">        <span class="comment"># edge: 2*D x E</span></span><br><span class="line"></span><br><span class="line">        edge_e = torch.exp(-self.leakyrelu(self.a.mm(edge_h).squeeze()))</span><br><span class="line">        <span class="keyword">assert</span> <span class="keyword">not</span> torch.isnan(edge_e).<span class="built_in">any</span>()</span><br><span class="line">        <span class="comment"># edge_e: E</span></span><br><span class="line"></span><br><span class="line">        e_rowsum = self.special_spmm(edge, edge_e, torch.Size([N, N]), torch.ones(size=(N,<span class="number">1</span>), device=dv))</span><br><span class="line">        <span class="comment"># e_rowsum: N x 1</span></span><br><span class="line"></span><br><span class="line">        edge_e = self.dropout(edge_e)</span><br><span class="line">        <span class="comment"># edge_e: E</span></span><br><span class="line"></span><br><span class="line">        h_prime = self.special_spmm(edge, edge_e, torch.Size([N, N]), h)</span><br><span class="line">        <span class="keyword">assert</span> <span class="keyword">not</span> torch.isnan(h_prime).<span class="built_in">any</span>()</span><br><span class="line">        <span class="comment"># h_prime: N x out</span></span><br><span class="line">        </span><br><span class="line">        h_prime = h_prime.div(e_rowsum)</span><br><span class="line">        <span class="comment"># h_prime: N x out</span></span><br><span class="line">        <span class="keyword">assert</span> <span class="keyword">not</span> torch.isnan(h_prime).<span class="built_in">any</span>()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.concat:</span><br><span class="line">            <span class="comment"># if this layer is not last layer,</span></span><br><span class="line">            <span class="keyword">return</span> F.elu(h_prime)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># if this layer is last layer,</span></span><br><span class="line">            <span class="keyword">return</span> h_prime</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__repr__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> self.__class__.__name__ + <span class="string">&#x27; (&#x27;</span> + <span class="built_in">str</span>(self.in_features) + <span class="string">&#x27; -&gt; &#x27;</span> + <span class="built_in">str</span>(self.out_features) + <span class="string">&#x27;)&#x27;</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><figcaption><span>utils.py</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> scipy.sparse <span class="keyword">as</span> sp</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">encode_onehot</span>(<span class="params">labels</span>):</span><br><span class="line">    <span class="comment"># The classes must be sorted before encoding to enable static class encoding.</span></span><br><span class="line">    <span class="comment"># In other words, make sure the first class always maps to index 0.</span></span><br><span class="line">    classes = <span class="built_in">sorted</span>(<span class="built_in">list</span>(<span class="built_in">set</span>(labels)))</span><br><span class="line">    classes_dict = &#123;c: np.identity(<span class="built_in">len</span>(classes))[i, :] <span class="keyword">for</span> i, c <span class="keyword">in</span> <span class="built_in">enumerate</span>(classes)&#125;</span><br><span class="line">    labels_onehot = np.array(<span class="built_in">list</span>(<span class="built_in">map</span>(classes_dict.get, labels)), dtype=np.int32)</span><br><span class="line">    <span class="keyword">return</span> labels_onehot</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">load_data</span>(<span class="params">path=<span class="string">&quot;./data/cora/&quot;</span>, dataset=<span class="string">&quot;cora&quot;</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Load citation network dataset (cora only for now)&quot;&quot;&quot;</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Loading &#123;&#125; dataset...&#x27;</span>.<span class="built_in">format</span>(dataset))</span><br><span class="line"></span><br><span class="line">    idx_features_labels = np.genfromtxt(<span class="string">&quot;&#123;&#125;&#123;&#125;.content&quot;</span>.<span class="built_in">format</span>(path, dataset), dtype=np.dtype(<span class="built_in">str</span>))</span><br><span class="line">    features = sp.csr_matrix(idx_features_labels[:, <span class="number">1</span>:-<span class="number">1</span>], dtype=np.float32)</span><br><span class="line">    labels = encode_onehot(idx_features_labels[:, -<span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># build graph</span></span><br><span class="line">    idx = np.array(idx_features_labels[:, <span class="number">0</span>], dtype=np.int32)</span><br><span class="line">    idx_map = &#123;j: i <span class="keyword">for</span> i, j <span class="keyword">in</span> <span class="built_in">enumerate</span>(idx)&#125;</span><br><span class="line">    <span class="comment"># 将边导入</span></span><br><span class="line">    edges_unordered = np.genfromtxt(<span class="string">&quot;&#123;&#125;&#123;&#125;.cites&quot;</span>.<span class="built_in">format</span>(path, dataset), dtype=np.int32)</span><br><span class="line">    <span class="comment"># 将点对应到dictionary中</span></span><br><span class="line">    edges = np.array(<span class="built_in">list</span>(<span class="built_in">map</span>(idx_map.get, edges_unordered.flatten())), dtype=np.int32).reshape(edges_unordered.shape)</span><br><span class="line">    <span class="comment"># 建立边的邻接矩阵</span></span><br><span class="line">    adj = sp.coo_matrix((np.ones(edges.shape[<span class="number">0</span>]), (edges[:, <span class="number">0</span>], edges[:, <span class="number">1</span>])), shape=(labels.shape[<span class="number">0</span>], labels.shape[<span class="number">0</span>]), dtype=np.float32)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># build symmetric adjacency matrix 构建对称矩阵</span></span><br><span class="line">    adj = adj + adj.T.multiply(adj.T &gt; adj) - adj.multiply(adj.T &gt; adj)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 归一化</span></span><br><span class="line">    features = normalize_features(features)</span><br><span class="line">    adj = normalize_adj(adj + sp.eye(adj.shape[<span class="number">0</span>]))</span><br><span class="line"></span><br><span class="line">    idx_train = <span class="built_in">range</span>(<span class="number">140</span>)</span><br><span class="line">    idx_val = <span class="built_in">range</span>(<span class="number">200</span>, <span class="number">500</span>)</span><br><span class="line">    idx_test = <span class="built_in">range</span>(<span class="number">500</span>, <span class="number">1500</span>)</span><br><span class="line"></span><br><span class="line">    adj = torch.FloatTensor(np.array(adj.todense()))</span><br><span class="line">    features = torch.FloatTensor(np.array(features.todense()))</span><br><span class="line">    labels = torch.LongTensor(np.where(labels)[<span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">    idx_train = torch.LongTensor(idx_train)</span><br><span class="line">    idx_val = torch.LongTensor(idx_val)</span><br><span class="line">    idx_test = torch.LongTensor(idx_test)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> adj, features, labels, idx_train, idx_val, idx_test</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">normalize_adj</span>(<span class="params">mx</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Row-normalize sparse matrix&quot;&quot;&quot;</span></span><br><span class="line">    rowsum = np.array(mx.<span class="built_in">sum</span>(<span class="number">1</span>))</span><br><span class="line">    r_inv_sqrt = np.power(rowsum, -<span class="number">0.5</span>).flatten()</span><br><span class="line">    r_inv_sqrt[np.isinf(r_inv_sqrt)] = <span class="number">0.</span></span><br><span class="line">    r_mat_inv_sqrt = sp.diags(r_inv_sqrt)</span><br><span class="line">    <span class="keyword">return</span> mx.dot(r_mat_inv_sqrt).transpose().dot(r_mat_inv_sqrt)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">normalize_features</span>(<span class="params">mx</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Row-normalize sparse matrix&quot;&quot;&quot;</span></span><br><span class="line">    rowsum = np.array(mx.<span class="built_in">sum</span>(<span class="number">1</span>))</span><br><span class="line">    r_inv = np.power(rowsum, -<span class="number">1</span>).flatten()</span><br><span class="line">    r_inv[np.isinf(r_inv)] = <span class="number">0.</span></span><br><span class="line">    r_mat_inv = sp.diags(r_inv)</span><br><span class="line">    mx = r_mat_inv.dot(mx)</span><br><span class="line">    <span class="keyword">return</span> mx</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">accuracy</span>(<span class="params">output, labels</span>):</span><br><span class="line">    preds = output.<span class="built_in">max</span>(<span class="number">1</span>)[<span class="number">1</span>].type_as(labels)</span><br><span class="line">    correct = preds.eq(labels).double()</span><br><span class="line">    correct = correct.<span class="built_in">sum</span>()</span><br><span class="line">    <span class="keyword">return</span> correct / <span class="built_in">len</span>(labels)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><figcaption><span>visualize_graph.py</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> graphviz <span class="keyword">import</span> Digraph</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> models</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">make_dot</span>(<span class="params">var, params</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot; Produces Graphviz representation of PyTorch autograd graph</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Blue nodes are the Variables that require grad, orange are Tensors</span></span><br><span class="line"><span class="string">    saved for backward in torch.autograd.Function</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        var: output Variable</span></span><br><span class="line"><span class="string">        params: dict of (name, Variable) to add names to node that</span></span><br><span class="line"><span class="string">            require grad (TODO: make optional)</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    param_map = &#123;<span class="built_in">id</span>(v): k <span class="keyword">for</span> k, v <span class="keyword">in</span> params.items()&#125;</span><br><span class="line">    <span class="built_in">print</span>(param_map)</span><br><span class="line">    </span><br><span class="line">    node_attr = <span class="built_in">dict</span>(style=<span class="string">&#x27;filled&#x27;</span>,</span><br><span class="line">                     shape=<span class="string">&#x27;box&#x27;</span>,</span><br><span class="line">                     align=<span class="string">&#x27;left&#x27;</span>,</span><br><span class="line">                     fontsize=<span class="string">&#x27;12&#x27;</span>,</span><br><span class="line">                     ranksep=<span class="string">&#x27;0.1&#x27;</span>,</span><br><span class="line">                     height=<span class="string">&#x27;0.2&#x27;</span>)</span><br><span class="line">    dot = Digraph(node_attr=node_attr, graph_attr=<span class="built_in">dict</span>(size=<span class="string">&quot;12,12&quot;</span>))</span><br><span class="line">    seen = <span class="built_in">set</span>()</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">size_to_str</span>(<span class="params">size</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="string">&#x27;(&#x27;</span>+(<span class="string">&#x27;, &#x27;</span>).join([<span class="string">&#x27;%d&#x27;</span>% v <span class="keyword">for</span> v <span class="keyword">in</span> size])+<span class="string">&#x27;)&#x27;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">add_nodes</span>(<span class="params">var</span>):</span><br><span class="line">        <span class="keyword">if</span> var <span class="keyword">not</span> <span class="keyword">in</span> seen:</span><br><span class="line">            <span class="keyword">if</span> torch.is_tensor(var):</span><br><span class="line">                dot.node(<span class="built_in">str</span>(<span class="built_in">id</span>(var)), size_to_str(var.size()), fillcolor=<span class="string">&#x27;orange&#x27;</span>)</span><br><span class="line">            <span class="keyword">elif</span> <span class="built_in">hasattr</span>(var, <span class="string">&#x27;variable&#x27;</span>):</span><br><span class="line">                u = var.variable</span><br><span class="line">                node_name = <span class="string">&#x27;%s\n %s&#x27;</span> % (param_map.get(<span class="built_in">id</span>(u)), size_to_str(u.size()))</span><br><span class="line">                dot.node(<span class="built_in">str</span>(<span class="built_in">id</span>(var)), node_name, fillcolor=<span class="string">&#x27;lightblue&#x27;</span>)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                dot.node(<span class="built_in">str</span>(<span class="built_in">id</span>(var)), <span class="built_in">str</span>(<span class="built_in">type</span>(var).__name__))</span><br><span class="line">            seen.add(var)</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">hasattr</span>(var, <span class="string">&#x27;next_functions&#x27;</span>):</span><br><span class="line">                <span class="keyword">for</span> u <span class="keyword">in</span> var.next_functions:</span><br><span class="line">                    <span class="keyword">if</span> u[<span class="number">0</span>] <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                        dot.edge(<span class="built_in">str</span>(<span class="built_in">id</span>(u[<span class="number">0</span>])), <span class="built_in">str</span>(<span class="built_in">id</span>(var)))</span><br><span class="line">                        add_nodes(u[<span class="number">0</span>])</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">hasattr</span>(var, <span class="string">&#x27;saved_tensors&#x27;</span>):</span><br><span class="line">                <span class="keyword">for</span> t <span class="keyword">in</span> var.saved_tensors:</span><br><span class="line">                    dot.edge(<span class="built_in">str</span>(<span class="built_in">id</span>(t)), <span class="built_in">str</span>(<span class="built_in">id</span>(var)))</span><br><span class="line">                    add_nodes(t)</span><br><span class="line">    add_nodes(var.grad_fn)</span><br><span class="line">    <span class="keyword">return</span> dot</span><br><span class="line"></span><br><span class="line">inputs = torch.randn(<span class="number">100</span>, <span class="number">50</span>).cuda()</span><br><span class="line">adj = torch.randn(<span class="number">100</span>, <span class="number">100</span>).cuda()</span><br><span class="line">model = models.SpGAT(<span class="number">50</span>, <span class="number">8</span>, <span class="number">7</span>, <span class="number">0.5</span>, <span class="number">0.01</span>, <span class="number">3</span>)</span><br><span class="line">model = model.cuda()</span><br><span class="line">y = model(inputs, adj)</span><br><span class="line"></span><br><span class="line">g = make_dot(y, model.state_dict())</span><br><span class="line">g.view()</span><br></pre></td></tr></table></figure>
<h1 id="参考文献">参考文献</h1>
<p><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzE3MTAuMTA5MDMucGRm">GRAPH ATTENTION
NETWORKS<i class="fa fa-external-link-alt"></i></span><br />
<span class="exturl" data-url="aHR0cHM6Ly9iYWlqaWFoYW8uYmFpZHUuY29tL3M/aWQ9MTY3MTAyODk2NDU0NDg4NDc0OSZ3ZnI9c3BpZGVyJmZvcj1wYw==">GAT
图注意力网络 Graph Attention Network<i class="fa fa-external-link-alt"></i></span><br />
<span class="exturl" data-url="aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQ1MzEyMTQxL2FydGljbGUvZGV0YWlscy8xMDYyOTExOTU=">图注意力网络(GAT)<i class="fa fa-external-link-alt"></i></span><br />
<span class="exturl" data-url="aHR0cHM6Ly93d3cuamlxaXpoaXhpbi5jb20vYXJ0aWNsZXMvMjAxOS0wMi0xOS03">深入理解图注意力机制<i class="fa fa-external-link-alt"></i></span><br />
<span class="exturl" data-url="aHR0cHM6Ly96aHVhbmxhbi56aGlodS5jb20vcC8zMzkwODc3MDY=">GAT详解<i class="fa fa-external-link-alt"></i></span></p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://soundmemories.github.io/2021/02/05/Graph/01.GCN/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avstar.png">
      <meta itemprop="name" content="SoundMemories">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="SoundMemories">
      <meta itemprop="description" content="今日事，今日毕">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | SoundMemories">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2021/02/05/Graph/01.GCN/" class="post-title-link" itemprop="url">GCN</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2021-02-05 00:00:00" itemprop="dateCreated datePublished" datetime="2021-02-05T00:00:00+08:00">2021-02-05</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Graph/" itemprop="url" rel="index"><span itemprop="name">Graph</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>5.6k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>20 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="图论">图论</h1>
<p>最开始是1707年由 Seven bridges
问题而来的，欧拉把这个问题转成图解决。近代图论1959年 Erdos 和 Renyi
发表的 Random Graph
一系列论文建立起来的全新领域。目前学术界和工业界是基于1999年 brabasi
提出的 Scale-free Network 进行的。</p>
<h2 id="degree-distribution">degree-distribution</h2>
<p><strong>度分布，即每个节点按边的数量分类，每类节点占总结点比例</strong>。一般
Random Graph 的度分布是
<strong>泊松分布</strong>（poisson，类似正态分布）。但基于 Scale-free
Network 的提出，我们发现很多网络结构是
<strong>幂律分布</strong>（power-law）。</p>
<p><img src="/images/GCN/随机网络.png" width="80%"><br />
<img src="/images/GCN/无标度网络.png" width="80%"></p>
<h2 id="distancehilbert-space">Distance(Hilbert Space)</h2>
<p>每个节点具有多维features，可以看成多维空间，计算Distance也可看作Similarity。为什么在
Hilbert Space ，因为要方便后面约束优化。</p>
<p>GCN要求：<br />
（1）weights <span class="math inline">\(\geq\)</span> 0<br />
（2）linear calculation<br />
（3）Inner product</p>
<p>Hilbert Space（括号表示内积）：<br />
（1）对称性：<span class="math inline">\((\vec{y}, \vec{x})\)</span> =
<span class="math inline">\((\vec{x}, \vec{y})\)</span><br />
（2）线性：<span class="math inline">\((a\vec{x\_1}+b \vec{x\_2},
\vec{y})\)</span> = <span class="math inline">\(a(\vec{x_1},
\vec{y})+b(\vec{x_2}, \vec{y})\)</span><br />
（3）半正定性：<span class="math inline">\((\vec{x}, \vec{x}) \geq
0\)</span>，if <span
class="math inline">\(\vec{x}=\vec{0}\)</span>，<span
class="math inline">\((\vec{x}, \vec{x}) = 0\)</span></p>
<p>Distance(Hilbert Space)：<span class="math inline">\(d(\vec{x},
\vec{y})=||\vec{x}-\vec{y}||=\sqrt{(\vec{x}-\vec{y},
\vec{x}-\vec{y})}\)</span></p>
<h2 id="adjacency-matrix">adjacency matrix</h2>
<p>邻接矩阵。阶为<span class="math inline">\(n\)</span>的图<span
class="math inline">\(G\)</span>的邻接矩阵<span
class="math inline">\(A\)</span>是<span class="math inline">\(n\times
n\)</span>的。将<span class="math inline">\(G\)</span>的顶点标签为<span
class="math inline">\(v_{1},v_{2},...,v_{n}\)</span>。若<span
class="math inline">\((v_{i},v_{j})\in E(G)\)</span>，<span
class="math inline">\(A_{ij}=1\)</span>，否则<span
class="math inline">\(A_{ij}=0\)</span>。也可以用大于0的值表示边的权值，例如可以用边权值表示一个点到另一个点的距离。</p>
<p>无向图的邻接矩阵计算方法是每条边为对应的单元加上1，而每个自环加上2。这样让某一节点的度数可以通过邻接矩阵的对应行或者列求和得到。<br />
<img src="/images/GCN/邻接矩阵.png" width="50%"></p>
<p>有向图的邻接矩阵可以是不对称的。我们可以定义有向图的邻接矩阵中的某个元素
<span class="math inline">\(A_{ij}\)</span> 代表：<br />
（1）从 <span class="math inline">\(i\)</span> 指向 <span
class="math inline">\(j\)</span> 的边数目。<br />
（2）从 <span class="math inline">\(j\)</span> 指向 <span
class="math inline">\(i\)</span> 的边数目。<br />
在第一种定义下，有向图的某个节点的入度可以通过对应的列（column）求和而得，出度可以通过对应的行（row）求和而得。在第二种定义下，入度可以通过对应的行（row）求和而得，出度可以通过对应的列（column）求和而得。<br />
<img src="/images/GCN/邻接矩阵2.png" width="50%"></p>
<h2 id="clustering-coefficient">Clustering coefficient</h2>
<p>聚类系数，一个图中的顶点之间结集成团的程度的系数。集聚系数分为整体与局部两种。整体集聚系数可以给出一个图中整体的集聚程度的评估，而局部集聚系数则可以测量图中每一个结点附近的集聚程度。</p>
<p><strong>分子</strong>：闭三点组（邻近三点组成“三角形”）数量。<br />
<strong>分母</strong>：闭三点组（邻近三点组成“三角形”）数量 +
开三点组（邻近三点组成“缺一条边的三角形”）数量。</p>
<p>整体聚类系数：对每个节点的聚类系数求和取均值。<strong>如果该值很大，表示图是很稠密的，节点和节点之间联系紧密</strong>。</p>
<h2 id="betweenness">Betweenness</h2>
<p>也叫Betweenness centrality，分node和edge两种计算方法：<br />
（1）<strong>node
Betweenness</strong>：计算经过一个点的最短路径的数量占所有最短路径数量比例。两点一组，遍历所有组，计算每组中经过一个点的最短路径的数量占该组最短路径数量比例，最后求和。<br />
（2）<strong>edge Betweenness</strong>：node Betweenness
换成边即可。</p>
<p><strong>这个值很大，表示此node或edge很重要，因为很多最短路径都要经过它，表现流通性</strong>。</p>
<p>缺点：需要遍历所有点找到所有最短路径，一般的算法有
迪杰斯特拉算法（Dijkstra） 和 弗洛伊德算法（Floyd），时间复杂度都为
<span class="math inline">\(O(n^3)\)</span>。</p>
<h1 id="gcn">GCN</h1>
<p>GCN（Graph Convolutional
Networks，图卷积神经网络），实际上跟CNN的作用一样，就是一个特征提取器，只不过它的对象是图数据。<br />
GCN精妙地设计了一种从图数据中提取特征的方法，从而让我们可以使用这些特征去对图数据进行<strong>节点分类</strong>（node
classification）、<strong>图分类</strong>（graph
classification）、<strong>边预测</strong>（link
prediction），还可以顺便得到<strong>图的嵌入表示</strong>（graph
embedding）。</p>
<p>GCN发展历史，那么肯定绕不过下面三篇论文：<br />
<span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzEzMTIuNjIwMy5wZGY=">Spectral Networks and Deep
Locally Connected Networks on Graphs<i class="fa fa-external-link-alt"></i></span> 2014年<br />
<span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzE2MDYuMDkzNzUucGRm">Convolutional Neural
Networks on Graphs with Fast Localized Spectral Filtering<i class="fa fa-external-link-alt"></i></span>
2016年<br />
<span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzE2MDkuMDI5MDcucGRm">Semi-Supervised
Classification with Graph Convolutional Networks<i class="fa fa-external-link-alt"></i></span> 2017年</p>
<p>在计算机科学领域、理论物理复杂网络领域的研究者在图（Graph）的空间域（spatial
domain）和频谱域（spectral
domain）分别提出了不同形式的图神经网络，并最终在2017年实现了空间域模型和频谱域模型的融合，即目前我们使用的第三代GCN。</p>
<p>对于其中理论和公式非常感兴趣的参考<span class="exturl" data-url="aHR0cDovL3h0ZjYxNS5jb20vMjAxOS8wMi8yNC9nY24v">图卷积神经网络理论基础<i class="fa fa-external-link-alt"></i></span>。</p>
<h2 id="fourier-transform">Fourier transform</h2>
<p><span class="math display">\[
F(w)=\frac{1}{2\pi}\int_{-\infty}^{+\infty} f(t)e^{-j\omega t} {\rm d}t
\]</span></p>
<p><span class="math inline">\(F(\omega)\)</span>
就是<strong>傅里叶变换</strong>，得到的就是<strong>频域曲线</strong>。每个频率<span
class="math inline">\(\omega\)</span>下都有对应的振幅<span
class="math inline">\(F(\omega)\)</span>。从几何上来看，<span
class="math inline">\(f(t)\)</span> 以 <span
class="math inline">\(e^{-j\omega t}\)</span> 为基函数投影，<span
class="math inline">\(F(w)\)</span> 就是以频率 <span
class="math inline">\(\omega\)</span> 对应基上的投影的坐标。</p>
<p>从数学角度来看，<span class="math inline">\(f(x)\)</span> 是函数
<span class="math inline">\(f\)</span> 在 <span
class="math inline">\(t\)</span>
处的取值，所有基都对该处取值有贡献，即把每个<span
class="math inline">\(F(w)\)</span> 投影到 <span
class="math inline">\(e^{-j\omega t}\)</span>
基方向上分量累加起来，得到的就是该点处的函数值。<br />
<span class="math display">\[
f(t) = \int_{-\infty}^{+\infty}F(w)e^{-j\omega t}\, {\rm
d}\omega=\sum_{\omega}F(w)e^{-j\omega t}
\]</span><br />
上面简化了一下，用 <span class="math inline">\(w\)</span>
代表频率。这个公式也叫做<strong>逆傅里叶变换</strong>。</p>
<h2 id="laplacian-operater">Laplacian operater</h2>
<p><span class="math display">\[
\Delta f = \Delta^2 f = \sum_{i=1}^{n}\frac{\partial^2 f}{\partial
x_i^2}
\]</span><br />
<span class="math inline">\(f\)</span>
是拉普拉斯算子作用的函数，求函数各向二阶导数再求和，定义为 <span
class="math inline">\(f\)</span> 上的拉普拉斯算子。<br />
可以理解为：<strong>二阶导数等于其在所有自由度上微扰之后获得的增益</strong>。<br />
更形象的理解：<strong>拉普拉斯算子就是在所有自由度上进行微小变化后获得的增益</strong>。</p>
<p>求 <span class="math inline">\(e^{-j\omega t}\)</span> 上的 Laplacian
operater：<br />
<span class="math display">\[
\Delta f = \Delta e^{-j\omega t} = \sum_{i=1}^{n}\frac{\partial^2
e^{-j\omega t}}{\partial t^2}=-\omega^2 e^{-j\omega t}
\]</span><br />
由此可知，<span class="math inline">\(e^{-j\omega t}\)</span> 是
<strong>Laplacian operater 的特征向量</strong>（满足特征方程 <span
class="math inline">\(A\vec{x}=\lambda \vec{x}\)</span>）。</p>
<h2 id="graph-laplacian-operater">Graph Laplacian operater</h2>
<p>Laplacian operater 推广到
Graph：假设<strong>图是一个完全图，即任意两个节点之间都有一条边，那么对一个节点进行微扰，它可能变成任意一个节点</strong>。即：<br />
<span class="math display">\[
f=(f_1,f_2...f_N)
\]</span><br />
是函数 <span class="math inline">\(f\)</span> 在节点 <span
class="math inline">\(1..N\)</span>
上的函数值，代表<strong>跟节点相关的信息</strong>，如节点属性等，此时可看作每一个节点是一个向量。</p>
<p>假设一个节点 <span class="math inline">\(f_i\)</span>
，其一阶邻域节点集合为 <span class="math inline">\(N_i\)</span> ，<span
class="math inline">\(f_j\)</span>为 <span
class="math inline">\(N_i\)</span> 集合的一个节点，对于任意节点 <span
class="math inline">\(f_i\)</span> ，对 <span
class="math inline">\(f_i\)</span>
节点进行微扰，它可能变为任意一个与他相邻的节点 <span
class="math inline">\(f_j \in
N_i\)</span>。前面提到，拉普拉斯算子就是在所有自由度上进行微小变化后获得的增益。对于
Graph 而言，从节点 <span class="math inline">\(i\)</span> 变化到节点
<span class="math inline">\(j\)</span> 增益是 <span
class="math inline">\(f_i−f_j\)</span>，即节点 <span
class="math inline">\(f_i\)</span> 的 <strong>Graph Laplacian
operater</strong>：<br />
<span class="math display">\[
\Delta f_i =\sum_{j \in N_i} (f_i - f_j)
\]</span><br />
通俗理解，当前节点的 Graph Laplacian operater 就是
<strong>当前节点和所有邻接节点的差值</strong>。</p>
<h2 id="laplacian-matrix">Laplacian Matrix</h2>
<p>把 Graph Laplacian operater
公式变换一下，<strong>考虑权重</strong>：<span
class="math inline">\(w_{ij}=0\)</span> 表示 <span
class="math inline">\(i,j\)</span> 不相邻，<span
class="math inline">\(w_{ij}=1\)</span> 表示 <span
class="math inline">\(i,j\)</span> 相邻，那么上面公式可以转换为：<br />
<span class="math display">\[
\begin{aligned}
\Delta f_i &amp;=\sum_{j \in N} w_{ij}(f_i - f_j)\\
&amp;=\sum_{j \in N} w_{ij}f_i - \sum_{j \in N} w_{ij}f_j\\
&amp;=d_if_i-w_if
\end{aligned}
\]</span><br />
其中：令 <span class="math inline">\(d_i=\sum_{j \in N}w_{ij}\)</span>
，表示节点 <span class="math inline">\(i\)</span> 的度。令 <span
class="math inline">\(w_i=[w_{i1},...,w_{iN}]\)</span> 行。令 <span
class="math inline">\(f=[f_1,...f_N]^T\)</span> 列。<br />
对于所有节点：<br />
<span class="math display">\[
\begin{aligned}
\Delta f&amp;=
\begin{bmatrix}
   d_1f_1-w_1f \\
   d_2f_2-w_2f \\
   \cdots \\
   d_Nf_N-w_Nf
\end{bmatrix}\\
&amp;=
\begin{equation*}
    \begin{bmatrix}
    d_1 &amp;0&amp;\cdots&amp;0 \\
    0&amp;d_2&amp;\cdots&amp;0\\
    \vdots&amp;\vdots&amp; \ddots&amp;\vdots \\
    0&amp;0&amp;\cdots&amp;d_N
    \end{bmatrix}
    \end{equation*}f-\begin{bmatrix}
   w_1\\
   w_2\\
   \cdots \\
   w_N
\end{bmatrix}f\\
&amp;=(D-W)f
\end{aligned}
\]</span></p>
<p><span class="math inline">\(D\)</span>
就是<strong>度矩阵</strong>（dgree matrix），<span
class="math inline">\(W\)</span>
就是<strong>邻接矩阵</strong>（adjacency matrix），<span
class="math inline">\(L=D-W\)</span>
就是<strong>拉普拉斯矩阵</strong>（Laplacian Matrix）。</p>
<p><img src="/images/GCN/LM.png" width="80%"></p>
<p>根据<span class="math inline">\(\Delta f = Lf\)</span>，那么可以看作
<strong>Laplacian operater 等于 Laplacian Matrix</strong> ，即<span
class="math inline">\(\Delta=L\)</span>。这样<strong>求 Graph Laplacian
operater 等价于求 Laplacian Matrix</strong>。</p>
<p>Laplacian Matrix
是<strong>半正定对称矩阵</strong>，因此拥有诸多优秀性质：</p>
<ul>
<li>对称矩阵一定n个线性无关的特征向量</li>
<li>半正定矩阵的特征值一定非负</li>
<li>对阵矩阵的特征向量相互正交，即所有特征向量构成的矩阵为正交矩阵</li>
</ul>
<p>对 Laplacian Matrix 进行特征分解：<br />
<span class="math display">\[
\Delta=L=U\Lambda U^T
\]</span><br />
其中，<span class="math inline">\(U\)</span>的每一列为<span
class="math inline">\(L\)</span>的<strong>特征向量</strong>，<span
class="math inline">\(\Lambda\)</span> 是<span
class="math inline">\(L\)</span>的<strong>特征值矩阵</strong>，<span
class="math inline">\(U^T\)</span>的每一行为<span
class="math inline">\(L\)</span>的<strong>特征向量</strong>。</p>
<h2 id="graph-fourier-transform">Graph Fourier transform</h2>
<p>前面提到，<span class="math inline">\(e^{-j\omega t}\)</span> 是
<span class="math inline">\(\Delta\)</span>
的<strong>特征向量</strong>，而后推导出：<span
class="math inline">\(\Delta=L=U\Lambda U^T\)</span> ，<span
class="math inline">\(U^T\)</span>的每一行为<span
class="math inline">\(L\)</span>的<strong>特征向量</strong><span
class="math inline">\(\phi_w\)</span>，因此我们可得到：</p>
<ul>
<li>频率<span class="math inline">\(w\)</span> <span
class="math inline">\(\to\)</span> 特征值<span
class="math inline">\(\lambda_w\)</span></li>
<li>正弦函数 <span class="math inline">\(e^{-j\omega t}\)</span> <span
class="math inline">\(\to\)</span> 特征向量<span
class="math inline">\(\phi_w\)</span></li>
<li>振幅<span class="math inline">\(F(w)\)</span> <span
class="math inline">\(\to\)</span> 振幅<span
class="math inline">\(F(\lambda_w)\)</span></li>
</ul>
<p>这样就把传统傅里叶变换推广到了图傅里叶变换。推广到矩阵形式：<br />
<span class="math display">\[
\hat{f} = U^Tf
\]</span><br />
逆变换：<br />
<span class="math display">\[
f = U\hat{f}
\]</span></p>
<h2 id="graph-convolution">Graph Convolution</h2>
<p><strong>卷积定理：函数卷积的傅里叶变换是函数傅立叶变换的乘积，即对于函数
<span class="math inline">\(f\)</span> 与 <span
class="math inline">\(g\)</span>
两者的卷积是其函数傅立叶变换乘积的逆变换</strong>。时域上的卷积-&gt;频域上的相乘后逆变换。从而方便计算，可以看作一种Mapping方式，把时域信号转成频域信号处理。<br />
<span class="math display">\[
f*g = \mathcal{F}^{-1}\{\mathcal{F}\{f\} \cdot \mathcal{F}\{g\}\}
\]</span><br />
其中，<span class="math inline">\(f\)</span> 是图信号，<span
class="math inline">\(g\)</span> 是卷积核。通过 Graph Fourier
transform：<br />
<span class="math display">\[
f*g=U(U^Tg\cdot U^Tf)
\]</span><br />
由于对 <span class="math inline">\(g\)</span> 和 <span
class="math inline">\(f\)</span> 进行傅里叶变换的结果为 <span
class="math inline">\(U^Tg\)</span> 和 <span
class="math inline">\(U^Tf\)</span>
都是一个列向量，所以也可以写成：<br />
<span class="math display">\[
f*g=U(U^Tg\odot U^Tf)
\]</span><br />
<span
class="math inline">\(\odot\)</span>表示哈达马积，对于两个向量，就是进行内积运算；对于维度相同的两个矩阵，就是对应元素的乘积运算。</p>
<p>通常把 <span class="math inline">\(U^Tg\)</span>
整体看作可学习的卷积核，这里把它写作 <span
class="math inline">\(g_{\theta}\)</span>（由参数 <span
class="math inline">\(\theta\)</span> 构成的对角矩阵 <span
class="math inline">\(diag(\theta)\)</span>）。最终图上的卷积公式：<br />
<span class="math display">\[
f*g=Ug_{\theta}U^Tf
\]</span><br />
由于参数 <span class="math inline">\(\theta\)</span> 的确定与 <span
class="math inline">\(L\)</span> 的特征值有关，可把 <span
class="math inline">\(g_{\theta}\)</span> 看作是特征值 <span
class="math inline">\(\Lambda\)</span> 的一个函数，那么可把 <span
class="math inline">\(g_{\theta}\)</span> 看成是拉普拉斯矩阵 <span
class="math inline">\(L\)</span>
的一系列特征值组成的对角矩阵的形式，即定义<span
class="math inline">\(g_{\theta}=diag(U^Tg)=g_{\theta}(\Lambda)\)</span>：<br />
<span class="math display">\[
f*g=Ug_{\theta}(\Lambda)U^Tf=U
\begin{equation*}
    \begin{bmatrix}
    \hat{g}(\lambda_1) &amp;   &amp;\\
    &amp; \ddots &amp; \\
    &amp; &amp; \hat{g}(\lambda_N)
    \end{bmatrix}
    \end{equation*}
U^Tf
\]</span></p>
<h2 id="graph-convolution-networks">Graph Convolution Networks</h2>
<p><strong>第一代GCN</strong>（Spectral CNN）：简单的把 <span
class="math inline">\(g_{\theta}\)</span>（由参数 <span
class="math inline">\(\theta\)</span> 构成的对角矩阵 <span
class="math inline">\(diag(\theta)\)</span>）看作是一个可学习参数的集合，其中
<span class="math inline">\(x\)</span> 是节点特征向量：<br />
<span class="math display">\[
f*g=x*g_{\theta}=Ug_{\theta}U^Tx
\]</span><br />
第一代GCN缺点：<br />
（1）计算复杂度高<span
class="math inline">\(O(n^2)\)</span>，每次计算都需要特征分解求U；每一次前向传播，都要计算<span
class="math inline">\(U,g_{\theta},U^T\)</span> 三者的乘积。<br />
（2）没有正则化（no normalization）。<br />
（3）没有考虑自身权重（no self-weight）。</p>
<hr />
<p><strong>第二代GCN</strong>（ChebNet）：定义特征向量对角矩阵的切比雪夫多项式为滤波器：<br />
<span class="math display">\[
g_{\theta&#39;}(\Lambda) \approx
\sum_{k=0}^{K}\theta_{k}^{&#39;}\Lambda^k=\sum_{k=0}^{K}\theta_{k}^{&#39;}T_{k}(\tilde{\Lambda})
\]</span><br />
其中：</p>
<ul>
<li><span
class="math inline">\(\tilde{\Lambda}=\frac{2}{\lambda_{max}}\Lambda-I_N\)</span>，<span
class="math inline">\(\lambda_{max}\)</span>是L的最大特征值。</li>
<li><span class="math inline">\(\theta \in \mathbb{R}^K\)</span>
是切比雪夫系数的向量。</li>
<li>切比雪夫多项式（类似泰勒展开）定义为：<span
class="math inline">\(T_{k}(x)=2xT_{k-1}(x)-T_{k-2}(x)\)</span>，其中
<span class="math inline">\(T_0(x)=1,T_1(x)=x\)</span>。</li>
</ul>
<p>就是利用Chebyshev多项式拟合卷积核的方法，来降低计算复杂度。但首先提出Chebyshev多项式K阶截断展开来拟合，并对
<span class="math inline">\(\Lambda\)</span>
进行归一化使其元素位于[-1,1]之间的是<span class="exturl" data-url="aHR0cHM6Ly9oYWwuaW5yaWEuZnIvaW5yaWEtMDA1NDE4NTUvZG9jdW1lbnQ=">Hammond et al.(2011)
：Wavelets on graphs via spectral graph
theory<i class="fa fa-external-link-alt"></i></span>，二代GCN借鉴了这一方法。</p>
<p>回到 <span class="math inline">\(g_{\theta}\)</span> 和输入 <span
class="math inline">\(x\)</span> 的卷积：<br />
<span class="math display">\[
\begin{aligned}
g_{\theta}*x &amp;= U \sum_{k=0}^{K}\theta_{k}^{&#39;}\Lambda^k U^Tx\\
&amp;=\sum_{k=0}^{K}\theta_{k}^{&#39;}(U \Lambda^kU^T) x\\
&amp;=\sum_{k=0}^{K}\theta_{k}^{&#39;}(U \Lambda U^T)^k x\\
&amp;=\sum_{k=0}^{K}\theta_{k}^{&#39;}L^{k}x
\end{aligned}
\]</span><br />
这里面就用到拉普拉斯矩阵 <span
class="math inline">\(L\)</span>。计算复杂度为 <span
class="math inline">\(O(kn^2)\)</span>。使用切比雪夫展开，其中 <span
class="math inline">\(\tilde{L}=\frac{2}{\lambda_{max}}L-I_N\)</span>：<br />
<span class="math display">\[
g_{\theta^{&#39;}}*x=\sum_{k=0}^{K}\theta_{k}^{&#39;}T_{x}(\tilde{L})x
\]</span></p>
<hr />
<p><strong>第三代GCN</strong>（一阶ChebNet）：只对切比雪夫展开到一阶，即
<span class="math inline">\(K=1,\lambda_{max}=2\)</span>，那么 <span
class="math inline">\(\tilde{L}=L-I_N\)</span>，且 <span
class="math inline">\(T_0(\tilde{L})=1,T_1(\tilde{L})=\tilde{L}\)</span>
，第二代公式可简化为：<br />
<span class="math display">\[
\begin{aligned}
g_{\theta^{&#39;}}*x &amp;=
\theta_0^{&#39;}T_0(\tilde{L})x+\theta_1^{&#39;}T_1(\tilde{L})x\\
&amp;=\theta_0^{&#39;}x+\theta_1^{&#39;}(L-I_N)x\\
\end{aligned}
\]</span><br />
对<span class="math inline">\(L\)</span>做归一化处理：<br />
<span class="math display">\[
\begin{aligned}
\hat{L}&amp;=D^{-\frac{1}{2}}(L)D^{-\frac{1}{2}}\\
&amp;=D^{-\frac{1}{2}}(D-W)D^{-\frac{1}{2}}\\
&amp;=I_N-D^{-\frac{1}{2}}WD^{-\frac{1}{2}}
\end{aligned}
\]</span><br />
代入到前式中得到：<br />
<span class="math display">\[
\theta_0^{\prime}x+\theta_1^{\prime}(L-I_N)x=\theta_0^{&#39;}x+(-\theta_1^{&#39;}(D^{-\frac{1}{2}}WD^{-\frac{1}{2}})x)
\]</span><br />
由于不希望 <span class="math inline">\(\theta_0^{\prime}\)</span> 和
<span class="math inline">\(\theta_1^{\prime}\)</span> 出现，所以假设
<span
class="math inline">\(\theta_0^{\prime}=-\theta_1^{\prime}=\theta\)</span>：<br />
<span class="math display">\[
g_{\theta^{&#39;}}*x=\theta(I_N+D^{-\frac{1}{2}}WD^{-\frac{1}{2}})x
\]</span><br />
注意 <span
class="math inline">\(I_N+D^{-\frac{1}{2}}WD^{-\frac{1}{2}}\)</span>
的特征值被限制在了[0,2]中。由于这一步输出可能作为下一层的输入，会再次与
<span
class="math inline">\(I_N+D^{-\frac{1}{2}}WD^{-\frac{1}{2}}\)</span>
相乘重复这样的操作将会导致数值不稳定、梯度消失/爆炸等问题。</p>
<p>为了解决该问题，引入renormalization（就是加了自环）：令 <span
class="math inline">\(\tilde{W}=W+I_N, \tilde{D}_i=\sum_j
\tilde{W}_{ij}\)</span>：<br />
<span class="math display">\[
I_N+D^{-\frac{1}{2}}WD^{-\frac{1}{2}} \approx
\tilde{D}^{-\frac{1}{2}}\tilde{W}\tilde{D}^{-\frac{1}{2}}
\]</span><br />
那么，带入之前的公式得到：<br />
<span class="math display">\[
\underbrace{\boldsymbol{g}_{\boldsymbol{\theta^{\prime}}} *
\boldsymbol{x}}_{\mathbb{R}^{n \times n}} =
\theta(\underbrace{\tilde{\boldsymbol{D}}^{-1/2}\tilde{\boldsymbol{W}}
\tilde{\boldsymbol{D}}^{-1/2}}_{\mathbb{R}^{n \times n}})
\underbrace{\boldsymbol{x}}_{\mathbb{R}^{n \times 1}}
\]</span><br />
推广到多通道和多卷积，则卷积结果写作矩阵形式如下：<br />
<span class="math display">\[
\underbrace{\boldsymbol{Z}}_{\mathbb{R}^{N \times F}} =
\underbrace{\tilde{\boldsymbol{D}}^{-1/2}\tilde{\boldsymbol{W}}
\tilde{\boldsymbol{D}}^{-1/2}}_{\mathbb{R}^{N \times N}}
\underbrace{\boldsymbol{X}}_{\mathbb{R}^{N \times C}} \ \
\underbrace{\boldsymbol{\Theta}}_{\mathbb{R}^{C \times F}}
\]</span><br />
其中，<span class="math inline">\(N\)</span>
是<strong>节点数量</strong>，<span class="math inline">\(C\)</span>
是通道数或者称作节点的<strong>特征维度</strong>，<span
class="math inline">\(F\)</span> 为<strong>卷积核数量</strong>。<span
class="math inline">\(D\)</span> 就是<strong>度矩阵</strong>，<span
class="math inline">\(W\)</span> 就是<strong>邻接矩阵</strong>，<span
class="math inline">\(X\)</span>
是节点的<strong>特征矩阵</strong>，<span
class="math inline">\(\Theta\)</span>
是<strong>卷积核参数矩阵</strong>，最终得到的卷积结果 <span
class="math inline">\(\boldsymbol{Z} \in \mathbb{R}^{N \times
F}\)</span>，即每个节点的卷积结果的维数等于卷积核数量。上述操作可以叠加多层，对
<span class="math inline">\(Z\)</span> 激活一下，然后将激活后的 <span
class="math inline">\(Z\)</span> 作为下一层的节点的特征矩阵。</p>
<p>第三代GCN特点总结：</p>
<ul>
<li>解决了计算复杂度高的问题：复杂度为<span
class="math inline">\(O(E)\)</span> (稀疏矩阵优化的话)，<span
class="math inline">\(E\)</span> 是图中边的几何。</li>
<li>只考虑1-hop，若要建模多hop，通过叠加层数，获得更大的感受野。（联想NLP中使用卷积操作语句序列时，也是通过叠加多层来达到获取长依赖的目的）。</li>
</ul>
<h1 id="code">Code</h1>
<p>作者给出了源码，分两个版本：</p>
<ul>
<li>tensorflow：<span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL3RraXBmL2djbg==">gcn<i class="fa fa-external-link-alt"></i></span></li>
<li>pytorch：<span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL3RraXBmL3B5Z2Nu">pygcn<i class="fa fa-external-link-alt"></i></span></li>
<li>数据集地址：<span class="exturl" data-url="aHR0cHM6Ly9saW5xcy1kYXRhLnNvZS51Y3NjLmVkdS9wdWJsaWMvbGJjL2NvcmEudGd6">cora<i class="fa fa-external-link-alt"></i></span></li>
</ul>
<p>cora数据集有2708个样本，每个样本由1433维特征表示，每个样本是一篇科学论文，每篇论文可能为7个类别，样本和样本之间包括了5429个连接。</p>
<p>模型输入：<br />
<strong>X</strong>：N×D的特征矩阵，N表示节点数量（cora数据集就是2708），D表示输入特征（cora数据集就是1433）。<br />
<strong>A</strong>：邻接矩阵。<br />
模型输出：<br />
<strong>Z</strong>：N×F的特征矩阵，F是每个输出节点的特征维度（这个维度自己设置）。</p>
<p>使用的公式：<br />
<span class="math inline">\(H^{(l+1)}=f(H^{(l)},A),\qquad H^{(0)}=X,
H^{(L)}=Z\)</span><br />
<span class="math inline">\(f(H^{(l)},A)=\sigma(AH^{(l)}W^{(l)}),\qquad
f(H^{(l)},A)=\sigma(\hat{D}^{-\frac{1}{2}}\hat{A}\hat{D}^{-\frac{1}{2}}H^{(l)}W^{(l)})\)</span></p>
<p>其中，<span class="math inline">\(\hat{A}=A+I\)</span>，<span
class="math inline">\(I\)</span>是对角矩阵(自环)，<span
class="math inline">\(\hat{A}\)</span>
是加上自环(节点本身信息)后的邻接矩阵。如果一个节点有非常多的邻居，那么函数<span
class="math inline">\(f\)</span>就会越来越大，所以加上一个归一化<span
class="math inline">\(\hat{D}\)</span>是<span
class="math inline">\(\hat{A}\)</span>的度矩阵，有两种方法<span
class="math inline">\(\hat{D}^{-1}A\)</span>和<span
class="math inline">\(\hat{D}^{-\frac{1}{2}}\hat{A}\hat{D}^{-\frac{1}{2}}\)</span>。</p>
<p>以pytorch版本为例：<br />
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">文件结构：</span><br><span class="line">├── data      // 图数据</span><br><span class="line">├── pygcn</span><br><span class="line">    ├── inits    // 初始化的一些公用函数</span><br><span class="line">    ├── layers     // GCN层的定义</span><br><span class="line">        ├── class GraphConvolution</span><br><span class="line">        ├── reset parameters</span><br><span class="line">        ├── forward</span><br><span class="line">    ├── models     // 模型结构定义</span><br><span class="line">        ├── class GCN</span><br><span class="line">        ├── forward</span><br><span class="line">    ├── train    // 训练</span><br><span class="line">        ├── def train</span><br><span class="line">        ├── def test </span><br><span class="line">    └── utils    //  工具函数的定义</span><br><span class="line">        ├── encode_onehot</span><br><span class="line">        ├── load_data</span><br><span class="line">        ├── normazlize</span><br><span class="line">        ├── accuracy</span><br><span class="line">        ├── sparse mx to torch sparse tensor</span><br><span class="line">├── setup.py //启动函数</span><br></pre></td></tr></table></figure></p>
<figure class="highlight python"><figcaption><span>model.py</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">from</span> pygcn.layers <span class="keyword">import</span> GraphConvolution</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">GCN</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, nfeat, nhid, nclass, dropout</span>):</span><br><span class="line">        <span class="built_in">super</span>(GCN, self).__init__()</span><br><span class="line"></span><br><span class="line">        self.gc1 = GraphConvolution(nfeat, nhid)  <span class="comment"># nfeat：N×D的D</span></span><br><span class="line">        self.gc2 = GraphConvolution(nhid, nclass) <span class="comment"># nclass：类别，这里是7类</span></span><br><span class="line">        self.dropout = dropout</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, adj</span>):</span><br><span class="line">        x = F.relu(self.gc1(x, adj)) <span class="comment"># 第一层输出+relu</span></span><br><span class="line">        x = F.dropout(x, self.dropout, training=self.training)</span><br><span class="line">        x = self.gc2(x, adj)  <span class="comment"># 第二层输出</span></span><br><span class="line">        <span class="keyword">return</span> F.log_softmax(x, dim=<span class="number">1</span>) <span class="comment"># 第二层输出+log_softmax</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><figcaption><span>layers.py</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> math</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> torch.nn.parameter <span class="keyword">import</span> Parameter</span><br><span class="line"><span class="keyword">from</span> torch.nn.modules.module <span class="keyword">import</span> Module</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">GraphConvolution</span>(<span class="title class_ inherited__">Module</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Simple GCN layer, similar to https://arxiv.org/abs/1609.02907</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_features, out_features, bias=<span class="literal">True</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(GraphConvolution, self).__init__()</span><br><span class="line">        self.in_features = in_features  <span class="comment"># 每层的输入维度</span></span><br><span class="line">        self.out_features = out_features   <span class="comment"># 每层的输出维度</span></span><br><span class="line">        self.weight = Parameter(torch.FloatTensor(in_features, out_features))</span><br><span class="line">        <span class="keyword">if</span> bias:</span><br><span class="line">            self.bias = Parameter(torch.FloatTensor(out_features))</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.register_parameter(<span class="string">&#x27;bias&#x27;</span>, <span class="literal">None</span>)</span><br><span class="line">        self.reset_parameters()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">reset_parameters</span>(<span class="params">self</span>): <span class="comment"># 参数初始化方法</span></span><br><span class="line">        stdv = <span class="number">1.</span> / math.sqrt(self.weight.size(<span class="number">1</span>))</span><br><span class="line">        self.weight.data.uniform_(-stdv, stdv)</span><br><span class="line">        <span class="keyword">if</span> self.bias <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            self.bias.data.uniform_(-stdv, stdv)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, <span class="built_in">input</span>, adj</span>): <span class="comment"># 实现AHW，第一次时H是X</span></span><br><span class="line">        support = torch.mm(<span class="built_in">input</span>, self.weight) <span class="comment"># 实现XW</span></span><br><span class="line">        <span class="comment"># Sparse matrix multiplication, https://github.com/tkipf/pygcn/issues/19</span></span><br><span class="line">        <span class="comment"># output = torch.spmm(adj, support) # spmm后续版本被移除了，使用sparse.mm替代</span></span><br><span class="line">        output = torch.sparse.mm(adj, support) <span class="comment"># 实现AXW</span></span><br><span class="line">        <span class="keyword">if</span> self.bias <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">return</span> output + self.bias</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> output</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__repr__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> self.__class__.__name__ + <span class="string">&#x27; (&#x27;</span> \</span><br><span class="line">               + <span class="built_in">str</span>(self.in_features) + <span class="string">&#x27; -&gt; &#x27;</span> \</span><br><span class="line">               + <span class="built_in">str</span>(self.out_features) + <span class="string">&#x27;)&#x27;</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><figcaption><span>util.py</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> scipy.sparse <span class="keyword">as</span> sp</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="comment"># focus</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">encode_onehot</span>(<span class="params">labels</span>):</span><br><span class="line">    classes = <span class="built_in">set</span>(labels)</span><br><span class="line">    classes_dict = &#123;c: np.identity(<span class="built_in">len</span>(classes))[i, :] <span class="keyword">for</span> i, c <span class="keyword">in</span></span><br><span class="line">                    <span class="built_in">enumerate</span>(classes)&#125;</span><br><span class="line">    labels_onehot = np.array(<span class="built_in">list</span>(<span class="built_in">map</span>(classes_dict.get, labels)),</span><br><span class="line">                             dtype=np.int32)</span><br><span class="line">    <span class="keyword">return</span> labels_onehot</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">load_data</span>(<span class="params">path=<span class="string">&quot;../data/cora/&quot;</span>, dataset=<span class="string">&quot;cora&quot;</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Load citation network dataset (cora only for now)&quot;&quot;&quot;</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Loading &#123;&#125; dataset...&#x27;</span>.<span class="built_in">format</span>(dataset))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 读进来的数据为：idx，features，labels</span></span><br><span class="line">    idx_features_labels = np.genfromtxt(<span class="string">&quot;&#123;&#125;&#123;&#125;.content&quot;</span>.<span class="built_in">format</span>(path, dataset),</span><br><span class="line">                                        dtype=np.dtype(<span class="built_in">str</span>))</span><br><span class="line">    <span class="comment"># csr_matrix数据存储成稀疏方式，格式为csr</span></span><br><span class="line">    features = sp.csr_matrix(idx_features_labels[:, <span class="number">1</span>:-<span class="number">1</span>], dtype=np.float32)</span><br><span class="line">    labels = encode_onehot(idx_features_labels[:, -<span class="number">1</span>]) <span class="comment"># 使用onehot编码类别 (2708, 7)</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># build graph</span></span><br><span class="line">    idx = np.array(idx_features_labels[:, <span class="number">0</span>], dtype=np.int32)</span><br><span class="line">    idx_map = &#123;j: i <span class="keyword">for</span> i, j <span class="keyword">in</span> <span class="built_in">enumerate</span>(idx)&#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 读进来的数据为：edges，unordered</span></span><br><span class="line">    <span class="comment"># [[     35,    1033],</span></span><br><span class="line">    <span class="comment">#  [     35,  103482],...]</span></span><br><span class="line">    edges_unordered = np.genfromtxt(<span class="string">&quot;&#123;&#125;&#123;&#125;.cites&quot;</span>.<span class="built_in">format</span>(path, dataset),</span><br><span class="line">                                    dtype=np.int32)</span><br><span class="line">    <span class="comment"># 转成对应map编号</span></span><br><span class="line">    <span class="comment"># [[ 163,  402],</span></span><br><span class="line">    <span class="comment">#  [ 163,  659],...]</span></span><br><span class="line">    edges = np.array(<span class="built_in">list</span>(<span class="built_in">map</span>(idx_map.get, edges_unordered.flatten())),</span><br><span class="line">                     dtype=np.int32).reshape(edges_unordered.shape)</span><br><span class="line">    <span class="comment"># (edges[:, 0], edges[:, 1])坐标点，(np.ones(edges.shape[0])每个坐标位置的值为1</span></span><br><span class="line">    <span class="comment"># 此步得到的是有向图邻接矩阵</span></span><br><span class="line">    adj = sp.coo_matrix((np.ones(edges.shape[<span class="number">0</span>]), (edges[:, <span class="number">0</span>], edges[:, <span class="number">1</span>])),</span><br><span class="line">                        shape=(labels.shape[<span class="number">0</span>], labels.shape[<span class="number">0</span>]),</span><br><span class="line">                        dtype=np.float32)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># build symmetric adjacency matrix !</span></span><br><span class="line">    <span class="comment"># 无向图，邻接矩阵是对称的，https://zhuanlan.zhihu.com/p/78191258</span></span><br><span class="line">    <span class="comment"># see https://github.com/tkipf/pygcn/issues/3</span></span><br><span class="line">    adj = adj + adj.T.multiply(adj.T &gt; adj) - adj.multiply(adj.T &gt; adj)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># see https://github.com/tkipf/pygcn/issues/47</span></span><br><span class="line">    <span class="comment"># 归一化防止梯度消失</span></span><br><span class="line">    features = normalize(features)</span><br><span class="line">    adj = normalize(adj + sp.eye(adj.shape[<span class="number">0</span>])) <span class="comment"># 加对角矩阵I，即A+I=\hat&#123;A&#125;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 切分数据</span></span><br><span class="line">    idx_train = <span class="built_in">range</span>(<span class="number">140</span>)</span><br><span class="line">    idx_val = <span class="built_in">range</span>(<span class="number">200</span>, <span class="number">500</span>)</span><br><span class="line">    idx_test = <span class="built_in">range</span>(<span class="number">500</span>, <span class="number">1500</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 转成Tensor</span></span><br><span class="line">    features = torch.FloatTensor(np.array(features.todense()))</span><br><span class="line">    labels = torch.LongTensor(np.where(labels)[<span class="number">1</span>])</span><br><span class="line">    adj = sparse_mx_to_torch_sparse_tensor(adj)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 转成Tensor</span></span><br><span class="line">    idx_train = torch.LongTensor(idx_train)</span><br><span class="line">    idx_val = torch.LongTensor(idx_val)</span><br><span class="line">    idx_test = torch.LongTensor(idx_test)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> adj, features, labels, idx_train, idx_val, idx_test</span><br><span class="line"></span><br><span class="line"><span class="comment"># focus</span></span><br><span class="line"><span class="comment"># 对特征矩阵features和邻接矩阵adj做标准化，防止梯度消失</span></span><br><span class="line"><span class="comment"># 每个值除以它所在行的和</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">normalize</span>(<span class="params">mx</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Row-normalize sparse matrix&quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># see https://github.com/tkipf/gcn/blob/master/gcn/utils.py#L122</span></span><br><span class="line">    <span class="comment"># 对每行求和得到rowsum</span></span><br><span class="line">    rowsum = np.array(mx.<span class="built_in">sum</span>(<span class="number">1</span>))</span><br><span class="line">    <span class="comment"># 求逆得到r_inv</span></span><br><span class="line">    r_inv = np.power(rowsum, -<span class="number">1</span>).flatten()</span><br><span class="line">    <span class="comment"># 如果某一行全为0，则r_inv算出来会等于无穷大，将这些行的r_inv置为0</span></span><br><span class="line">    r_inv[np.isinf(r_inv)] = <span class="number">0.</span></span><br><span class="line">    r_mat_inv = sp.diags(r_inv)</span><br><span class="line">    mx = r_mat_inv.dot(mx)</span><br><span class="line">    <span class="keyword">return</span> mx</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">accuracy</span>(<span class="params">output, labels</span>):</span><br><span class="line">    preds = output.<span class="built_in">max</span>(<span class="number">1</span>)[<span class="number">1</span>].type_as(labels)</span><br><span class="line">    correct = preds.eq(labels).double()</span><br><span class="line">    correct = correct.<span class="built_in">sum</span>()</span><br><span class="line">    <span class="keyword">return</span> correct / <span class="built_in">len</span>(labels)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">sparse_mx_to_torch_sparse_tensor</span>(<span class="params">sparse_mx</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Convert a scipy sparse matrix to a torch sparse tensor.&quot;&quot;&quot;</span></span><br><span class="line">    sparse_mx = sparse_mx.tocoo().astype(np.float32)</span><br><span class="line">    indices = torch.from_numpy(</span><br><span class="line">        np.vstack((sparse_mx.row, sparse_mx.col)).astype(np.int64))</span><br><span class="line">    values = torch.from_numpy(sparse_mx.data)</span><br><span class="line">    shape = torch.Size(sparse_mx.shape)</span><br><span class="line">    <span class="keyword">return</span> torch.sparse.FloatTensor(indices, values, shape)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><figcaption><span>train.py</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> division</span><br><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> print_function</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> pygcn.utils <span class="keyword">import</span> load_data, accuracy</span><br><span class="line"><span class="keyword">from</span> pygcn.models <span class="keyword">import</span> GCN</span><br><span class="line"></span><br><span class="line"><span class="comment"># Training settings</span></span><br><span class="line">parser = argparse.ArgumentParser()</span><br><span class="line">parser.add_argument(<span class="string">&#x27;--no-cuda&#x27;</span>, action=<span class="string">&#x27;store_true&#x27;</span>, default=<span class="literal">False</span>,</span><br><span class="line">                    <span class="built_in">help</span>=<span class="string">&#x27;Disables CUDA training.&#x27;</span>)</span><br><span class="line">parser.add_argument(<span class="string">&#x27;--fastmode&#x27;</span>, action=<span class="string">&#x27;store_true&#x27;</span>, default=<span class="literal">False</span>,</span><br><span class="line">                    <span class="built_in">help</span>=<span class="string">&#x27;Validate during training pass.&#x27;</span>)</span><br><span class="line">parser.add_argument(<span class="string">&#x27;--seed&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">int</span>, default=<span class="number">42</span>, <span class="built_in">help</span>=<span class="string">&#x27;Random seed.&#x27;</span>)</span><br><span class="line">parser.add_argument(<span class="string">&#x27;--epochs&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">int</span>, default=<span class="number">200</span>,</span><br><span class="line">                    <span class="built_in">help</span>=<span class="string">&#x27;Number of epochs to train.&#x27;</span>)</span><br><span class="line">parser.add_argument(<span class="string">&#x27;--lr&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">float</span>, default=<span class="number">0.01</span>,</span><br><span class="line">                    <span class="built_in">help</span>=<span class="string">&#x27;Initial learning rate.&#x27;</span>)</span><br><span class="line">parser.add_argument(<span class="string">&#x27;--weight_decay&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">float</span>, default=<span class="number">5e-4</span>,</span><br><span class="line">                    <span class="built_in">help</span>=<span class="string">&#x27;Weight decay (L2 loss on parameters).&#x27;</span>)</span><br><span class="line">parser.add_argument(<span class="string">&#x27;--hidden&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">int</span>, default=<span class="number">16</span>,</span><br><span class="line">                    <span class="built_in">help</span>=<span class="string">&#x27;Number of hidden units.&#x27;</span>)</span><br><span class="line">parser.add_argument(<span class="string">&#x27;--dropout&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">float</span>, default=<span class="number">0.5</span>,</span><br><span class="line">                    <span class="built_in">help</span>=<span class="string">&#x27;Dropout rate (1 - keep probability).&#x27;</span>)</span><br><span class="line"></span><br><span class="line">args = parser.parse_args()</span><br><span class="line">args.cuda = <span class="keyword">not</span> args.no_cuda <span class="keyword">and</span> torch.cuda.is_available()</span><br><span class="line"></span><br><span class="line">np.random.seed(args.seed)</span><br><span class="line">torch.manual_seed(args.seed)</span><br><span class="line"><span class="keyword">if</span> args.cuda:</span><br><span class="line">    torch.cuda.manual_seed(args.seed)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Load data</span></span><br><span class="line">adj, features, labels, idx_train, idx_val, idx_test = load_data()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Model and optimizer</span></span><br><span class="line">model = GCN(nfeat=features.shape[<span class="number">1</span>],</span><br><span class="line">            nhid=args.hidden,</span><br><span class="line">            nclass=labels.<span class="built_in">max</span>().item() + <span class="number">1</span>,</span><br><span class="line">            dropout=args.dropout)</span><br><span class="line">optimizer = optim.Adam(model.parameters(),</span><br><span class="line">                       lr=args.lr, weight_decay=args.weight_decay)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> args.cuda:</span><br><span class="line">    model.cuda()</span><br><span class="line">    features = features.cuda()</span><br><span class="line">    adj = adj.cuda()</span><br><span class="line">    labels = labels.cuda()</span><br><span class="line">    idx_train = idx_train.cuda()</span><br><span class="line">    idx_val = idx_val.cuda()</span><br><span class="line">    idx_test = idx_test.cuda()</span><br><span class="line"></span><br><span class="line"><span class="comment"># focus</span></span><br><span class="line"><span class="comment"># 这里训练时给140个带标签，输入的是全部数据特征，整体是个半监督的任务</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">train</span>(<span class="params">epoch</span>):</span><br><span class="line">    t = time.time()</span><br><span class="line">    model.train()</span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    output = model(features, adj)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># classify each node</span></span><br><span class="line">    <span class="comment"># 只考虑 train ids 计算 loss</span></span><br><span class="line">    <span class="comment"># see https://github.com/tkipf/pygcn/issues/50</span></span><br><span class="line">    <span class="comment"># 如果输出用softmax，这里就用交叉熵损失cross_entropy</span></span><br><span class="line">    <span class="comment"># 这里使用负对数似然损失nll_loss，因为前面输出用的是log_softmax</span></span><br><span class="line">    <span class="comment"># torch.nn.CrossEntropyLoss、cross_entropy都是上面两个函数的组合nll_loss(log_softmax(input))</span></span><br><span class="line">    loss_train = F.nll_loss(output[idx_train], labels[idx_train])</span><br><span class="line">    acc_train = accuracy(output[idx_train], labels[idx_train])</span><br><span class="line">    loss_train.backward()</span><br><span class="line">    optimizer.step()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> args.fastmode:</span><br><span class="line">        <span class="comment"># Evaluate validation set performance separately,</span></span><br><span class="line">        <span class="comment"># deactivates dropout during validation run.</span></span><br><span class="line">        model.<span class="built_in">eval</span>()</span><br><span class="line">        output = model(features, adj)</span><br><span class="line"></span><br><span class="line">    loss_val = F.nll_loss(output[idx_val], labels[idx_val])</span><br><span class="line">    acc_val = accuracy(output[idx_val], labels[idx_val])</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Epoch: &#123;:04d&#125;&#x27;</span>.<span class="built_in">format</span>(epoch+<span class="number">1</span>),</span><br><span class="line">          <span class="string">&#x27;loss_train: &#123;:.4f&#125;&#x27;</span>.<span class="built_in">format</span>(loss_train.item()),</span><br><span class="line">          <span class="string">&#x27;acc_train: &#123;:.4f&#125;&#x27;</span>.<span class="built_in">format</span>(acc_train.item()),</span><br><span class="line">          <span class="string">&#x27;loss_val: &#123;:.4f&#125;&#x27;</span>.<span class="built_in">format</span>(loss_val.item()),</span><br><span class="line">          <span class="string">&#x27;acc_val: &#123;:.4f&#125;&#x27;</span>.<span class="built_in">format</span>(acc_val.item()),</span><br><span class="line">          <span class="string">&#x27;time: &#123;:.4f&#125;s&#x27;</span>.<span class="built_in">format</span>(time.time() - t))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">test</span>():</span><br><span class="line">    model.<span class="built_in">eval</span>()</span><br><span class="line">    output = model(features, adj)</span><br><span class="line">    loss_test = F.nll_loss(output[idx_test], labels[idx_test])</span><br><span class="line">    acc_test = accuracy(output[idx_test], labels[idx_test])</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Test set results:&quot;</span>,</span><br><span class="line">          <span class="string">&quot;loss= &#123;:.4f&#125;&quot;</span>.<span class="built_in">format</span>(loss_test.item()),</span><br><span class="line">          <span class="string">&quot;accuracy= &#123;:.4f&#125;&quot;</span>.<span class="built_in">format</span>(acc_test.item()))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Train model</span></span><br><span class="line">t_total = time.time()</span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(args.epochs):</span><br><span class="line">    train(epoch)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Optimization Finished!&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Total time elapsed: &#123;:.4f&#125;s&quot;</span>.<span class="built_in">format</span>(time.time() - t_total))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Testing</span></span><br><span class="line">test()</span><br></pre></td></tr></table></figure>
<h1 id="参考文献">参考文献</h1>
<p><span class="exturl" data-url="aHR0cHM6Ly96aC53aWtpcGVkaWEub3JnL3dpa2kvJUU5JTlBJThGJUU2JTlDJUJBJUU1JTlCJUJF">随机图<i class="fa fa-external-link-alt"></i></span><br />
<span class="exturl" data-url="aHR0cHM6Ly96aC53aWtpcGVkaWEub3JnL3dpa2kvJUU2JTk3JUEwJUU1JUIwJUJBJUU1JUJBJUE2JUU3JUJEJTkxJUU3JUJCJTlD">无尺度网络<i class="fa fa-external-link-alt"></i></span>，<span class="exturl" data-url="aHR0cHM6Ly93d3cuY25ibG9ncy5jb20vbGVlengvcC85NDM2ODIwLmh0bWw=">Scale Free
Network<i class="fa fa-external-link-alt"></i></span></p>
<p><span class="exturl" data-url="aHR0cHM6Ly96aC53aWtpcGVkaWEub3JnL3dpa2kvJUU1JUJBJUE2JUU1JTg4JTg2JUU1JUI4JTgz">度分布<i class="fa fa-external-link-alt"></i></span><br />
<span class="exturl" data-url="aHR0cHM6Ly96aC53aWtpcGVkaWEub3JnL3dpa2kvJUU5JTgyJUJCJUU2JThFJUE1JUU3JTlGJUE5JUU5JTk4JUI1">邻接矩阵<i class="fa fa-external-link-alt"></i></span><br />
<span class="exturl" data-url="aHR0cHM6Ly9lbi53aWtpcGVkaWEub3JnL3dpa2kvQ2x1c3RlcmluZ19jb2VmZmljaWVudA==">Clustering
coefficient<i class="fa fa-external-link-alt"></i></span><br />
<span class="exturl" data-url="aHR0cHM6Ly9lbi53aWtpcGVkaWEub3JnL3dpa2kvQmV0d2Vlbm5lc3M=">Betweenness<i class="fa fa-external-link-alt"></i></span><br />
<span class="exturl" data-url="aHR0cHM6Ly93d3cuemhpaHUuY29tL3F1ZXN0aW9uLzE5OTY3Nzc4">如何理解希尔伯特空间？<i class="fa fa-external-link-alt"></i></span>，<span class="exturl" data-url="aHR0cHM6Ly96aC53aWtpcGVkaWEub3JnL3dpa2kvJUU1JUI4JThDJUU1JUIwJTk0JUU0JUJDJUFGJUU3JTg5JUI5JUU3JUE5JUJBJUU5JTk3JUI0">希尔伯特空间<i class="fa fa-external-link-alt"></i></span><br />
<span class="exturl" data-url="aHR0cHM6Ly9lbi53aWtpcGVkaWEub3JnL3dpa2kvRm91cmllcl90cmFuc2Zvcm0=">Fourier
transform<i class="fa fa-external-link-alt"></i></span>，<span class="exturl" data-url="aHR0cHM6Ly96aC53aWtpcGVkaWEub3JnL3dpa2kvJUU1JTgyJTg1JUU5JTg3JThDJUU1JThGJUI2JUU1JThGJTk4JUU2JThEJUEy">傅里叶变换<i class="fa fa-external-link-alt"></i></span></p>
<p><span class="exturl" data-url="aHR0cDovL3RraXBmLmdpdGh1Yi5pby9ncmFwaC1jb252b2x1dGlvbmFsLW5ldHdvcmtzLw==">GRAPH
CONVOLUTIONAL NETWORKS<i class="fa fa-external-link-alt"></i></span><br />
<span class="exturl" data-url="aHR0cHM6Ly93d3cuc29odS5jb20vYS8zNDI2MzQyOTFfNjUxODkz">跳出公式，看清全局，图神经网络（GCN）原理详解<i class="fa fa-external-link-alt"></i></span><br />
<span class="exturl" data-url="aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3l5bDQyNDUyNS9hcnRpY2xlL2RldGFpbHMvMTAwMDU4MjY0I0dDTl84Mjg=">图卷积网络
GCN Graph Convolutional Network（谱域GCN）的理解和详细推导<i class="fa fa-external-link-alt"></i></span><br />
<span class="exturl" data-url="aHR0cDovL3h0ZjYxNS5jb20vMjAxOS8wMi8yNC9nY24v">图卷积神经网络理论基础<i class="fa fa-external-link-alt"></i></span><br />
<span class="exturl" data-url="aHR0cHM6Ly96aHVhbmxhbi56aGlodS5jb20vcC83NjAwMTA4MA==">GNN综述——从入门到入门<i class="fa fa-external-link-alt"></i></span><br />
<span class="exturl" data-url="aHR0cHM6Ly93d3cuY25ibG9ncy5jb20vbnhmLXJhYmJpdDc1L3AvMTEzMDYxOTguaHRtbA==">GCN总结<i class="fa fa-external-link-alt"></i></span></p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://soundmemories.github.io/2021/02/03/Graph/00.%E5%9B%BE%E6%B7%B1%E5%BA%A6%E8%A1%A8%E7%A4%BA/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avstar.png">
      <meta itemprop="name" content="SoundMemories">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="SoundMemories">
      <meta itemprop="description" content="今日事，今日毕">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | SoundMemories">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2021/02/03/Graph/00.%E5%9B%BE%E6%B7%B1%E5%BA%A6%E8%A1%A8%E7%A4%BA/" class="post-title-link" itemprop="url">图深度表示</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2021-02-03 00:00:00" itemprop="dateCreated datePublished" datetime="2021-02-03T00:00:00+08:00">2021-02-03</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Graph/" itemprop="url" rel="index"><span itemprop="name">Graph</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>3k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>11 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="图结构">图结构</h1>
<p>在现实中，很多情景的构成是不规则的图结构，比如社交网络、金融网络、化学分子结构等等。</p>
<h1 id="什么是图表示学习">什么是图表示学习？</h1>
<p>简单讲就是把图结构映射到向量空间（也叫graph
embedding），或者由向量空间映射到图结构（也叫 graph generate）。</p>
<p>我们为什么这么做呢？或者说将图映射到向量空间的优势是什么？<br />
（1）向量表示相对传统图表示（邻接矩阵、邻接表）对现有机器学习算法更友好。<br />
（2）可以更好的将拓扑信息和节点本身特征结合。</p>
<h1 id="基于图结构的表示学习">基于图结构的表示学习</h1>
<p>图论、数据挖掘角度：如何在学习到向量的表示中保留尽可能多的图拓扑结构的信息。</p>
<p>节点的向量表示只来源于图的拓扑结构（nxn
的邻接矩阵表达的图结构），只是对图结构的单一表示，缺乏对图节点特征消息的表示。下图d远远小于n。</p>
<p><img src="/images/图深度表示/图表示1.png" width="100%"></p>
<h2 id="传统方法">传统方法</h2>
<p>第一个想法就是<strong>降维</strong>（nxn -&gt;
nxd），利用现有的降维方法实现，比如PCA、LDA等等。致命缺点：时间复杂度高；<strong>不能保留图中节点和节点之间的拓扑信息</strong>。</p>
<ul>
<li><p>这里有个问题：什么叫做<strong>保留拓扑信息</strong>？</p></li>
<li><p>目标：<strong>在拓扑域里面‘邻近’&lt;==&gt;
在向量域‘邻近’</strong>。</p></li>
<li><p>问题：<strong>如何建模节点之间的邻近信息？ -&gt;
如何定义'邻近'？</strong></p></li>
<li><p>定义邻近的方法很多：<strong>共同出现、高阶邻近（n-hop邻居）、团体邻近（属于某一个团体）</strong>。</p></li>
</ul>
<p>所以现有算法都是围绕着 <strong>定义‘邻近’</strong> 和
<strong>求解‘邻近’</strong> 这两个点展开。</p>
<h2 id="deepwalk">Deepwalk</h2>
<ul>
<li><strong>动机</strong>：如何动态的建模邻居信息？</li>
<li><strong>1-hop建模</strong>：对于某个节点只看其邻居节点。两个相邻的结点就可以定义为邻近。
<ul>
<li>太局部，忽略了图上的一些全局信息。</li>
</ul></li>
<li><strong>n-hop建模</strong>：对于某个节点考虑其n步邻居节点。两个n阶临近的结点也可以定义为邻近。
<ul>
<li>组合爆炸，复杂度高，且没必要。</li>
</ul></li>
<li>解决方法：
<ul>
<li>考虑有限步的情况，例如只考虑1，2 hop，即 LINE 2015。</li>
<li>使用采样的方式————随机游走（Random Walk）思想的 Deepwalk 2014。</li>
</ul></li>
</ul>
<p><strong>Deepwalk</strong>：<strong>利用随机游走采样生成的序列去定义节点间的邻近关系</strong>。<strong>在足够多的采样情况下，可以很好的刻画节点之间的邻近信息</strong>。<strong>这样就把图信息，转成了序列信息，通过Word2Vec把序列向量化即可（每个点看成词）</strong>。<br />
总结：<br />
（1）使用定长的随机游走去采样图中节点的邻近关系。<br />
（2）节点-&gt;词语，随机游走序列-&gt;句子。<br />
（3）使用自然语言处理相关模型（例如word2vec）对随机游走得到的序列进行表示学习。</p>
<p>基于Random Walk的思路出现了很多 XXX2vec 的论文，基本套路都一样。</p>
<h2 id="node2vec">Node2vec</h2>
<p><strong>动机</strong>：简单的随机游走采样不够好（不能体现出BFS/DFS性质）。<br />
<strong>核心思想</strong>：等概率跳 -&gt; 人工设计概率来跳。</p>
<p>当从结点 t 跳跃到结点 v 之后，算法下一步从结点 v
向邻居结点跳跃的概率是不同的。<br />
<img src="/images/图深度表示/node2vec.png" width="50%"></p>
<p>从结点 v 回跳到上一个结点 t 的 <span
class="math inline">\(\alpha\)</span> 为 <span
class="math inline">\(\frac{1}{p}\)</span>，从结点 v 跳到 t、v
的公共邻居结点的 <span class="math inline">\(\alpha\)</span> 为
1，从结点 v 跳到其他邻居的 <span class="math inline">\(\alpha\)</span>
为 <span class="math inline">\(\frac{1}{q}\)</span>。<br />
<img src="/images/图深度表示/node2vec2.png" width="50%"></p>
<p>我们发现，当 p 比较小的时候，结点间的跳转类似于
BFS，结点间的“接近”就可以理解为结点在<strong>邻接关系</strong>上“接近”；当
q 比较小的时候，结点间的跳转类似于
DFS，节点间的“接近”就可以视作是<strong>结构上相似</strong>。<br />
<img src="/images/图深度表示/node2vec3.png" width="50%"></p>
<h2 id="struc2vec">Struc2vec</h2>
<p><strong>动机</strong>：保留局部结构一致性。<br />
<strong>核心思想</strong>：在原来的图上构建一个新图。</p>
<h2 id="metapath2vec">Metapath2vec</h2>
<p><strong>动机</strong>：异构图上存在不同类型的节点，这些节点不能等同看待，其间关系可能存在一些固定模式。<br />
<strong>核心思路</strong>：使用预定义的Meta-Path来进行Random Walk。</p>
<h1 id="基于图特征的学习图神经网络">基于图特征的学习（图神经网络）</h1>
<p>节点的向量表示既包含了图的拓扑信息（nxn
的邻接矩阵表达的图结构）也包含了节点的特征向量集合（nxf
的特征向量）。<br />
<img src="/images/图深度表示/图表示2.png" width="100%"></p>
<p>机器学习、特征工程角度：如何通过有效利用图拓扑结构信息结合现有的特征向量得到新的特征。<br />
比如：图像-&gt;向量，视频-&gt;向量...。可以不严谨的说<strong>所有深度学习问题都可以归结为表示学习的问题</strong>。<br />
<strong>挑战</strong>：如何利用我们在图片/视频上取得的成功经验来应对图特征的表示学习问题？</p>
<p><strong>卷积神经网络</strong>（Convolutional Neural
Network）：表示学习利器。<br />
从图的角度看图像上的CNN：在欧式空间上的格点图（平移不变性、多尺度结构）。<br />
<strong>目标</strong>：将在欧式空间上的CNN扩展到拓扑空间————<strong>图卷积</strong>。</p>
<h2 id="gcn">GCN</h2>
<p>GCN(Graph Convolutional Networks，图卷积神经网络):</p>
<ul>
<li><strong>输入</strong>：邻接矩阵（节点数×节点数），特征矩阵（节点数×输入特征数）。</li>
<li><strong>输出</strong>：新的特征矩阵（节点数×输出特征数）。</li>
<li>网络层面：多层网络可以叠加。</li>
<li>节点层面：节点<strong>自身特征</strong>和其<strong>邻域特征</strong>的聚合。</li>
</ul>
<p><img src="/images/图深度表示/GCN1.png" width="100%"></p>
<p>公式如下：<br />
<img src="/images/图深度表示/GCN2.png" width="40%"></p>
<p><span
class="math inline">\(\tilde{A}=A+I_N\)</span>：带自环的邻接矩阵。<br />
<span class="math inline">\(\tilde{D}=\sum_j
\tilde{A}_{ij}\)</span>：度矩阵。<br />
<span class="math inline">\(H\)</span>：特征矩阵。<br />
<span class="math inline">\(W\)</span>：模型参数。<br />
<span class="math inline">\(\sigma(.)\)</span>：激活函数。</p>
<p><strong>两层GCN构造&amp;损失函数</strong>：<br />
<img src="/images/图深度表示/GCN3.png" width="50%"><br />
<img src="/images/图深度表示/GCN4.png" width="25%"></p>
<p><strong>GCN的推导思路</strong>：在图的拓扑空间近似在谱空间中的图滤波的操作，减少可学习参数。</p>
<p><strong>从另一个角度理解GCN</strong>：对<strong>邻居节点</strong>特征的<strong>带权重</strong>（<span
class="math inline">\(\tilde{D}^{-\frac{1}{2}}\)</span>）的<strong>聚合</strong>（<span
class="math inline">\(\tilde{A}H^{(l)}\)</span>）。</p>
<h2 id="graphsage">GraphSAGE</h2>
<p>对<strong>聚合</strong>和<strong>邻居节点</strong>进行了扩展定义：<br />
（1）<strong>聚合</strong>：Mean Pooling/Max Pooling/LSTM，etc。<br />
（2）<strong>邻居节点</strong>：Fix-length sample -&gt;
可以用来加速GCN计算。</p>
<p><img src="/images/图深度表示/GraphSAGE.png" width="100%"></p>
<h2 id="gat">GAT</h2>
<p>GAT（GRAPH ATTENTION
NETWORKS，图注意力网络）：对<strong>权重</strong>（<span
class="math inline">\(\tilde{D}^{-\frac{1}{2}}\)</span>）进行了扩展。<br />
（1）GCN中使用的邻接矩阵权重是提前给定的<span
class="math inline">\(\tilde{D}^{-\frac{1}{2}}\)</span>。<br />
（2）图注意力网络引入了<strong>自注意力机制</strong>，利用当前节点的特征以及其邻居节点的特征计算邻居节点的重要性，把该重要性作为新的邻接矩阵进行卷积计算。<br />
（3）有势：利用节点特征的相似性更能反映邻接信息。<br />
<img src="/images/图深度表示/GAT1.png" width="80%"></p>
<p><img src="/images/图深度表示/GAT2.png" width="50%"><br />
<img src="/images/图深度表示/GAT3.png" width="40%"></p>
<h1 id="图学习面临的挑战">图学习面临的挑战</h1>
<h2
id="如何将图神经网络模型做到更大的图上如何做大">如何将图神经网络模型做到更大的图上（如何做大）？</h2>
<p>因为<span
class="math inline">\(\tilde{D}^{-\frac{1}{2}}\tilde{A}\tilde{D}^{-\frac{1}{2}}\)</span>中邻接矩阵用到所有节点，难以处理超大图。即<strong>对所有邻接节点进行聚合并不高效</strong>。</p>
<p><strong>思路</strong>：采样，采用一部分点/边来进行运算。<br />
<strong>FastGCN</strong>：<br />
（1）把图节点特征看作有一个隐含概率分布产生，利用该分布对每一层的所有节点<strong>整体采样</strong>，避免了采样点个数的指数增加。<br />
（2）采样的目标是尽量减少采样的方差-&gt;基于节点degree的采样。<br />
（3）<strong>缺点</strong>：没有考虑层间点和点的关系。</p>
<p>为了克服这个缺点出现了层间采样的方法：<br />
<strong>ASGCN</strong>：FastGCN采样方式并不合理，在图极大而采样比例极少时，层间连接会急剧减少。<br />
（1）自顶向下Layer-dependent的采样方式。<br />
（2）在控制每层采样个数的同时，确保上下两层之间的连接是密集。<br />
（3）通过公式证明了可以保证采样无偏和减小采样方差。<br />
（4）扩展：加入了残差连接，能考虑二阶邻居的信息传播。在采样设置下，实现了注意力机制。</p>
<h2
id="如何有效训练更复杂的图神经网络模型如何做深">如何有效训练更复杂的图神经网络模型（如何做深）？</h2>
<p>为什么不能做深？<br />
（1）过拟合（Overfitting）：参数数量过多造成的泛化性降低。<br />
（2）<strong>过平滑</strong>（Over-Smoothing）：<strong>多层的邻居聚合造成的特征均化</strong>。</p>
<p>Over-Smoothing的定义：经过L层特征聚合后特征收敛到一个和输入特征无关的子空间M的现象。<br />
<img src="/images/图深度表示/Dropedge2.png" width="20%"></p>
<p><strong>挑战</strong>：如何减弱Over-Smoothing？<br />
<strong>DropEdge</strong>：<strong>在每个epoch训练前，随机丢掉一定比例的边</strong>。<br />
<img src="/images/图深度表示/Dropedge.png" width="20%"></p>
<p>为什么DropEdge可以减弱Over-Smoothing？<br />
（1）<strong>DropEdge可以减缓收敛到子空间M的速度</strong>。<img src="/images/图深度表示/Dropedge3.png" width="20%"><br />
（2）<strong>DropEdge可以减少收敛过程中的信息损失</strong>。<img src="/images/图深度表示/Dropedge4.png" width="30%"></p>
<p>由此通过减弱Over-Smoothing的影响，可以使我们可以成功在更复杂更深层的图神经网络上进行训练，并且提升精度。</p>
<h1 id="应用">应用</h1>
<p>药物属性预测和可解释性问题：DualMPNN。所有属性预测数据上大幅超越SOTA算法并提供了模型的可解释性。<br />
复杂层次图学习问题：SEAL算法。基于复杂层次图结构的GNN模型。应用于安全场景中的群分类任务。<br />
社交网络谣言检测：Bi-GCN。首创双向GCN结构并将其应用于谣言检测问题。<br />
统一黑盒攻击框架：GF-Attacker。首个可以对于多种图模型进行黑盒攻击的攻击框架。</p>
<h2 id="seal">SEAL</h2>
<p><strong>背景</strong>：在实际数据中，图相互之间的关系可以建模成图，即层级图结构。比如QQ群和QQ群的关系、学术论文引用（不同领域间的引用构成层次图，领域内的文章引用构成实例图）。<br />
<img src="/images/图深度表示/SEAL.png" width="70%"></p>
<p><strong>问题</strong>：如何预测实例图的分类标签？<br />
<strong>挑战</strong>：<br />
（1）<strong>如何利用统一长度的向量来表示具有不同大小的实例图</strong>？</p>
<ul>
<li>在不同层级下学习图的表示：
<ul>
<li>节点层级：<span class="math inline">\(G(V,E) -&gt;
H^{n×v}\)</span></li>
<li>层图级：<span class="math inline">\(G(V,E) -&gt; \it
e^{v}\)</span></li>
</ul></li>
<li>自注意力图表示学习（Self-Attentive Graph Embedding）
<ul>
<li>图大小不变性————自注意力机制（<span class="math inline">\(\it e \in
R^{r×v}\)</span>）</li>
<li>节点重要性——————自注意力机制</li>
<li>排列不变性——————GCN Smoothing</li>
</ul></li>
</ul>
<p><img src="/images/图深度表示/SEAL2.png" width="80%"></p>
<p>（2）<strong>如何在不同层级去融合实例图和层次图的信息</strong>？</p>
<ul>
<li><strong>实例图层次</strong>（Instance Classifier）：Graph Level
Learning （SEGA）。</li>
<li><strong>层次图层次</strong>（Hierarchical Classifier）：Node Level
Learning（GCN）。</li>
<li><strong>特征共享</strong>：将实例图的输出作为层次图模型的输入。<br />
<img src="/images/图深度表示/SEAL3.png" width="90%"></li>
</ul>
<h1 id="时间线">时间线</h1>
<p><img src="/images/图深度表示/GNN.png" width="50%"></p>
<h1 id="参考文献">参考文献</h1>
<p><span class="exturl" data-url="aHR0cHM6Ly96aC53aWtpcGVkaWEub3JnL3dpa2kvJUU1JTlCJUJFXyglRTYlOTUlQjAlRTUlQUQlQTYp">图
(数学)<i class="fa fa-external-link-alt"></i></span><br />
<span class="exturl" data-url="aHR0cHM6Ly96aC53aWtpcGVkaWEub3JnL3dpa2kvJUU1JTlCJUJFJUU4JUFFJUJB">图论<i class="fa fa-external-link-alt"></i></span><br />
<span class="exturl" data-url="aHR0cHM6Ly96aC53aWtpcGVkaWEub3JnL3dpa2kvJUU5JTgyJUJCJUU2JThFJUE1JUU3JTlGJUE5JUU5JTk4JUI1">邻接矩阵<i class="fa fa-external-link-alt"></i></span><br />
<span class="exturl" data-url="aHR0cHM6Ly93d3cuYmlsaWJpbGkuY29tL3ZpZGVvL2F2ODM1MTk3NjU/ZnJvbT1zZWFyY2gmc2VpZD00MjExNDE0NTk3NTQ4MjM5Njc2">图深度表示（GNN）的基础和前沿进展<i class="fa fa-external-link-alt"></i></span><br />
<span class="exturl" data-url="aHR0cHM6Ly93d3cuY25ibG9ncy5jb20vbnhmLXJhYmJpdDc1L3AvMTEzMDYxOTguaHRtbA==">GCN总结<i class="fa fa-external-link-alt"></i></span></p>
<p><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzE0MDMuNjY1Mi5wZGYlQzMlQUYlQzIlQkMlRTIlODAlQkE=">DeepWalk:
Online Learning of Social Representations<i class="fa fa-external-link-alt"></i></span> DeepWalk 2014年<br />
<span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzE1MDMuMDM1NzgucGRmJUMyJUEwJUUzJTgwJTkwV1dX">LINE:
Large-scale Information Network Embedding<i class="fa fa-external-link-alt"></i></span> LINE 2015年<br />
<span class="exturl" data-url="aHR0cHM6Ly93d3ctY3MtZmFjdWx0eS5zdGFuZm9yZC5lZHUvcGVvcGxlL2p1cmUvcHVicy9ub2RlMnZlYy1rZGQxNi5wZGY=">node2vec:
Scalable Feature Learning for Networks<i class="fa fa-external-link-alt"></i></span> node2vec 2016年<br />
<span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzE3MDQuMDMxNjUucGRm">struc2vec: Learning Node
Representations from Structural Identity<i class="fa fa-external-link-alt"></i></span> struc2vec 2017年<br />
<span class="exturl" data-url="aHR0cHM6Ly9kbC5hY20ub3JnL2RvaS9wZGYvMTAuMTE0NS8zMDk3OTgzLjMwOTgwMzY=">metapath2vec:
Scalable Representation Learning for Heterogeneous Networks<i class="fa fa-external-link-alt"></i></span>
metapath2vec 2017年</p>
<p><span class="exturl" data-url="aHR0cHM6Ly9vcGVucmV2aWV3Lm5ldC9wZGY/aWQ9U0pVNGF5WWds">SEMI-SUPERVISED
CLASSIFICATION WITH GRAPH CONVOLUTIONAL NETWORKS<i class="fa fa-external-link-alt"></i></span> GCN 2017年<br />
<span class="exturl" data-url="aHR0cHM6Ly9wcm9jZWVkaW5ncy5uZXVyaXBzLmNjL3BhcGVyLzIwMTcvZmlsZS81ZGQ5ZGI1ZTAzM2RhOWM2ZmI1YmE4M2M3YTdlYmVhOS1QYXBlci5wZGY=">Inductive
Representation Learning on Large Graphs<i class="fa fa-external-link-alt"></i></span> GraphSAGE 2017年<br />
<span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzE3MTAuMTA5MDMucGRm">GRAPH ATTENTION
NETWORKS<i class="fa fa-external-link-alt"></i></span> GAT 2018年</p>
<p><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzE4MDEuMTAyNDcucGRm">FASTGCN: FAST LEARNING
WITH GRAPH CONVOLU TIONAL NETWORKS VIA IMPORTANCE SAMPLING<i class="fa fa-external-link-alt"></i></span> FASTGCN
2018年<br />
<span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzE4MDkuMDUzNDMucGRm">Adaptive Sampling Towards
Fast Graph Representation Learning<i class="fa fa-external-link-alt"></i></span> ASGCN 2018年</p>
<p><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzE5MDcuMTA5MDMucGRm">DROPEDGE: TOWARDS DEEP
GRAPH CONVOLU TIONAL NETWORKS ON NODE CLASSIFICATION<i class="fa fa-external-link-alt"></i></span> DROPEDGE
2020年<br />
<span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzE5MDQuMDUwMDMucGRm">Semi-Supervised Graph
Classification: A Hierarchical Graph Perspective<i class="fa fa-external-link-alt"></i></span> SEAL 2019年</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://soundmemories.github.io/2020/10/17/Machine%20Learning/40.cheatsheet/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avstar.png">
      <meta itemprop="name" content="SoundMemories">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="SoundMemories">
      <meta itemprop="description" content="今日事，今日毕">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | SoundMemories">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2020/10/17/Machine%20Learning/40.cheatsheet/" class="post-title-link" itemprop="url">cheatsheet</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2020-10-17 00:00:00" itemprop="dateCreated datePublished" datetime="2020-10-17T00:00:00+08:00">2020-10-17</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Machine-Learning/" itemprop="url" rel="index"><span itemprop="name">Machine Learning</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>2.9k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>11 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="总结">总结</h1>
<h2 id="math">Math</h2>
<ol type="1">
<li><p>MLE<br />
<span class="math display">\[
\theta_{MLE}=\mathop{argmax}\limits _{\theta}\log
p(X|\theta)\mathop{=}\limits _{iid}\mathop{argmax}\limits
_{\theta}\sum\limits _{i=1}^{N}\log p(x_{i}|\theta)
\]</span></p></li>
<li><p>MAP<br />
<span class="math display">\[
\theta_{MAP}=\mathop{argmax}\limits
_{\theta}p(\theta|X)=\mathop{argmax}\limits _{\theta}p(X|\theta)\cdot
p(\theta)
\]</span></p></li>
<li><p>Gaussian Distribution<br />
<span class="math display">\[
\begin{align}&amp;p(x|\mu,\Sigma)=\frac{1}{(2\pi)^{p/2}|\Sigma|^{1/2}}e^{-\frac{1}{2}(x-\mu)^{T}\Sigma^{-1}(x-\mu)}\\
&amp;\Delta=(x-\mu)^{T}\Sigma^{-1}(x-\mu)=\sum\limits
_{i=1}^{p}(x-\mu)^{T}u_{i}\frac{1}{\lambda_{i}}u_{i}^{T}(x-\mu)=\sum\limits
_{i=1}^{p}\frac{y_{i}^{2}}{\lambda_{i}}
\end{align}
\]</span></p></li>
<li><p>已知 <span class="math inline">\(x\sim\mathcal{N}(\mu,\Sigma),
y\sim Ax+b\)</span>，有：<br />
<span class="math display">\[
\begin{align}y\sim\mathcal{N}(A\mu+b, A\Sigma A^T)
\end{align}
\]</span></p></li>
<li><p>记 <span class="math inline">\(x=(x_1,
x_2,\cdots,x_p)^T=(x_{a,m\times 1},
x_{b,n\times1})^T,\mu=(\mu_{a,m\times1},
\mu_{b,n\times1}),\Sigma=\begin{pmatrix}\Sigma_{aa}&amp;\Sigma_{ab}\\\Sigma_{ba}&amp;\Sigma_{bb}\end{pmatrix}\)</span>，已知
<span
class="math inline">\(x\sim\mathcal{N}(\mu,\Sigma)\)</span>，则：<br />
<span class="math display">\[
\begin{align}&amp;x_a\sim\mathcal{N}(\mu_a,\Sigma_{aa})\\
&amp;x_b|x_a\sim\mathcal{N}(\mu_{b|a},\Sigma_{b|a})\\
&amp;\mu_{b|a}=\Sigma_{ba}\Sigma_{aa}^{-1}(x_a-\mu_a)+\mu_b\\
&amp;\Sigma_{b|a}=\Sigma_{bb}-\Sigma_{ba}\Sigma_{aa}^{-1}\Sigma_{ab}
\end{align}
\]</span></p></li>
</ol>
<h2 id="linear-regression">Linear Regression</h2>
<h3 id="model">Model</h3>
<ol type="1">
<li><p>Dataset:<br />
<span class="math display">\[
\mathcal{D}=\{(x_1, y_1),(x_2, y_2),\cdots,(x_N, y_N)\}
\]</span></p></li>
<li><p>Notation:<br />
<span class="math display">\[
X=(x_1,x_2,\cdots,x_N)^T,Y=(y_1,y_2,\cdots,y_N)^T
\]</span></p></li>
<li><p>Model:<br />
<span class="math display">\[
f(w)=w^Tx
\]</span></p></li>
</ol>
<h3 id="loss-function">Loss Function</h3>
<ol type="1">
<li>最小二乘误差/高斯噪声的MLE<br />
<span class="math display">\[
L(w)=\sum\limits_{i=1}^N||w^Tx_i-y_i||^2_2
\]</span></li>
</ol>
<h3 id="闭式解">闭式解</h3>
<p><span class="math display">\[
\begin{align}\hat{w}=(X^TX)^{-1}X^TY=X^+Y\\
X=U\Sigma V^T\\
X^+=V\Sigma^{-1}U^T
\end{align}
\]</span></p>
<h3 id="正则化">正则化</h3>
<p><span class="math display">\[
\begin{align}
L1-Gaussian \
priori&amp;:\mathop{argmin}\limits_wL(w)+\lambda||w||_1,\lambda\gt0\\
L2-Laplasian\
priori-Sparsity&amp;:\mathop{argmin}\limits_wL(w)+\lambda||w||^2_2,\lambda
\gt 0
\end{align}
\]</span></p>
<h2 id="linear-classification">Linear Classification</h2>
<h3 id="hard">Hard</h3>
<h4 id="pca">PCA</h4>
<ol type="1">
<li><p>Idea: 在线性模型上加入激活函数</p></li>
<li><p>Loss Function:</p></li>
</ol>
<p><span class="math display">\[
L(w)=\sum\limits_{x_i\in\mathcal{D}_{wrong}}-y_iw^Tx_i
\]</span></p>
<ol start="3" type="1">
<li>Parameters:</li>
</ol>
<p><span class="math display">\[
w^{t+1}\leftarrow w^{t}+\lambda y_ix_i
\]</span></p>
<h4 id="fisher">Fisher</h4>
<ol type="1">
<li><p>Idea: 投影，类内小，类间大。</p></li>
<li><p>Loss Function:<br />
<span class="math display">\[
\begin{align}&amp;J(w)=\frac{w^TS_bw}{w^TS_ww}\\
&amp;S_b=(\overline{x_{c1}}-\overline{x_{c2}})(\overline{x_{c1}}-\overline{x_{c2}})^T\\
&amp;S_w=S_1+S_2
\end{align}
\]</span></p></li>
<li><p>闭式解，投影方向:<br />
<span class="math display">\[
S_w^{-1}(\overline{x_{c1}}-\overline{x_{c2}})
\]</span></p></li>
</ol>
<h3 id="soft">Soft</h3>
<h4 id="判别模型">判别模型</h4>
<h5 id="logistic-regression">Logistic Regression</h5>
<ol type="1">
<li><p>Idea，激活函数:<br />
<span class="math display">\[
\begin{align}p(C_1|x)&amp;=\frac{1}{1+\exp(-a)}\\
a&amp;=w^Tx
\end{align}
\]</span></p></li>
<li><p>Loss Function(交叉熵):<br />
<span class="math display">\[
\hat{w}=\mathop{argmax}_wJ(w)=\mathop{argmax}_w\sum\limits_{i=1}^N(y_i\log
p_1+(1-y_i)\log p_0)
\]</span></p></li>
<li><p>解法，SGD<br />
<span class="math display">\[
J&#39;(w)=\sum\limits_{i=1}^N(y_i-p_1)x_i
\]</span></p></li>
</ol>
<h4 id="生成模型">生成模型</h4>
<h5 id="gda">GDA</h5>
<ol type="1">
<li><p>Model</p>
<ol type="1">
<li><span class="math inline">\(y\sim Bernoulli(\phi)\)</span></li>
<li><span
class="math inline">\(x|y=1\sim\mathcal{N}(\mu_1,\Sigma)\)</span></li>
<li><span
class="math inline">\(x|y=0\sim\mathcal{N}(\mu_0,\Sigma)\)</span></li>
</ol></li>
<li><p>MAP<br />
<span class="math display">\[
\begin{align}
&amp;\mathop{argmax}_{\phi,\mu_0,\mu_1,\Sigma}\log p(X|Y)p(Y)\nonumber\\
&amp;=\mathop{argmax}_{\phi,\mu_0,\mu_1,\Sigma}\sum\limits_{i=1}^N((1-y_i)\log\mathcal{N}(\mu_0,\Sigma)+y_i\log
\mathcal{N}(\mu_1,\Sigma)+y_i\log\phi+(1-y_i)\log(1-\phi))
\end{align}
\]</span></p></li>
<li><p>解<br />
<span class="math display">\[
\begin{align}\phi&amp;=\frac{N_1}{N}\\
\mu_1&amp;=\frac{\sum\limits_{i=1}^Ny_ix_i}{N_1}\\
\mu_0&amp;=\frac{\sum\limits_{i=1}^N(1-y_i)x_i}{N_0}\\
\Sigma&amp;=\frac{N_1S_1+N_2S_2}{N}
\end{align}
\]</span></p></li>
</ol>
<h5 id="naive-bayesian">Naive Bayesian</h5>
<ol type="1">
<li><p>Model, 对单个数据点的各个维度作出限制<br />
<span class="math display">\[
x_i\perp x_j|y,\forall\  i\ne j
\]</span></p>
<ol type="1">
<li><span class="math inline">\(x_i\)</span> 为连续变量：<span
class="math inline">\(p(x_i|y)=\mathcal{N}(\mu_i,\sigma_i^2)\)</span></li>
<li><span class="math inline">\(x_i\)</span>
为离散变量：类别分布（Categorical）：<span
class="math inline">\(p(x_i=i|y)=\theta_i,\sum\limits_{i=1}^K\theta_i=1\)</span></li>
<li><span class="math inline">\(p(y)=\phi^y(1-\phi)^{1-y}\)</span></li>
</ol></li>
<li><p>解：和GDA相同</p></li>
</ol>
<h2 id="dimension-reduction">Dimension Reduction</h2>
<p>中心化：<br />
<span class="math display">\[
\begin{align}S
&amp;=\frac{1}{N}X^T(E_N-\frac{1}{N}\mathbb{I}_{N1}\mathbb{I}_{1N})(E_N-\frac{1}{N}\mathbb{I}_{N1}\mathbb{I}_{1N})^TX\nonumber\\
&amp;=\frac{1}{N}X^TH^2X=\frac{1}{N}X^THX
\end{align}
\]</span></p>
<h3 id="pca-1">PCA</h3>
<ol type="1">
<li><p>Idea:
坐标变换，寻找线性无关的新基矢，取信息损失最小的前几个维度</p></li>
<li><p>Loss Function:<br />
<span class="math display">\[
\begin{align}J
&amp;=\sum\limits_{j=1}^qu_j^TSu_j\ ,\ s.t.\ u_j^Tu_j=1
\end{align}
\]</span></p></li>
<li><p>解：</p>
<ol type="1">
<li><p>特征分解法<br />
<span class="math display">\[
S=U\Lambda U^T
\]</span></p></li>
<li><p>SVD for X/S<br />
<span class="math display">\[
\begin{align}HX=U\Sigma V^T\\
S=\frac{1}{N}V\Sigma^T\Sigma V^T
\\new\ co=HX\cdot V\end{align}
\]</span></p></li>
<li><p>SVD for T<br />
<span class="math display">\[
\begin{align}T=HXX^TH=U\Sigma\Sigma^TU^T\\
new\ co=U\Sigma
\end{align}
\]</span></p></li>
</ol></li>
</ol>
<h3 id="p-pca">p-PCA</h3>
<ol type="1">
<li><p>Model:<br />
<span class="math display">\[
\begin{align}
z&amp;\sim\mathcal{N}(\mathbb{O}_{q1},\mathbb{I}_{qq})\\
x&amp;=Wz+\mu+\varepsilon\\
\varepsilon&amp;\sim\mathcal{N}(0,\sigma^2\mathbb{I}_{pp})
\end{align}
\]</span></p></li>
<li><p>Learning: E-M</p></li>
<li><p>Inference:<br />
<span class="math display">\[
p(z|x)=\mathcal{N}(W^T(WW^T+\sigma^2\mathbb{I})^{-1}(x-\mu),\mathbb{I}-W^T(WW^T+\sigma^2\mathbb{I})^{-1}W)
\]</span></p></li>
</ol>
<h2 id="svm">SVM</h2>
<ol type="1">
<li>强对偶关系：凸优化+（松弛）Slater 条件-&gt;强对偶。</li>
<li>参数求解：KKT条件
<ol type="1">
<li>可行域</li>
<li>互补松弛+梯度为0</li>
</ol></li>
</ol>
<h3 id="hard-margin">Hard-margin</h3>
<ol type="1">
<li><p>Idea: 最大化间隔</p></li>
<li><p>Model:<br />
<span class="math display">\[
\mathop{argmin}_{w,b}\frac{1}{2}w^Tw\ s.t.\
y_i(w^Tx_i+b)\ge1,i=1,2,\cdots,N
\]</span></p></li>
<li><p>对偶问题<br />
<span class="math display">\[
\max_{\lambda}-\frac{1}{2}\sum\limits_{i=1}^N\sum\limits_{j=1}^N\lambda_i\lambda_jy_iy_jx_i^Tx_j+\sum\limits_{i=1}^N\lambda_i,\
s.t.\ \lambda_i\ge0
\]</span></p></li>
<li><p>模型参数<br />
<span class="math display">\[
\hat{w}=\sum\limits_{i=1}^N\lambda_iy_ix_i\\
\hat{b}=y_k-w^Tx_k=y_k-\sum\limits_{i=1}^N\lambda_iy_ix_i^Tx_k,\exists
k,1-y_k(w^Tx_k+b)=0
\]</span></p></li>
</ol>
<h3 id="soft-margin">Soft-margin</h3>
<ol type="1">
<li><p>Idea:允许少量错误</p></li>
<li><p>Model:<br />
<span class="math display">\[
error=\sum\limits_{i=1}^N\max\{0,1-y_i(w^Tx_i+b)\}\\
\mathop{argmin}_{w,b}\frac{1}{2}w^Tw+C\sum\limits_{i=1}^N\xi_i\ s.t.\
y_i(w^Tx_i+b)\ge1-\xi_i,\xi_i\ge0,i=1,2,\cdots,N
\]</span></p></li>
</ol>
<h3 id="kernel">Kernel</h3>
<p>对称的正定函数都可以作为正定核。</p>
<h2 id="exp-family">Exp Family</h2>
<ol type="1">
<li><p>表达式<br />
<span class="math display">\[
p(x|\eta)=h(x)\exp(\eta^T\phi(x)-A(\eta))=\frac{1}{\exp(A(\eta))}h(x)\exp(\eta^T\phi(x))
\]</span></p></li>
<li><p>对数配分函数<br />
<span class="math display">\[
\begin{align}
A&#39;(\eta)=\mathbb{E}_{p(x|\eta)}[\phi(x)]\\
A&#39;&#39;(\eta)=Var_{p(x|\eta)}[\phi(x)]
\end{align}
\]</span></p></li>
<li><p>指数族分布满足最大熵定理</p></li>
</ol>
<h2 id="pgm">PGM</h2>
<h3 id="representation">Representation</h3>
<ol type="1">
<li><p>有向图<br />
<span class="math display">\[
p(x_1,x_2,\cdots,x_p)=\prod\limits_{i=1}^pp(x_i|x_{parent(i)})
\]</span><br />
D-separation<br />
<span class="math display">\[
p(x_i|x_{-i})=\frac{p(x)}{\int
p(x)dx_{i}}=\frac{\prod\limits_{j=1}^pp(x_j|x_{parents(j)})}{\int\prod\limits_{j=1}^pp(x_j|x_{parents(j)})dx_i}=\frac{p(x_i|x_{parents(i)})p(x_{child(i)}|x_i)}{\int
p(x_i|x_{parents(i)})p(x_{child(i)}|x_i)dx_i}
\]</span></p></li>
<li><p>无向图<br />
<span class="math display">\[
\begin{align}p(x)=\frac{1}{Z}\prod\limits_{i=1}^{K}\phi(x_{ci})\\
Z=\sum\limits_{x\in\mathcal{X}}\prod\limits_{i=1}^{K}\phi(x_{ci})\\
\phi(x_{ci})=\exp(-E(x_{ci}))
\end{align}
\]</span></p></li>
<li><p>有向转无向</p>
<ol type="1">
<li>将每个节点的父节点两两相连</li>
<li>将有向边替换为无向边</li>
</ol></li>
</ol>
<h3 id="learning">Learning</h3>
<p>参数学习-EM</p>
<ol type="1">
<li><p>目的：解决具有隐变量的混合模型的参数估计（极大似然估计）</p></li>
<li><p>参数：<br />
<span class="math display">\[
\theta_{MLE}=\mathop{argmax}\limits_\theta\log p(x|\theta)
\]</span></p></li>
<li><p>迭代求解：<br />
<span class="math display">\[
\theta^{t+1}=\mathop{argmax}\limits_{\theta}\int_z\log
[p(x,z|\theta)]p(z|x,\theta^t)dz=\mathbb{E}_{z|x,\theta^t}[\log
p(x,z|\theta)]
\]</span></p></li>
<li><p>原理<br />
<span class="math display">\[
\log p(x|\theta^t)\le\log p(x|\theta^{t+1})
\]</span></p></li>
<li><p>广义EM</p>
<ol type="1">
<li><p>E step：<br />
<span class="math display">\[
\hat{q}^{t+1}(z)=\mathop{argmax}_q\int_zq^t(z)\log\frac{p(x,z|\theta)}{q^t(z)}dz,fixed\
\theta
\]</span></p></li>
<li><p>M step：<br />
<span class="math display">\[
\hat{\theta}=\mathop{argmax}_\theta
\int_zq^{t+1}(z)\log\frac{p(x,z|\theta)}{q^{t+1}(z)}dz,fixed\ \hat{q}
\]</span></p></li>
</ol></li>
</ol>
<h3 id="inference">Inference</h3>
<ol type="1">
<li><p>精确推断</p>
<ol type="1">
<li><p>VE</p></li>
<li><p>BP<br />
<span class="math display">\[
m_{j\to i}(i)=\sum\limits_j\phi_j(j)\phi_{ij}(ij)\prod\limits_{k\in
Neighbour(j)-i}m_{k\to j}(j)
\]</span></p></li>
<li><p>MP<br />
<span class="math display">\[
m_{j\to i}=\max\limits_{j}\phi_j\phi_{ij}\prod\limits_{k\in
Neighbour(j)-i}m_{k\to j}
\]</span></p></li>
</ol></li>
<li><p>近似推断</p>
<ol type="1">
<li><p>确定性近似，VI</p>
<ol type="1">
<li><p>变分表达式<br />
<span class="math display">\[
\hat{q}(Z)=\mathop{argmax}_{q(Z)}L(q)
\]</span></p></li>
<li><p>平均场近似下的 VI-坐标上升<br />
<span class="math display">\[
\mathbb{E}_{\prod\limits_{i\ne j}q_i(Z_i)}[\log p(X,Z)]=\log
\hat{p}(X,Z_j)\\
q_j(Z_j)=\hat{p}(X,Z_j)
\]</span></p></li>
<li><p>SGVI-变成优化问题，重参数法<br />
<span class="math display">\[
\begin{aligned}
\mathop{argmax}_{q(Z)}L(q)=\mathop{argmax}_{\phi}L(\phi)\\
\nabla_\phi L(\phi)=\mathbb{E}_{q_\phi}[(\nabla_\phi\log q_\phi)(\log
p_\theta(x^i,z)-\log q_\phi(z))]\\
=\mathbb{E}_{p(\varepsilon)}[\nabla_z[\log p_\theta(x^i,z)-\log
q_\phi(z)]\nabla_\phi g_\phi(\varepsilon,x^i)]\\
z=g_\phi(\varepsilon,x^i),\varepsilon\sim p(\varepsilon)
\end{aligned}
\]</span></p></li>
</ol></li>
<li><p>随机性近似</p>
<ol type="1">
<li><p>蒙特卡洛方法采样</p>
<ol type="1">
<li><p>CDF 采样</p></li>
<li><p>拒绝采样， <span class="math inline">\(q(z)\)</span>，使得 <span
class="math inline">\(\forall z_i,Mq(z_i)\ge
p(z_i)\)</span>，拒绝因子：<span
class="math inline">\(\alpha=\frac{p(z^i)}{Mq(z^i)}\le1\)</span></p></li>
<li><p>重要性采样<br />
<span class="math display">\[
\mathbb{E}_{p(z)}[f(z)]=\int p(z)f(z)dz=\int
\frac{p(z)}{q(z)}f(z)q(z)dz\simeq\frac{1}{N}\sum\limits_{i=1}^Nf(z_i)\frac{p(z_i)}{q(z_i)}
\]</span></p></li>
<li><p>重要性重采样：重要性采样+重采样</p></li>
</ol></li>
<li><p>MCMC：构建马尔可夫链概率序列，使其收敛到平稳分布 <span
class="math inline">\(p(z)\)</span>。</p>
<ol type="1">
<li><p>转移矩阵（提议分布）<br />
<span class="math display">\[
p(z)\cdot Q_{z\to z^*}\alpha(z,z^*)=p(z^*)\cdot Q_{z^*\to
z}\alpha(z^*,z)\\
\alpha(z,z^*)=\min\{1,\frac{p(z^*)Q_{z^*\to z}}{p(z)Q_{z\to z^*}}\}
\]</span></p></li>
<li><p>算法（MH）：</p>
<ol type="1">
<li>通过在0，1之间均匀分布取点 <span
class="math inline">\(u\)</span></li>
<li>生成 <span class="math inline">\(z^*\sim
Q(z^*|z^{i-1})\)</span></li>
<li>计算 <span class="math inline">\(\alpha\)</span> 值</li>
<li>如果 <span class="math inline">\(\alpha\ge u\)</span>，则 <span
class="math inline">\(z^i=z^*\)</span>，否则 <span
class="math inline">\(z^{i}=z^{i-1}\)</span></li>
</ol></li>
</ol></li>
<li><p>Gibbs 采样：给定初始值 <span
class="math inline">\(z_1^0,z_2^0,\cdots\)</span>在 <span
class="math inline">\(t+1\)</span> 时刻，采样 <span
class="math inline">\(z_i^{t+1}\sim
p(z_i|z_{-i})\)</span>，从第一个维度一个个采样。</p></li>
</ol></li>
</ol></li>
</ol>
<h2 id="gmm">GMM</h2>
<ol type="1">
<li><p>Model<br />
<span class="math display">\[
p(x)=\sum\limits_{k=1}^Kp_k\mathcal{N}(x|\mu_k,\Sigma_k)
\]</span></p></li>
<li><p>求解-EM<br />
<span class="math display">\[
\begin{align}Q(\theta,\theta^t)&amp;=\sum\limits_z[\log\prod\limits_{i=1}^Np(x_i,z_i|\theta)]\prod
\limits_{i=1}^Np(z_i|x_i,\theta^t)\nonumber\\
&amp;=\sum\limits_z[\sum\limits_{i=1}^N\log p(x_i,z_i|\theta)]\prod
\limits_{i=1}^Np(z_i|x_i,\theta^t)\nonumber\\
&amp;=\sum\limits_{i=1}^N\sum\limits_{z_i}\log
p(x_i,z_i|\theta)p(z_i|x_i,\theta^t)\nonumber\\
&amp;=\sum\limits_{i=1}^N\sum\limits_{z_i}\log
p_{z_i}\mathcal{N(x_i|\mu_{z_i},\Sigma_{z_i})}\frac{p_{z_i}^t\mathcal{N}(x_i|\mu_{z_i}^t,\Sigma_{z_i}^t)}{\sum\limits_kp_k^t\mathcal{N}(x_i|\mu_k^t,\Sigma_k^t)}
\end{align}
\]</span></p>
<p><span class="math display">\[
p_k^{t+1}=\frac{1}{N}\sum\limits_{i=1}^Np(z_i=k|x_i,\theta^t)
\]</span></p></li>
</ol>
<h2 id="序列模型-hmmldsparticle">序列模型-HMM，LDS，Particle</h2>
<ol type="1">
<li><p>假设：</p>
<ol type="1">
<li><p>齐次 Markov 假设（未来只依赖于当前）：<br />
<span class="math display">\[
p(i_{t+1}|i_t,i_{t-1},\cdots,i_1,o_t,o_{t-1},\cdots,o_1)=p(i_{t+1}|i_t)
\]</span></p></li>
<li><p>观测独立假设：<br />
<span class="math display">\[
p(o_t|i_t,i_{t-1},\cdots,i_1,o_{t-1},\cdots,o_1)=p(o_t|i_t)
\]</span></p></li>
</ol></li>
<li><p>参数<br />
<span class="math display">\[
\lambda=(\pi,A,B)
\]</span></p></li>
</ol>
<h3 id="离散线性隐变量-hmm">离散线性隐变量-HMM</h3>
<ol type="1">
<li><p>Evaluation：<span
class="math inline">\(p(O|\lambda)\)</span>，Forward-Backward 算法<br />
<span class="math display">\[
p(O|\lambda)=\sum\limits_{i=1}^Np(O,i_T=q_i|\lambda)=\sum\limits_{i=1}^N\alpha_T(i)=\sum\limits_{i=1}^Nb_i(o_1)\pi_i\beta_1(i)\\
\alpha_{t+1}(j)=\sum\limits_{i=1}^Nb_{j}(o_t)a_{ij}\alpha_t(i)\\
\beta_t(i)=\sum\limits_{j=1}^Nb_j(o_{t+1})a_{ij}\beta_{t+1}(j)
\]</span></p></li>
<li><p>Learning：<span
class="math inline">\(\lambda=\mathop{argmax}\limits_{\lambda}p(O|\lambda)\)</span>，EM
算法（Baum-Welch）<br />
<span class="math display">\[
\lambda^{t+1}=\mathop{argmax}_\lambda\sum\limits_I\log
p(O,I|\lambda)p(O,I|\lambda^t)\\=\sum\limits_I[\log
\pi_{i_1}+\sum\limits_{t=2}^T\log
a_{i_{t-1},i_t}+\sum\limits_{t=1}^T\log b_{i_t}(o_t)]p(O,I|\lambda^t)
\]</span></p></li>
<li><p>Decoding：<span
class="math inline">\(I=\mathop{argmax}\limits_{I}p(I|O,\lambda)\)</span>，Viterbi
算法-动态规划<br />
<span class="math display">\[
\delta_{t}(j)=\max\limits_{i_1,\cdots,i_{t-1}}p(o_1,\cdots,o_t,i_1,\cdots,i_{t-1},i_t=q_i)\\\delta_{t+1}(j)=\max\limits_{1\le
i\le
N}\delta_t(i)a_{ij}b_j(o_{t+1})\\\psi_{t+1}(j)=\mathop{argmax}\limits_{1\le
i\le N}\delta_t(i)a_{ij}
\]</span></p></li>
</ol>
<h3 id="连续线性隐变量-lds">连续线性隐变量-LDS</h3>
<ol type="1">
<li><p>Model<br />
<span class="math display">\[
\begin{align}
p(z_t|z_{t-1})&amp;\sim\mathcal{N}(A\cdot z_{t-1}+B,Q)\\
p(x_t|z_t)&amp;\sim\mathcal{N}(C\cdot z_t+D,R)\\
z_1&amp;\sim\mathcal{N}(\mu_1,\Sigma_1)
\end{align}
\]</span></p></li>
<li><p>滤波<br />
<span class="math display">\[
p(z_t|x_{1:t})=p(x_{1:t},z_t)/p(x_{1:t})\propto
p(x_{1:t},z_t)\\=p(x_t|z_t)p(z_t|x_{1:t-1})p(x_{1:t-1})\propto
p(x_t|z_t)p(z_t|x_{1:t-1})
\]</span></p></li>
<li><p>递推求解-线性高斯模型</p>
<ol type="1">
<li><p>Prediction<br />
<span class="math display">\[
p(z_t|x_{1:t-1})=\int_{z_{t-1}}p(z_t|z_{t-1})p(z_{t-1}|x_{1:t-1})dz_{t-1}=\int_{z_{t-1}}\mathcal{N}(Az_{t-1}+B,Q)\mathcal{N}(\mu_{t-1},\Sigma_{t-1})dz_{t-1}
\]</span></p></li>
<li><p>Update:<br />
<span class="math display">\[
p(z_t|x_{1:t})\propto p(x_t|z_t)p(z_t|x_{1:t-1}
\]</span></p></li>
</ol></li>
</ol>
<h3 id="连续非线性隐变量-粒子滤波">连续非线性隐变量-粒子滤波</h3>
<p>通过采样(SIR)解决：<br />
<span class="math display">\[
\mathbb{E}[f(z)]=\int_zf(z)p(z)dz=\int_zf(z)\frac{p(z)}{q(z)}q(z)dz=\sum\limits_{i=1}^Nf(z_i)\frac{p(z_i)}{q(z_i)}
\]</span></p>
<ol type="1">
<li><p>采样<br />
<span class="math display">\[
w_t^i\propto\frac{p(x_t|z_t)p(z_t|z_{t-1})}{q(z_t|z_{1:t-1},x_{1:t})}w_{t-1}^i\\
q(z_t|z_{1:t-1},x_{1:t})=p(z_t|z_{t-1})
\]</span></p></li>
<li><p>重采样</p></li>
</ol>
<h2 id="crf">CRF</h2>
<ol type="1">
<li><p>PDF<br />
<span class="math display">\[
p(Y=y|X=x)=\frac{1}{Z(x,\theta)}\exp[\theta^TH(y_t,y_{t-1},x)]
\]</span></p></li>
<li><p>边缘概率<br />
<span class="math display">\[
\begin{aligned}
p(y_t=i|x)=\sum\limits_{y_{1:t-1}}\sum\limits_{y_{t+1:T}}\frac{1}{Z}\prod\limits_{t&#39;=1}^T\phi_{t&#39;}(y_{t&#39;-1},y_{t&#39;},x)\\
p(y_t=i|x)=\frac{1}{Z}\Delta_l\Delta_r\\
\Delta_l=\sum\limits_{y_{1:t-1}}\phi_{1}(y_0,y_1,x)\phi_2(y_1,y_2,x)\cdots\phi_{t-1}(y_{t-2},y_{t-1},x)\phi_t(y_{t-1},y_t=i,x)\\
\Delta_r=\sum\limits_{y_{t+1:T}}\phi_{t+1}(y_t=i,y_{t+1},x)\phi_{t+2}(y_{t+1},y_{t+2},x)\cdots\phi_T(y_{T-1},y_T,x)
\end{aligned}
\]</span></p>
<p><span class="math display">\[
\begin{aligned}
\alpha_t(i)=\Delta_l=\sum\limits_{j\in
S}\phi_t(y_{t-1}=j,y_t=i,x)\alpha_{t-1}(j)\\
\Delta_r=\beta_t(i)=\sum\limits_{j\in
S}\phi_{t+1}(y_t=i,y_{t+1}=j,x)\beta_{t+1}(j)
\end{aligned}
\]</span></p></li>
<li><p>学习<br />
<span class="math display">\[
\nabla_\lambda
L=\sum\limits_{i=1}^N\sum\limits_{t=1}^T[f(y_{t-1},y_t,x^i)-\sum\limits_{y_{t-1}}\sum\limits_{y_t}p(y_{t-1},y_t|x^i)f(y_{t-1},y_t,x^i)]
\]</span></p></li>
</ol>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://soundmemories.github.io/2020/10/16/Machine%20Learning/39.ApproInference/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avstar.png">
      <meta itemprop="name" content="SoundMemories">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="SoundMemories">
      <meta itemprop="description" content="今日事，今日毕">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | SoundMemories">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2020/10/16/Machine%20Learning/39.ApproInference/" class="post-title-link" itemprop="url">ApproInference</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2020-10-16 00:00:00" itemprop="dateCreated datePublished" datetime="2020-10-16T00:00:00+08:00">2020-10-16</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Machine-Learning/" itemprop="url" rel="index"><span itemprop="name">Machine Learning</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>367</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>1 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="近似推断">近似推断</h1>
<p>这一讲中的近似推断具体描述在深度生成模型中的近似推断。推断的目的有下面几个部分：</p>
<ol type="1">
<li>推断本身，根据结果（观测）得到原因（隐变量）。</li>
<li>为参数的学习提供帮助。</li>
</ol>
<p>但是推断本身是一个困难的额任务，计算复杂度往往很高，对于无向图，由于节点之间的联系过多，那么因子分解很难进行，并且相互之间都有耦合，于是很难求解，仅仅在某些情况如
RBM
中可解，在有向图中，常常由于条件独立性问题，如两个节点之间条件相关（explain
away），于是求解这些节点的条件概率就很困难，仅仅在某些概率假设情况下可解如高斯模型，于是需要近似推断。</p>
<p>事实上，我们常常讲推断问题变为优化问题，即：<br />
<span class="math display">\[
Log-likehood:\sum\limits_{v\in V}\log p(v)
\]</span><br />
对上面这个问题，由于：<br />
<span class="math display">\[
\log
p(v)=\log\frac{p(v,h)}{p(h|v)}=\log\frac{p(v,h)}{q(h|v)}+\log\frac{q(h|v)}{p(h|v)}
\]</span><br />
左右两边对 <span class="math inline">\(h\)</span> 积分：<br />
<span class="math display">\[
\int_h\log p(v)\cdot q(h|v)dh=\log p(v)
\]</span><br />
右边积分有：<br />
<span class="math display">\[
\mathbb{E}_{q(h|v)}[\log\frac{p(v,h)}{q(h|v)}]+KL(q(h|v)||p(h|v))=\mathbb{E}_{q(h|v)}[\log
p(v,h)]+H(q)+KL(q||p)
\]</span><br />
其中前两项是 ELBO，于是这就变成一个优化 ELBO 的问题。</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://soundmemories.github.io/2020/10/15/Machine%20Learning/37.NN/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avstar.png">
      <meta itemprop="name" content="SoundMemories">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="SoundMemories">
      <meta itemprop="description" content="今日事，今日毕">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | SoundMemories">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2020/10/15/Machine%20Learning/37.NN/" class="post-title-link" itemprop="url">NN</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2020-10-15 00:00:00" itemprop="dateCreated datePublished" datetime="2020-10-15T00:00:00+08:00">2020-10-15</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Machine-Learning/" itemprop="url" rel="index"><span itemprop="name">Machine Learning</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>601</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>2 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="前馈神经网络">前馈神经网络</h1>
<p>机器学习我们已经知道可以分为两大流派：</p>
<ol type="1">
<li><p>频率派，这个流派的方法叫做统计学习，根据具体问题有下面的算法：</p>
<ol type="1">
<li><p>正则化，L1，L2 等</p></li>
<li><p>核化，如核支撑向量机</p></li>
<li><p>集成化，AdaBoost，RandomForest</p></li>
<li><p>层次化，神经网络，神经网络有各种不同的模型，有代表性的有：</p>
<ol type="1">
<li>多层感知机</li>
<li>Autoencoder</li>
<li>CNN</li>
<li>RNN</li>
</ol>
<p>这几种模型又叫做深度神经网络。</p></li>
</ol></li>
<li><p>贝叶斯派，这个流派的方法叫概率图模型，根据图特点分为：</p>
<ol type="1">
<li>有向图-贝叶斯网络，加入层次化后有深度有向网络，包括
<ol type="1">
<li>Sigmoid Belief Network</li>
<li>Variational Autoencoder</li>
<li>GAN</li>
</ol></li>
<li>无向图-马尔可夫网络，加入层次化后有深度玻尔兹曼机。</li>
<li>混合，加入层次化后有深度信念网络</li>
</ol>
<p>这几个加入层次化后的模型叫做深度生成网络。</p></li>
</ol>
<p>从广义来说，深度学习包括深度生成网络和深度神经网络。</p>
<h2 id="from-pla-to-dl">From PLA to DL</h2>
<ul>
<li>1958，PLA</li>
<li>1969，PLA 不能解决 XOR 等非线性数据</li>
<li>1981，MLP，多层感知机的出现解决了上面的问题</li>
<li>1986，BP 算法应用在 MLP 上，RNN</li>
<li>1989，CNN，Univeral Approximation
Theorem，但是于此同时，由于深度和宽度的相对效率不知道，并且无法解决 BP
算法的梯度消失问题</li>
<li>1993，1995，SVM + kernel，AdaBoost，RandomForest，这些算法的发展，DL
逐渐没落</li>
<li>1997，LSTM</li>
<li>2006，基于 RBM 的 深度信念网络和深度自编码</li>
<li>2009，GPU的发展</li>
<li>2011，在语音方面的应用</li>
<li>2012，ImageNet</li>
<li>2013，VAE</li>
<li>2014，GAN</li>
<li>2016，AlphaGo</li>
<li>2018，GNN</li>
</ul>
<p>DL 不是一个新的东西，其近年来的大发展主要原因如下：</p>
<ol type="1">
<li>数据量变大</li>
<li>分布式计算的发展</li>
<li>硬件算力的发展</li>
</ol>
<h2 id="非线性问题">非线性问题</h2>
<p>对于非线性的问题，有三种方法：</p>
<ol type="1">
<li>非线性转换，将低维空间转换到高维空间（Cover
定理），从而变为一个线性问题。</li>
<li>核方法，由于非线性转换是变换为高维空间，因此可能导致维度灾难，并且可能很难得到这个变换函数，核方法不直接寻找这个转换，而是寻找一个内积。</li>
<li>神经网络方法，将复合运算变为基本的线性运算的组合。</li>
</ol>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://soundmemories.github.io/2020/10/15/Machine%20Learning/38.PartitionFunction/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avstar.png">
      <meta itemprop="name" content="SoundMemories">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="SoundMemories">
      <meta itemprop="description" content="今日事，今日毕">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | SoundMemories">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2020/10/15/Machine%20Learning/38.PartitionFunction/" class="post-title-link" itemprop="url">PartitionFunction</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2020-10-15 00:00:00" itemprop="dateCreated datePublished" datetime="2020-10-15T00:00:00+08:00">2020-10-15</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Machine-Learning/" itemprop="url" rel="index"><span itemprop="name">Machine Learning</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>1.5k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>5 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="配分函数">配分函数</h1>
<p>在学习和推断中，对于一个概率的归一化因子很难处理，这个归一化因子和配分函数相关。假设一个概率分布：<br />
<span class="math display">\[
p(x|\theta)=\frac{1}{Z(\theta)}\hat{p}(x|\theta),Z(\theta)=\int\hat{p}(x|\theta)dx
\]</span></p>
<h2 id="包含配分函数的-mle">包含配分函数的 MLE</h2>
<p>在学习任务中，采用最大似然：<br />
<span class="math display">\[
\begin{align}
\hat{\theta}&amp;=\mathop{argmax}_{\theta}p(x|\theta)=\mathop{argmax}_\theta\sum\limits_{i=1}^N\log
p(x_i|\theta)\nonumber\\
&amp;=\mathop{argmax}_\theta\sum\limits_{i=1}^N\log
\hat{p}(x|\theta)-N\log Z(\theta)\nonumber\\
&amp;=\mathop{argmax}_{\theta}\frac{1}{N}\sum\limits_{i=1}^N\log
\hat{p}(x|\theta)-\log Z(\theta)=\mathop{argmax}_\theta l(\theta)
\end{align}
\]</span><br />
求导：<br />
<span class="math display">\[
\begin{align}\nabla_\theta\log
Z(\theta)&amp;=\frac{1}{Z(\theta)}\nabla_\theta Z(\theta)\nonumber\\
&amp;=\frac{p(x|\theta)}{\hat{p}(x|\theta)}\int\nabla_\theta
\hat{p}(x|\theta)dx\nonumber\\
&amp;=\int\frac{p(x|\theta)}{\hat{p}(x|\theta)}\nabla_\theta\hat{p}(x|\theta)dx\nonumber\\
&amp;=\mathbb{E}_{p(x|\theta)}[\nabla_\theta\log\hat{p}(x|\theta)]
\end{align}
\]</span><br />
由于这个表达式和未知的概率相关，于是无法直接精确求解，需要近似采样，如果没有这一项，那么可以采用梯度下降，但是存在配分函数就无法直接采用梯度下降了。</p>
<p>上面这个期望值，是对模型假设的概率分布，定义真实概率分布为 <span
class="math inline">\(p_{data}\)</span>，于是，<span
class="math inline">\(l(\theta)\)</span>
中的第一项的梯度可以看成是从这个概率分布中采样出来的 <span
class="math inline">\(N\)</span> 个点求和平均，可以近似期望值。<br />
<span class="math display">\[
\nabla_\theta
l(\theta)=\mathbb{E}_{p_{data}}[\nabla_\theta\log\hat{p}(x|\theta)]-\mathbb{E}_{p(x|\theta)}[\nabla_\theta\log\hat{p}(x|\theta)]
\]</span><br />
于是，相当于真实分布和模型假设越接近越好。上面这个式子第一项叫做正相，第二项叫做负相。为了得到负相的值，需要采用各种采样方法，如
MCMC。</p>
<p>采样得到 <span class="math inline">\(\hat{x}_{1-m}\sim
p_{model}(x|\theta^t)\)</span>，那么：<br />
<span class="math display">\[
\theta^{t+1}=\theta^t+\eta(\sum\limits_{i=1}^m\nabla_\theta \log
\hat{p}(x_i|\theta^t)-\sum\limits_{i=1}^m\nabla_\theta\log
\hat{p}(\hat{x_i}|\theta^t))
\]</span><br />
这个算法也叫做基于 MCMC
采样的梯度上升。每次通过采样得到的样本叫做幻想粒子，如果这些幻想粒子区域的概率高于实际分布，那么最大化参数的结果就是降低这些部分的概率。</p>
<h2 id="对比散度-cd-learning">对比散度-CD Learning</h2>
<p>上面对于负相的采样，最大的问题是，采样到达平稳分布的步骤数量是未知的。对比散度的方法，是对上述的采样是的初始值作出限制，直接采样
<span
class="math inline">\(\hat{x}_i=x_i\)</span>，这样可以缩短采样的混合时间。这个算法叫做
CD-k 算法，<span class="math inline">\(k\)</span>
就是初始化后进行的演化时间，很多时候，即使 <span
class="math inline">\(k=1\)</span> 也是可以的。</p>
<p>我们看 MLE 的表达式：<br />
<span class="math display">\[
\begin{align}\hat{\theta}&amp;=\mathop{argmax}_{\theta}p(x|\theta)=\mathop{argmax}_{\theta}\frac{1}{N}\sum\limits_{i=1}^N\log
p(x_i|\theta)=\mathbb{E}_{p_{data}}[\log p_{model}(x|\theta)]\nonumber\\
&amp;=\mathop{argmax}_\theta\int p_{data}\log p_{model}dx\nonumber\\
&amp;=\mathop{argmax}_\theta\int p_{data}\log
\frac{p_{model}}{p_{data}}dx\nonumber\\
&amp;=\mathop{argmin}_\theta KL(p_{data}||p_{model})
\end{align}
\]</span><br />
对于 CD-k 的采样过程，可以将初始值这些点表示为：<br />
<span class="math display">\[
p^0=p_{data}
\]</span><br />
而我们的模型需要采样过程达到平稳分布：<br />
<span class="math display">\[
p^\infty=p_{model}
\]</span><br />
因此，我们需要的是 <span
class="math inline">\(KL(p^0||p^\infty)\)</span>。定义 CD：<br />
<span class="math display">\[
KL(p^0||p^\infty)-KL(p^k||p^\infty)
\]</span><br />
这就是 CD-k 算法第 <span class="math inline">\(k\)</span>
次采样的目标函数。</p>
<h2 id="rbm-的学习问题">RBM 的学习问题</h2>
<p>RBM 的参数为：<br />
<span class="math display">\[
\begin{align}
h=(h_1,\cdots,h_m)^T\\
v=(v_1,\cdots,v_n)^T\\
w=(w_{ij})_{mn}\\
\alpha=(\alpha_1,\cdots,\alpha_n)^T\\
\beta=(\beta_1,\cdots,\beta_m)^T
\end{align}
\]</span><br />
学习问题关注的概率分布为：<br />
<span class="math display">\[
\begin{align}
\log p(v)&amp;=\log\sum\limits_{h}p(h,v)\nonumber\\
&amp;=\log\sum\limits_h\frac{1}{Z}\exp(-E(v,h))\nonumber\\
&amp;=\log\sum\limits_{h}\exp(-E(v,h))-\log\sum\limits_{v,h}\exp(-E(h,v))
\end{align}
\]</span><br />
对上面这个式子求导第一项：<br />
<span class="math display">\[
\frac{\partial
\log\sum\limits_{h}\exp(-E(v,h))}{\partial\theta}=-\frac{\sum\limits_h\exp(-E(v,h))\frac{\partial
E(v,h)}{\partial\theta}}{\sum\limits_{h}\exp(-E(v,h))}\\
=-\sum\limits_h\frac{\exp(-E(v,h))\frac{\partial
E(v,h)}{\partial\theta}}{\sum\limits_{h}\exp(-E(v,h))}=-\sum\limits_hp(h|v)\frac{\partial
E(v,h)}{\partial\theta}
\]</span><br />
第二项：<br />
<span class="math display">\[
\frac{\partial
\log\sum\limits_{v,h}\exp(-E(h,v))}{\partial\theta}=-\sum\limits_{h,v}\frac{\exp(-E(v,h))\frac{\partial
E(v,h)}{\partial\theta}}{\sum\limits_{h,v}\exp(-E(v,h))}=-\sum\limits_{v,h}p(v,h)\frac{\partial
E(v,h)}{\partial\theta}
\]</span><br />
所以有：<br />
<span class="math display">\[
\frac{\partial}{\partial\theta}\log
p(v)=-\sum\limits_hp(h|v)\frac{\partial
E(v,h)}{\partial\theta}+\sum\limits_{v,h}p(v,h)\frac{\partial
E(v,h)}{\partial\theta}
\]</span><br />
将 RBM 的模型假设代入：<br />
<span class="math display">\[
E(v,h)=-(h^Twv+\alpha^Tv+\beta^Th)
\]</span></p>
<ol type="1">
<li><p><span class="math inline">\(w_{ij}\)</span>：<br />
<span class="math display">\[
\frac{\partial}{\partial w_{ij}}E(v,h)=-h_iv_j
\]</span><br />
于是：<br />
<span class="math display">\[
\frac{\partial}{\partial\theta}\log
p(v)=\sum\limits_{h}p(h|v)h_iv_j-\sum\limits_{h,v}p(h,v)h_iv_j
\]</span><br />
第一项：<br />
<span class="math display">\[
\sum\limits_{h_1,h_2,\cdots,h_m}p(h_1,h_2,\cdots,h_m|v)h_iv_j=\sum\limits_{h_i}p(h_i|v)h_iv_j=p(h_i=1|v)v_j
\]</span><br />
这里假设了 <span class="math inline">\(h_i\)</span> 是二元变量。</p>
<p>第二项：<br />
<span class="math display">\[
\sum\limits_{h,v}p(h,v)h_iv_j=\sum\limits_{h,v}p(v)p(h|v)h_iv_j=\sum\limits_vp(v)p(h_i=1|v)v_j
\]</span><br />
这个求和是指数阶的，于是需要采样解决，我么使用 CD-k 方法。</p>
<p>对于第一项，可以直接使用训练样本得到，第二项采用 CD-k
采样方法，首先使用样本 <span
class="math inline">\(v^0=v\)</span>，然后采样得到 <span
class="math inline">\(h^0\)</span>，然后采样得到 <span
class="math inline">\(v^1\)</span>，这样顺次进行，最终得到 <span
class="math inline">\(v^k\)</span>，对于每个样本都得到一个 <span
class="math inline">\(v^k\)</span>，最终采样得到 <span
class="math inline">\(N\)</span> 个 $v^k $，于是第二项就是：<br />
<span class="math display">\[
p(h_i=1|v^k)v_j^k
\]</span><br />
具体的算法为：</p>
<ol type="1">
<li>对每一个样本中的 <span class="math inline">\(v\)</span>，进行采样：
<ol type="1">
<li>使用这个样本初始化采样</li>
<li>进行 <span class="math inline">\(k\)</span> 次采样（0-k-1）：
<ol type="1">
<li><span class="math inline">\(h_i^l\sim p(h_i|v^l)\)</span></li>
<li><span class="math inline">\(v_i^{l+1}\sim p(v_i|h^l)\)</span></li>
</ol></li>
<li>将这些采样出来的结果累加进梯度中</li>
</ol></li>
<li>重复进行上述过程，最终的梯度除以 <span
class="math inline">\(N\)</span></li>
</ol></li>
</ol>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://soundmemories.github.io/2020/10/14/Machine%20Learning/36.Spectral/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avstar.png">
      <meta itemprop="name" content="SoundMemories">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="SoundMemories">
      <meta itemprop="description" content="今日事，今日毕">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | SoundMemories">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2020/10/14/Machine%20Learning/36.Spectral/" class="post-title-link" itemprop="url">Spectral</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2020-10-14 00:00:00" itemprop="dateCreated datePublished" datetime="2020-10-14T00:00:00+08:00">2020-10-14</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Machine-Learning/" itemprop="url" rel="index"><span itemprop="name">Machine Learning</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>792</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>3 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="谱聚类">谱聚类</h1>
<p>聚类问题可以分为两种思路：</p>
<ol type="1">
<li>Compactness，这类有 K-means，GMM
等，但是这类算法只能处理凸集，为了处理非凸的样本集，必须引入核技巧。</li>
<li>Connectivity，这类以谱聚类为代表。</li>
</ol>
<p>谱聚类是一种基于无向带权图的聚类方法。这个图用 <span
class="math inline">\(G=(V,E)\)</span> 表示，其中 <span
class="math inline">\(V=\{1,2,\cdots,N\}\)</span>，<span
class="math inline">\(E=\{w_{ij}\}\)</span>，这里 <span
class="math inline">\(w_{ij}\)</span>
就是边的权重，这里权重取为相似度，<span
class="math inline">\(W=(w_{ij})\)</span>
是相似度矩阵，定义相似度（径向核）：<br />
<span class="math display">\[
w_{ij}=k(x_i,x_j)=\exp(-\frac{||x_i-x_j||_2^2}{2\sigma^2}),(i,j)\in E\\
w_{ij}=0,(i,j)\notin E
\]</span><br />
下面定义图的分割，这种分割就相当于聚类的结果。定义 <span
class="math inline">\(w(A,B)\)</span>：<br />
<span class="math display">\[
A\subset V,B\subset V,A\cap B=\emptyset,w(A,B)=\sum\limits_{i\in A,j\in
B}w_{ij}
\]</span><br />
假设一共有 <span class="math inline">\(K\)</span> 个类别，对这个图的分割
<span
class="math inline">\(CUT(V)=CUT(A_1,A_2,\cdots,A_K)=\sum\limits_{k=1}^Kw(A_k,\overline{A_k})=\sum\limits_{k=1}^K[w(A_k,V)-w(A_k,A_k)]\)</span></p>
<p>于是，我们的目标就是 <span
class="math inline">\(\min\limits_{A_k}CUT(V)\)</span>。</p>
<p>为了平衡每一类内部的权重不同，我们做归一化的操作，定义每一个集合的度，首先，对单个节点的度定义：<br />
<span class="math display">\[
d_i=\sum\limits_{j=1}^Nw_{ij}
\]</span><br />
其次，每个集合：<br />
<span class="math display">\[
\Delta_k=degree(A_k)=\sum\limits_{i\in A_k}d_i
\]</span><br />
于是：<br />
<span class="math display">\[
N(CUT)=\sum\limits_{k=1}^K\frac{w(A_k,\overline{A_k})}{\sum\limits_{i\in
A_k}d_i}
\]</span><br />
所以目标函数就是最小化这个式子。</p>
<p>谱聚类的模型就是：<br />
<span class="math display">\[
\{\hat{A}_k\}_{k=1}^K=\mathop{argmin}_{A_k}N(CUT)
\]</span><br />
引入指示向量：<br />
<span class="math display">\[
\begin{cases}
y_i\in \{0,1\}^K\\
\sum\limits_{j=1}^Ky_{ij}=1
\end{cases}
\]</span><br />
其中，<span class="math inline">\(y_{ij}\)</span> 表示第 <span
class="math inline">\(i\)</span> 个样本属于 <span
class="math inline">\(j\)</span> 个类别，记：<span
class="math inline">\(Y=(y_1,y_2,\cdots,y_N)^T\)</span>。所以：<br />
<span class="math display">\[
\hat{Y}=\mathop{argmin}_YN(CUT)
\]</span><br />
将 <span class="math inline">\(N(CUT)\)</span>
写成对角矩阵的形式，于是：<br />
<span class="math display">\[
\begin{align}N(CUT)&amp;=Trace[diag(\frac{w(A_1,\overline{A_1})}{\sum\limits_{i\in
A_1}d_i},\frac{w(A_2,\overline{A_2})}{\sum\limits_{i\in
A_2}d_i},\cdots,\frac{w(A_K,\overline{A_K})}{\sum\limits_{i\in
A_K}d_i})]\nonumber\\
&amp;=Trace[diag(w(A_1,\overline{A_1}),w(A_2,\overline{A_2}),\cdots,w(A_K,\overline{A_K}))\cdot
diag(\sum\limits_{i\in A_1}d_i,\cdots,\sum\limits_{i\in
A_K}d_i)^{-1}]\nonumber\\
&amp;=Trace[O\cdot P^{-1}]
\end{align}
\]</span><br />
我们已经知道 <span class="math inline">\(Y,w\)</span>
这两个矩阵，我们希望求得 <span class="math inline">\(O,P\)</span>。</p>
<p>由于：<br />
<span class="math display">\[
Y^TY=\sum\limits_{i=1}^Ny_iy_i^T
\]</span><br />
对于 <span class="math inline">\(y_iy_i^T\)</span>，只在对角线上的 <span
class="math inline">\(k\times k\)</span> 处为 1，所以：<br />
<span class="math display">\[
Y^TY=diag(N_1,N_2,\cdots,N_K)
\]</span><br />
其中，<span class="math inline">\(N_i\)</span> 表示有 <span
class="math inline">\(N_i\)</span> 个样本属于 <span
class="math inline">\(i\)</span>，即 <span
class="math inline">\(N_k=\sum\limits_{k\in A_k}1\)</span>。</p>
<p>引入对角矩阵，根据 <span class="math inline">\(d_i\)</span> 的定义，
<span
class="math inline">\(D=diag(d_1,d_2,\cdots,d_N)=diag(w_{NN}\mathbb{I}_{N1})\)</span>，于是：<br />
<span class="math display">\[
P=Y^TDY
\]</span><br />
对另一项 <span
class="math inline">\(O=diag(w(A_1,\overline{A_1}),w(A_2,\overline{A_2}),\cdots,w(A_K,\overline{A_K})\)</span>：<br />
<span class="math display">\[
O=diag(w(A_i,V))-diag(w(A_i,A_i))=diag(\sum\limits_{j\in
A_i}d_j)-diag(w(A_i,A_i))
\]</span><br />
其中，第一项已知，第二项可以写成 <span
class="math inline">\(Y^TwY\)</span>，这是由于：<br />
<span class="math display">\[
Y^TwY=\sum\limits_{i=1}^N\sum\limits_{j=1}^Ny_iy_j^Tw_{ij}
\]</span><br />
于是这个矩阵的第 <span class="math inline">\(lm\)</span>
项可以写为：<br />
<span class="math display">\[
\sum\limits_{i\in A_l,j\in A_m}w_{ij}
\]</span><br />
这个矩阵的对角线上的项和 <span class="math inline">\(w(A_i,A_i)\)</span>
相同，所以取迹后的取值不会变化。</p>
<p>所以：<br />
<span class="math display">\[
N(CUT)=Trace[(Y^T(D-w))Y)\cdot(Y^TDY)^{-1}]
\]</span><br />
其中，$ L=D-w$ 叫做拉普拉斯矩阵。</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://soundmemories.github.io/2020/10/13/Machine%20Learning/35.RBM/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avstar.png">
      <meta itemprop="name" content="SoundMemories">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="SoundMemories">
      <meta itemprop="description" content="今日事，今日毕">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | SoundMemories">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2020/10/13/Machine%20Learning/35.RBM/" class="post-title-link" itemprop="url">RBM</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2020-10-13 00:00:00" itemprop="dateCreated datePublished" datetime="2020-10-13T00:00:00+08:00">2020-10-13</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Machine-Learning/" itemprop="url" rel="index"><span itemprop="name">Machine Learning</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>1k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>4 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="受限玻尔兹曼机">受限玻尔兹曼机</h1>
<p>玻尔兹曼机是一种存在隐节点的无向图模型。在图模型中最简单的是朴素贝叶斯模型（朴素贝叶斯假设），引入单个隐变量后，发展出了
GMM，如果单个隐变量变成序列的隐变量，就得到了状态空间模型（引入齐次马尔可夫假设和观测独立假设就有HMM，Kalman
Filter，Particle
Filter），为了引入观测变量之间的关联，引入了一种最大熵模型-MEMM，为了克服
MEMM 中的局域问题，又引入了 CRF，CRF
是一个无向图，其中，破坏了齐次马尔可夫假设，如果隐变量是一个链式结构，那么又叫线性链
CRF。</p>
<p>在无向图的基础上，引入隐变量得到了玻尔兹曼机，这个图模型的概率密度函数是一个指数族分布。对隐变量和观测变量作出一定的限制，就得到了受限玻尔兹曼机（RBM）。</p>
<p>我们看到，不同的概率图模型对下面几个特点作出假设：</p>
<ol type="1">
<li>方向-边的性质</li>
<li>离散/连续/混合-点的性质</li>
<li>条件独立性-边的性质</li>
<li>隐变量-节点的性质</li>
<li>指数族-结构特点</li>
</ol>
<p>将观测变量和隐变量分别记为 <span
class="math inline">\(v,h,h=\{h_1,\cdots,h_m\},v=\{v_1,\cdots,v_n\}\)</span>。我们知道，无向图根据最大团的分解，可以写为玻尔兹曼分布的形式
<span
class="math inline">\(p(x)=\frac{1}{Z}\prod\limits_{i=1}^K\psi_i(x_{ci})=\frac{1}{Z}\exp(-\sum\limits_{i=1}^KE(x_{ci}))\)</span>，这也是一个指数族分布。</p>
<p>一个玻尔兹曼机存在一系列的问题，在其推断任务中，想要精确推断，是无法进行的，想要近似推断，计算量过大。为了解决这个问题，一种简化的玻尔兹曼机-受限玻尔兹曼机作出了假设，所有隐变量内部以及观测变量内部没有连接，只在隐变量和观测变量之间有连接，这样一来：<br />
<span class="math display">\[
p(x)=p(h,v)=\frac{1}{Z}\exp(-E(v,h))
\]</span><br />
其中能量函数 <span class="math inline">\(E(v,h)\)</span>
可以写出三个部分，包括与节点集合相关的两项以及与边 <span
class="math inline">\(w\)</span> 相关的一项，记为：<br />
<span class="math display">\[
E(v,h)=-(h^Twv+\alpha^T v+\beta^T h)
\]</span><br />
所以：<br />
<span class="math display">\[
p(x)=\frac{1}{Z}\exp(h^Twv)\exp(\alpha^T v)\exp(\beta^T
h)=\frac{1}{Z}\prod_{i=1}^m\prod_{j=1}^n\exp(h_iw_{ij}v_j)\prod_{j=1}^n\exp(\alpha_jv_j)\prod_{i=1}^m\exp(\beta_ih_i)
\]</span><br />
上面这个式子也和 RBM 的因子图一一对应。</p>
<h2 id="推断">推断</h2>
<p>推断任务包括求后验概率 $ p(v|h),p(h|v)$ 以及求边缘概率 <span
class="math inline">\(p(v)\)</span>。</p>
<h3 id="phv"><span class="math inline">\(p(h|v)\)</span></h3>
<p>对于一个无向图，满足局域的 Markov 性质，即 <span
class="math inline">\(p(h_1|h-\{h_1\},v)=p(h_1|Neighbour(h_1))=p(h_1|v)\)</span>。我们可以得到：<br />
<span class="math display">\[
p(h|v)=\prod_{i=1}^mp(h_i|v)
\]</span><br />
考虑 Binary RBM，所有的隐变量只有两个取值 <span
class="math inline">\(0,1\)</span>：<br />
<span class="math display">\[
p(h_l=1|v)=\frac{p(h_l=1,h_{-l},v)}{p(h_{-l},v)}=\frac{p(h_l=1,h_{-l},v)}{p(h_l=1,h_{-l},v)+p(h_l=0,h_{-l},v)}
\]</span><br />
将能量函数写成和 <span class="math inline">\(l\)</span>
相关或不相关的两项：<br />
<span class="math display">\[
E(v,h)=-(\sum\limits_{i=1,i\ne
l}^m\sum\limits_{j=1}^nh_iw_{ij}v_j+h_l\sum\limits_{j=1}^nw_{lj}v_j+\sum\limits_{j=1}^n\alpha_j
v_j+\sum\limits_{i=1,i\ne l}^m\beta_ih_i+\beta_lh_l)
\]</span><br />
定义：<span
class="math inline">\(h_lH_l(v)=h_l\sum\limits_{j=1}^nw_{lj}v_j+\beta_lh_l,\overline{H}(h_{-l},v)=\sum\limits_{i=1,i\ne
l}^m\sum\limits_{j=1}^nh_iw_{ij}v_j+\sum\limits_{j=1}^n\alpha_j
v_j+\sum\limits_{i=1,i\ne l}^m\beta_ih_i\)</span>。</p>
<p>代入，有：<br />
<span class="math display">\[
p(h_l=1|v)=\frac{\exp(H_l(v)+\overline{H}(h_{-l},v))}{\exp(H_l(v)+\overline{H}(h_{-l},v))+\exp(\overline{H}(h_{-l},v))}=\frac{1}{1+\exp(-H_l(v))}=\sigma(H_l(v))
\]</span><br />
于是就得到了后验概率。对于 <span class="math inline">\(v\)</span>
的后验是对称的，所以类似的可以求解。</p>
<h3 id="pv"><span class="math inline">\(p(v)\)</span></h3>
<p><span class="math display">\[
\begin{align}p(v)&amp;=\sum\limits_hp(h,v)=\sum\limits_h\frac{1}{Z}\exp(h^Twv+\alpha^Tv+\beta^Th)\nonumber\\
&amp;=\exp(\alpha^Tv)\frac{1}{Z}\sum\limits_{h_1}\exp(h_1w_1v+\beta_1h_1)\cdots\sum\limits_{h_m}\exp(h_mw_mv+\beta_mh_m)\nonumber\\
&amp;=\exp(\alpha^Tv)\frac{1}{Z}(1+\exp(w_1v+\beta_1))\cdots(1+\exp(w_mv+\beta_m))\nonumber\\
&amp;=\frac{1}{Z}\exp(\alpha^Tv+\sum\limits_{i=1}^m\log(1+\exp(w_iv+\beta_i)))
\end{align}
\]</span></p>
<p>其中，<span class="math inline">\(\log(1+\exp(x))\)</span> 叫做
Softplus 函数。</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://soundmemories.github.io/2020/10/12/Machine%20Learning/34.GaussianProcess/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avstar.png">
      <meta itemprop="name" content="SoundMemories">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="SoundMemories">
      <meta itemprop="description" content="今日事，今日毕">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | SoundMemories">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2020/10/12/Machine%20Learning/34.GaussianProcess/" class="post-title-link" itemprop="url">GaussianProcess</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2020-10-12 00:00:00" itemprop="dateCreated datePublished" datetime="2020-10-12T00:00:00+08:00">2020-10-12</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Machine-Learning/" itemprop="url" rel="index"><span itemprop="name">Machine Learning</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>1.2k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>4 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="高斯过程回归">高斯过程回归</h1>
<p>将一维高斯分布推广到多变量中就得到了高斯网络，将多变量推广到无限维，就得到了高斯过程，高斯过程是定义在连续域（时间空间）上的无限多个高维随机变量所组成的随机过程。</p>
<p>在时间轴上的任意一个点都满足高斯分布吗，将这些点的集合叫做高斯过程的一个样本。</p>
<blockquote>
<p>对于时间轴上的序列 <span class="math inline">\(\xi_t\)</span>，如果
<span class="math inline">\(\forall n\in N^+，t_i\in T\)</span>，有
<span class="math inline">\(\xi_{t_1-t_n}\sim
\mathcal{N}(\mu_{t_1-t_n},\Sigma_{t_1-t_n})\)</span>， 那么 <span
class="math inline">\(\{\xi_t\}_{t\in T}\)</span> 是一个高斯过程。</p>
<p>高斯过程有两个参数（高斯过程存在性定理），均值函数 <span
class="math inline">\(m(t)=\mathbb{E}[\xi_t]\)</span> 和协方差函数 <span
class="math inline">\(k(s,t)=\mathbb{E}[(\xi_s-\mathbb{E}[\xi_s])(\xi_t-\mathbb{E}[\xi_t])]\)</span>。</p>
</blockquote>
<p>我们将贝叶斯线性回归添加核技巧的这个模型叫做高斯过程回归，高斯过程回归分为两种视角：</p>
<ol type="1">
<li>权空间的视角-核贝叶斯线性回归，相当于 <span
class="math inline">\(x\)</span> 为 <span
class="math inline">\(t\)</span>，在每个时刻的高斯分布来源于权重，根据上面的推导，预测的函数依然是高斯分布。</li>
<li>函数空间的视角-高斯分布通过函数 <span
class="math inline">\(f(x)\)</span> 来体现。</li>
</ol>
<h2 id="核贝叶斯线性回归">核贝叶斯线性回归</h2>
<p>贝叶斯线性回归可以通过加入核函数的方法来解决非线性函数的问题，将
<span class="math inline">\(f(x)=x^Tw\)</span> 这个函数变为 <span
class="math inline">\(f(x)=\phi(x)^Tw\)</span>（当然这个时候，$ _p$
也要变为更高维度的），变换到更高维的空间，有：<br />
<span class="math display">\[
\begin{align}f(x^*)\sim
\mathcal{N}(\phi(x^*)^{T}\sigma^{-2}A^{-1}\Phi^TY,\phi(x^*)^{T}A^{-1}\phi(x^*))\\
A=\sigma^{-2}\Phi^T\Phi+\Sigma_p^{-1}
\end{align}
\]</span><br />
其中，<span
class="math inline">\(\Phi=(\phi(x_1),\phi(x_2),\cdots,\phi(x_N))^T\)</span>。</p>
<p>为了求解 <span class="math inline">\(A^{-1}\)</span>，可以利用
Woodbury Formula，<span
class="math inline">\(A=\Sigma_p^{-1},C=\sigma^{-2}\mathbb{I}\)</span>：<br />
<span class="math display">\[
(A+UCV)^{-1}=A^{-1}-A^{-1}U(C^{-1}+VA^{-1}U)^{-1}VA^{-1}
\]</span><br />
所以 <span
class="math inline">\(A^{-1}=\Sigma_p-\Sigma_p\Phi^T(\sigma^2\mathbb{I}+\Phi\Sigma_p\Phi^T)^{-1}\Phi\Sigma_p\)</span></p>
<p>也可以用另一种方法：<br />
<span class="math display">\[
\begin{align}
A&amp;=\sigma^{-2}\Phi^T\Phi+\Sigma_p^{-1}\nonumber\\
\Leftrightarrow
A\Sigma_p&amp;=\sigma^{-2}\Phi^T\Phi\Sigma_p+\mathbb{I}\nonumber\\
\Leftrightarrow
A\Sigma_p\Phi^T&amp;=\sigma^{-2}\Phi^T\Phi\Sigma_p\Phi^T+\Phi^T=\sigma^{-2}\Phi^T(k+\sigma^2\mathbb{I})\nonumber\\
\Leftrightarrow
\Sigma_p\Phi^T&amp;=\sigma^{-2}A^{-1}\Phi^T(k+\sigma^2\mathbb{I})\nonumber\\
\Leftrightarrow
\sigma^{-2}A^{-1}\Phi^T&amp;=\Sigma_p\Phi^T(k+\sigma^2\mathbb{I})^{-1}\nonumber\\
\Leftrightarrow
\phi(x^*)^T\sigma^{-2}A^{-1}\Phi^T&amp;=\phi(x^*)^T\Sigma_p\Phi^T(k+\sigma^2\mathbb{I})^{-1}
\end{align}
\]</span><br />
上面的左边的式子就是变换后的均值，而右边的式子就是不含 <span
class="math inline">\(A^{-1}\)</span> 的式子，其中 <span
class="math inline">\(k=\Phi\Sigma_p\Phi^T\)</span>。</p>
<p>根据 <span class="math inline">\(A^{-1}\)</span> 得到方差为：<br />
<span class="math display">\[
\phi(x^*)^T\Sigma_p\phi(x^*)-\phi(x^*)^T\Sigma_p\Phi^T(\sigma^2\mathbb{I}+k)^{-1}\Phi\Sigma_p\phi(x^*)
\]</span><br />
上面定义了：<br />
<span class="math display">\[
k=\Phi\Sigma_p\Phi^T
\]</span><br />
我们看到，在均值和方差中，含有下面四项：<br />
<span class="math display">\[
\phi(x^*)^T\Sigma_p\Phi^T,\phi(x^*)^T\Sigma_p\phi(x^*),\phi(x^*)^T\Sigma_p\Phi^T,\Phi\Sigma_p\phi(x^*)
\]</span><br />
展开后，可以看到，有共同的项：<span
class="math inline">\(k(x,x&#39;)=\phi(x)^T\Sigma_p\phi(x‘)\)</span>。由于
<span class="math inline">\(\Sigma_p\)</span>
是正定对称的方差矩阵，所以，这是一个核函数。</p>
<p>对于高斯过程中的协方差：<br />
<span class="math display">\[
k(t,s)=Cov[f(x),f(x&#39;)]=\mathbb{E}[\phi(x)^Tww^T\phi(x&#39;)]=\phi(x)^T\mathbb{E}[ww^T]\phi(x&#39;)=\phi(x)^T\Sigma_p\phi(x&#39;)
\]</span><br />
我们可以看到，这个就对应着上面的核函数。因此我们看到 <span
class="math inline">\(\{f(x)\}\)</span> 组成的组合就是一个高斯过程。</p>
<h2 id="函数空间的观点">函数空间的观点</h2>
<p>相比权重空间，我们也可以直接关注 <span
class="math inline">\(f\)</span>
这个空间，对于预测任务，这就是类似于求：<br />
<span class="math display">\[
p(y^*|X,Y,x^*)=\int_fp(y^*|f,X,Y,x^*)p(f|X,Y,x^*)df
\]</span><br />
对于数据集来说，取 <span
class="math inline">\(f(X)\sim\mathcal{N}(\mu(X),k(X,X)),Y=f(X)+\varepsilon\sim\mathcal{N}(\mu(X),k(X,X)+\sigma^2\mathbb{I})\)</span>。预测任务的目的是给定一个新数据序列
<span
class="math inline">\(X^\ast=(x_1^\ast,\cdots,x_M^\ast)^T\)</span>，得到
<span
class="math inline">\(Y^\ast=f(X^\ast)+\varepsilon\)</span>。我们可以写出：<br />
<span class="math display">\[
\begin{pmatrix}Y\\f(X^*)\end{pmatrix}\sim\mathcal{N}\left(\begin{pmatrix}\mu(X)\\\mu(X^*)\end{pmatrix},\begin{pmatrix}k(X,X)+\sigma^2\mathbb{I}&amp;k(X,X^*)\\k(X^*,X)&amp;k(X^*,X^*)\end{pmatrix}\right)
\]</span><br />
根据高斯分布的方法：<br />
<span class="math display">\[
\begin{align}x=\begin{pmatrix}x_a\\x_b\end{pmatrix}\sim\mathcal{N}\left(\begin{pmatrix}\mu_a\\\mu_b\end{pmatrix},\begin{pmatrix}\Sigma_{aa}&amp;\Sigma_{ab}\\\Sigma_{ba}&amp;\Sigma_{bb}\end{pmatrix}\right)\\
x_b|x_a\sim\mathcal{N}(\mu_{b|a},\Sigma_{b|a})\\
\mu_{b|a}=\Sigma_{ba}\Sigma_{aa}^{-1}(x_a-\mu_a)+\mu_b\\
\Sigma_{b|a}=\Sigma_{bb}-\Sigma_{ba}\Sigma_{aa}^{-1}\Sigma_{ab}
\end{align}
\]</span><br />
可以直接写出：<br />
<span class="math display">\[
\begin{align}
p(f(X^*)|X,Y,X^*)=p(f(X^*)|Y)\\
=\mathcal{N}(k(X^*,X)[k(X,X)+\sigma^2\mathbb{I}]^{-1}(Y-\mu(X))+\mu(X^*),\\
k(X^*,X^*)-k(X^*,X)[k(X,X)+\sigma^2\mathbb{I}]^{1}k(X,X^*))
\end{align}
\]</span><br />
所以对于 <span
class="math inline">\(Y=f(X^*)+\varepsilon\)</span>：<br />
<span class="math display">\[
\begin{align}
\mathcal{N}(k(X^*,X)[k(X,X)+\sigma^2\mathbb{I}]^{-1}(Y-\mu(X))+\mu(X^*),\\
k(X^*,X^*)-k(X^*,X)[k(X,X)+\sigma^2\mathbb{I}]^{1}k(X,X^*)+\sigma^2\mathbb{I})
\end{align}
\]</span><br />
我们看到，函数空间的观点更加简单易于求解。</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




  <nav class="pagination">
    <a class="extend prev" rel="prev" title="上一页" aria-label="上一页" href="/page/2/"><i class="fa fa-angle-left"></i></a><a class="page-number" href="/">1</a><a class="page-number" href="/page/2/">2</a><span class="page-number current">3</span><a class="page-number" href="/page/4/">4</a><span class="space">&hellip;</span><a class="page-number" href="/page/13/">13</a><a class="extend next" rel="next" title="下一页" aria-label="下一页" href="/page/4/"><i class="fa fa-angle-right"></i></a>
  </nav>

</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="copyright">
    &copy; 2019 – 
    <span itemprop="copyrightYear">2023</span>
    <span class="with-love">
      <i class="fa fa-heart"></i>
    </span>
    <span class="author" itemprop="copyrightHolder">SoundMemories</span>
  </div>
  <div class="powered-by">由 <span class="exturl" data-url="aHR0cHM6Ly9oZXhvLmlv">Hexo</span> & <span class="exturl" data-url="aHR0cHM6Ly90aGVtZS1uZXh0LmpzLm9yZy9tdXNlLw==">NexT.Muse</span> 强力驱动
  </div>

    </div>
  </footer>

  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>
  <div class="sidebar-dimmer"></div>
  <div class="back-to-top" role="button" aria-label="返回顶部">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>
  <a role="button" class="book-mark-link book-mark-link-fixed"></a>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/next-theme-pjax/0.6.0/pjax.min.js" integrity="sha256-vxLn1tSKWD4dqbMRyv940UYw4sXgMtYcK6reefzZrao=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.7.0/jquery.min.js" integrity="sha256-2Pmvv0kuTBOenSvLm6bvfBSSHrUJ+3A7x6P5Ebd07/g=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.js" integrity="sha256-yt2kYMy0w8AbtF89WXb2P1rfjcP/HTHLT7097U8Y5b8=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/medium-zoom/1.0.8/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/muse.js"></script><script src="/js/next-boot.js"></script><script src="/js/bookmark.js"></script><script src="/js/pjax.js"></script>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-generator-searchdb/1.4.1/search.js" integrity="sha256-1kfA5uHPf65M5cphT2dvymhkuyHPQp5A53EGZOnOLmc=" crossorigin="anonymous"></script>
<script src="/js/third-party/search/local-search.js"></script>

  <script class="next-config" data-name="pdf" type="application/json">{"object_url":{"url":"https://cdnjs.cloudflare.com/ajax/libs/pdfobject/2.2.12/pdfobject.min.js","integrity":"sha256-g2xji1rlE3KsGVClvuxTbcR0Kn2+wtQADSff2Tbb4zA="},"url":"/lib/pdf/web/viewer.html"}</script>
  <script src="/js/third-party/tags/pdf.js"></script>

  <script class="next-config" data-name="mermaid" type="application/json">{"enable":true,"theme":{"light":"neutral","dark":"neutral"},"js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mermaid/10.2.3/mermaid.min.js","integrity":"sha256-JFptYy4KzJ5OQP+Q9fubNf3cxpPPmZKqUOovyEONKrQ="}}</script>
  <script src="/js/third-party/tags/mermaid.js"></script>


  <script src="/js/third-party/fancybox.js"></script>



  




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"none","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>


  <script src="https://cdnjs.cloudflare.com/ajax/libs/quicklink/2.3.0/quicklink.umd.js" integrity="sha256-yvJQOINiH9fWemHn0vCA5lsHWJaHs6/ZmO+1Ft04SvM=" crossorigin="anonymous"></script>
  <script class="next-config" data-name="quicklink" type="application/json">{"enable":true,"home":true,"archive":true,"delay":true,"timeout":3000,"priority":true,"url":"https://soundmemories.github.io/page/3/"}</script>
  <script src="/js/third-party/quicklink.js"></script>

</body>
</html>
