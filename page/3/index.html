<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.2.0">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="https://fonts.loli.net/css?family=Monda:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">

<link rel="stylesheet" href="//cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.1/css/all.min.css">
  <link rel="stylesheet" href="//cdn.jsdelivr.net/npm/animate.css@3.1.1/animate.min.css">
  <link rel="stylesheet" href="//cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.css">

<script class="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"soundmemories.github.io","root":"/","images":"/images","scheme":"Muse","version":"8.0.2","exturl":true,"sidebar":{"position":"left","display":"always","padding":18,"offset":12},"copycode":true,"bookmark":{"enable":true,"color":"#222","save":"auto"},"fancybox":true,"mediumzoom":true,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"},"path":"/search.xml","localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false}};
  </script>

  <meta name="description" content="今日事，今日毕">
<meta property="og:type" content="website">
<meta property="og:title" content="SoundMemories">
<meta property="og:url" content="https://soundmemories.github.io/page/3/index.html">
<meta property="og:site_name" content="SoundMemories">
<meta property="og:description" content="今日事，今日毕">
<meta property="og:locale" content="zh_CN">
<meta property="article:author" content="SoundMemories">
<meta name="twitter:card" content="summary">


<link rel="canonical" href="https://soundmemories.github.io/page/3/">


<script data-pjax class="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'zh-CN'
  };
</script>

  <title>SoundMemories</title>
  






  <noscript>
  <style>
  body { margin-top: 2rem; }

  .use-motion .menu-item,
  .use-motion .sidebar,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header {
    visibility: visible;
  }

  .use-motion .header,
  .use-motion .site-brand-container .toggle,
  .use-motion .footer { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle,
  .use-motion .custom-logo-image {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line {
    transform: scaleX(1);
  }

  .search-pop-overlay, .sidebar-nav { display: none; }
  .sidebar-panel { display: block; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <h1 class="site-title">SoundMemories</h1>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu">
        <li class="menu-item menu-item-主页">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>主页</a>

  </li>
        <li class="menu-item menu-item-分类">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a>

  </li>
        <li class="menu-item menu-item-标签">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a>

  </li>
        <li class="menu-item menu-item-归档">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" maxlength="80"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="search-result-container no-result">
  <div class="search-result-icon">
    <i class="fa fa-spinner fa-pulse fa-5x"></i>
  </div>
</div>

    </div>
  </div>

</div>
        
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>

  <aside class="sidebar">

    <div class="sidebar-inner sidebar-overview-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>
	   
		  
      <div class="sidebar-panel-container">
        <!--noindex-->
        <section class="post-toc-wrap sidebar-panel">
        </section>
        <!--/noindex-->

        <section class="site-overview-wrap sidebar-panel">
          <div class="site-author site-overview-item animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="SoundMemories"
      src="https://i.loli.net/2020/11/04/6JhNuwtBe4adylS.png">
  <p class="site-author-name" itemprop="name">SoundMemories</p>
  <div class="site-description" itemprop="description">今日事，今日毕</div>
</div>
<div class="site-state-wrap site-overview-item animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives">
          <span class="site-state-item-count">117</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
        <span class="site-state-item-count">8</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
        <span class="site-state-item-count">8</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author site-overview-item animated">
      <span class="links-of-author-item">
        <span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL3NvdW5kbWVtb3JpZXM=" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;soundmemories"><i class="fab fa-github fa-fw"></i>GitHub</span>
      </span>
      <span class="links-of-author-item">
        <span class="exturl" data-url="bWFpbHRvOnNvdW5kbWVtb3JpZXNAMTYzLmNvbQ==" title="E-Mail → mailto:soundmemories@163.com"><i class="fa fa-envelope fa-fw"></i>E-Mail</span>
      </span>
  </div>
  <div class="cc-license site-overview-item animated" itemprop="license">
    <span class="exturl cc-opacity" data-url="aHR0cHM6Ly9jcmVhdGl2ZWNvbW1vbnMub3JnL2xpY2Vuc2VzL2J5LW5jLXNhLzQuMC8="><img src="/images/cc-by-nc-sa.svg" alt="Creative Commons"></span>
  </div>



        </section>
      </div>
    </div>
  </aside>
  <div class="sidebar-dimmer"></div>


    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>
  <a role="button" class="book-mark-link book-mark-link-fixed"></a>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


    <div class="main-inner index posts-expand">
      

      
    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://soundmemories.github.io/2021/02/03/Graph/00.%E5%9B%BE%E6%B7%B1%E5%BA%A6%E8%A1%A8%E7%A4%BA/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="https://i.loli.net/2020/11/04/6JhNuwtBe4adylS.png">
      <meta itemprop="name" content="SoundMemories">
      <meta itemprop="description" content="今日事，今日毕">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="SoundMemories">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/02/03/Graph/00.%E5%9B%BE%E6%B7%B1%E5%BA%A6%E8%A1%A8%E7%A4%BA/" class="post-title-link" itemprop="url">图深度表示</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2021-02-03 00:00:00" itemprop="dateCreated datePublished" datetime="2021-02-03T00:00:00+08:00">2021-02-03</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Graph/" itemprop="url" rel="index"><span itemprop="name">Graph</span></a>
        </span>
    </span>

  
      </div>
      <div class="post-meta">
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>4.7k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>4 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="图结构"><a href="#图结构" class="headerlink" title="图结构"></a>图结构</h1><p>在现实中，很多情景的构成是不规则的图结构，比如社交网络、金融网络、化学分子结构等等。</p>
<h1 id="什么是图表示学习？"><a href="#什么是图表示学习？" class="headerlink" title="什么是图表示学习？"></a>什么是图表示学习？</h1><p>简单讲就是把图结构映射到向量空间（也叫graph embedding），或者由向量空间映射到图结构（也叫 graph generate）。</p>
<p>我们为什么这么做呢？或者说将图映射到向量空间的优势是什么？<br>（1）向量表示相对传统图表示（邻接矩阵、邻接表）对现有机器学习算法更友好。<br>（2）可以更好的将拓扑信息和节点本身特征结合。</p>
<h1 id="基于图结构的表示学习"><a href="#基于图结构的表示学习" class="headerlink" title="基于图结构的表示学习"></a>基于图结构的表示学习</h1><p>图论、数据挖掘角度：如何在学习到向量的表示中保留尽可能多的图拓扑结构的信息。</p>
<p>节点的向量表示只来源于图的拓扑结构（nxn 的邻接矩阵表达的图结构），只是对图结构的单一表示，缺乏对图节点特征消息的表示。下图d远远小于n。</p>
<p><img src="/images/图深度表示/图表示1.png" width="100%"></p>
<h2 id="传统方法"><a href="#传统方法" class="headerlink" title="传统方法"></a>传统方法</h2><p>第一个想法就是<strong>降维</strong>（nxn -&gt; nxd），利用现有的降维方法实现，比如PCA、LDA等等。致命缺点：时间复杂度高；<strong>不能保留图中节点和节点之间的拓扑信息</strong>。</p>
<ul>
<li>这里有个问题：什么叫做<strong>保留拓扑信息</strong>？</li>
<li><p>目标：<strong>在拓扑域里面‘邻近’&lt;==&gt; 在向量域‘邻近’</strong>。</p>
</li>
<li><p>问题：<strong>如何建模节点之间的邻近信息？ -&gt; 如何定义’邻近’？</strong></p>
</li>
<li>定义邻近的方法很多：<strong>共同出现、高阶邻近（n-hop邻居）、团体邻近（属于某一个团体）</strong>。</li>
</ul>
<p>所以现有算法都是围绕着 <strong>定义‘邻近’</strong> 和 <strong>求解‘邻近’</strong> 这两个点展开。</p>
<h2 id="Deepwalk"><a href="#Deepwalk" class="headerlink" title="Deepwalk"></a>Deepwalk</h2><ul>
<li><strong>动机</strong>：如何动态的建模邻居信息？</li>
<li><strong>1-hop建模</strong>：对于某个节点只看其邻居节点。两个相邻的结点就可以定义为邻近。<ul>
<li>太局部，忽略了图上的一些全局信息。</li>
</ul>
</li>
<li><strong>n-hop建模</strong>：对于某个节点考虑其n步邻居节点。两个n阶临近的结点也可以定义为邻近。<ul>
<li>组合爆炸，复杂度高，且没必要。</li>
</ul>
</li>
<li>解决方法：<ul>
<li>考虑有限步的情况，例如只考虑1，2 hop，即 LINE 2015。</li>
<li>使用采样的方式————随机游走（Random Walk）思想的 Deepwalk 2014。</li>
</ul>
</li>
</ul>
<p><strong>Deepwalk</strong>：<strong>利用随机游走采样生成的序列去定义节点间的邻近关系</strong>。<strong>在足够多的采样情况下，可以很好的刻画节点之间的邻近信息</strong>。<strong>这样就把图信息，转成了序列信息，通过Word2Vec把序列向量化即可（每个点看成词）</strong>。<br>总结：<br>（1）使用定长的随机游走去采样图中节点的邻近关系。<br>（2）节点-&gt;词语，随机游走序列-&gt;句子。<br>（3）使用自然语言处理相关模型（例如word2vec）对随机游走得到的序列进行表示学习。</p>
<p>基于Random Walk的思路出现了很多 XXX2vec 的论文，基本套路都一样。</p>
<h2 id="Node2vec"><a href="#Node2vec" class="headerlink" title="Node2vec"></a>Node2vec</h2><p><strong>动机</strong>：简单的随机游走采样不够好（不能体现出BFS/DFS性质）。<br><strong>核心思想</strong>：等概率跳 -&gt; 人工设计概率来跳。</p>
<p>当从结点 t 跳跃到结点 v 之后，算法下一步从结点 v 向邻居结点跳跃的概率是不同的。<br><img src="/images/图深度表示/node2vec.png" width="50%"></p>
<p>从结点 v 回跳到上一个结点 t 的 $\alpha$ 为 $\frac{1}{p}$，从结点 v 跳到 t、v 的公共邻居结点的 $\alpha$ 为 1，从结点 v 跳到其他邻居的 $\alpha$ 为 $\frac{1}{q}$。<br><img src="/images/图深度表示/node2vec2.png" width="50%"></p>
<p>我们发现，当 p 比较小的时候，结点间的跳转类似于 BFS，结点间的“接近”就可以理解为结点在<strong>邻接关系</strong>上“接近”；当 q 比较小的时候，结点间的跳转类似于 DFS，节点间的“接近”就可以视作是<strong>结构上相似</strong>。<br><img src="/images/图深度表示/node2vec3.png" width="50%"></p>
<h2 id="Struc2vec"><a href="#Struc2vec" class="headerlink" title="Struc2vec"></a>Struc2vec</h2><p><strong>动机</strong>：保留局部结构一致性。<br><strong>核心思想</strong>：在原来的图上构建一个新图。</p>
<h2 id="Metapath2vec"><a href="#Metapath2vec" class="headerlink" title="Metapath2vec"></a>Metapath2vec</h2><p><strong>动机</strong>：异构图上存在不同类型的节点，这些节点不能等同看待，其间关系可能存在一些固定模式。<br><strong>核心思路</strong>：使用预定义的Meta-Path来进行Random Walk。</p>
<h1 id="基于图特征的学习（图神经网络）"><a href="#基于图特征的学习（图神经网络）" class="headerlink" title="基于图特征的学习（图神经网络）"></a>基于图特征的学习（图神经网络）</h1><p>节点的向量表示既包含了图的拓扑信息（nxn 的邻接矩阵表达的图结构）也包含了节点的特征向量集合（nxf 的特征向量）。<br><img src="/images/图深度表示/图表示2.png" width="100%"></p>
<p>机器学习、特征工程角度：如何通过有效利用图拓扑结构信息结合现有的特征向量得到新的特征。<br>比如：图像-&gt;向量，视频-&gt;向量…。可以不严谨的说<strong>所有深度学习问题都可以归结为表示学习的问题</strong>。<br><strong>挑战</strong>：如何利用我们在图片/视频上取得的成功经验来应对图特征的表示学习问题？</p>
<p><strong>卷积神经网络</strong>（Convolutional Neural Network）：表示学习利器。<br>从图的角度看图像上的CNN：在欧式空间上的格点图（平移不变性、多尺度结构）。<br><strong>目标</strong>：将在欧式空间上的CNN扩展到拓扑空间————<strong>图卷积</strong>。</p>
<h2 id="GCN"><a href="#GCN" class="headerlink" title="GCN"></a>GCN</h2><p>GCN（Graph Convolutional Networks，图卷积神经网络）：</p>
<ul>
<li><strong>输入</strong>：邻接矩阵（节点数×节点数），特征矩阵（节点数×输入特征数）。</li>
<li><strong>输出</strong>：新的特征矩阵（节点数×输出特征数）。</li>
<li>网络层面：多层网络可以叠加。</li>
<li>节点层面：节点<strong>自身特征</strong>和其<strong>邻域特征</strong>的聚合。<br><img src="/images/图深度表示/GCN1.png" width="100%"></li>
</ul>
<p>公式如下：<br><img src="/images/图深度表示/GCN2.png" width="40%"></p>
<p>$\tilde{A}=A+I_N$：带自环的邻接矩阵。<br>$\tilde{D}=\sum_j \tilde{A}_{ij}$：度矩阵。<br>$H$：特征矩阵。<br>$W$：模型参数。<br>$\sigma(.)$：激活函数。</p>
<p><strong>两层GCN构造&amp;损失函数</strong>：<br><img src="/images/图深度表示/GCN3.png" width="50%"><br><img src="/images/图深度表示/GCN4.png" width="25%"></p>
<p><strong>GCN的推导思路</strong>：在图的拓扑空间近似在谱空间中的图滤波的操作，减少可学习参数。</p>
<p><strong>从另一个角度理解GCN</strong>：对<strong>邻居节点</strong>特征的<strong>带权重</strong>（$\tilde{D}^{-\frac{1}{2}}$）的<strong>聚合</strong>（$\tilde{A}H^{(l)}$）。</p>
<h2 id="GraphSAGE"><a href="#GraphSAGE" class="headerlink" title="GraphSAGE"></a>GraphSAGE</h2><p>对<strong>聚合</strong>和<strong>邻居节点</strong>进行了扩展定义：<br>（1）<strong>聚合</strong>：Mean Pooling/Max Pooling/LSTM，etc。<br>（2）<strong>邻居节点</strong>：Fix-length sample -&gt; 可以用来加速GCN计算。</p>
<p><img src="/images/图深度表示/GraphSAGE.png" width="100%"></p>
<h2 id="GAT"><a href="#GAT" class="headerlink" title="GAT"></a>GAT</h2><p>GAT（GRAPH ATTENTION NETWORKS，图注意力网络）：对<strong>权重</strong>（$\tilde{D}^{-\frac{1}{2}}$）进行了扩展。<br>（1）GCN中使用的邻接矩阵权重是提前给定的$\tilde{D}^{-\frac{1}{2}}$。<br>（2）图注意力网络引入了<strong>自注意力机制</strong>，利用当前节点的特征以及其邻居节点的特征计算邻居节点的重要性，把该重要性作为新的邻接矩阵进行卷积计算。<br>（3）有势：利用节点特征的相似性更能反映邻接信息。<br><img src="/images/图深度表示/GAT1.png" width="80%"></p>
<p><img src="/images/图深度表示/GAT2.png" width="50%"><br><img src="/images/图深度表示/GAT3.png" width="40%"></p>
<h1 id="图学习面临的挑战"><a href="#图学习面临的挑战" class="headerlink" title="图学习面临的挑战"></a>图学习面临的挑战</h1><h2 id="如何将图神经网络模型做到更大的图上（如何做大）？"><a href="#如何将图神经网络模型做到更大的图上（如何做大）？" class="headerlink" title="如何将图神经网络模型做到更大的图上（如何做大）？"></a>如何将图神经网络模型做到更大的图上（如何做大）？</h2><p>因为$\tilde{D}^{-\frac{1}{2}}\tilde{A}\tilde{D}^{-\frac{1}{2}}$中邻接矩阵用到所有节点，难以处理超大图。即<strong>对所有邻接节点进行聚合并不高效</strong>。</p>
<p><strong>思路</strong>：采样，采用一部分点/边来进行运算。<br><strong>FastGCN</strong>：<br>（1）把图节点特征看作有一个隐含概率分布产生，利用该分布对每一层的所有节点<strong>整体采样</strong>，避免了采样点个数的指数增加。<br>（2）采样的目标是尽量减少采样的方差-&gt;基于节点degree的采样。<br>（3）<strong>缺点</strong>：没有考虑层间点和点的关系。</p>
<p>为了克服这个缺点出现了层间采样的方法：<br><strong>ASGCN</strong>：FastGCN采样方式并不合理，在图极大而采样比例极少时，层间连接会急剧减少。<br>（1）自顶向下Layer-dependent的采样方式。<br>（2）在控制每层采样个数的同时，确保上下两层之间的连接是密集。<br>（3）通过公式证明了可以保证采样无偏和减小采样方差。<br>（4）扩展：加入了残差连接，能考虑二阶邻居的信息传播。在采样设置下，实现了注意力机制。</p>
<h2 id="如何有效训练更复杂的图神经网络模型（如何做深）？"><a href="#如何有效训练更复杂的图神经网络模型（如何做深）？" class="headerlink" title="如何有效训练更复杂的图神经网络模型（如何做深）？"></a>如何有效训练更复杂的图神经网络模型（如何做深）？</h2><p>为什么不能做深？<br>（1）过拟合（Overfitting）：参数数量过多造成的泛化性降低。<br>（2）<strong>过平滑</strong>（Over-Smoothing）：<strong>多层的邻居聚合造成的特征均化</strong>。</p>
<p>Over-Smoothing的定义：经过L层特征聚合后特征收敛到一个和输入特征无关的子空间M的现象。<br><img src="/images/图深度表示/Dropedge2.png" width="20%"></p>
<p><strong>挑战</strong>：如何减弱Over-Smoothing？<br><strong>DropEdge</strong>：<strong>在每个epoch训练前，随机丢掉一定比例的边</strong>。<br><img src="/images/图深度表示/Dropedge.png" width="20%"></p>
<p>为什么DropEdge可以减弱Over-Smoothing？<br>（1）<strong>DropEdge可以减缓收敛到子空间M的速度</strong>。<img src="/images/图深度表示/Dropedge3.png" width="20%"><br>（2）<strong>DropEdge可以减少收敛过程中的信息损失</strong>。<img src="/images/图深度表示/Dropedge4.png" width="30%"></p>
<p>由此通过减弱Over-Smoothing的影响，可以使我们可以成功在更复杂更深层的图神经网络上进行训练，并且提升精度。</p>
<h1 id="应用"><a href="#应用" class="headerlink" title="应用"></a>应用</h1><p>药物属性预测和可解释性问题：DualMPNN。所有属性预测数据上大幅超越SOTA算法并提供了模型的可解释性。<br>复杂层次图学习问题：SEAL算法。基于复杂层次图结构的GNN模型。应用于安全场景中的群分类任务。<br>社交网络谣言检测：Bi-GCN。首创双向GCN结构并将其应用于谣言检测问题。<br>统一黑盒攻击框架：GF-Attacker。首个可以对于多种图模型进行黑盒攻击的攻击框架。</p>
<h2 id="SEAL"><a href="#SEAL" class="headerlink" title="SEAL"></a>SEAL</h2><p><strong>背景</strong>：在实际数据中，图相互之间的关系可以建模成图，即层级图结构。比如QQ群和QQ群的关系、学术论文引用（不同领域间的引用构成层次图，领域内的文章引用构成实例图）。<br><img src="/images/图深度表示/SEAL.png" width="70%"></p>
<p><strong>问题</strong>：如何预测实例图的分类标签？<br><strong>挑战</strong>：<br>（1）<strong>如何利用统一长度的向量来表示具有不同大小的实例图</strong>？</p>
<ul>
<li>在不同层级下学习图的表示：<ul>
<li>节点层级：$G(V,E) -&gt; H^{n×v}$</li>
<li>层图级：$G(V,E) -&gt; \it e^{v}$</li>
</ul>
</li>
<li>自注意力图表示学习（Self-Attentive Graph Embedding）<ul>
<li>图大小不变性————自注意力机制（$\it e \in R^{r×v}$）</li>
<li>节点重要性——————自注意力机制</li>
<li>排列不变性——————GCN Smoothing</li>
</ul>
</li>
</ul>
<p><img src="/images/图深度表示/SEAL2.png" width="80%"></p>
<p>（2）<strong>如何在不同层级去融合实例图和层次图的信息</strong>？</p>
<ul>
<li><strong>实例图层次</strong>（Instance Classifier）：Graph Level Learning （SEGA）。</li>
<li><strong>层次图层次</strong>（Hierarchical Classifier）：Node Level Learning（GCN）。</li>
<li><strong>特征共享</strong>：将实例图的输出作为层次图模型的输入。<br><img src="/images/图深度表示/SEAL3.png" width="90%"></li>
</ul>
<h1 id="时间线"><a href="#时间线" class="headerlink" title="时间线"></a>时间线</h1><p><img src="/images/图深度表示/GNN.png" width="70%"></p>
<h1 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h1><p><span class="exturl" data-url="aHR0cHM6Ly96aC53aWtpcGVkaWEub3JnL3dpa2kvJUU1JTlCJUJFXyglRTYlOTUlQjAlRTUlQUQlQTY=">图 (数学)<i class="fa fa-external-link-alt"></i></span>)<br><span class="exturl" data-url="aHR0cHM6Ly96aC53aWtpcGVkaWEub3JnL3dpa2kvJUU1JTlCJUJFJUU4JUFFJUJB">图论<i class="fa fa-external-link-alt"></i></span><br><span class="exturl" data-url="aHR0cHM6Ly96aC53aWtpcGVkaWEub3JnL3dpa2kvJUU5JTgyJUJCJUU2JThFJUE1JUU3JTlGJUE5JUU5JTk4JUI1">邻接矩阵<i class="fa fa-external-link-alt"></i></span><br><span class="exturl" data-url="aHR0cHM6Ly93d3cuYmlsaWJpbGkuY29tL3ZpZGVvL2F2ODM1MTk3NjU/ZnJvbT1zZWFyY2gmYW1wO3NlaWQ9NDIxMTQxNDU5NzU0ODIzOTY3Ng==">图深度表示（GNN）的基础和前沿进展<i class="fa fa-external-link-alt"></i></span><br><span class="exturl" data-url="aHR0cHM6Ly93d3cuY25ibG9ncy5jb20vbnhmLXJhYmJpdDc1L3AvMTEzMDYxOTguaHRtbA==">GCN总结<i class="fa fa-external-link-alt"></i></span></p>
<p><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzE0MDMuNjY1Mi5wZGYlQzMlQUYlQzIlQkMlRTIlODAlQkE=">DeepWalk: Online Learning of Social Representations<i class="fa fa-external-link-alt"></i></span> DeepWalk 2014年<br><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzE1MDMuMDM1NzgucGRmJUMyJUEwJUUzJTgwJTkwV1dX">LINE: Large-scale Information Network Embedding<i class="fa fa-external-link-alt"></i></span>  LINE 2015年<br><span class="exturl" data-url="aHR0cHM6Ly93d3ctY3MtZmFjdWx0eS5zdGFuZm9yZC5lZHUvcGVvcGxlL2p1cmUvcHVicy9ub2RlMnZlYy1rZGQxNi5wZGY=">node2vec: Scalable Feature Learning for Networks<i class="fa fa-external-link-alt"></i></span>  node2vec 2016年<br><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzE3MDQuMDMxNjUucGRm">struc2vec: Learning Node Representations from Structural Identity<i class="fa fa-external-link-alt"></i></span>  struc2vec 2017年<br><span class="exturl" data-url="aHR0cHM6Ly9kbC5hY20ub3JnL2RvaS9wZGYvMTAuMTE0NS8zMDk3OTgzLjMwOTgwMzY=">metapath2vec: Scalable Representation Learning for Heterogeneous Networks<i class="fa fa-external-link-alt"></i></span>  metapath2vec 2017年</p>
<p><span class="exturl" data-url="aHR0cHM6Ly9vcGVucmV2aWV3Lm5ldC9wZGY/aWQ9U0pVNGF5WWds">SEMI-SUPERVISED CLASSIFICATION WITH GRAPH CONVOLUTIONAL NETWORKS<i class="fa fa-external-link-alt"></i></span> GCN 2017年<br><span class="exturl" data-url="aHR0cHM6Ly9wcm9jZWVkaW5ncy5uZXVyaXBzLmNjL3BhcGVyLzIwMTcvZmlsZS81ZGQ5ZGI1ZTAzM2RhOWM2ZmI1YmE4M2M3YTdlYmVhOS1QYXBlci5wZGY=">Inductive Representation Learning on Large Graphs<i class="fa fa-external-link-alt"></i></span> GraphSAGE 2017年<br><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzE3MTAuMTA5MDMucGRm">GRAPH ATTENTION NETWORKS<i class="fa fa-external-link-alt"></i></span> GAT 2018年</p>
<p><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzE4MDEuMTAyNDcucGRm">FASTGCN: FAST LEARNING WITH GRAPH CONVOLU TIONAL NETWORKS VIA IMPORTANCE SAMPLING<i class="fa fa-external-link-alt"></i></span> FASTGCN 2018年<br><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzE4MDkuMDUzNDMucGRm">Adaptive Sampling Towards Fast Graph Representation Learning<i class="fa fa-external-link-alt"></i></span>  ASGCN 2018年</p>
<p><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzE5MDcuMTA5MDMucGRm">DROPEDGE: TOWARDS DEEP GRAPH CONVOLU TIONAL NETWORKS ON NODE CLASSIFICATION<i class="fa fa-external-link-alt"></i></span> DROPEDGE 2020年<br><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzE5MDQuMDUwMDMucGRm">Semi-Supervised Graph Classification: A Hierarchical Graph Perspective<i class="fa fa-external-link-alt"></i></span> SEAL 2019年</p>

      
    </div>

    
    
    
      


    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://soundmemories.github.io/2020/10/17/Machine%20Learning/40.cheatsheet/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="https://i.loli.net/2020/11/04/6JhNuwtBe4adylS.png">
      <meta itemprop="name" content="SoundMemories">
      <meta itemprop="description" content="今日事，今日毕">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="SoundMemories">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/10/17/Machine%20Learning/40.cheatsheet/" class="post-title-link" itemprop="url">cheatsheet</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2020-10-17 00:00:00" itemprop="dateCreated datePublished" datetime="2020-10-17T00:00:00+08:00">2020-10-17</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Machine-Learning/" itemprop="url" rel="index"><span itemprop="name">Machine Learning</span></a>
        </span>
    </span>

  
      </div>
      <div class="post-meta">
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>11k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>10 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><h2 id="Math"><a href="#Math" class="headerlink" title="Math"></a>Math</h2><ol>
<li>MLE<script type="math/tex; mode=display">
\theta_{MLE}=\mathop{argmax}\limits _{\theta}\log p(X|\theta)\mathop{=}\limits _{iid}\mathop{argmax}\limits _{\theta}\sum\limits _{i=1}^{N}\log p(x_{i}|\theta)</script></li>
</ol>
<ol>
<li><p>MAP</p>
<script type="math/tex; mode=display">
\theta_{MAP}=\mathop{argmax}\limits _{\theta}p(\theta|X)=\mathop{argmax}\limits _{\theta}p(X|\theta)\cdot p(\theta)</script></li>
<li><p>Gaussian Distribution</p>
<script type="math/tex; mode=display">
\begin{align}&p(x|\mu,\Sigma)=\frac{1}{(2\pi)^{p/2}|\Sigma|^{1/2}}e^{-\frac{1}{2}(x-\mu)^{T}\Sigma^{-1}(x-\mu)}\\
&\Delta=(x-\mu)^{T}\Sigma^{-1}(x-\mu)=\sum\limits _{i=1}^{p}(x-\mu)^{T}u_{i}\frac{1}{\lambda_{i}}u_{i}^{T}(x-\mu)=\sum\limits _{i=1}^{p}\frac{y_{i}^{2}}{\lambda_{i}}
\end{align}</script></li>
<li><p>已知 $x\sim\mathcal{N}(\mu,\Sigma), y\sim Ax+b$，有：</p>
<script type="math/tex; mode=display">
\begin{align}y\sim\mathcal{N}(A\mu+b, A\Sigma A^T)
\end{align}</script></li>
<li><p>记 $x=(x_1, x_2,\cdots,x_p)^T=(x_{a,m\times 1}, x_{b,n\times1})^T,\mu=(\mu_{a,m\times1}, \mu_{b,n\times1}),\Sigma=\begin{pmatrix}\Sigma_{aa}&amp;\Sigma_{ab}\\\Sigma_{ba}&amp;\Sigma_{bb}\end{pmatrix}$，已知 $x\sim\mathcal{N}(\mu,\Sigma)$，则：</p>
<script type="math/tex; mode=display">
\begin{align}&x_a\sim\mathcal{N}(\mu_a,\Sigma_{aa})\\
&x_b|x_a\sim\mathcal{N}(\mu_{b|a},\Sigma_{b|a})\\
&\mu_{b|a}=\Sigma_{ba}\Sigma_{aa}^{-1}(x_a-\mu_a)+\mu_b\\
&\Sigma_{b|a}=\Sigma_{bb}-\Sigma_{ba}\Sigma_{aa}^{-1}\Sigma_{ab}
\end{align}</script></li>
</ol>
<h2 id="Linear-Regression"><a href="#Linear-Regression" class="headerlink" title="Linear Regression"></a>Linear Regression</h2><h3 id="Model"><a href="#Model" class="headerlink" title="Model"></a>Model</h3><ol>
<li><p>Dataset: </p>
<script type="math/tex; mode=display">
\mathcal{D}=\{(x_1, y_1),(x_2, y_2),\cdots,(x_N, y_N)\}</script></li>
<li><p>Notation:</p>
<script type="math/tex; mode=display">
X=(x_1,x_2,\cdots,x_N)^T,Y=(y_1,y_2,\cdots,y_N)^T</script></li>
<li><p>Model:</p>
<script type="math/tex; mode=display">
f(w)=w^Tx</script></li>
</ol>
<h3 id="Loss-Function"><a href="#Loss-Function" class="headerlink" title="Loss Function"></a>Loss Function</h3><ol>
<li>最小二乘误差/高斯噪声的MLE<script type="math/tex; mode=display">
L(w)=\sum\limits_{i=1}^N||w^Tx_i-y_i||^2_2</script></li>
</ol>
<h3 id="闭式解"><a href="#闭式解" class="headerlink" title="闭式解"></a>闭式解</h3><script type="math/tex; mode=display">
\begin{align}\hat{w}=(X^TX)^{-1}X^TY=X^+Y\\
X=U\Sigma V^T\\
X^+=V\Sigma^{-1}U^T
\end{align}</script><h3 id="正则化"><a href="#正则化" class="headerlink" title="正则化"></a>正则化</h3><script type="math/tex; mode=display">
\begin{align}
L1-Gaussian \ priori&:\mathop{argmin}\limits_wL(w)+\lambda||w||_1,\lambda\gt0\\
L2-Laplasian\ priori-Sparsity&:\mathop{argmin}\limits_wL(w)+\lambda||w||^2_2,\lambda \gt 0
\end{align}</script><h2 id="Linear-Classification"><a href="#Linear-Classification" class="headerlink" title="Linear Classification"></a>Linear Classification</h2><h3 id="Hard"><a href="#Hard" class="headerlink" title="Hard"></a>Hard</h3><h4 id="PCA"><a href="#PCA" class="headerlink" title="PCA"></a>PCA</h4><ol>
<li><p>Idea: 在线性模型上加入激活函数</p>
</li>
<li><p>Loss Function:</p>
</li>
</ol>
<script type="math/tex; mode=display">
L(w)=\sum\limits_{x_i\in\mathcal{D}_{wrong}}-y_iw^Tx_i</script><ol>
<li>Parameters:</li>
</ol>
<script type="math/tex; mode=display">
w^{t+1}\leftarrow w^{t}+\lambda y_ix_i</script><h4 id="Fisher"><a href="#Fisher" class="headerlink" title="Fisher"></a>Fisher</h4><ol>
<li><p>Idea: 投影，类内小，类间大。</p>
</li>
<li><p>Loss Function:</p>
<script type="math/tex; mode=display">
\begin{align}&J(w)=\frac{w^TS_bw}{w^TS_ww}\\
&S_b=(\overline{x_{c1}}-\overline{x_{c2}})(\overline{x_{c1}}-\overline{x_{c2}})^T\\
&S_w=S_1+S_2
\end{align}</script></li>
<li><p>闭式解，投影方向:</p>
<script type="math/tex; mode=display">
S_w^{-1}(\overline{x_{c1}}-\overline{x_{c2}})</script></li>
</ol>
<h3 id="Soft"><a href="#Soft" class="headerlink" title="Soft"></a>Soft</h3><h4 id="判别模型"><a href="#判别模型" class="headerlink" title="判别模型"></a>判别模型</h4><h5 id="Logistic-Regression"><a href="#Logistic-Regression" class="headerlink" title="Logistic Regression"></a>Logistic Regression</h5><ol>
<li><p>Idea，激活函数:</p>
<script type="math/tex; mode=display">
\begin{align}p(C_1|x)&=\frac{1}{1+\exp(-a)}\\
a&=w^Tx
\end{align}</script></li>
<li><p>Loss Function(交叉熵):</p>
<script type="math/tex; mode=display">
\hat{w}=\mathop{argmax}_wJ(w)=\mathop{argmax}_w\sum\limits_{i=1}^N(y_i\log p_1+(1-y_i)\log p_0)</script></li>
<li><p>解法，SGD</p>
<script type="math/tex; mode=display">
J'(w)=\sum\limits_{i=1}^N(y_i-p_1)x_i</script></li>
</ol>
<h4 id="生成模型"><a href="#生成模型" class="headerlink" title="生成模型"></a>生成模型</h4><h5 id="GDA"><a href="#GDA" class="headerlink" title="GDA"></a>GDA</h5><ol>
<li><p>Model</p>
<ol>
<li>$y\sim Bernoulli(\phi)$</li>
<li>$x|y=1\sim\mathcal{N}(\mu_1,\Sigma)$</li>
<li>$x|y=0\sim\mathcal{N}(\mu_0,\Sigma)$</li>
</ol>
</li>
<li><p>MAP</p>
<script type="math/tex; mode=display">
\begin{align}
&\mathop{argmax}_{\phi,\mu_0,\mu_1,\Sigma}\log p(X|Y)p(Y)\nonumber\\
&=\mathop{argmax}_{\phi,\mu_0,\mu_1,\Sigma}\sum\limits_{i=1}^N((1-y_i)\log\mathcal{N}(\mu_0,\Sigma)+y_i\log \mathcal{N}(\mu_1,\Sigma)+y_i\log\phi+(1-y_i)\log(1-\phi))
\end{align}</script></li>
<li><p>解</p>
<script type="math/tex; mode=display">
\begin{align}\phi&=\frac{N_1}{N}\\
\mu_1&=\frac{\sum\limits_{i=1}^Ny_ix_i}{N_1}\\
\mu_0&=\frac{\sum\limits_{i=1}^N(1-y_i)x_i}{N_0}\\
\Sigma&=\frac{N_1S_1+N_2S_2}{N}
\end{align}</script></li>
</ol>
<h5 id="Naive-Bayesian"><a href="#Naive-Bayesian" class="headerlink" title="Naive Bayesian"></a>Naive Bayesian</h5><ol>
<li><p>Model, 对单个数据点的各个维度作出限制</p>
<script type="math/tex; mode=display">
x_i\perp x_j|y,\forall\  i\ne j</script><ol>
<li>$x_i$ 为连续变量：$p(x_i|y)=\mathcal{N}(\mu_i,\sigma_i^2)$</li>
<li>$x_i$ 为离散变量：类别分布（Categorical）：$p(x_i=i|y)=\theta_i,\sum\limits_{i=1}^K\theta_i=1$</li>
<li>$p(y)=\phi^y(1-\phi)^{1-y}$</li>
</ol>
</li>
<li><p>解：和GDA相同</p>
</li>
</ol>
<h2 id="Dimension-Reduction"><a href="#Dimension-Reduction" class="headerlink" title="Dimension Reduction"></a>Dimension Reduction</h2><p>中心化：</p>
<script type="math/tex; mode=display">
\begin{align}S
&=\frac{1}{N}X^T(E_N-\frac{1}{N}\mathbb{I}_{N1}\mathbb{I}_{1N})(E_N-\frac{1}{N}\mathbb{I}_{N1}\mathbb{I}_{1N})^TX\nonumber\\
&=\frac{1}{N}X^TH^2X=\frac{1}{N}X^THX
\end{align}</script><h3 id="PCA-1"><a href="#PCA-1" class="headerlink" title="PCA"></a>PCA</h3><ol>
<li><p>Idea: 坐标变换，寻找线性无关的新基矢，取信息损失最小的前几个维度</p>
</li>
<li><p>Loss Function:</p>
<script type="math/tex; mode=display">
\begin{align}J
&=\sum\limits_{j=1}^qu_j^TSu_j\ ,\ s.t.\ u_j^Tu_j=1
\end{align}</script></li>
<li><p>解：</p>
<ol>
<li><p>特征分解法</p>
<script type="math/tex; mode=display">
S=U\Lambda U^T</script></li>
<li><p>SVD for X/S</p>
<script type="math/tex; mode=display">
\begin{align}HX=U\Sigma V^T\\
S=\frac{1}{N}V\Sigma^T\Sigma V^T
\\new\ co=HX\cdot V\end{align}</script></li>
<li><p>SVD for T</p>
<script type="math/tex; mode=display">
\begin{align}T=HXX^TH=U\Sigma\Sigma^TU^T\\
new\ co=U\Sigma
\end{align}</script></li>
</ol>
</li>
</ol>
<h3 id="p-PCA"><a href="#p-PCA" class="headerlink" title="p-PCA"></a>p-PCA</h3><ol>
<li><p>Model:</p>
<script type="math/tex; mode=display">
\begin{align}
z&\sim\mathcal{N}(\mathbb{O}_{q1},\mathbb{I}_{qq})\\
x&=Wz+\mu+\varepsilon\\
\varepsilon&\sim\mathcal{N}(0,\sigma^2\mathbb{I}_{pp})
\end{align}</script></li>
<li><p>Learning: E-M</p>
</li>
<li><p>Inference:</p>
<script type="math/tex; mode=display">
p(z|x)=\mathcal{N}(W^T(WW^T+\sigma^2\mathbb{I})^{-1}(x-\mu),\mathbb{I}-W^T(WW^T+\sigma^2\mathbb{I})^{-1}W)</script></li>
</ol>
<h2 id="SVM"><a href="#SVM" class="headerlink" title="SVM"></a>SVM</h2><ol>
<li>强对偶关系：凸优化+（松弛）Slater 条件-&gt;强对偶。</li>
<li>参数求解：KKT条件<ol>
<li>可行域</li>
<li>互补松弛+梯度为0</li>
</ol>
</li>
</ol>
<h3 id="Hard-margin"><a href="#Hard-margin" class="headerlink" title="Hard-margin"></a>Hard-margin</h3><ol>
<li><p>Idea: 最大化间隔</p>
</li>
<li><p>Model:</p>
<script type="math/tex; mode=display">
\mathop{argmin}_{w,b}\frac{1}{2}w^Tw\ s.t.\ y_i(w^Tx_i+b)\ge1,i=1,2,\cdots,N</script></li>
<li><p>对偶问题</p>
<script type="math/tex; mode=display">
\max_{\lambda}-\frac{1}{2}\sum\limits_{i=1}^N\sum\limits_{j=1}^N\lambda_i\lambda_jy_iy_jx_i^Tx_j+\sum\limits_{i=1}^N\lambda_i,\ s.t.\ \lambda_i\ge0</script></li>
<li><p>模型参数</p>
<script type="math/tex; mode=display">
\hat{w}=\sum\limits_{i=1}^N\lambda_iy_ix_i\\
\hat{b}=y_k-w^Tx_k=y_k-\sum\limits_{i=1}^N\lambda_iy_ix_i^Tx_k,\exists k,1-y_k(w^Tx_k+b)=0</script></li>
</ol>
<h3 id="Soft-margin"><a href="#Soft-margin" class="headerlink" title="Soft-margin"></a>Soft-margin</h3><ol>
<li><p>Idea:允许少量错误</p>
</li>
<li><p>Model:</p>
<script type="math/tex; mode=display">
error=\sum\limits_{i=1}^N\max\{0,1-y_i(w^Tx_i+b)\}\\
\mathop{argmin}_{w,b}\frac{1}{2}w^Tw+C\sum\limits_{i=1}^N\xi_i\ s.t.\ y_i(w^Tx_i+b)\ge1-\xi_i,\xi_i\ge0,i=1,2,\cdots,N</script></li>
</ol>
<h3 id="Kernel"><a href="#Kernel" class="headerlink" title="Kernel"></a>Kernel</h3><p>对称的正定函数都可以作为正定核。</p>
<h2 id="Exp-Family"><a href="#Exp-Family" class="headerlink" title="Exp Family"></a>Exp Family</h2><ol>
<li><p>表达式</p>
<script type="math/tex; mode=display">
p(x|\eta)=h(x)\exp(\eta^T\phi(x)-A(\eta))=\frac{1}{\exp(A(\eta))}h(x)\exp(\eta^T\phi(x))</script></li>
<li><p>对数配分函数</p>
<script type="math/tex; mode=display">
\begin{align} 
A'(\eta)=\mathbb{E}_{p(x|\eta)}[\phi(x)]\\
A''(\eta)=Var_{p(x|\eta)}[\phi(x)]
\end{align}</script></li>
<li><p>指数族分布满足最大熵定理</p>
</li>
</ol>
<h2 id="PGM"><a href="#PGM" class="headerlink" title="PGM"></a>PGM</h2><h3 id="Representation"><a href="#Representation" class="headerlink" title="Representation"></a>Representation</h3><ol>
<li>有向图<script type="math/tex; mode=display">
p(x_1,x_2,\cdots,x_p)=\prod\limits_{i=1}^pp(x_i|x_{parent(i)})</script>D-separation<script type="math/tex; mode=display">
p(x_i|x_{-i})=\frac{p(x)}{\int p(x)dx_{i}}=\frac{\prod\limits_{j=1}^pp(x_j|x_{parents(j)})}{\int\prod\limits_{j=1}^pp(x_j|x_{parents(j)})dx_i}=\frac{p(x_i|x_{parents(i)})p(x_{child(i)}|x_i)}{\int p(x_i|x_{parents(i)})p(x_{child(i)}|x_i)dx_i}</script></li>
</ol>
<ol>
<li><p>无向图</p>
<script type="math/tex; mode=display">
\begin{align}p(x)=\frac{1}{Z}\prod\limits_{i=1}^{K}\phi(x_{ci})\\
Z=\sum\limits_{x\in\mathcal{X}}\prod\limits_{i=1}^{K}\phi(x_{ci})\\
\phi(x_{ci})=\exp(-E(x_{ci}))
\end{align}</script></li>
<li><p>有向转无向</p>
<ol>
<li>将每个节点的父节点两两相连</li>
<li>将有向边替换为无向边</li>
</ol>
</li>
</ol>
<h3 id="Learning"><a href="#Learning" class="headerlink" title="Learning"></a>Learning</h3><p>参数学习-EM</p>
<ol>
<li><p>目的：解决具有隐变量的混合模型的参数估计（极大似然估计）</p>
</li>
<li><p>参数：</p>
<script type="math/tex; mode=display">
\theta_{MLE}=\mathop{argmax}\limits_\theta\log p(x|\theta)</script></li>
</ol>
<ol>
<li><p>迭代求解：</p>
<script type="math/tex; mode=display">
\theta^{t+1}=\mathop{argmax}\limits_{\theta}\int_z\log [p(x,z|\theta)]p(z|x,\theta^t)dz=\mathbb{E}_{z|x,\theta^t}[\log p(x,z|\theta)]</script></li>
<li><p>原理</p>
<script type="math/tex; mode=display">
\log p(x|\theta^t)\le\log p(x|\theta^{t+1})</script></li>
<li><p>广义EM</p>
<ol>
<li><p>E step：</p>
<script type="math/tex; mode=display">
\hat{q}^{t+1}(z)=\mathop{argmax}_q\int_zq^t(z)\log\frac{p(x,z|\theta)}{q^t(z)}dz,fixed\ \theta</script></li>
<li><p>M step：</p>
<script type="math/tex; mode=display">
\hat{\theta}=\mathop{argmax}_\theta \int_zq^{t+1}(z)\log\frac{p(x,z|\theta)}{q^{t+1}(z)}dz,fixed\ \hat{q}</script></li>
</ol>
</li>
</ol>
<h3 id="Inference"><a href="#Inference" class="headerlink" title="Inference"></a>Inference</h3><ol>
<li><p>精确推断</p>
<ol>
<li><p>VE</p>
</li>
<li><p>BP</p>
<script type="math/tex; mode=display">
m_{j\to i}(i)=\sum\limits_j\phi_j(j)\phi_{ij}(ij)\prod\limits_{k\in Neighbour(j)-i}m_{k\to j}(j)</script></li>
<li><p>MP</p>
<script type="math/tex; mode=display">
m_{j\to i}=\max\limits_{j}\phi_j\phi_{ij}\prod\limits_{k\in Neighbour(j)-i}m_{k\to j}</script></li>
</ol>
</li>
<li><p>近似推断</p>
<ol>
<li><p>确定性近似，VI</p>
<ol>
<li><p>变分表达式</p>
<script type="math/tex; mode=display">
\hat{q}(Z)=\mathop{argmax}_{q(Z)}L(q)</script></li>
<li><p>平均场近似下的 VI-坐标上升</p>
<script type="math/tex; mode=display">
\mathbb{E}_{\prod\limits_{i\ne j}q_i(Z_i)}[\log p(X,Z)]=\log \hat{p}(X,Z_j)\\
q_j(Z_j)=\hat{p}(X,Z_j)</script></li>
<li><p>SGVI-变成优化问题，重参数法</p>
<script type="math/tex; mode=display">
\begin{aligned}
\mathop{argmax}_{q(Z)}L(q)=\mathop{argmax}_{\phi}L(\phi)\\
\nabla_\phi L(\phi)=\mathbb{E}_{q_\phi}[(\nabla_\phi\log q_\phi)(\log p_\theta(x^i,z)-\log q_\phi(z))]\\
=\mathbb{E}_{p(\varepsilon)}[\nabla_z[\log p_\theta(x^i,z)-\log q_\phi(z)]\nabla_\phi g_\phi(\varepsilon,x^i)]\\
z=g_\phi(\varepsilon,x^i),\varepsilon\sim p(\varepsilon)
\end{aligned}</script></li>
</ol>
</li>
<li><p>随机性近似</p>
<ol>
<li><p>蒙特卡洛方法采样</p>
<ol>
<li><p>CDF 采样</p>
</li>
<li><p>拒绝采样， $q(z)$，使得 $\forall z_i,Mq(z_i)\ge p(z_i)$，拒绝因子：$\alpha=\frac{p(z^i)}{Mq(z^i)}\le1$</p>
</li>
<li><p>重要性采样</p>
<script type="math/tex; mode=display">
\mathbb{E}_{p(z)}[f(z)]=\int p(z)f(z)dz=\int \frac{p(z)}{q(z)}f(z)q(z)dz\simeq\frac{1}{N}\sum\limits_{i=1}^Nf(z_i)\frac{p(z_i)}{q(z_i)}</script></li>
<li><p>重要性重采样：重要性采样+重采样</p>
</li>
</ol>
</li>
<li><p>MCMC：构建马尔可夫链概率序列，使其收敛到平稳分布 $p(z)$。</p>
<ol>
<li><p>转移矩阵（提议分布）</p>
<script type="math/tex; mode=display">
p(z)\cdot Q_{z\to z^*}\alpha(z,z^*)=p(z^*)\cdot Q_{z^*\to z}\alpha(z^*,z)\\
\alpha(z,z^*)=\min\{1,\frac{p(z^*)Q_{z^*\to z}}{p(z)Q_{z\to z^*}}\}</script></li>
<li><p>算法（MH）：</p>
<ol>
<li>通过在0，1之间均匀分布取点 $u$</li>
<li>生成 $z^<em>\sim Q(z^</em>|z^{i-1})$</li>
<li>计算 $\alpha$ 值</li>
<li>如果 $\alpha\ge u$，则 $z^i=z^*$，否则 $z^{i}=z^{i-1}$</li>
</ol>
</li>
</ol>
</li>
<li><p>Gibbs 采样：给定初始值 $z_1^0,z_2^0,\cdots$在 $t+1$ 时刻，采样 $z_i^{t+1}\sim p(z_i|z_{-i})$，从第一个维度一个个采样。</p>
</li>
</ol>
</li>
</ol>
</li>
</ol>
<h2 id="GMM"><a href="#GMM" class="headerlink" title="GMM"></a>GMM</h2><ol>
<li><p>Model</p>
<script type="math/tex; mode=display">
p(x)=\sum\limits_{k=1}^Kp_k\mathcal{N}(x|\mu_k,\Sigma_k)</script></li>
<li><p>求解-EM</p>
<script type="math/tex; mode=display">
\begin{align}Q(\theta,\theta^t)&=\sum\limits_z[\log\prod\limits_{i=1}^Np(x_i,z_i|\theta)]\prod \limits_{i=1}^Np(z_i|x_i,\theta^t)\nonumber\\
&=\sum\limits_z[\sum\limits_{i=1}^N\log p(x_i,z_i|\theta)]\prod \limits_{i=1}^Np(z_i|x_i,\theta^t)\nonumber\\
&=\sum\limits_{i=1}^N\sum\limits_{z_i}\log p(x_i,z_i|\theta)p(z_i|x_i,\theta^t)\nonumber\\
&=\sum\limits_{i=1}^N\sum\limits_{z_i}\log p_{z_i}\mathcal{N(x_i|\mu_{z_i},\Sigma_{z_i})}\frac{p_{z_i}^t\mathcal{N}(x_i|\mu_{z_i}^t,\Sigma_{z_i}^t)}{\sum\limits_kp_k^t\mathcal{N}(x_i|\mu_k^t,\Sigma_k^t)}
\end{align}</script><script type="math/tex; mode=display">
p_k^{t+1}=\frac{1}{N}\sum\limits_{i=1}^Np(z_i=k|x_i,\theta^t)</script></li>
</ol>
<h2 id="序列模型-HMM，LDS，Particle"><a href="#序列模型-HMM，LDS，Particle" class="headerlink" title="序列模型-HMM，LDS，Particle"></a>序列模型-HMM，LDS，Particle</h2><ol>
<li><p>假设：</p>
<ol>
<li><p>齐次 Markov 假设（未来只依赖于当前）：</p>
<script type="math/tex; mode=display">
p(i_{t+1}|i_t,i_{t-1},\cdots,i_1,o_t,o_{t-1},\cdots,o_1)=p(i_{t+1}|i_t)</script></li>
<li><p>观测独立假设：</p>
<script type="math/tex; mode=display">
p(o_t|i_t,i_{t-1},\cdots,i_1,o_{t-1},\cdots,o_1)=p(o_t|i_t)</script></li>
</ol>
</li>
<li><p>参数</p>
<script type="math/tex; mode=display">
\lambda=(\pi,A,B)</script></li>
</ol>
<h3 id="离散线性隐变量-HMM"><a href="#离散线性隐变量-HMM" class="headerlink" title="离散线性隐变量-HMM"></a>离散线性隐变量-HMM</h3><ol>
<li><p>Evaluation：$p(O|\lambda)$，Forward-Backward 算法</p>
<script type="math/tex; mode=display">
p(O|\lambda)=\sum\limits_{i=1}^Np(O,i_T=q_i|\lambda)=\sum\limits_{i=1}^N\alpha_T(i)=\sum\limits_{i=1}^Nb_i(o_1)\pi_i\beta_1(i)\\
\alpha_{t+1}(j)=\sum\limits_{i=1}^Nb_{j}(o_t)a_{ij}\alpha_t(i)\\
\beta_t(i)=\sum\limits_{j=1}^Nb_j(o_{t+1})a_{ij}\beta_{t+1}(j)</script></li>
<li><p>Learning：$\lambda=\mathop{argmax}\limits_{\lambda}p(O|\lambda)$，EM 算法（Baum-Welch）</p>
<script type="math/tex; mode=display">
\lambda^{t+1}=\mathop{argmax}_\lambda\sum\limits_I\log p(O,I|\lambda)p(O,I|\lambda^t)\\=\sum\limits_I[\log \pi_{i_1}+\sum\limits_{t=2}^T\log a_{i_{t-1},i_t}+\sum\limits_{t=1}^T\log b_{i_t}(o_t)]p(O,I|\lambda^t)</script></li>
<li><p>Decoding：$I=\mathop{argmax}\limits_{I}p(I|O,\lambda)$，Viterbi 算法-动态规划</p>
<script type="math/tex; mode=display">
\delta_{t}(j)=\max\limits_{i_1,\cdots,i_{t-1}}p(o_1,\cdots,o_t,i_1,\cdots,i_{t-1},i_t=q_i)\\\delta_{t+1}(j)=\max\limits_{1\le i\le N}\delta_t(i)a_{ij}b_j(o_{t+1})\\\psi_{t+1}(j)=\mathop{argmax}\limits_{1\le i\le N}\delta_t(i)a_{ij}</script></li>
</ol>
<h3 id="连续线性隐变量-LDS"><a href="#连续线性隐变量-LDS" class="headerlink" title="连续线性隐变量-LDS"></a>连续线性隐变量-LDS</h3><ol>
<li><p>Model</p>
<script type="math/tex; mode=display">
\begin{align}
p(z_t|z_{t-1})&\sim\mathcal{N}(A\cdot z_{t-1}+B,Q)\\
p(x_t|z_t)&\sim\mathcal{N}(C\cdot z_t+D,R)\\
z_1&\sim\mathcal{N}(\mu_1,\Sigma_1)
\end{align}</script></li>
<li><p>滤波</p>
<script type="math/tex; mode=display">
p(z_t|x_{1:t})=p(x_{1:t},z_t)/p(x_{1:t})\propto p(x_{1:t},z_t)\\=p(x_t|z_t)p(z_t|x_{1:t-1})p(x_{1:t-1})\propto p(x_t|z_t)p(z_t|x_{1:t-1})</script></li>
<li><p>递推求解-线性高斯模型</p>
<ol>
<li><p>Prediction</p>
<script type="math/tex; mode=display">
p(z_t|x_{1:t-1})=\int_{z_{t-1}}p(z_t|z_{t-1})p(z_{t-1}|x_{1:t-1})dz_{t-1}=\int_{z_{t-1}}\mathcal{N}(Az_{t-1}+B,Q)\mathcal{N}(\mu_{t-1},\Sigma_{t-1})dz_{t-1}</script></li>
<li><p>Update:</p>
<script type="math/tex; mode=display">
p(z_t|x_{1:t})\propto p(x_t|z_t)p(z_t|x_{1:t-1}</script></li>
</ol>
</li>
</ol>
<h3 id="连续非线性隐变量-粒子滤波"><a href="#连续非线性隐变量-粒子滤波" class="headerlink" title="连续非线性隐变量-粒子滤波"></a>连续非线性隐变量-粒子滤波</h3><p>通过采样(SIR)解决：</p>
<script type="math/tex; mode=display">
\mathbb{E}[f(z)]=\int_zf(z)p(z)dz=\int_zf(z)\frac{p(z)}{q(z)}q(z)dz=\sum\limits_{i=1}^Nf(z_i)\frac{p(z_i)}{q(z_i)}</script><ol>
<li><p>采样</p>
<script type="math/tex; mode=display">
w_t^i\propto\frac{p(x_t|z_t)p(z_t|z_{t-1})}{q(z_t|z_{1:t-1},x_{1:t})}w_{t-1}^i\\
q(z_t|z_{1:t-1},x_{1:t})=p(z_t|z_{t-1})</script></li>
<li><p>重采样</p>
</li>
</ol>
<h2 id="CRF"><a href="#CRF" class="headerlink" title="CRF"></a>CRF</h2><ol>
<li><p>PDF</p>
<script type="math/tex; mode=display">
p(Y=y|X=x)=\frac{1}{Z(x,\theta)}\exp[\theta^TH(y_t,y_{t-1},x)]</script></li>
<li><p>边缘概率</p>
<script type="math/tex; mode=display">
\begin{aligned}
p(y_t=i|x)=\sum\limits_{y_{1:t-1}}\sum\limits_{y_{t+1:T}}\frac{1}{Z}\prod\limits_{t'=1}^T\phi_{t'}(y_{t'-1},y_{t'},x)\\
p(y_t=i|x)=\frac{1}{Z}\Delta_l\Delta_r\\
\Delta_l=\sum\limits_{y_{1:t-1}}\phi_{1}(y_0,y_1,x)\phi_2(y_1,y_2,x)\cdots\phi_{t-1}(y_{t-2},y_{t-1},x)\phi_t(y_{t-1},y_t=i,x)\\
\Delta_r=\sum\limits_{y_{t+1:T}}\phi_{t+1}(y_t=i,y_{t+1},x)\phi_{t+2}(y_{t+1},y_{t+2},x)\cdots\phi_T(y_{T-1},y_T,x)
\end{aligned}</script><script type="math/tex; mode=display">
\begin{aligned}
\alpha_t(i)=\Delta_l=\sum\limits_{j\in S}\phi_t(y_{t-1}=j,y_t=i,x)\alpha_{t-1}(j)\\
\Delta_r=\beta_t(i)=\sum\limits_{j\in S}\phi_{t+1}(y_t=i,y_{t+1}=j,x)\beta_{t+1}(j)
\end{aligned}</script></li>
<li><p>学习</p>
<script type="math/tex; mode=display">
\nabla_\lambda L=\sum\limits_{i=1}^N\sum\limits_{t=1}^T[f(y_{t-1},y_t,x^i)-\sum\limits_{y_{t-1}}\sum\limits_{y_t}p(y_{t-1},y_t|x^i)f(y_{t-1},y_t,x^i)]</script></li>
</ol>
<div class="note info"><p>文章转载自<span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL3NodWh1YWkwMDc=">Jie Zhou<i class="fa fa-external-link-alt"></i></span>的<span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL3NodWh1YWkwMDcvTWFjaGluZS1MZWFybmluZy1TZXNzaW9u">Machine-Learning-Session<i class="fa fa-external-link-alt"></i></span>。</p>
</div>
      
    </div>

    
    
    
      


    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://soundmemories.github.io/2020/10/16/Machine%20Learning/39.ApproInference/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="https://i.loli.net/2020/11/04/6JhNuwtBe4adylS.png">
      <meta itemprop="name" content="SoundMemories">
      <meta itemprop="description" content="今日事，今日毕">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="SoundMemories">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/10/16/Machine%20Learning/39.ApproInference/" class="post-title-link" itemprop="url">ApproInference</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2020-10-16 00:00:00" itemprop="dateCreated datePublished" datetime="2020-10-16T00:00:00+08:00">2020-10-16</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Machine-Learning/" itemprop="url" rel="index"><span itemprop="name">Machine Learning</span></a>
        </span>
    </span>

  
      </div>
      <div class="post-meta">
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>646</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>1 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="近似推断"><a href="#近似推断" class="headerlink" title="近似推断"></a>近似推断</h1><p>这一讲中的近似推断具体描述在深度生成模型中的近似推断。推断的目的有下面几个部分：</p>
<ol>
<li>推断本身，根据结果（观测）得到原因（隐变量）。</li>
<li>为参数的学习提供帮助。</li>
</ol>
<p>但是推断本身是一个困难的额任务，计算复杂度往往很高，对于无向图，由于节点之间的联系过多，那么因子分解很难进行，并且相互之间都有耦合，于是很难求解，仅仅在某些情况如 RBM 中可解，在有向图中，常常由于条件独立性问题，如两个节点之间条件相关（explain away），于是求解这些节点的条件概率就很困难，仅仅在某些概率假设情况下可解如高斯模型，于是需要近似推断。</p>
<p>事实上，我们常常讲推断问题变为优化问题，即：</p>
<script type="math/tex; mode=display">
Log-likehood:\sum\limits_{v\in V}\log p(v)</script><p>对上面这个问题，由于：</p>
<script type="math/tex; mode=display">
\log p(v)=\log\frac{p(v,h)}{p(h|v)}=\log\frac{p(v,h)}{q(h|v)}+\log\frac{q(h|v)}{p(h|v)}</script><p>左右两边对 $h$ 积分：</p>
<script type="math/tex; mode=display">
\int_h\log p(v)\cdot q(h|v)dh=\log p(v)</script><p>右边积分有：</p>
<script type="math/tex; mode=display">
\mathbb{E}_{q(h|v)}[\log\frac{p(v,h)}{q(h|v)}]+KL(q(h|v)||p(h|v))=\mathbb{E}_{q(h|v)}[\log p(v,h)]+H(q)+KL(q||p)</script><p>其中前两项是 ELBO，于是这就变成一个优化 ELBO 的问题。</p>
<div class="note info"><p>文章转载自<span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL3NodWh1YWkwMDc=">Jie Zhou<i class="fa fa-external-link-alt"></i></span>的<span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL3NodWh1YWkwMDcvTWFjaGluZS1MZWFybmluZy1TZXNzaW9u">Machine-Learning-Session<i class="fa fa-external-link-alt"></i></span>。</p>
</div>
      
    </div>

    
    
    
      


    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://soundmemories.github.io/2020/10/15/Machine%20Learning/37.NN/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="https://i.loli.net/2020/11/04/6JhNuwtBe4adylS.png">
      <meta itemprop="name" content="SoundMemories">
      <meta itemprop="description" content="今日事，今日毕">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="SoundMemories">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/10/15/Machine%20Learning/37.NN/" class="post-title-link" itemprop="url">NN</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2020-10-15 00:00:00" itemprop="dateCreated datePublished" datetime="2020-10-15T00:00:00+08:00">2020-10-15</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Machine-Learning/" itemprop="url" rel="index"><span itemprop="name">Machine Learning</span></a>
        </span>
    </span>

  
      </div>
      <div class="post-meta">
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>915</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>1 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="前馈神经网络"><a href="#前馈神经网络" class="headerlink" title="前馈神经网络"></a>前馈神经网络</h1><p>机器学习我们已经知道可以分为两大流派：</p>
<ol>
<li><p>频率派，这个流派的方法叫做统计学习，根据具体问题有下面的算法：</p>
<ol>
<li><p>正则化，L1，L2 等</p>
</li>
<li><p>核化，如核支撑向量机</p>
</li>
<li><p>集成化，AdaBoost，RandomForest</p>
</li>
<li><p>层次化，神经网络，神经网络有各种不同的模型，有代表性的有：</p>
<ol>
<li>多层感知机</li>
<li>Autoencoder</li>
<li>CNN</li>
<li>RNN</li>
</ol>
<p>这几种模型又叫做深度神经网络。</p>
</li>
</ol>
</li>
<li><p>贝叶斯派，这个流派的方法叫概率图模型，根据图特点分为：</p>
<ol>
<li>有向图-贝叶斯网络，加入层次化后有深度有向网络，包括<ol>
<li>Sigmoid Belief Network</li>
<li>Variational Autoencoder</li>
<li>GAN</li>
</ol>
</li>
<li>无向图-马尔可夫网络，加入层次化后有深度玻尔兹曼机。</li>
<li>混合，加入层次化后有深度信念网络</li>
</ol>
<p>这几个加入层次化后的模型叫做深度生成网络。</p>
</li>
</ol>
<p>从广义来说，深度学习包括深度生成网络和深度神经网络。</p>
<h2 id="From-PLA-to-DL"><a href="#From-PLA-to-DL" class="headerlink" title="From PLA to DL"></a>From PLA to DL</h2><ul>
<li>1958，PLA</li>
<li>1969，PLA 不能解决 XOR 等非线性数据</li>
<li>1981，MLP，多层感知机的出现解决了上面的问题</li>
<li>1986，BP 算法应用在 MLP 上，RNN</li>
<li>1989，CNN，Univeral Approximation Theorem，但是于此同时，由于深度和宽度的相对效率不知道，并且无法解决 BP 算法的梯度消失问题</li>
<li>1993，1995，SVM + kernel，AdaBoost，RandomForest，这些算法的发展，DL 逐渐没落</li>
<li>1997，LSTM</li>
<li>2006，基于 RBM 的 深度信念网络和深度自编码</li>
<li>2009，GPU的发展</li>
<li>2011，在语音方面的应用</li>
<li>2012，ImageNet</li>
<li>2013，VAE</li>
<li>2014，GAN</li>
<li>2016，AlphaGo</li>
<li>2018，GNN</li>
</ul>
<p>DL 不是一个新的东西，其近年来的大发展主要原因如下：</p>
<ol>
<li>数据量变大</li>
<li>分布式计算的发展</li>
<li>硬件算力的发展</li>
</ol>
<h2 id="非线性问题"><a href="#非线性问题" class="headerlink" title="非线性问题"></a>非线性问题</h2><p>对于非线性的问题，有三种方法：</p>
<ol>
<li>非线性转换，将低维空间转换到高维空间（Cover 定理），从而变为一个线性问题。</li>
<li>核方法，由于非线性转换是变换为高维空间，因此可能导致维度灾难，并且可能很难得到这个变换函数，核方法不直接寻找这个转换，而是寻找一个内积。</li>
<li>神经网络方法，将复合运算变为基本的线性运算的组合。</li>
</ol>
<div class="note info"><p>文章转载自<span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL3NodWh1YWkwMDc=">Jie Zhou<i class="fa fa-external-link-alt"></i></span>的<span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL3NodWh1YWkwMDcvTWFjaGluZS1MZWFybmluZy1TZXNzaW9u">Machine-Learning-Session<i class="fa fa-external-link-alt"></i></span>。</p>
</div>
      
    </div>

    
    
    
      


    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://soundmemories.github.io/2020/10/15/Machine%20Learning/38.PartitionFunction/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="https://i.loli.net/2020/11/04/6JhNuwtBe4adylS.png">
      <meta itemprop="name" content="SoundMemories">
      <meta itemprop="description" content="今日事，今日毕">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="SoundMemories">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/10/15/Machine%20Learning/38.PartitionFunction/" class="post-title-link" itemprop="url">PartitionFunction</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2020-10-15 00:00:00" itemprop="dateCreated datePublished" datetime="2020-10-15T00:00:00+08:00">2020-10-15</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Machine-Learning/" itemprop="url" rel="index"><span itemprop="name">Machine Learning</span></a>
        </span>
    </span>

  
      </div>
      <div class="post-meta">
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>4.1k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>4 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="配分函数"><a href="#配分函数" class="headerlink" title="配分函数"></a>配分函数</h1><p>在学习和推断中，对于一个概率的归一化因子很难处理，这个归一化因子和配分函数相关。假设一个概率分布：</p>
<script type="math/tex; mode=display">
p(x|\theta)=\frac{1}{Z(\theta)}\hat{p}(x|\theta),Z(\theta)=\int\hat{p}(x|\theta)dx</script><h2 id="包含配分函数的-MLE"><a href="#包含配分函数的-MLE" class="headerlink" title="包含配分函数的 MLE"></a>包含配分函数的 MLE</h2><p>在学习任务中，采用最大似然：</p>
<script type="math/tex; mode=display">
\begin{align}
\hat{\theta}&=\mathop{argmax}_{\theta}p(x|\theta)=\mathop{argmax}_\theta\sum\limits_{i=1}^N\log p(x_i|\theta)\nonumber\\
&=\mathop{argmax}_\theta\sum\limits_{i=1}^N\log \hat{p}(x|\theta)-N\log Z(\theta)\nonumber\\
&=\mathop{argmax}_{\theta}\frac{1}{N}\sum\limits_{i=1}^N\log \hat{p}(x|\theta)-\log Z(\theta)=\mathop{argmax}_\theta l(\theta)
\end{align}</script><p>求导：</p>
<script type="math/tex; mode=display">
\begin{align}\nabla_\theta\log Z(\theta)&=\frac{1}{Z(\theta)}\nabla_\theta Z(\theta)\nonumber\\
&=\frac{p(x|\theta)}{\hat{p}(x|\theta)}\int\nabla_\theta \hat{p}(x|\theta)dx\nonumber\\
&=\int\frac{p(x|\theta)}{\hat{p}(x|\theta)}\nabla_\theta\hat{p}(x|\theta)dx\nonumber\\
&=\mathbb{E}_{p(x|\theta)}[\nabla_\theta\log\hat{p}(x|\theta)]
\end{align}</script><p>由于这个表达式和未知的概率相关，于是无法直接精确求解，需要近似采样，如果没有这一项，那么可以采用梯度下降，但是存在配分函数就无法直接采用梯度下降了。</p>
<p>上面这个期望值，是对模型假设的概率分布，定义真实概率分布为 $p_{data}$，于是，$l(\theta)$ 中的第一项的梯度可以看成是从这个概率分布中采样出来的 $N$ 个点求和平均，可以近似期望值。</p>
<script type="math/tex; mode=display">
\nabla_\theta l(\theta)=\mathbb{E}_{p_{data}}[\nabla_\theta\log\hat{p}(x|\theta)]-\mathbb{E}_{p(x|\theta)}[\nabla_\theta\log\hat{p}(x|\theta)]</script><p>于是，相当于真实分布和模型假设越接近越好。上面这个式子第一项叫做正相，第二项叫做负相。为了得到负相的值，需要采用各种采样方法，如 MCMC。</p>
<p>采样得到 $\hat{x}_{1-m}\sim p_{model}(x|\theta^t)$，那么：</p>
<script type="math/tex; mode=display">
\theta^{t+1}=\theta^t+\eta(\sum\limits_{i=1}^m\nabla_\theta \log \hat{p}(x_i|\theta^t)-\sum\limits_{i=1}^m\nabla_\theta\log \hat{p}(\hat{x_i}|\theta^t))</script><p>这个算法也叫做基于 MCMC 采样的梯度上升。每次通过采样得到的样本叫做幻想粒子，如果这些幻想粒子区域的概率高于实际分布，那么最大化参数的结果就是降低这些部分的概率。</p>
<h2 id="对比散度-CD-Learning"><a href="#对比散度-CD-Learning" class="headerlink" title="对比散度-CD Learning"></a>对比散度-CD Learning</h2><p>上面对于负相的采样，最大的问题是，采样到达平稳分布的步骤数量是未知的。对比散度的方法，是对上述的采样是的初始值作出限制，直接采样 $\hat{x}_i=x_i$，这样可以缩短采样的混合时间。这个算法叫做 CD-k 算法，$k$ 就是初始化后进行的演化时间，很多时候，即使 $k=1$ 也是可以的。</p>
<p>我们看 MLE 的表达式：</p>
<script type="math/tex; mode=display">
\begin{align}\hat{\theta}&=\mathop{argmax}_{\theta}p(x|\theta)=\mathop{argmax}_{\theta}\frac{1}{N}\sum\limits_{i=1}^N\log p(x_i|\theta)=\mathbb{E}_{p_{data}}[\log p_{model}(x|\theta)]\nonumber\\
&=\mathop{argmax}_\theta\int p_{data}\log p_{model}dx\nonumber\\
&=\mathop{argmax}_\theta\int p_{data}\log \frac{p_{model}}{p_{data}}dx\nonumber\\
&=\mathop{argmin}_\theta KL(p_{data}||p_{model})
\end{align}</script><p>对于 CD-k 的采样过程，可以将初始值这些点表示为：</p>
<script type="math/tex; mode=display">
p^0=p_{data}</script><p>而我们的模型需要采样过程达到平稳分布：</p>
<script type="math/tex; mode=display">
p^\infty=p_{model}</script><p>因此，我们需要的是 $KL(p^0||p^\infty)$。定义 CD：</p>
<script type="math/tex; mode=display">
KL(p^0||p^\infty)-KL(p^k||p^\infty)</script><p>这就是 CD-k 算法第 $k$ 次采样的目标函数。</p>
<h2 id="RBM-的学习问题"><a href="#RBM-的学习问题" class="headerlink" title="RBM 的学习问题"></a>RBM 的学习问题</h2><p>RBM 的参数为：</p>
<script type="math/tex; mode=display">
\begin{align}
h=(h_1,\cdots,h_m)^T\\
v=(v_1,\cdots,v_n)^T\\
w=(w_{ij})_{mn}\\
\alpha=(\alpha_1,\cdots,\alpha_n)^T\\
\beta=(\beta_1,\cdots,\beta_m)^T
\end{align}</script><p>学习问题关注的概率分布为：</p>
<script type="math/tex; mode=display">
\begin{align}
\log p(v)&=\log\sum\limits_{h}p(h,v)\nonumber\\
&=\log\sum\limits_h\frac{1}{Z}\exp(-E(v,h))\nonumber\\
&=\log\sum\limits_{h}\exp(-E(v,h))-\log\sum\limits_{v,h}\exp(-E(h,v))
\end{align}</script><p>对上面这个式子求导第一项：</p>
<script type="math/tex; mode=display">
\frac{\partial \log\sum\limits_{h}\exp(-E(v,h))}{\partial\theta}=-\frac{\sum\limits_h\exp(-E(v,h))\frac{\partial E(v,h)}{\partial\theta}}{\sum\limits_{h}\exp(-E(v,h))}\\
=-\sum\limits_h\frac{\exp(-E(v,h))\frac{\partial E(v,h)}{\partial\theta}}{\sum\limits_{h}\exp(-E(v,h))}=-\sum\limits_hp(h|v)\frac{\partial E(v,h)}{\partial\theta}</script><p>第二项：</p>
<script type="math/tex; mode=display">
\frac{\partial \log\sum\limits_{v,h}\exp(-E(h,v))}{\partial\theta}=-\sum\limits_{h,v}\frac{\exp(-E(v,h))\frac{\partial E(v,h)}{\partial\theta}}{\sum\limits_{h,v}\exp(-E(v,h))}=-\sum\limits_{v,h}p(v,h)\frac{\partial E(v,h)}{\partial\theta}</script><p>所以有：</p>
<script type="math/tex; mode=display">
\frac{\partial}{\partial\theta}\log p(v)=-\sum\limits_hp(h|v)\frac{\partial E(v,h)}{\partial\theta}+\sum\limits_{v,h}p(v,h)\frac{\partial E(v,h)}{\partial\theta}</script><p>将 RBM 的模型假设代入：</p>
<script type="math/tex; mode=display">
E(v,h)=-(h^Twv+\alpha^Tv+\beta^Th)</script><ol>
<li><p>$w_{ij}$：</p>
<script type="math/tex; mode=display">
\frac{\partial}{\partial w_{ij}}E(v,h)=-h_iv_j</script><p>于是：</p>
<script type="math/tex; mode=display">
\frac{\partial}{\partial\theta}\log p(v)=\sum\limits_{h}p(h|v)h_iv_j-\sum\limits_{h,v}p(h,v)h_iv_j</script><p>第一项：</p>
<script type="math/tex; mode=display">
\sum\limits_{h_1,h_2,\cdots,h_m}p(h_1,h_2,\cdots,h_m|v)h_iv_j=\sum\limits_{h_i}p(h_i|v)h_iv_j=p(h_i=1|v)v_j</script><p>这里假设了 $h_i$ 是二元变量。</p>
<p>第二项：</p>
<script type="math/tex; mode=display">
\sum\limits_{h,v}p(h,v)h_iv_j=\sum\limits_{h,v}p(v)p(h|v)h_iv_j=\sum\limits_vp(v)p(h_i=1|v)v_j</script><p>这个求和是指数阶的，于是需要采样解决，我么使用 CD-k 方法。</p>
<p>对于第一项，可以直接使用训练样本得到，第二项采用 CD-k 采样方法，首先使用样本 $v^0=v$，然后采样得到 $h^0$，然后采样得到 $v^1$，这样顺次进行，最终得到 $v^k$，对于每个样本都得到一个 $v^k$，最终采样得到 $N$ 个 $v^k $，于是第二项就是：</p>
<script type="math/tex; mode=display">
p(h_i=1|v^k)v_j^k</script><p>具体的算法为：</p>
<ol>
<li>对每一个样本中的 $v$，进行采样：<ol>
<li>使用这个样本初始化采样</li>
<li>进行 $k$ 次采样（0-k-1）：<ol>
<li>$h_i^l\sim p(h_i|v^l)$</li>
<li>$v_i^{l+1}\sim p(v_i|h^l)$</li>
</ol>
</li>
<li>将这些采样出来的结果累加进梯度中</li>
</ol>
</li>
<li>重复进行上述过程，最终的梯度除以 $N$</li>
</ol>
</li>
</ol>
<div class="note info"><p>文章转载自<span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL3NodWh1YWkwMDc=">Jie Zhou<i class="fa fa-external-link-alt"></i></span>的<span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL3NodWh1YWkwMDcvTWFjaGluZS1MZWFybmluZy1TZXNzaW9u">Machine-Learning-Session<i class="fa fa-external-link-alt"></i></span>。</p>
</div>
      
    </div>

    
    
    
      


    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://soundmemories.github.io/2020/10/14/Machine%20Learning/36.Spectral/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="https://i.loli.net/2020/11/04/6JhNuwtBe4adylS.png">
      <meta itemprop="name" content="SoundMemories">
      <meta itemprop="description" content="今日事，今日毕">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="SoundMemories">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/10/14/Machine%20Learning/36.Spectral/" class="post-title-link" itemprop="url">Spectral</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2020-10-14 00:00:00" itemprop="dateCreated datePublished" datetime="2020-10-14T00:00:00+08:00">2020-10-14</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Machine-Learning/" itemprop="url" rel="index"><span itemprop="name">Machine Learning</span></a>
        </span>
    </span>

  
      </div>
      <div class="post-meta">
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>2.2k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>2 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="谱聚类"><a href="#谱聚类" class="headerlink" title="谱聚类"></a>谱聚类</h1><p>聚类问题可以分为两种思路：</p>
<ol>
<li>Compactness，这类有 K-means，GMM 等，但是这类算法只能处理凸集，为了处理非凸的样本集，必须引入核技巧。</li>
<li>Connectivity，这类以谱聚类为代表。</li>
</ol>
<p>谱聚类是一种基于无向带权图的聚类方法。这个图用 $G=(V,E)$ 表示，其中 $V=\{1,2,\cdots,N\}$，$E=\{w_{ij}\}$，这里 $w_{ij}$ 就是边的权重，这里权重取为相似度，$W=(w_{ij})$ 是相似度矩阵，定义相似度（径向核）：</p>
<script type="math/tex; mode=display">
w_{ij}=k(x_i,x_j)=\exp(-\frac{||x_i-x_j||_2^2}{2\sigma^2}),(i,j)\in E\\
w_{ij}=0,(i,j)\notin E</script><p>下面定义图的分割，这种分割就相当于聚类的结果。定义 $w(A,B)$：</p>
<script type="math/tex; mode=display">
A\subset V,B\subset V,A\cap B=\emptyset,w(A,B)=\sum\limits_{i\in A,j\in B}w_{ij}</script><p>假设一共有 $K$ 个类别，对这个图的分割 $CUT(V)=CUT(A_1,A_2,\cdots,A_K)=\sum\limits_{k=1}^Kw(A_k,\overline{A_k})=\sum\limits_{k=1}^K[w(A_k,V)-w(A_k,A_k)]$</p>
<p>于是，我们的目标就是 $\min\limits_{A_k}CUT(V)$。</p>
<p>为了平衡每一类内部的权重不同，我们做归一化的操作，定义每一个集合的度，首先，对单个节点的度定义：</p>
<script type="math/tex; mode=display">
d_i=\sum\limits_{j=1}^Nw_{ij}</script><p>其次，每个集合：</p>
<script type="math/tex; mode=display">
\Delta_k=degree(A_k)=\sum\limits_{i\in A_k}d_i</script><p>于是：</p>
<script type="math/tex; mode=display">
N(CUT)=\sum\limits_{k=1}^K\frac{w(A_k,\overline{A_k})}{\sum\limits_{i\in A_k}d_i}</script><p>所以目标函数就是最小化这个式子。</p>
<p>谱聚类的模型就是：</p>
<script type="math/tex; mode=display">
\{\hat{A}_k\}_{k=1}^K=\mathop{argmin}_{A_k}N(CUT)</script><p>引入指示向量：</p>
<script type="math/tex; mode=display">
\begin{cases}
y_i\in \{0,1\}^K\\
\sum\limits_{j=1}^Ky_{ij}=1
\end{cases}</script><p>其中，$y_{ij}$ 表示第 $i$ 个样本属于 $j$ 个类别，记：$Y=(y_1,y_2,\cdots,y_N)^T$。所以：</p>
<script type="math/tex; mode=display">
\hat{Y}=\mathop{argmin}_YN(CUT)</script><p>将 $N(CUT)$ 写成对角矩阵的形式，于是：</p>
<script type="math/tex; mode=display">
\begin{align}N(CUT)&=Trace[diag(\frac{w(A_1,\overline{A_1})}{\sum\limits_{i\in A_1}d_i},\frac{w(A_2,\overline{A_2})}{\sum\limits_{i\in A_2}d_i},\cdots,\frac{w(A_K,\overline{A_K})}{\sum\limits_{i\in A_K}d_i})]\nonumber\\
&=Trace[diag(w(A_1,\overline{A_1}),w(A_2,\overline{A_2}),\cdots,w(A_K,\overline{A_K}))\cdot diag(\sum\limits_{i\in A_1}d_i,\cdots,\sum\limits_{i\in A_K}d_i)^{-1}]\nonumber\\
&=Trace[O\cdot P^{-1}]
\end{align}</script><p>我们已经知道 $Y,w$ 这两个矩阵，我们希望求得 $O,P$。</p>
<p>由于：</p>
<script type="math/tex; mode=display">
Y^TY=\sum\limits_{i=1}^Ny_iy_i^T</script><p>对于 $y_iy_i^T$，只在对角线上的 $k\times k$ 处为 1，所以：</p>
<script type="math/tex; mode=display">
Y^TY=diag(N_1,N_2,\cdots,N_K)</script><p>其中，$N_i$ 表示有 $N_i$ 个样本属于 $i$，即 $N_k=\sum\limits_{k\in A_k}1$。</p>
<p>引入对角矩阵，根据 $d_i$ 的定义， $D=diag(d_1,d_2,\cdots,d_N)=diag(w_{NN}\mathbb{I}_{N1})$，于是：</p>
<script type="math/tex; mode=display">
P=Y^TDY</script><p>对另一项 $O=diag(w(A_1,\overline{A_1}),w(A_2,\overline{A_2}),\cdots,w(A_K,\overline{A_K})$：</p>
<script type="math/tex; mode=display">
O=diag(w(A_i,V))-diag(w(A_i,A_i))=diag(\sum\limits_{j\in A_i}d_j)-diag(w(A_i,A_i))</script><p>其中，第一项已知，第二项可以写成 $Y^TwY$，这是由于：</p>
<script type="math/tex; mode=display">
Y^TwY=\sum\limits_{i=1}^N\sum\limits_{j=1}^Ny_iy_j^Tw_{ij}</script><p>于是这个矩阵的第 $lm$ 项可以写为：</p>
<script type="math/tex; mode=display">
\sum\limits_{i\in A_l,j\in A_m}w_{ij}</script><p>这个矩阵的对角线上的项和 $w(A_i,A_i)$ 相同，所以取迹后的取值不会变化。</p>
<p>所以：</p>
<script type="math/tex; mode=display">
N(CUT)=Trace[(Y^T(D-w))Y)\cdot(Y^TDY)^{-1}]</script><p>其中，$ L=D-w$ 叫做拉普拉斯矩阵。</p>
<div class="note info"><p>文章转载自<span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL3NodWh1YWkwMDc=">Jie Zhou<i class="fa fa-external-link-alt"></i></span>的<span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL3NodWh1YWkwMDcvTWFjaGluZS1MZWFybmluZy1TZXNzaW9u">Machine-Learning-Session<i class="fa fa-external-link-alt"></i></span>。</p>
</div>
      
    </div>

    
    
    
      


    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://soundmemories.github.io/2020/10/13/Machine%20Learning/35.RBM/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="https://i.loli.net/2020/11/04/6JhNuwtBe4adylS.png">
      <meta itemprop="name" content="SoundMemories">
      <meta itemprop="description" content="今日事，今日毕">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="SoundMemories">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/10/13/Machine%20Learning/35.RBM/" class="post-title-link" itemprop="url">RBM</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2020-10-13 00:00:00" itemprop="dateCreated datePublished" datetime="2020-10-13T00:00:00+08:00">2020-10-13</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Machine-Learning/" itemprop="url" rel="index"><span itemprop="name">Machine Learning</span></a>
        </span>
    </span>

  
      </div>
      <div class="post-meta">
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>2.4k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>2 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="受限玻尔兹曼机"><a href="#受限玻尔兹曼机" class="headerlink" title="受限玻尔兹曼机"></a>受限玻尔兹曼机</h1><p>玻尔兹曼机是一种存在隐节点的无向图模型。在图模型中最简单的是朴素贝叶斯模型（朴素贝叶斯假设），引入单个隐变量后，发展出了 GMM，如果单个隐变量变成序列的隐变量，就得到了状态空间模型（引入齐次马尔可夫假设和观测独立假设就有HMM，Kalman Filter，Particle Filter），为了引入观测变量之间的关联，引入了一种最大熵模型-MEMM，为了克服 MEMM 中的局域问题，又引入了 CRF，CRF 是一个无向图，其中，破坏了齐次马尔可夫假设，如果隐变量是一个链式结构，那么又叫线性链 CRF。</p>
<p>在无向图的基础上，引入隐变量得到了玻尔兹曼机，这个图模型的概率密度函数是一个指数族分布。对隐变量和观测变量作出一定的限制，就得到了受限玻尔兹曼机（RBM）。</p>
<p>我们看到，不同的概率图模型对下面几个特点作出假设：</p>
<ol>
<li>方向-边的性质</li>
<li>离散/连续/混合-点的性质</li>
<li>条件独立性-边的性质</li>
<li>隐变量-节点的性质</li>
<li>指数族-结构特点</li>
</ol>
<p>将观测变量和隐变量分别记为 $v,h,h=\{h_1,\cdots,h_m\},v=\{v_1,\cdots,v_n\}$。我们知道，无向图根据最大团的分解，可以写为玻尔兹曼分布的形式 $p(x)=\frac{1}{Z}\prod\limits_{i=1}^K\psi_i(x_{ci})=\frac{1}{Z}\exp(-\sum\limits_{i=1}^KE(x_{ci}))$，这也是一个指数族分布。</p>
<p>一个玻尔兹曼机存在一系列的问题，在其推断任务中，想要精确推断，是无法进行的，想要近似推断，计算量过大。为了解决这个问题，一种简化的玻尔兹曼机-受限玻尔兹曼机作出了假设，所有隐变量内部以及观测变量内部没有连接，只在隐变量和观测变量之间有连接，这样一来：</p>
<script type="math/tex; mode=display">
p(x)=p(h,v)=\frac{1}{Z}\exp(-E(v,h))</script><p>其中能量函数 $E(v,h)$ 可以写出三个部分，包括与节点集合相关的两项以及与边 $w$ 相关的一项，记为：</p>
<script type="math/tex; mode=display">
E(v,h)=-(h^Twv+\alpha^T v+\beta^T h)</script><p>所以：</p>
<script type="math/tex; mode=display">
p(x)=\frac{1}{Z}\exp(h^Twv)\exp(\alpha^T v)\exp(\beta^T h)=\frac{1}{Z}\prod_{i=1}^m\prod_{j=1}^n\exp(h_iw_{ij}v_j)\prod_{j=1}^n\exp(\alpha_jv_j)\prod_{i=1}^m\exp(\beta_ih_i)</script><p>上面这个式子也和 RBM 的因子图一一对应。</p>
<h2 id="推断"><a href="#推断" class="headerlink" title="推断"></a>推断</h2><p>推断任务包括求后验概率 $ p(v|h),p(h|v)$ 以及求边缘概率 $p(v)$。</p>
<h3 id="p-h-v"><a href="#p-h-v" class="headerlink" title="$p(h|v)$"></a>$p(h|v)$</h3><p>对于一个无向图，满足局域的 Markov 性质，即 $p(h_1|h-\{h_1\},v)=p(h_1|Neighbour(h_1))=p(h_1|v)$。我们可以得到：</p>
<script type="math/tex; mode=display">
p(h|v)=\prod_{i=1}^mp(h_i|v)</script><p>考虑 Binary RBM，所有的隐变量只有两个取值 $0,1$：</p>
<script type="math/tex; mode=display">
p(h_l=1|v)=\frac{p(h_l=1,h_{-l},v)}{p(h_{-l},v)}=\frac{p(h_l=1,h_{-l},v)}{p(h_l=1,h_{-l},v)+p(h_l=0,h_{-l},v)}</script><p>将能量函数写成和 $l$ 相关或不相关的两项：</p>
<script type="math/tex; mode=display">
E(v,h)=-(\sum\limits_{i=1,i\ne l}^m\sum\limits_{j=1}^nh_iw_{ij}v_j+h_l\sum\limits_{j=1}^nw_{lj}v_j+\sum\limits_{j=1}^n\alpha_j v_j+\sum\limits_{i=1,i\ne l}^m\beta_ih_i+\beta_lh_l)</script><p>定义：$h_lH_l(v)=h_l\sum\limits_{j=1}^nw_{lj}v_j+\beta_lh_l,\overline{H}(h_{-l},v)=\sum\limits_{i=1,i\ne l}^m\sum\limits_{j=1}^nh_iw_{ij}v_j+\sum\limits_{j=1}^n\alpha_j v_j+\sum\limits_{i=1,i\ne l}^m\beta_ih_i$。</p>
<p>代入，有：</p>
<script type="math/tex; mode=display">
p(h_l=1|v)=\frac{\exp(H_l(v)+\overline{H}(h_{-l},v))}{\exp(H_l(v)+\overline{H}(h_{-l},v))+\exp(\overline{H}(h_{-l},v))}=\frac{1}{1+\exp(-H_l(v))}=\sigma(H_l(v))</script><p>于是就得到了后验概率。对于 $v$ 的后验是对称的，所以类似的可以求解。</p>
<h3 id="p-v"><a href="#p-v" class="headerlink" title="$p(v)$"></a>$p(v)$</h3><script type="math/tex; mode=display">
\begin{align}p(v)&=\sum\limits_hp(h,v)=\sum\limits_h\frac{1}{Z}\exp(h^Twv+\alpha^Tv+\beta^Th)\nonumber\\
&=\exp(\alpha^Tv)\frac{1}{Z}\sum\limits_{h_1}\exp(h_1w_1v+\beta_1h_1)\cdots\sum\limits_{h_m}\exp(h_mw_mv+\beta_mh_m)\nonumber\\
&=\exp(\alpha^Tv)\frac{1}{Z}(1+\exp(w_1v+\beta_1))\cdots(1+\exp(w_mv+\beta_m))\nonumber\\
&=\frac{1}{Z}\exp(\alpha^Tv+\sum\limits_{i=1}^m\log(1+\exp(w_iv+\beta_i)))
\end{align}</script><p>其中，$\log(1+\exp(x))$ 叫做 Softplus 函数。</p>
<div class="note info"><p>文章转载自<span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL3NodWh1YWkwMDc=">Jie Zhou<i class="fa fa-external-link-alt"></i></span>的<span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL3NodWh1YWkwMDcvTWFjaGluZS1MZWFybmluZy1TZXNzaW9u">Machine-Learning-Session<i class="fa fa-external-link-alt"></i></span>。</p>
</div>
      
    </div>

    
    
    
      


    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://soundmemories.github.io/2020/10/12/Machine%20Learning/34.GaussianProcess/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="https://i.loli.net/2020/11/04/6JhNuwtBe4adylS.png">
      <meta itemprop="name" content="SoundMemories">
      <meta itemprop="description" content="今日事，今日毕">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="SoundMemories">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/10/12/Machine%20Learning/34.GaussianProcess/" class="post-title-link" itemprop="url">GaussianProcess</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2020-10-12 00:00:00" itemprop="dateCreated datePublished" datetime="2020-10-12T00:00:00+08:00">2020-10-12</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Machine-Learning/" itemprop="url" rel="index"><span itemprop="name">Machine Learning</span></a>
        </span>
    </span>

  
      </div>
      <div class="post-meta">
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>3.6k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>3 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="高斯过程回归"><a href="#高斯过程回归" class="headerlink" title="高斯过程回归"></a>高斯过程回归</h1><p>将一维高斯分布推广到多变量中就得到了高斯网络，将多变量推广到无限维，就得到了高斯过程，高斯过程是定义在连续域（时间空间）上的无限多个高维随机变量所组成的随机过程。</p>
<p>在时间轴上的任意一个点都满足高斯分布吗，将这些点的集合叫做高斯过程的一个样本。</p>
<blockquote>
<p>  对于时间轴上的序列 $\xi_t$，如果 $\forall n\in N^+，t_i\in T$，有 $\xi_{t_1-t_n}\sim \mathcal{N}(\mu_{t_1-t_n},\Sigma_{t_1-t_n})$，  那么 $\{\xi_t\}_{t\in T}$ 是一个高斯过程。</p>
<p>  高斯过程有两个参数（高斯过程存在性定理），均值函数 $m(t)=\mathbb{E}[\xi_t]$ 和协方差函数 $k(s,t)=\mathbb{E}[(\xi_s-\mathbb{E}[\xi_s])(\xi_t-\mathbb{E}[\xi_t])]$。</p>
</blockquote>
<p>我们将贝叶斯线性回归添加核技巧的这个模型叫做高斯过程回归，高斯过程回归分为两种视角：</p>
<ol>
<li>权空间的视角-核贝叶斯线性回归，相当于 $x$ 为 $t$，在每个时刻的高斯分布来源于权重，根据上面的推导，预测的函数依然是高斯分布。</li>
<li>函数空间的视角-高斯分布通过函数 $f(x)$ 来体现。</li>
</ol>
<h2 id="核贝叶斯线性回归"><a href="#核贝叶斯线性回归" class="headerlink" title="核贝叶斯线性回归"></a>核贝叶斯线性回归</h2><p>贝叶斯线性回归可以通过加入核函数的方法来解决非线性函数的问题，将 $f(x)=x^Tw$ 这个函数变为 $f(x)=\phi(x)^Tw$（当然这个时候，$ \Sigma_p$ 也要变为更高维度的），变换到更高维的空间，有：</p>
<script type="math/tex; mode=display">
\begin{align}f(x^*)\sim \mathcal{N}(\phi(x^*)^{T}\sigma^{-2}A^{-1}\Phi^TY,\phi(x^*)^{T}A^{-1}\phi(x^*))\\
A=\sigma^{-2}\Phi^T\Phi+\Sigma_p^{-1}
\end{align}</script><p>其中，$\Phi=(\phi(x_1),\phi(x_2),\cdots,\phi(x_N))^T$。</p>
<p>为了求解 $A^{-1}$，可以利用 Woodbury Formula，$A=\Sigma_p^{-1},C=\sigma^{-2}\mathbb{I}$：</p>
<script type="math/tex; mode=display">
(A+UCV)^{-1}=A^{-1}-A^{-1}U(C^{-1}+VA^{-1}U)^{-1}VA^{-1}</script><p>所以 $A^{-1}=\Sigma_p-\Sigma_p\Phi^T(\sigma^2\mathbb{I}+\Phi\Sigma_p\Phi^T)^{-1}\Phi\Sigma_p$</p>
<p>也可以用另一种方法：</p>
<script type="math/tex; mode=display">
\begin{align}
A&=\sigma^{-2}\Phi^T\Phi+\Sigma_p^{-1}\nonumber\\
\Leftrightarrow A\Sigma_p&=\sigma^{-2}\Phi^T\Phi\Sigma_p+\mathbb{I}\nonumber\\
\Leftrightarrow A\Sigma_p\Phi^T&=\sigma^{-2}\Phi^T\Phi\Sigma_p\Phi^T+\Phi^T=\sigma^{-2}\Phi^T(k+\sigma^2\mathbb{I})\nonumber\\
\Leftrightarrow \Sigma_p\Phi^T&=\sigma^{-2}A^{-1}\Phi^T(k+\sigma^2\mathbb{I})\nonumber\\
\Leftrightarrow \sigma^{-2}A^{-1}\Phi^T&=\Sigma_p\Phi^T(k+\sigma^2\mathbb{I})^{-1}\nonumber\\
\Leftrightarrow \phi(x^*)^T\sigma^{-2}A^{-1}\Phi^T&=\phi(x^*)^T\Sigma_p\Phi^T(k+\sigma^2\mathbb{I})^{-1}
\end{align}</script><p>上面的左边的式子就是变换后的均值，而右边的式子就是不含 $A^{-1}$ 的式子，其中 $k=\Phi\Sigma_p\Phi^T$。</p>
<p>根据 $A^{-1}$ 得到方差为：</p>
<script type="math/tex; mode=display">
\phi(x^*)^T\Sigma_p\phi(x^*)-\phi(x^*)^T\Sigma_p\Phi^T(\sigma^2\mathbb{I}+k)^{-1}\Phi\Sigma_p\phi(x^*)</script><p>上面定义了：</p>
<script type="math/tex; mode=display">
k=\Phi\Sigma_p\Phi^T</script><p>我们看到，在均值和方差中，含有下面四项：</p>
<script type="math/tex; mode=display">
\phi(x^*)^T\Sigma_p\Phi^T,\phi(x^*)^T\Sigma_p\phi(x^*),\phi(x^*)^T\Sigma_p\Phi^T,\Phi\Sigma_p\phi(x^*)</script><p>展开后，可以看到，有共同的项：$k(x,x’)=\phi(x)^T\Sigma_p\phi(x‘)$。由于 $\Sigma_p$ 是正定对称的方差矩阵，所以，这是一个核函数。</p>
<p>对于高斯过程中的协方差：</p>
<script type="math/tex; mode=display">
k(t,s)=Cov[f(x),f(x')]=\mathbb{E}[\phi(x)^Tww^T\phi(x')]=\phi(x)^T\mathbb{E}[ww^T]\phi(x')=\phi(x)^T\Sigma_p\phi(x')</script><p>我们可以看到，这个就对应着上面的核函数。因此我们看到 $\{f(x)\}$ 组成的组合就是一个高斯过程。</p>
<h2 id="函数空间的观点"><a href="#函数空间的观点" class="headerlink" title="函数空间的观点"></a>函数空间的观点</h2><p>相比权重空间，我们也可以直接关注 $f$ 这个空间，对于预测任务，这就是类似于求：</p>
<script type="math/tex; mode=display">
p(y^*|X,Y,x^*)=\int_fp(y^*|f,X,Y,x^*)p(f|X,Y,x^*)df</script><p>对于数据集来说，取 $f(X)\sim\mathcal{N}(\mu(X),k(X,X)),Y=f(X)+\varepsilon\sim\mathcal{N}(\mu(X),k(X,X)+\sigma^2\mathbb{I})$。预测任务的目的是给定一个新数据序列 $X^\ast=(x_1^\ast,\cdots,x_M^\ast)^T$，得到 $Y^\ast=f(X^\ast)+\varepsilon$。我们可以写出：</p>
<script type="math/tex; mode=display">
\begin{pmatrix}Y\\f(X^*)\end{pmatrix}\sim\mathcal{N}\left(\begin{pmatrix}\mu(X)\\\mu(X^*)\end{pmatrix},\begin{pmatrix}k(X,X)+\sigma^2\mathbb{I}&k(X,X^*)\\k(X^*,X)&k(X^*,X^*)\end{pmatrix}\right)</script><p>根据高斯分布的方法：</p>
<script type="math/tex; mode=display">
\begin{align}x=\begin{pmatrix}x_a\\x_b\end{pmatrix}\sim\mathcal{N}\left(\begin{pmatrix}\mu_a\\\mu_b\end{pmatrix},\begin{pmatrix}\Sigma_{aa}&\Sigma_{ab}\\\Sigma_{ba}&\Sigma_{bb}\end{pmatrix}\right)\\
x_b|x_a\sim\mathcal{N}(\mu_{b|a},\Sigma_{b|a})\\
\mu_{b|a}=\Sigma_{ba}\Sigma_{aa}^{-1}(x_a-\mu_a)+\mu_b\\
\Sigma_{b|a}=\Sigma_{bb}-\Sigma_{ba}\Sigma_{aa}^{-1}\Sigma_{ab}
\end{align}</script><p>可以直接写出：</p>
<script type="math/tex; mode=display">
\begin{align}
p(f(X^*)|X,Y,X^*)=p(f(X^*)|Y)\\
=\mathcal{N}(k(X^*,X)[k(X,X)+\sigma^2\mathbb{I}]^{-1}(Y-\mu(X))+\mu(X^*),\\
k(X^*,X^*)-k(X^*,X)[k(X,X)+\sigma^2\mathbb{I}]^{1}k(X,X^*))
\end{align}</script><p>所以对于 $Y=f(X^*)+\varepsilon$：</p>
<script type="math/tex; mode=display">
\begin{align}
\mathcal{N}(k(X^*,X)[k(X,X)+\sigma^2\mathbb{I}]^{-1}(Y-\mu(X))+\mu(X^*),\\
k(X^*,X^*)-k(X^*,X)[k(X,X)+\sigma^2\mathbb{I}]^{1}k(X,X^*)+\sigma^2\mathbb{I})
\end{align}</script><p>我们看到，函数空间的观点更加简单易于求解。</p>
<div class="note info"><p>文章转载自<span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL3NodWh1YWkwMDc=">Jie Zhou<i class="fa fa-external-link-alt"></i></span>的<span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL3NodWh1YWkwMDcvTWFjaGluZS1MZWFybmluZy1TZXNzaW9u">Machine-Learning-Session<i class="fa fa-external-link-alt"></i></span>。</p>
</div>
      
    </div>

    
    
    
      


    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://soundmemories.github.io/2020/10/11/Machine%20Learning/33.BayesianLR/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="https://i.loli.net/2020/11/04/6JhNuwtBe4adylS.png">
      <meta itemprop="name" content="SoundMemories">
      <meta itemprop="description" content="今日事，今日毕">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="SoundMemories">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/10/11/Machine%20Learning/33.BayesianLR/" class="post-title-link" itemprop="url">BayesianLR</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2020-10-11 00:00:00" itemprop="dateCreated datePublished" datetime="2020-10-11T00:00:00+08:00">2020-10-11</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Machine-Learning/" itemprop="url" rel="index"><span itemprop="name">Machine Learning</span></a>
        </span>
    </span>

  
      </div>
      <div class="post-meta">
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>1.6k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>1 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="贝叶斯线性回归"><a href="#贝叶斯线性回归" class="headerlink" title="贝叶斯线性回归"></a>贝叶斯线性回归</h1><p> 我们知道，线性回归当噪声为高斯分布的时候，最小二乘损失导出的结果相当于对概率模型应用 MLE，引入参数的先验时，先验分布是高斯分布，那么 MAP的结果相当于岭回归的正则化，如果先验是拉普拉斯分布，那么相当于 Lasso 的正则化。这两种方案都是点估计方法。我们希望利用贝叶斯方法来求解参数的后验分布。</p>
<p>线性回归的模型假设为：</p>
<script type="math/tex; mode=display">
\begin{align}f(x)=w^Tx
\\y=f(x)+\varepsilon\\
\varepsilon\sim\mathcal{N}(0,\sigma^2)
\end{align}</script><p>在贝叶斯方法中，需要解决推断和预测两个问题。</p>
<h2 id="推断"><a href="#推断" class="headerlink" title="推断"></a>推断</h2><p>引入高斯先验：</p>
<script type="math/tex; mode=display">
p(w)=\mathcal{N}(0,\Sigma_p)</script><p>对参数的后验分布进行推断：</p>
<script type="math/tex; mode=display">
p(w|X,Y)=\frac{p(w,Y|X)}{p(Y|X)}=\frac{p(Y|w,X)p(w|X)}{\int p(Y|w,X)p(w|X)dw}</script><p>分母和参数无关，由于 $p(w|X)=p(w)$，代入先验得到：</p>
<script type="math/tex; mode=display">
p(w|X,Y)\propto \prod\limits_{i=1}^N\mathcal{N}(y_i|w^Tx_i,\sigma^2)\cdot\mathcal{N}(0,\Sigma_p)</script><p>高斯分布取高斯先验的共轭分布依然是高斯分布，于是可以得到后验分布也是一个高斯分布。第一项：</p>
<script type="math/tex; mode=display">
\begin{align}\prod\limits_{i=1}^N\mathcal{N}(y_i|w^Tx_i,\sigma^2)&=\frac{1}{(2\pi)^{N/2}\sigma^N}\exp(-\frac{1}{2\sigma^2}\sum\limits_{i=1}^N(y_i-w^Tx_i)^2)\nonumber\\
&=\frac{1}{(2\pi)^{N/2}\sigma^N}\exp(-\frac{1}{2}(Y-Xw)^T(\sigma^{-2}\mathbb{I})(Y-Xw))
\nonumber\\&=\mathcal{N}(Xw,\sigma^2\mathbb{I})
\end{align}</script><p>代入上面的式子：</p>
<script type="math/tex; mode=display">
p(w|X,Y)\propto\exp(-\frac{1}{2\sigma^2}(Y-Xw)^T\sigma^{-2}\mathbb{I}(Y-Xw)-\frac{1}{2}w^T\Sigma_p^{-1}w)</script><p>假定最后得到的高斯分布为：$\mathcal{N}(\mu_w,\Sigma_w)$。对于上面的分布，采用配方的方式来得到最终的分布，指数上面的二次项为：</p>
<script type="math/tex; mode=display">
-\frac{1}{2\sigma^2}w^TX^TXw-\frac{1}{2}w^T\Sigma_p^{-1}w</script><p>于是：</p>
<script type="math/tex; mode=display">
\Sigma_w^{-1}=\sigma^{-2}X^TX+\Sigma_p^{-1}=A</script><p>一次项：</p>
<script type="math/tex; mode=display">
\frac{1}{2\sigma^2}2Y^TXw=\sigma^{-2}Y^TXw</script><p>于是：</p>
<script type="math/tex; mode=display">
\mu_w^T\Sigma_w^{-1}=\sigma^{-2}Y^TX\Rightarrow\mu_w=\sigma^{-2}A^{-1}X^TY</script><h2 id="预测"><a href="#预测" class="headerlink" title="预测"></a>预测</h2><p>给定一个 $x^\ast$，求解 $y^\ast$，所以 $f(x^\ast)=x^{\ast T}w$，代入参数后验，有 $x^{\ast T}w\sim \mathcal{N}(x^{\ast T}\mu_w,x^{\ast T}\Sigma_wx^\ast)$，添上噪声项：</p>
<script type="math/tex; mode=display">
p(y^*|X,Y,x^*)=\int_wp(y^*|w,X,Y,x^*)p(w|X,Y,x^*)dw=\int_wp(y^*|w,x^*)p(w|X,Y)dw\\
=\mathcal{N}(x^{*T}\mu_w,x^{*T}\Sigma_wx^*+\sigma^2)</script><div class="note info"><p>文章转载自<span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL3NodWh1YWkwMDc=">Jie Zhou<i class="fa fa-external-link-alt"></i></span>的<span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL3NodWh1YWkwMDcvTWFjaGluZS1MZWFybmluZy1TZXNzaW9u">Machine-Learning-Session<i class="fa fa-external-link-alt"></i></span>。</p>
</div>
      
    </div>

    
    
    
      


    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://soundmemories.github.io/2020/10/10/Machine%20Learning/32.GaussianNetwork/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="https://i.loli.net/2020/11/04/6JhNuwtBe4adylS.png">
      <meta itemprop="name" content="SoundMemories">
      <meta itemprop="description" content="今日事，今日毕">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="SoundMemories">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/10/10/Machine%20Learning/32.GaussianNetwork/" class="post-title-link" itemprop="url">GaussianNetwork</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2020-10-10 00:00:00" itemprop="dateCreated datePublished" datetime="2020-10-10T00:00:00+08:00">2020-10-10</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Machine-Learning/" itemprop="url" rel="index"><span itemprop="name">Machine Learning</span></a>
        </span>
    </span>

  
      </div>
      <div class="post-meta">
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>1.8k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>2 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="高斯网络"><a href="#高斯网络" class="headerlink" title="高斯网络"></a>高斯网络</h1><p>高斯图模型（高斯网络）是一种随机变量为连续的有向或者无向图。有向图版本的高斯图是高斯贝叶斯网络，无向版本的叫高斯马尔可夫网络。</p>
<p>高斯网络的每一个节点都是高斯分布：$\mathcal{N}(\mu_i,\Sigma_i)$，于是所有节点的联合分布就是一个高斯分布，均值为 $\mu$，方差为 $\Sigma$。</p>
<p>对于边缘概率，我们有下面三个结论：</p>
<ol>
<li><p>对于方差矩阵，可以得到独立性条件：$x_i\perp x_j\Leftrightarrow\sigma_{ij}=0$，这个叫做全局独立性。</p>
</li>
<li><p>我们看方差矩阵的逆（精度矩阵或信息矩阵）：$\Lambda=\Sigma^{-1}=(\lambda_{ij})_{pp}$，有定理：</p>
<blockquote>
<p>  $x_i\perp x_j|(X-\{x_i,x_j\})\Leftrightarrow\lambda_{ij}=0$</p>
</blockquote>
<p>因此，我们使用精度矩阵来表示条件独立性。</p>
</li>
<li><p>对于任意一个无向图中的节点 $x_i$，$x_i|(X-x_i)\sim \mathcal{N}(\sum\limits_{j\ne i}\frac{\lambda_{ij}}{\lambda_{ii}}x_j,\lambda_{ii}^{-1})$</p>
<p>也就是其他所有分量的线性组合，即所有与它有链接的分量的线性组合。</p>
</li>
</ol>
<h2 id="高斯贝叶斯网络-GBN"><a href="#高斯贝叶斯网络-GBN" class="headerlink" title="高斯贝叶斯网络 GBN"></a>高斯贝叶斯网络 GBN</h2><p>高斯贝叶斯网络可以看成是 LDS 的一个推广，LDS 的假设是相邻时刻的变量之间的依赖关系，因此是一个局域模型，而高斯贝叶斯网络，每一个节点的父亲节点不一定只有一个，因此可以看成是一个全局的模型。根据有向图的因子分解：</p>
<script type="math/tex; mode=display">
p(x)=\prod\limits_{i=1}^pp(x_i|x_{Parents(i)})</script><p>对里面每一项，假设每一个特征是一维的，可以写成线性组合：</p>
<script type="math/tex; mode=display">
p(x_i|x_{Parents(i)})=\mathcal{N}(x_i|\mu_i+W_i^Tx_{Parents(i)},\sigma^2_i)</script><p>将随机变量写成：</p>
<script type="math/tex; mode=display">
x_i=\mu_i+\sum\limits_{j\in x_{Parents(i)}}w_{ij}(x_j-\mu_j)+\sigma_i\varepsilon_i,\varepsilon_i\sim \mathcal{N}(0,1)</script><p>写成矩阵形式，并且对 $w$ 进行扩展：</p>
<script type="math/tex; mode=display">
x-\mu=W(x-\mu)+S\varepsilon</script><p>其中，$S=diag(\sigma_i)$。所以有：$x-\mu=(\mathbb{I}-W)^{-1}S\varepsilon$</p>
<p>由于：</p>
<script type="math/tex; mode=display">
Cov(x)=Cov(x-\mu)</script><p>可以得到协方差矩阵。</p>
<h2 id="高斯马尔可夫网络-GMN"><a href="#高斯马尔可夫网络-GMN" class="headerlink" title="高斯马尔可夫网络 GMN"></a>高斯马尔可夫网络 GMN</h2><p>对于无向图版本的高斯网络，可以写成：</p>
<script type="math/tex; mode=display">
p(x)=\frac{1}{Z}\prod\limits_{i=1}^p\phi_i(x_i)\prod\limits_{i,j\in X}\phi_{i,j}(x_i,x_j)</script><p>为了将高斯分布和这个式子结合，我们写出高斯分布和变量相关的部分：</p>
<script type="math/tex; mode=display">
\begin{align}p(x)&\propto \exp(-\frac{1}{2}(x-\mu)^T\Sigma^{-1}(x-\mu))\nonumber\\
&=\exp(-\frac{1}{2}(x^T\Lambda x-2\mu^T\Lambda x+\mu^T\Lambda\mu))\nonumber\\
&=\exp(-\frac{1}{2}x^T\Lambda x+(\Lambda\mu)^Tx)
\end{align}</script><p>可以看到，这个式子与无向图分解中的两个部分对应，我们记 $h=\Lambda\mu$为 Potential Vector。其中和 $x_i$ 相关的为：$x_i:-\frac{1}{2}\lambda_{ii}x_i^2+h_ix_i$，与 $x_i,x_j$ 相关的是：$x_i,x_j:-\lambda_{ij}x_ix_j$，这里利用了精度矩阵为对称矩阵的性质。我们看到，这里也可以看出，$x_i,x_j$ 构成的一个势函数，只和 $\lambda_{ij}$ 有关，于是 $x_i\perp x_j|(X-\{x_i,x_j\})\Leftrightarrow\lambda_{ij}=0 $。</p>
<div class="note info"><p>文章转载自<span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL3NodWh1YWkwMDc=">Jie Zhou<i class="fa fa-external-link-alt"></i></span>的<span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL3NodWh1YWkwMDcvTWFjaGluZS1MZWFybmluZy1TZXNzaW9u">Machine-Learning-Session<i class="fa fa-external-link-alt"></i></span>。</p>
</div>
      
    </div>

    
    
    
      


    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>





  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/2/"><i class="fa fa-angle-left" aria-label="上一页"></i></a><a class="page-number" href="/">1</a><a class="page-number" href="/page/2/">2</a><span class="page-number current">3</span><a class="page-number" href="/page/4/">4</a><span class="space">&hellip;</span><a class="page-number" href="/page/12/">12</a><a class="extend next" rel="next" href="/page/4/"><i class="fa fa-angle-right" aria-label="下一页"></i></a>
  </nav>



      

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      const activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      const commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

    </div>
  </main>

  <footer class="footer">
    <div class="footer-inner">
      

      

<div class="copyright">
  
  &copy; 2019 – 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">SoundMemories</span>
</div>
  <div class="powered-by">由 <span class="exturl theme-link" data-url="aHR0cHM6Ly9oZXhvLmlv">Hexo</span> & <span class="exturl theme-link" data-url="aHR0cHM6Ly90aGVtZS1uZXh0LmpzLm9yZy9tdXNlLw==">NexT.Muse</span> 强力驱动
  </div>

    </div>
  </footer>

  
  <script src="//cdn.jsdelivr.net/npm/animejs@3.2.1/lib/anime.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/@next-theme/pjax@0.4.0/pjax.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/jquery@3.5.1/dist/jquery.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js"></script>
<script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/muse.js"></script><script src="/js/next-boot.js"></script><script src="/js/bookmark.js"></script>
  <script>
var pjax = new Pjax({
  selectors: [
    'head title',
    '.page-configurations',
    '.main-inner',
    '.post-toc-wrap',
    '.languages',
    '.pjax'
  ],
  analytics: false,
  cacheBust: false,
  scrollRestoration: false,
  scrollTo: !CONFIG.bookmark.enable
});

document.addEventListener('pjax:success', () => {
  pjax.executeScripts(document.querySelectorAll('script[data-pjax], .pjax script'));
  NexT.boot.refresh();
  // Define Motion Sequence & Bootstrap Motion.
  if (CONFIG.motion.enable) {
    NexT.motion.integrator
      .init()
      .add(NexT.motion.middleWares.subMenu)
      .add(NexT.motion.middleWares.postList)
      .bootstrap();
  }
  const hasTOC = document.querySelector('.post-toc');
  document.querySelector('.sidebar-inner').classList.toggle('sidebar-nav-active', hasTOC);
  document.querySelector(hasTOC ? '.sidebar-nav-toc' : '.sidebar-nav-overview').click();
  NexT.utils.updateSidebarPosition();
});
</script>


  




  <script src="/js/local-search.js"></script>








<script data-pjax>
document.querySelectorAll('.pdfobject-container').forEach(element => {
  const url = element.dataset.target;
  const pdfOpenParams = {
    navpanes : 0,
    toolbar  : 0,
    statusbar: 0,
    pagemode : 'thumbs',
    view     : 'FitH'
  };
  const pdfOpenFragment = '#' + Object.entries(pdfOpenParams).map(([key, value]) => `${key}=${encodeURIComponent(value)}`).join('&');
  const fullURL = `/lib/pdf/web/viewer.html?file=${encodeURIComponent(url)}${pdfOpenFragment}`;

  if (NexT.utils.supportsPDFs()) {
    element.innerHTML = `<embed class="pdfobject" src="${url + pdfOpenFragment}" type="application/pdf" style="height: ${element.dataset.height};">`;
  } else {
    element.innerHTML = `<iframe src="${fullURL}" style="height: ${element.dataset.height};" frameborder="0"></iframe>`;
  }
});
</script>


<script data-pjax>
if (document.querySelectorAll('.mermaid').length) {
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/mermaid@8.8.2/dist/mermaid.min.js', () => {
    mermaid.init({
      theme    : 'neutral',
      logLevel : 3,
      flowchart: { curve     : 'linear' },
      gantt    : { axisFormat: '%m/%d/%Y' },
      sequence : { actorMargin: 50 }
    }, '.mermaid');
  }, window.mermaid);
}
</script>





  








    <div class="pjax">
  

  
      <script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              const target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    const script = document.createElement('script');
    script.src = '//cdn.jsdelivr.net/npm/mathjax@3.1.2/es5/tex-mml-chtml.js';
    script.defer = true;
    document.head.appendChild(script);
  } else {
    MathJax.startup.document.state(0);
    MathJax.typesetClear();
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  
  <script src="//cdn.jsdelivr.net/npm/quicklink@2.0.0/dist/quicklink.umd.js"></script>
  <script>
      window.addEventListener('load', () => {
      quicklink.listen({
        timeout : 3000,
        priority: true,
        ignores : [uri => uri.includes('#'),uri => uri === 'https://soundmemories.github.io/page/3/',]
      });
      });
  </script>

    </div>
</body>
</html>
