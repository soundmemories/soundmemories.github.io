<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222" media="(prefers-color-scheme: light)">
<meta name="theme-color" content="#222" media="(prefers-color-scheme: dark)"><meta name="generator" content="Hexo 6.3.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="https://fonts.loli.net/css?family=Monda:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">

<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css" integrity="sha256-HtsXJanqjKTc8vVQjO4YMhiqFoXkfBsjBWcX91T1jr8=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.css" integrity="sha256-Vzbj7sDDS/woiFS3uNKo8eIuni59rjyNGtXfstRzStA=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"soundmemories.github.io","root":"/","images":"/images","scheme":"Muse","darkmode":true,"version":"8.17.1","exturl":true,"sidebar":{"position":"left","display":"always","padding":18,"offset":12},"copycode":{"enable":true,"style":null},"bookmark":{"enable":true,"color":"#222","save":"auto"},"mediumzoom":true,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"},"path":"/search.xml","localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false}}</script><script src="/js/config.js"></script>

    <meta name="description" content="今日事，今日毕">
<meta property="og:type" content="website">
<meta property="og:title" content="SoundMemories">
<meta property="og:url" content="https://soundmemories.github.io/page/4/index.html">
<meta property="og:site_name" content="SoundMemories">
<meta property="og:description" content="今日事，今日毕">
<meta property="og:locale" content="zh_CN">
<meta property="article:author" content="SoundMemories">
<meta name="twitter:card" content="summary">


<link rel="canonical" href="https://soundmemories.github.io/page/4/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":true,"isPost":false,"lang":"zh-CN","comments":"","permalink":"","path":"page/4/index.html","title":""}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>SoundMemories</title>
  








  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <h1 class="site-title">SoundMemories</h1>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="搜索" role="button">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签<span class="badge">9</span></a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类<span class="badge">9</span></a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档<span class="badge">126</span></a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup"><div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" maxlength="80"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close" role="button">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="search-result-container no-result">
  <div class="search-result-icon">
    <i class="fa fa-spinner fa-pulse fa-5x"></i>
  </div>
</div>

    </div>
  </div>

</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-overview-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="SoundMemories"
      src="/images/avstar.png">
  <p class="site-author-name" itemprop="name">SoundMemories</p>
  <div class="site-description" itemprop="description">今日事，今日毕</div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">126</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">9</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">9</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author animated">
      <span class="links-of-author-item">
        <span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL3NvdW5kbWVtb3JpZXM=" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;soundmemories"><i class="fab fa-github fa-fw"></i>GitHub</span>
      </span>
      <span class="links-of-author-item">
        <span class="exturl" data-url="bWFpbHRvOnNvdW5kbWVtb3JpZXNAMTYzLmNvbQ==" title="E-Mail → mailto:soundmemories@163.com"><i class="fa fa-envelope fa-fw"></i>E-Mail</span>
      </span>
  </div>
  <div class="cc-license animated" itemprop="license">
    <span class="exturl cc-opacity" data-url="aHR0cHM6Ly9jcmVhdGl2ZWNvbW1vbnMub3JnL2xpY2Vuc2VzL2J5LW5jLXNhLzQuMC8="><img src="https://cdnjs.cloudflare.com/ajax/libs/creativecommons-vocabulary/2020.11.3/assets/license_badges/small/by_nc_sa.svg" alt="Creative Commons"></span>
  </div>

        </div>
      </div>
    </div>

    
        <div class="pjax">
        </div>
  </aside>


    </div>

    <div class="main-inner index posts-expand">

    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://soundmemories.github.io/2020/10/15/ML/37.NN/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avstar.png">
      <meta itemprop="name" content="SoundMemories">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="SoundMemories">
      <meta itemprop="description" content="今日事，今日毕">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | SoundMemories">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2020/10/15/ML/37.NN/" class="post-title-link" itemprop="url">NN</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2020-10-15 00:00:00" itemprop="dateCreated datePublished" datetime="2020-10-15T00:00:00+08:00">2020-10-15</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/ML/" itemprop="url" rel="index"><span itemprop="name">ML</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>601</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>2 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="前馈神经网络">前馈神经网络</h1>
<p>机器学习我们已经知道可以分为两大流派：</p>
<ol type="1">
<li><p>频率派，这个流派的方法叫做统计学习，根据具体问题有下面的算法：</p>
<ol type="1">
<li><p>正则化，L1，L2 等</p></li>
<li><p>核化，如核支撑向量机</p></li>
<li><p>集成化，AdaBoost，RandomForest</p></li>
<li><p>层次化，神经网络，神经网络有各种不同的模型，有代表性的有：</p>
<ol type="1">
<li>多层感知机</li>
<li>Autoencoder</li>
<li>CNN</li>
<li>RNN</li>
</ol>
<p>这几种模型又叫做深度神经网络。</p></li>
</ol></li>
<li><p>贝叶斯派，这个流派的方法叫概率图模型，根据图特点分为：</p>
<ol type="1">
<li>有向图-贝叶斯网络，加入层次化后有深度有向网络，包括
<ol type="1">
<li>Sigmoid Belief Network</li>
<li>Variational Autoencoder</li>
<li>GAN</li>
</ol></li>
<li>无向图-马尔可夫网络，加入层次化后有深度玻尔兹曼机。</li>
<li>混合，加入层次化后有深度信念网络</li>
</ol>
<p>这几个加入层次化后的模型叫做深度生成网络。</p></li>
</ol>
<p>从广义来说，深度学习包括深度生成网络和深度神经网络。</p>
<h2 id="from-pla-to-dl">From PLA to DL</h2>
<ul>
<li>1958，PLA</li>
<li>1969，PLA 不能解决 XOR 等非线性数据</li>
<li>1981，MLP，多层感知机的出现解决了上面的问题</li>
<li>1986，BP 算法应用在 MLP 上，RNN</li>
<li>1989，CNN，Univeral Approximation
Theorem，但是于此同时，由于深度和宽度的相对效率不知道，并且无法解决 BP
算法的梯度消失问题</li>
<li>1993，1995，SVM + kernel，AdaBoost，RandomForest，这些算法的发展，DL
逐渐没落</li>
<li>1997，LSTM</li>
<li>2006，基于 RBM 的 深度信念网络和深度自编码</li>
<li>2009，GPU的发展</li>
<li>2011，在语音方面的应用</li>
<li>2012，ImageNet</li>
<li>2013，VAE</li>
<li>2014，GAN</li>
<li>2016，AlphaGo</li>
<li>2018，GNN</li>
</ul>
<p>DL 不是一个新的东西，其近年来的大发展主要原因如下：</p>
<ol type="1">
<li>数据量变大</li>
<li>分布式计算的发展</li>
<li>硬件算力的发展</li>
</ol>
<h2 id="非线性问题">非线性问题</h2>
<p>对于非线性的问题，有三种方法：</p>
<ol type="1">
<li>非线性转换，将低维空间转换到高维空间（Cover
定理），从而变为一个线性问题。</li>
<li>核方法，由于非线性转换是变换为高维空间，因此可能导致维度灾难，并且可能很难得到这个变换函数，核方法不直接寻找这个转换，而是寻找一个内积。</li>
<li>神经网络方法，将复合运算变为基本的线性运算的组合。</li>
</ol>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://soundmemories.github.io/2020/10/14/ML/36.Spectral/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avstar.png">
      <meta itemprop="name" content="SoundMemories">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="SoundMemories">
      <meta itemprop="description" content="今日事，今日毕">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | SoundMemories">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2020/10/14/ML/36.Spectral/" class="post-title-link" itemprop="url">Spectral</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2020-10-14 00:00:00" itemprop="dateCreated datePublished" datetime="2020-10-14T00:00:00+08:00">2020-10-14</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/ML/" itemprop="url" rel="index"><span itemprop="name">ML</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>792</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>3 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="谱聚类">谱聚类</h1>
<p>聚类问题可以分为两种思路：</p>
<ol type="1">
<li>Compactness，这类有 K-means，GMM
等，但是这类算法只能处理凸集，为了处理非凸的样本集，必须引入核技巧。</li>
<li>Connectivity，这类以谱聚类为代表。</li>
</ol>
<p>谱聚类是一种基于无向带权图的聚类方法。这个图用 <span
class="math inline">\(G=(V,E)\)</span> 表示，其中 <span
class="math inline">\(V=\{1,2,\cdots,N\}\)</span>，<span
class="math inline">\(E=\{w_{ij}\}\)</span>，这里 <span
class="math inline">\(w_{ij}\)</span>
就是边的权重，这里权重取为相似度，<span
class="math inline">\(W=(w_{ij})\)</span>
是相似度矩阵，定义相似度（径向核）：<br />
<span class="math display">\[
w_{ij}=k(x_i,x_j)=\exp(-\frac{||x_i-x_j||_2^2}{2\sigma^2}),(i,j)\in E\\
w_{ij}=0,(i,j)\notin E
\]</span><br />
下面定义图的分割，这种分割就相当于聚类的结果。定义 <span
class="math inline">\(w(A,B)\)</span>：<br />
<span class="math display">\[
A\subset V,B\subset V,A\cap B=\emptyset,w(A,B)=\sum\limits_{i\in A,j\in
B}w_{ij}
\]</span><br />
假设一共有 <span class="math inline">\(K\)</span> 个类别，对这个图的分割
<span
class="math inline">\(CUT(V)=CUT(A_1,A_2,\cdots,A_K)=\sum\limits_{k=1}^Kw(A_k,\overline{A_k})=\sum\limits_{k=1}^K[w(A_k,V)-w(A_k,A_k)]\)</span></p>
<p>于是，我们的目标就是 <span
class="math inline">\(\min\limits_{A_k}CUT(V)\)</span>。</p>
<p>为了平衡每一类内部的权重不同，我们做归一化的操作，定义每一个集合的度，首先，对单个节点的度定义：<br />
<span class="math display">\[
d_i=\sum\limits_{j=1}^Nw_{ij}
\]</span><br />
其次，每个集合：<br />
<span class="math display">\[
\Delta_k=degree(A_k)=\sum\limits_{i\in A_k}d_i
\]</span><br />
于是：<br />
<span class="math display">\[
N(CUT)=\sum\limits_{k=1}^K\frac{w(A_k,\overline{A_k})}{\sum\limits_{i\in
A_k}d_i}
\]</span><br />
所以目标函数就是最小化这个式子。</p>
<p>谱聚类的模型就是：<br />
<span class="math display">\[
\{\hat{A}_k\}_{k=1}^K=\mathop{argmin}_{A_k}N(CUT)
\]</span><br />
引入指示向量：<br />
<span class="math display">\[
\begin{cases}
y_i\in \{0,1\}^K\\
\sum\limits_{j=1}^Ky_{ij}=1
\end{cases}
\]</span><br />
其中，<span class="math inline">\(y_{ij}\)</span> 表示第 <span
class="math inline">\(i\)</span> 个样本属于 <span
class="math inline">\(j\)</span> 个类别，记：<span
class="math inline">\(Y=(y_1,y_2,\cdots,y_N)^T\)</span>。所以：<br />
<span class="math display">\[
\hat{Y}=\mathop{argmin}_YN(CUT)
\]</span><br />
将 <span class="math inline">\(N(CUT)\)</span>
写成对角矩阵的形式，于是：<br />
<span class="math display">\[
\begin{align}N(CUT)&amp;=Trace[diag(\frac{w(A_1,\overline{A_1})}{\sum\limits_{i\in
A_1}d_i},\frac{w(A_2,\overline{A_2})}{\sum\limits_{i\in
A_2}d_i},\cdots,\frac{w(A_K,\overline{A_K})}{\sum\limits_{i\in
A_K}d_i})]\nonumber\\
&amp;=Trace[diag(w(A_1,\overline{A_1}),w(A_2,\overline{A_2}),\cdots,w(A_K,\overline{A_K}))\cdot
diag(\sum\limits_{i\in A_1}d_i,\cdots,\sum\limits_{i\in
A_K}d_i)^{-1}]\nonumber\\
&amp;=Trace[O\cdot P^{-1}]
\end{align}
\]</span><br />
我们已经知道 <span class="math inline">\(Y,w\)</span>
这两个矩阵，我们希望求得 <span class="math inline">\(O,P\)</span>。</p>
<p>由于：<br />
<span class="math display">\[
Y^TY=\sum\limits_{i=1}^Ny_iy_i^T
\]</span><br />
对于 <span class="math inline">\(y_iy_i^T\)</span>，只在对角线上的 <span
class="math inline">\(k\times k\)</span> 处为 1，所以：<br />
<span class="math display">\[
Y^TY=diag(N_1,N_2,\cdots,N_K)
\]</span><br />
其中，<span class="math inline">\(N_i\)</span> 表示有 <span
class="math inline">\(N_i\)</span> 个样本属于 <span
class="math inline">\(i\)</span>，即 <span
class="math inline">\(N_k=\sum\limits_{k\in A_k}1\)</span>。</p>
<p>引入对角矩阵，根据 <span class="math inline">\(d_i\)</span> 的定义，
<span
class="math inline">\(D=diag(d_1,d_2,\cdots,d_N)=diag(w_{NN}\mathbb{I}_{N1})\)</span>，于是：<br />
<span class="math display">\[
P=Y^TDY
\]</span><br />
对另一项 <span
class="math inline">\(O=diag(w(A_1,\overline{A_1}),w(A_2,\overline{A_2}),\cdots,w(A_K,\overline{A_K})\)</span>：<br />
<span class="math display">\[
O=diag(w(A_i,V))-diag(w(A_i,A_i))=diag(\sum\limits_{j\in
A_i}d_j)-diag(w(A_i,A_i))
\]</span><br />
其中，第一项已知，第二项可以写成 <span
class="math inline">\(Y^TwY\)</span>，这是由于：<br />
<span class="math display">\[
Y^TwY=\sum\limits_{i=1}^N\sum\limits_{j=1}^Ny_iy_j^Tw_{ij}
\]</span><br />
于是这个矩阵的第 <span class="math inline">\(lm\)</span>
项可以写为：<br />
<span class="math display">\[
\sum\limits_{i\in A_l,j\in A_m}w_{ij}
\]</span><br />
这个矩阵的对角线上的项和 <span class="math inline">\(w(A_i,A_i)\)</span>
相同，所以取迹后的取值不会变化。</p>
<p>所以：<br />
<span class="math display">\[
N(CUT)=Trace[(Y^T(D-w))Y)\cdot(Y^TDY)^{-1}]
\]</span><br />
其中，$ L=D-w$ 叫做拉普拉斯矩阵。</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://soundmemories.github.io/2020/10/13/ML/35.RBM/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avstar.png">
      <meta itemprop="name" content="SoundMemories">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="SoundMemories">
      <meta itemprop="description" content="今日事，今日毕">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | SoundMemories">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2020/10/13/ML/35.RBM/" class="post-title-link" itemprop="url">RBM</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2020-10-13 00:00:00" itemprop="dateCreated datePublished" datetime="2020-10-13T00:00:00+08:00">2020-10-13</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/ML/" itemprop="url" rel="index"><span itemprop="name">ML</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>1k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>4 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="受限玻尔兹曼机">受限玻尔兹曼机</h1>
<p>玻尔兹曼机是一种存在隐节点的无向图模型。在图模型中最简单的是朴素贝叶斯模型（朴素贝叶斯假设），引入单个隐变量后，发展出了
GMM，如果单个隐变量变成序列的隐变量，就得到了状态空间模型（引入齐次马尔可夫假设和观测独立假设就有HMM，Kalman
Filter，Particle
Filter），为了引入观测变量之间的关联，引入了一种最大熵模型-MEMM，为了克服
MEMM 中的局域问题，又引入了 CRF，CRF
是一个无向图，其中，破坏了齐次马尔可夫假设，如果隐变量是一个链式结构，那么又叫线性链
CRF。</p>
<p>在无向图的基础上，引入隐变量得到了玻尔兹曼机，这个图模型的概率密度函数是一个指数族分布。对隐变量和观测变量作出一定的限制，就得到了受限玻尔兹曼机（RBM）。</p>
<p>我们看到，不同的概率图模型对下面几个特点作出假设：</p>
<ol type="1">
<li>方向-边的性质</li>
<li>离散/连续/混合-点的性质</li>
<li>条件独立性-边的性质</li>
<li>隐变量-节点的性质</li>
<li>指数族-结构特点</li>
</ol>
<p>将观测变量和隐变量分别记为 <span
class="math inline">\(v,h,h=\{h_1,\cdots,h_m\},v=\{v_1,\cdots,v_n\}\)</span>。我们知道，无向图根据最大团的分解，可以写为玻尔兹曼分布的形式
<span
class="math inline">\(p(x)=\frac{1}{Z}\prod\limits_{i=1}^K\psi_i(x_{ci})=\frac{1}{Z}\exp(-\sum\limits_{i=1}^KE(x_{ci}))\)</span>，这也是一个指数族分布。</p>
<p>一个玻尔兹曼机存在一系列的问题，在其推断任务中，想要精确推断，是无法进行的，想要近似推断，计算量过大。为了解决这个问题，一种简化的玻尔兹曼机-受限玻尔兹曼机作出了假设，所有隐变量内部以及观测变量内部没有连接，只在隐变量和观测变量之间有连接，这样一来：<br />
<span class="math display">\[
p(x)=p(h,v)=\frac{1}{Z}\exp(-E(v,h))
\]</span><br />
其中能量函数 <span class="math inline">\(E(v,h)\)</span>
可以写出三个部分，包括与节点集合相关的两项以及与边 <span
class="math inline">\(w\)</span> 相关的一项，记为：<br />
<span class="math display">\[
E(v,h)=-(h^Twv+\alpha^T v+\beta^T h)
\]</span><br />
所以：<br />
<span class="math display">\[
p(x)=\frac{1}{Z}\exp(h^Twv)\exp(\alpha^T v)\exp(\beta^T
h)=\frac{1}{Z}\prod_{i=1}^m\prod_{j=1}^n\exp(h_iw_{ij}v_j)\prod_{j=1}^n\exp(\alpha_jv_j)\prod_{i=1}^m\exp(\beta_ih_i)
\]</span><br />
上面这个式子也和 RBM 的因子图一一对应。</p>
<h2 id="推断">推断</h2>
<p>推断任务包括求后验概率 $ p(v|h),p(h|v)$ 以及求边缘概率 <span
class="math inline">\(p(v)\)</span>。</p>
<h3 id="phv"><span class="math inline">\(p(h|v)\)</span></h3>
<p>对于一个无向图，满足局域的 Markov 性质，即 <span
class="math inline">\(p(h_1|h-\{h_1\},v)=p(h_1|Neighbour(h_1))=p(h_1|v)\)</span>。我们可以得到：<br />
<span class="math display">\[
p(h|v)=\prod_{i=1}^mp(h_i|v)
\]</span><br />
考虑 Binary RBM，所有的隐变量只有两个取值 <span
class="math inline">\(0,1\)</span>：<br />
<span class="math display">\[
p(h_l=1|v)=\frac{p(h_l=1,h_{-l},v)}{p(h_{-l},v)}=\frac{p(h_l=1,h_{-l},v)}{p(h_l=1,h_{-l},v)+p(h_l=0,h_{-l},v)}
\]</span><br />
将能量函数写成和 <span class="math inline">\(l\)</span>
相关或不相关的两项：<br />
<span class="math display">\[
E(v,h)=-(\sum\limits_{i=1,i\ne
l}^m\sum\limits_{j=1}^nh_iw_{ij}v_j+h_l\sum\limits_{j=1}^nw_{lj}v_j+\sum\limits_{j=1}^n\alpha_j
v_j+\sum\limits_{i=1,i\ne l}^m\beta_ih_i+\beta_lh_l)
\]</span><br />
定义：<span
class="math inline">\(h_lH_l(v)=h_l\sum\limits_{j=1}^nw_{lj}v_j+\beta_lh_l,\overline{H}(h_{-l},v)=\sum\limits_{i=1,i\ne
l}^m\sum\limits_{j=1}^nh_iw_{ij}v_j+\sum\limits_{j=1}^n\alpha_j
v_j+\sum\limits_{i=1,i\ne l}^m\beta_ih_i\)</span>。</p>
<p>代入，有：<br />
<span class="math display">\[
p(h_l=1|v)=\frac{\exp(H_l(v)+\overline{H}(h_{-l},v))}{\exp(H_l(v)+\overline{H}(h_{-l},v))+\exp(\overline{H}(h_{-l},v))}=\frac{1}{1+\exp(-H_l(v))}=\sigma(H_l(v))
\]</span><br />
于是就得到了后验概率。对于 <span class="math inline">\(v\)</span>
的后验是对称的，所以类似的可以求解。</p>
<h3 id="pv"><span class="math inline">\(p(v)\)</span></h3>
<p><span class="math display">\[
\begin{align}p(v)&amp;=\sum\limits_hp(h,v)=\sum\limits_h\frac{1}{Z}\exp(h^Twv+\alpha^Tv+\beta^Th)\nonumber\\
&amp;=\exp(\alpha^Tv)\frac{1}{Z}\sum\limits_{h_1}\exp(h_1w_1v+\beta_1h_1)\cdots\sum\limits_{h_m}\exp(h_mw_mv+\beta_mh_m)\nonumber\\
&amp;=\exp(\alpha^Tv)\frac{1}{Z}(1+\exp(w_1v+\beta_1))\cdots(1+\exp(w_mv+\beta_m))\nonumber\\
&amp;=\frac{1}{Z}\exp(\alpha^Tv+\sum\limits_{i=1}^m\log(1+\exp(w_iv+\beta_i)))
\end{align}
\]</span></p>
<p>其中，<span class="math inline">\(\log(1+\exp(x))\)</span> 叫做
Softplus 函数。</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://soundmemories.github.io/2020/10/12/ML/34.GaussianProcess/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avstar.png">
      <meta itemprop="name" content="SoundMemories">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="SoundMemories">
      <meta itemprop="description" content="今日事，今日毕">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | SoundMemories">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2020/10/12/ML/34.GaussianProcess/" class="post-title-link" itemprop="url">GaussianProcess</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2020-10-12 00:00:00" itemprop="dateCreated datePublished" datetime="2020-10-12T00:00:00+08:00">2020-10-12</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/ML/" itemprop="url" rel="index"><span itemprop="name">ML</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>1.2k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>4 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="高斯过程回归">高斯过程回归</h1>
<p>将一维高斯分布推广到多变量中就得到了高斯网络，将多变量推广到无限维，就得到了高斯过程，高斯过程是定义在连续域（时间空间）上的无限多个高维随机变量所组成的随机过程。</p>
<p>在时间轴上的任意一个点都满足高斯分布吗，将这些点的集合叫做高斯过程的一个样本。</p>
<blockquote>
<p>对于时间轴上的序列 <span class="math inline">\(\xi_t\)</span>，如果
<span class="math inline">\(\forall n\in N^+，t_i\in T\)</span>，有
<span class="math inline">\(\xi_{t_1-t_n}\sim
\mathcal{N}(\mu_{t_1-t_n},\Sigma_{t_1-t_n})\)</span>， 那么 <span
class="math inline">\(\{\xi_t\}_{t\in T}\)</span> 是一个高斯过程。</p>
<p>高斯过程有两个参数（高斯过程存在性定理），均值函数 <span
class="math inline">\(m(t)=\mathbb{E}[\xi_t]\)</span> 和协方差函数 <span
class="math inline">\(k(s,t)=\mathbb{E}[(\xi_s-\mathbb{E}[\xi_s])(\xi_t-\mathbb{E}[\xi_t])]\)</span>。</p>
</blockquote>
<p>我们将贝叶斯线性回归添加核技巧的这个模型叫做高斯过程回归，高斯过程回归分为两种视角：</p>
<ol type="1">
<li>权空间的视角-核贝叶斯线性回归，相当于 <span
class="math inline">\(x\)</span> 为 <span
class="math inline">\(t\)</span>，在每个时刻的高斯分布来源于权重，根据上面的推导，预测的函数依然是高斯分布。</li>
<li>函数空间的视角-高斯分布通过函数 <span
class="math inline">\(f(x)\)</span> 来体现。</li>
</ol>
<h2 id="核贝叶斯线性回归">核贝叶斯线性回归</h2>
<p>贝叶斯线性回归可以通过加入核函数的方法来解决非线性函数的问题，将
<span class="math inline">\(f(x)=x^Tw\)</span> 这个函数变为 <span
class="math inline">\(f(x)=\phi(x)^Tw\)</span>（当然这个时候，$ _p$
也要变为更高维度的），变换到更高维的空间，有：<br />
<span class="math display">\[
\begin{align}f(x^*)\sim
\mathcal{N}(\phi(x^*)^{T}\sigma^{-2}A^{-1}\Phi^TY,\phi(x^*)^{T}A^{-1}\phi(x^*))\\
A=\sigma^{-2}\Phi^T\Phi+\Sigma_p^{-1}
\end{align}
\]</span><br />
其中，<span
class="math inline">\(\Phi=(\phi(x_1),\phi(x_2),\cdots,\phi(x_N))^T\)</span>。</p>
<p>为了求解 <span class="math inline">\(A^{-1}\)</span>，可以利用
Woodbury Formula，<span
class="math inline">\(A=\Sigma_p^{-1},C=\sigma^{-2}\mathbb{I}\)</span>：<br />
<span class="math display">\[
(A+UCV)^{-1}=A^{-1}-A^{-1}U(C^{-1}+VA^{-1}U)^{-1}VA^{-1}
\]</span><br />
所以 <span
class="math inline">\(A^{-1}=\Sigma_p-\Sigma_p\Phi^T(\sigma^2\mathbb{I}+\Phi\Sigma_p\Phi^T)^{-1}\Phi\Sigma_p\)</span></p>
<p>也可以用另一种方法：<br />
<span class="math display">\[
\begin{align}
A&amp;=\sigma^{-2}\Phi^T\Phi+\Sigma_p^{-1}\nonumber\\
\Leftrightarrow
A\Sigma_p&amp;=\sigma^{-2}\Phi^T\Phi\Sigma_p+\mathbb{I}\nonumber\\
\Leftrightarrow
A\Sigma_p\Phi^T&amp;=\sigma^{-2}\Phi^T\Phi\Sigma_p\Phi^T+\Phi^T=\sigma^{-2}\Phi^T(k+\sigma^2\mathbb{I})\nonumber\\
\Leftrightarrow
\Sigma_p\Phi^T&amp;=\sigma^{-2}A^{-1}\Phi^T(k+\sigma^2\mathbb{I})\nonumber\\
\Leftrightarrow
\sigma^{-2}A^{-1}\Phi^T&amp;=\Sigma_p\Phi^T(k+\sigma^2\mathbb{I})^{-1}\nonumber\\
\Leftrightarrow
\phi(x^*)^T\sigma^{-2}A^{-1}\Phi^T&amp;=\phi(x^*)^T\Sigma_p\Phi^T(k+\sigma^2\mathbb{I})^{-1}
\end{align}
\]</span><br />
上面的左边的式子就是变换后的均值，而右边的式子就是不含 <span
class="math inline">\(A^{-1}\)</span> 的式子，其中 <span
class="math inline">\(k=\Phi\Sigma_p\Phi^T\)</span>。</p>
<p>根据 <span class="math inline">\(A^{-1}\)</span> 得到方差为：<br />
<span class="math display">\[
\phi(x^*)^T\Sigma_p\phi(x^*)-\phi(x^*)^T\Sigma_p\Phi^T(\sigma^2\mathbb{I}+k)^{-1}\Phi\Sigma_p\phi(x^*)
\]</span><br />
上面定义了：<br />
<span class="math display">\[
k=\Phi\Sigma_p\Phi^T
\]</span><br />
我们看到，在均值和方差中，含有下面四项：<br />
<span class="math display">\[
\phi(x^*)^T\Sigma_p\Phi^T,\phi(x^*)^T\Sigma_p\phi(x^*),\phi(x^*)^T\Sigma_p\Phi^T,\Phi\Sigma_p\phi(x^*)
\]</span><br />
展开后，可以看到，有共同的项：<span
class="math inline">\(k(x,x&#39;)=\phi(x)^T\Sigma_p\phi(x‘)\)</span>。由于
<span class="math inline">\(\Sigma_p\)</span>
是正定对称的方差矩阵，所以，这是一个核函数。</p>
<p>对于高斯过程中的协方差：<br />
<span class="math display">\[
k(t,s)=Cov[f(x),f(x&#39;)]=\mathbb{E}[\phi(x)^Tww^T\phi(x&#39;)]=\phi(x)^T\mathbb{E}[ww^T]\phi(x&#39;)=\phi(x)^T\Sigma_p\phi(x&#39;)
\]</span><br />
我们可以看到，这个就对应着上面的核函数。因此我们看到 <span
class="math inline">\(\{f(x)\}\)</span> 组成的组合就是一个高斯过程。</p>
<h2 id="函数空间的观点">函数空间的观点</h2>
<p>相比权重空间，我们也可以直接关注 <span
class="math inline">\(f\)</span>
这个空间，对于预测任务，这就是类似于求：<br />
<span class="math display">\[
p(y^*|X,Y,x^*)=\int_fp(y^*|f,X,Y,x^*)p(f|X,Y,x^*)df
\]</span><br />
对于数据集来说，取 <span
class="math inline">\(f(X)\sim\mathcal{N}(\mu(X),k(X,X)),Y=f(X)+\varepsilon\sim\mathcal{N}(\mu(X),k(X,X)+\sigma^2\mathbb{I})\)</span>。预测任务的目的是给定一个新数据序列
<span
class="math inline">\(X^\ast=(x_1^\ast,\cdots,x_M^\ast)^T\)</span>，得到
<span
class="math inline">\(Y^\ast=f(X^\ast)+\varepsilon\)</span>。我们可以写出：<br />
<span class="math display">\[
\begin{pmatrix}Y\\f(X^*)\end{pmatrix}\sim\mathcal{N}\left(\begin{pmatrix}\mu(X)\\\mu(X^*)\end{pmatrix},\begin{pmatrix}k(X,X)+\sigma^2\mathbb{I}&amp;k(X,X^*)\\k(X^*,X)&amp;k(X^*,X^*)\end{pmatrix}\right)
\]</span><br />
根据高斯分布的方法：<br />
<span class="math display">\[
\begin{align}x=\begin{pmatrix}x_a\\x_b\end{pmatrix}\sim\mathcal{N}\left(\begin{pmatrix}\mu_a\\\mu_b\end{pmatrix},\begin{pmatrix}\Sigma_{aa}&amp;\Sigma_{ab}\\\Sigma_{ba}&amp;\Sigma_{bb}\end{pmatrix}\right)\\
x_b|x_a\sim\mathcal{N}(\mu_{b|a},\Sigma_{b|a})\\
\mu_{b|a}=\Sigma_{ba}\Sigma_{aa}^{-1}(x_a-\mu_a)+\mu_b\\
\Sigma_{b|a}=\Sigma_{bb}-\Sigma_{ba}\Sigma_{aa}^{-1}\Sigma_{ab}
\end{align}
\]</span><br />
可以直接写出：<br />
<span class="math display">\[
\begin{align}
p(f(X^*)|X,Y,X^*)=p(f(X^*)|Y)\\
=\mathcal{N}(k(X^*,X)[k(X,X)+\sigma^2\mathbb{I}]^{-1}(Y-\mu(X))+\mu(X^*),\\
k(X^*,X^*)-k(X^*,X)[k(X,X)+\sigma^2\mathbb{I}]^{1}k(X,X^*))
\end{align}
\]</span><br />
所以对于 <span
class="math inline">\(Y=f(X^*)+\varepsilon\)</span>：<br />
<span class="math display">\[
\begin{align}
\mathcal{N}(k(X^*,X)[k(X,X)+\sigma^2\mathbb{I}]^{-1}(Y-\mu(X))+\mu(X^*),\\
k(X^*,X^*)-k(X^*,X)[k(X,X)+\sigma^2\mathbb{I}]^{1}k(X,X^*)+\sigma^2\mathbb{I})
\end{align}
\]</span><br />
我们看到，函数空间的观点更加简单易于求解。</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://soundmemories.github.io/2020/10/11/ML/33.BayesianLR/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avstar.png">
      <meta itemprop="name" content="SoundMemories">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="SoundMemories">
      <meta itemprop="description" content="今日事，今日毕">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | SoundMemories">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2020/10/11/ML/33.BayesianLR/" class="post-title-link" itemprop="url">BayesianLR</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2020-10-11 00:00:00" itemprop="dateCreated datePublished" datetime="2020-10-11T00:00:00+08:00">2020-10-11</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/ML/" itemprop="url" rel="index"><span itemprop="name">ML</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>618</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>2 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="贝叶斯线性回归">贝叶斯线性回归</h1>
<p>我们知道，线性回归当噪声为高斯分布的时候，最小二乘损失导出的结果相当于对概率模型应用
MLE，引入参数的先验时，先验分布是高斯分布，那么
MAP的结果相当于岭回归的正则化，如果先验是拉普拉斯分布，那么相当于 Lasso
的正则化。这两种方案都是点估计方法。我们希望利用贝叶斯方法来求解参数的后验分布。</p>
<p>线性回归的模型假设为：<br />
<span class="math display">\[
\begin{align}f(x)=w^Tx
\\y=f(x)+\varepsilon\\
\varepsilon\sim\mathcal{N}(0,\sigma^2)
\end{align}
\]</span><br />
在贝叶斯方法中，需要解决推断和预测两个问题。</p>
<h2 id="推断">推断</h2>
<p>引入高斯先验：<br />
<span class="math display">\[
p(w)=\mathcal{N}(0,\Sigma_p)
\]</span><br />
对参数的后验分布进行推断：<br />
<span class="math display">\[
p(w|X,Y)=\frac{p(w,Y|X)}{p(Y|X)}=\frac{p(Y|w,X)p(w|X)}{\int
p(Y|w,X)p(w|X)dw}
\]</span><br />
分母和参数无关，由于 <span
class="math inline">\(p(w|X)=p(w)\)</span>，代入先验得到：<br />
<span class="math display">\[
p(w|X,Y)\propto
\prod\limits_{i=1}^N\mathcal{N}(y_i|w^Tx_i,\sigma^2)\cdot\mathcal{N}(0,\Sigma_p)
\]</span><br />
高斯分布取高斯先验的共轭分布依然是高斯分布，于是可以得到后验分布也是一个高斯分布。第一项：<br />
<span class="math display">\[
\begin{align}\prod\limits_{i=1}^N\mathcal{N}(y_i|w^Tx_i,\sigma^2)&amp;=\frac{1}{(2\pi)^{N/2}\sigma^N}\exp(-\frac{1}{2\sigma^2}\sum\limits_{i=1}^N(y_i-w^Tx_i)^2)\nonumber\\
&amp;=\frac{1}{(2\pi)^{N/2}\sigma^N}\exp(-\frac{1}{2}(Y-Xw)^T(\sigma^{-2}\mathbb{I})(Y-Xw))
\nonumber\\&amp;=\mathcal{N}(Xw,\sigma^2\mathbb{I})
\end{align}
\]</span><br />
代入上面的式子：<br />
<span class="math display">\[
p(w|X,Y)\propto\exp(-\frac{1}{2\sigma^2}(Y-Xw)^T\sigma^{-2}\mathbb{I}(Y-Xw)-\frac{1}{2}w^T\Sigma_p^{-1}w)
\]</span><br />
假定最后得到的高斯分布为：<span
class="math inline">\(\mathcal{N}(\mu_w,\Sigma_w)\)</span>。对于上面的分布，采用配方的方式来得到最终的分布，指数上面的二次项为：<br />
<span class="math display">\[
-\frac{1}{2\sigma^2}w^TX^TXw-\frac{1}{2}w^T\Sigma_p^{-1}w
\]</span><br />
于是：<br />
<span class="math display">\[
\Sigma_w^{-1}=\sigma^{-2}X^TX+\Sigma_p^{-1}=A
\]</span><br />
一次项：<br />
<span class="math display">\[
\frac{1}{2\sigma^2}2Y^TXw=\sigma^{-2}Y^TXw
\]</span><br />
于是：<br />
<span class="math display">\[
\mu_w^T\Sigma_w^{-1}=\sigma^{-2}Y^TX\Rightarrow\mu_w=\sigma^{-2}A^{-1}X^TY
\]</span></p>
<h2 id="预测">预测</h2>
<p>给定一个 <span class="math inline">\(x^\ast\)</span>，求解 <span
class="math inline">\(y^\ast\)</span>，所以 <span
class="math inline">\(f(x^\ast)=x^{\ast T}w\)</span>，代入参数后验，有
<span class="math inline">\(x^{\ast T}w\sim \mathcal{N}(x^{\ast
T}\mu_w,x^{\ast T}\Sigma_wx^\ast)\)</span>，添上噪声项：<br />
<span class="math display">\[
p(y^*|X,Y,x^*)=\int_wp(y^*|w,X,Y,x^*)p(w|X,Y,x^*)dw=\int_wp(y^*|w,x^*)p(w|X,Y)dw\\
=\mathcal{N}(x^{*T}\mu_w,x^{*T}\Sigma_wx^*+\sigma^2)
\]</span></p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://soundmemories.github.io/2020/10/10/ML/32.GaussianNetwork/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avstar.png">
      <meta itemprop="name" content="SoundMemories">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="SoundMemories">
      <meta itemprop="description" content="今日事，今日毕">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | SoundMemories">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2020/10/10/ML/32.GaussianNetwork/" class="post-title-link" itemprop="url">GaussianNetwork</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2020-10-10 00:00:00" itemprop="dateCreated datePublished" datetime="2020-10-10T00:00:00+08:00">2020-10-10</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/ML/" itemprop="url" rel="index"><span itemprop="name">ML</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>753</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>3 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="高斯网络">高斯网络</h1>
<p>高斯图模型（高斯网络）是一种随机变量为连续的有向或者无向图。有向图版本的高斯图是高斯贝叶斯网络，无向版本的叫高斯马尔可夫网络。</p>
<p>高斯网络的每一个节点都是高斯分布：<span
class="math inline">\(\mathcal{N}(\mu_i,\Sigma_i)\)</span>，于是所有节点的联合分布就是一个高斯分布，均值为
<span class="math inline">\(\mu\)</span>，方差为 <span
class="math inline">\(\Sigma\)</span>。</p>
<p>对于边缘概率，我们有下面三个结论：</p>
<ol type="1">
<li><p>对于方差矩阵，可以得到独立性条件：<span
class="math inline">\(x_i\perp
x_j\Leftrightarrow\sigma_{ij}=0\)</span>，这个叫做全局独立性。</p></li>
<li><p>我们看方差矩阵的逆（精度矩阵或信息矩阵）：<span
class="math inline">\(\Lambda=\Sigma^{-1}=(\lambda_{ij})_{pp}\)</span>，有定理：</p>
<blockquote>
<p><span class="math inline">\(x_i\perp
x_j|(X-\{x_i,x_j\})\Leftrightarrow\lambda_{ij}=0\)</span></p>
</blockquote>
<p>因此，我们使用精度矩阵来表示条件独立性。</p></li>
<li><p>对于任意一个无向图中的节点 <span
class="math inline">\(x_i\)</span>，<span
class="math inline">\(x_i|(X-x_i)\sim \mathcal{N}(\sum\limits_{j\ne
i}\frac{\lambda_{ij}}{\lambda_{ii}}x_j,\lambda_{ii}^{-1})\)</span></p>
<p>也就是其他所有分量的线性组合，即所有与它有链接的分量的线性组合。</p></li>
</ol>
<h2 id="高斯贝叶斯网络-gbn">高斯贝叶斯网络 GBN</h2>
<p>高斯贝叶斯网络可以看成是 LDS 的一个推广，LDS
的假设是相邻时刻的变量之间的依赖关系，因此是一个局域模型，而高斯贝叶斯网络，每一个节点的父亲节点不一定只有一个，因此可以看成是一个全局的模型。根据有向图的因子分解：<br />
<span class="math display">\[
p(x)=\prod\limits_{i=1}^pp(x_i|x_{Parents(i)})
\]</span><br />
对里面每一项，假设每一个特征是一维的，可以写成线性组合：<br />
<span class="math display">\[
p(x_i|x_{Parents(i)})=\mathcal{N}(x_i|\mu_i+W_i^Tx_{Parents(i)},\sigma^2_i)
\]</span><br />
将随机变量写成：<br />
<span class="math display">\[
x_i=\mu_i+\sum\limits_{j\in
x_{Parents(i)}}w_{ij}(x_j-\mu_j)+\sigma_i\varepsilon_i,\varepsilon_i\sim
\mathcal{N}(0,1)
\]</span><br />
写成矩阵形式，并且对 <span class="math inline">\(w\)</span>
进行扩展：<br />
<span class="math display">\[
x-\mu=W(x-\mu)+S\varepsilon
\]</span><br />
其中，<span
class="math inline">\(S=diag(\sigma_i)\)</span>。所以有：<span
class="math inline">\(x-\mu=(\mathbb{I}-W)^{-1}S\varepsilon\)</span></p>
<p>由于：<br />
<span class="math display">\[
Cov(x)=Cov(x-\mu)
\]</span><br />
可以得到协方差矩阵。</p>
<h2 id="高斯马尔可夫网络-gmn">高斯马尔可夫网络 GMN</h2>
<p>对于无向图版本的高斯网络，可以写成：<br />
<span class="math display">\[
p(x)=\frac{1}{Z}\prod\limits_{i=1}^p\phi_i(x_i)\prod\limits_{i,j\in
X}\phi_{i,j}(x_i,x_j)
\]</span><br />
为了将高斯分布和这个式子结合，我们写出高斯分布和变量相关的部分：<br />
<span class="math display">\[
\begin{align}p(x)&amp;\propto
\exp(-\frac{1}{2}(x-\mu)^T\Sigma^{-1}(x-\mu))\nonumber\\
&amp;=\exp(-\frac{1}{2}(x^T\Lambda x-2\mu^T\Lambda
x+\mu^T\Lambda\mu))\nonumber\\
&amp;=\exp(-\frac{1}{2}x^T\Lambda x+(\Lambda\mu)^Tx)
\end{align}
\]</span><br />
可以看到，这个式子与无向图分解中的两个部分对应，我们记 <span
class="math inline">\(h=\Lambda\mu\)</span>为 Potential Vector。其中和
<span class="math inline">\(x_i\)</span> 相关的为：<span
class="math inline">\(x_i:-\frac{1}{2}\lambda_{ii}x_i^2+h_ix_i\)</span>，与
<span class="math inline">\(x_i,x_j\)</span> 相关的是：<span
class="math inline">\(x_i,x_j:-\lambda_{ij}x_ix_j\)</span>，这里利用了精度矩阵为对称矩阵的性质。我们看到，这里也可以看出，<span
class="math inline">\(x_i,x_j\)</span> 构成的一个势函数，只和 <span
class="math inline">\(\lambda_{ij}\)</span> 有关，于是
$x_ix_j|(X-{x_i,x_j})_{ij}=0 $。</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://soundmemories.github.io/2020/10/09/ML/31.CRF/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avstar.png">
      <meta itemprop="name" content="SoundMemories">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="SoundMemories">
      <meta itemprop="description" content="今日事，今日毕">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | SoundMemories">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2020/10/09/ML/31.CRF/" class="post-title-link" itemprop="url">CRF</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2020-10-09 00:00:00" itemprop="dateCreated datePublished" datetime="2020-10-09T00:00:00+08:00">2020-10-09</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/ML/" itemprop="url" rel="index"><span itemprop="name">ML</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>2.6k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>9 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="条件随机场">条件随机场</h1>
<p>我们知道，分类问题可以分为<strong>硬分类</strong>和<strong>软分类</strong>两种。其中<strong>硬分类</strong>有
SVM（支持向量机），PLA（感知机算法），LDA（线性判别分析）
等。<strong>软分类</strong>问题大体上可以分为<strong>概率生成</strong>和<strong>概率判别</strong>模型，其中较为有名的<strong>判别模型</strong>有
Logistic 回归，<strong>生成模型</strong>有朴素贝叶斯模型。Logistic
回归模型的损失函数为交叉熵，这类模型也叫对数线性模型，一般地，又叫做最大熵模型，这类模型和指数族分布的概率假设是一致的。对朴素贝叶斯假设，如果将其中的单元素的条件独立性做推广到一系列的隐变量，那么，由此得到的模型又被称为动态模型，比较有代表性的如
HMM（隐马尔可夫模型），从概率意义上，当HMM的发射矩阵服从高斯分布时，HMM也可以看成是
GMM（高斯混合模型） 在时序上面的推广。</p>
<p>我们看到，一般地，如果将<strong>最大熵模型</strong>（判别模型）和
<strong>HMM</strong>（生成模型） 相结合，那么这种模型叫做<strong>最大熵
Markov 模型</strong>（MEMM），严格讲它是概率判别模型：</p>
<pre class="mermaid">
graph LR
	x4((x4))--&gt;y4
	x2((x2))--&gt;y2
	x1((x1))--&gt;y1
	x3((x3))--&gt;y3
	y1--&gt;y2;
	y2--&gt;y3;
y3--&gt;y4;
</pre>
<p>这个图就是将 HMM 的<strong>观测变量 <span
class="math inline">\(x\)</span>
方向反向</strong>且是有向图，应用在分类中，隐变量就是输出的分类。这样
HMM 中的两个假设就不成立了，特别是观测之间不是完全独立的了。即与HMM的
<span class="math inline">\(x_i\)</span> 只依赖 <span
class="math inline">\(y_i\)</span> 不一样，MEMM当前隐藏状态 <span
class="math inline">\(y_i\)</span> 应该是依赖当前时刻的观测节点 <span
class="math inline">\(x_i\)</span> 和上一时刻的隐藏节点 <span
class="math inline">\(y_{i-1}\)</span>，依次类推<span
class="math inline">\(y_{i-1}\)</span>也和 <span
class="math inline">\(x_{i-1}\)</span> 有关，所以 <span
class="math inline">\(x_i\)</span> 和 <span
class="math inline">\(x_{i-1}\)</span>
有关，从而打破了观测独立假设。</p>
<p>HMM 是一种<strong>生成式模型</strong>，其建模对象为 <span
class="math inline">\(p(X,Y|\lambda)\)</span>，根据 HMM 的概率图，<span
class="math inline">\(p(X,Y|\lambda)=\prod\limits_{t=1}^Tp(x_t,y_t|\lambda,y_{t-1})\)</span>。我们看到，观测独立性假设是一个很强的假设，如果我们有一个文本样本，那么观测独立性假设就假定了所有的单词之间没有关联。</p>
<p>MEMM 是一种<strong>判别式模型</strong>，建模对象是 <span
class="math inline">\(p(Y|X,\lambda)\)</span>，我们看概率图（有向图），给定
<span class="math inline">\(y_t\)</span>，<span
class="math inline">\(x_t,x_{t-1}\)</span>
是不独立的，这样，观测独立假设就不成立了。根据概率图，<span
class="math inline">\(p(Y|X,\lambda)=\prod\limits_{t=1}^Tp(y_t|y_{t-1},X,\lambda)\)</span>。</p>
<p>MEMM 的缺陷是<strong>标注偏差问题</strong>（Label Bias
Problem），其必须满足<strong>局域的概率归一化</strong>，我们看到，在上面的概率图中，<span
class="math inline">\(p(y_t|y_{t-1},x_t)\)</span>， 这个概率，如果 <span
class="math inline">\(p(y_t|y_{t-1})\)</span>
非常接近1，那么事实上，观测变量 <span class="math inline">\(x\)</span>
是什么都不会影响这个概率了，这样观测变量就没用了。</p>
<p>对于这个问题，我们将 <span class="math inline">\(y\)</span>
之间的箭头转为直线转为无向图（线性链条件随机场），这样就只要满足<strong>全局归一化了</strong>（破坏齐次
Markov 假设）。</p>
<pre class="mermaid">
graph LR
	x4((x4))--&gt;y4
		x2((x2))--&gt;y2
			x1((x1))--&gt;y1
				x3((x3))--&gt;y3
	y1---y2;
	y2---y3;
y3---y4;
</pre>
<h2 id="crf-的-pdf">CRF 的 PDF</h2>
<p>线性链的 CRF 的 PDF 为 <span
class="math inline">\(p(Y|X)=\dfrac{1}{Z}\exp\sum\limits_{t=1}^T(F_t(y_{t-1},y_t,x_{1:T}))\)</span>，两两形成了最大团，其中
<span class="math inline">\(y_0\)</span>
是随意外加的一个元素作为初始状态。作为第一个简化，我们假设每个团的势函数相同
<span class="math inline">\(F_t=F\)</span>。</p>
<p>对于这个 <span
class="math inline">\(F\)</span>，我们进一步，可以将其写为
$ F(y_{t-1},y_t,X)=<em>{y</em>{t-1},X}+<em>{y</em>{t},X}+<em>{y_t,y</em>{t-1},X}$这三个部分，前两个部分表示状态函数，最后一个表示转移函数，由于整体的求和，第一部分重复可以省略，可以简化为
$
F(y_{t-1},y_t,X)=<em>{y</em>{t},X}+<em>{y_t,y</em>{t-1},X}$，我们可以设计一个表达式将这两部分参数化：<br />
<span class="math display">\[
\begin{align}
\Delta_{y_t,y_{t-1},X}&amp;=\sum\limits_{k=1}^K\lambda_kf_k(y_{t-1},y_t,X)\\
\Delta_{y_{t},X}&amp;=\sum\limits_{l=1}^L\eta_lg_l(y_t,X)
\end{align}
\]</span><br />
其中 $g,f $ 叫做特征函数，对于 <span class="math inline">\(y\)</span> 有
<span class="math inline">\(S\)</span> 种元素，<span
class="math inline">\(K\)</span> 和 <span
class="math inline">\(y_{t-1}\)</span> 和 <span
class="math inline">\(y_{t}\)</span> 有关，<span
class="math inline">\(L\)</span> 和 <span
class="math inline">\(y_t\)</span>有关，所以 <span
class="math inline">\(K\le S^2,L\le S\)</span>。</p>
<p>然后代入概率密度函数中：<br />
<span class="math display">\[
\begin{align}
p(Y|X)=\frac{1}{Z}\exp\sum\limits_{t=1}^T[\sum\limits_{k=1}^K\lambda_kf_k(y_{t-1},y_t,X)+\sum\limits_{l=1}^L\eta_lg_l(y_t,X)]
\end{align}
\]</span></p>
<h3 id="条件随机场的简化">条件随机场的简化</h3>
<p>将其写成向量的形式：定义 <span
class="math inline">\(y=(y_1,y_2,\cdots,y_T)^T,x=(x_1,x_2,\cdots,x_T)^T,\lambda=(\lambda_1,\lambda_2,\cdots,\lambda_K)^T,\eta=(\eta_1,\eta_2,\cdots,\eta_L)^T\)</span>，并且有
<span
class="math inline">\(f=(f_1,f_2,\cdots,f_K)^T,g=(g_1,g_2,\cdots,g_L)^T\)</span>，于是：<br />
<span class="math display">\[
\begin{align}
p(Y=y|X=x)=\frac{1}{Z}\exp\sum\limits_{t=1}^T[\lambda^Tf(y_{t-1},y_t,x)+\eta^Tg(y_t,x)]
\end{align}
\]</span><br />
不妨记：<span
class="math inline">\(\theta=(\lambda,\eta)^T,H=(\sum\limits_{t=1}^Tf,\sum\limits_{t=1}^Tg)^T\)</span>，这俩都是
<span class="math inline">\(K+L\)</span> 维度：<br />
<span class="math display">\[
\begin{align}
p(Y=y|X=x)=\frac{1}{Z(x,\theta)}\exp[\theta^TH(y_t,y_{t-1},x)]
\end{align}
\]</span><br />
上面这个式子是一个指数族分布，于是 <span
class="math inline">\(Z\)</span> 是配分函数。</p>
<h3 id="个问题">3个问题</h3>
<p>CRF 需要解决下面几个问题：<br />
1. Learning：参数估计问题，CRF是求 <span
class="math inline">\(\theta\)</span> 参数。对 <span
class="math inline">\(N\)</span> 个 <span
class="math inline">\(T\)</span> 维样本，<span
class="math inline">\(\hat{\theta}=\mathop{argmax}\limits_{\theta}\prod\limits_{i=1}^Np(y^i|x^i)\)</span>，这里用上标表示样本的编号。<br />
2. Inference：<br />
1. 边缘概率：<span class="math inline">\(p(y_t|x)\)</span><br />
2. 条件概率：一般在生成模型中较为关注，CRF中不关注<br />
3. MAP 推断(Decoding)：<span
class="math inline">\(\hat{y}=\mathop{argmax}\limits_{y}p(y|x)\)</span>
的序列 <span class="math inline">\(y\)</span></p>
<p>总体来说3个基本问题：<strong>概率计算问题</strong>(边缘概率)、<strong>学习问题</strong>(参数估计)、<strong>预测问题</strong>(Decoding)。</p>
<h2 id="边缘概率">边缘概率</h2>
<p>边缘概率这个问题描述为，根据学习任务得到的参数，给定了 <span
class="math inline">\(p(Y=y|X=x)\)</span>，求解 <span
class="math inline">\(p(y_t=i|x)\)</span>。根据无向图可以给出：<br />
<span class="math display">\[
\begin{align}
p(y_t=i|x)=\sum\limits_{y_{1:t-1},y_{t+1:T}}p(y|x)=\sum\limits_{y_{1:t-1}}\sum\limits_{y_{t+1:T}}\frac{1}{Z}\prod\limits_{t&#39;=1}^T\phi_{t&#39;}(y_{t&#39;-1},y_{t&#39;},x)
\end{align}
\]</span><br />
我们看到上面的式子，直接计算的复杂度很高，这是由于求和的复杂度在 <span
class="math inline">\(O(S^T)\)</span>，求积的复杂度在 <span
class="math inline">\(O(T)\)</span>，所以整体复杂度为 <span
class="math inline">\(O(TS^T)\)</span>。我们需要调整求和符号的顺序，从而降低复杂度。</p>
<p>首先，将两个求和分为：<br />
<span class="math display">\[
\begin{align}&amp;p(y_t=i|x)=\frac{1}{Z}\Delta_l\Delta_r\\
&amp;\Delta_l=\sum\limits_{y_{1:t-1}}\phi_{1}(y_0,y_1,x)\phi_2(y_1,y_2,x)\cdots\phi_{t-1}(y_{t-2},y_{t-1},x)\phi_t(y_{t-1},y_t=i,x)\\
&amp;\Delta_r=\sum\limits_{y_{t+1:T}}\phi_{t+1}(y_t=i,y_{t+1},x)\phi_{t+2}(y_{t+1},y_{t+2},x)\cdots\phi_T(y_{T-1},y_T,x)
\end{align}
\]</span><br />
对于 <span class="math inline">\(\Delta_l\)</span>：<span
class="math inline">\(\phi_{t-1}(y_{t-2},y_{t-1},x)\phi_t(y_{t-1},y_t=i,x)\)</span>
对 <span class="math inline">\(y_{t-1}\)</span> 积分得到 <span
class="math inline">\(\sum\limits_{y_{t-1}}\phi_t(y_{t-1},y_t=i,x)\)</span>
，那么从左向右，一步一步积分将 <span class="math inline">\(y_t\)</span>
消掉：<br />
<span class="math display">\[
\begin{align}
\Delta_l=\sum\limits_{y_{t-1}}\phi_t(y_{t-1},y_t=i,x)\sum\limits_{y_{t-2}}\phi_{t-1}(y_{t-2},y_{t-1},x)\cdots\sum\limits_{y_0}\phi_1(y_0,y_1,x)
\end{align}
\]</span><br />
引入：<br />
<span class="math display">\[
\begin{align}
\alpha_t(i)=\Delta_l
\end{align}
\]</span><br />
这里 <span
class="math inline">\(\alpha_t(i)=y_1y_2...y_{t-1}(y_t=i)\)</span>，依此类推
<span
class="math inline">\(\alpha_{t-1}(j)=y_1y_2...y_{t-2}(y_{t-1}=j)\)</span>，于是：<br />
<span class="math display">\[
\begin{align}
\alpha_{t}(i)&amp;=\sum\limits_{y_{t-1}}\phi_t(y_{t-1}=j,y_t=i,x)\alpha_{t-1}(j)\\
&amp;=\sum\limits_{j\in S}\phi_t(y_{t-1}=j,y_t=i,x)\alpha_{t-1}(j)
\end{align}
\]</span><br />
这样我们得到了一个递推式。</p>
<p>类似地，<span
class="math inline">\(\Delta_r=\beta_t(i)=\sum\limits_{j\in
S}\phi_{t+1}(y_t=i,y_{t+1}=j,x)\beta_{t+1}(j)\)</span>。这个方法和 HMM
中的前向-后向算法类似，就是概率图模型中精确推断的变量消除算法（信念传播）。</p>
<h2 id="参数估计">参数估计</h2>
<p>在进行各种类型的推断之前，还需要对参数 <span
class="math inline">\(\theta=(\lambda,\eta)\)</span> 进行学习：<br />
<span class="math display">\[
\begin{align}\hat{\theta}&amp;=\mathop{argmax}_{\theta}\prod\limits_{i=1}^Np(y^i|x^i)\\
&amp;=\mathop{argmax}_\theta\sum\limits_{i=1}^N\log p(y^i|x^i)\\
&amp;=\mathop{argmax}_\theta\sum\limits_{i=1}^N\log
[\frac{1}{Z(x^i,\lambda,\eta)}\exp\sum\limits_{t=1}^T[\lambda^Tf(y_{t-1}^i,y_t^i,x^i)+\eta^Tg(y_t^i,x^i)]]\\
&amp;=\mathop{argmax}_\theta\sum\limits_{i=1}^N[-\log
Z(x^i,\lambda,\eta)+\sum\limits_{t=1}^T[\lambda^Tf(y_{t-1}^i,y_t^i,x^i)+\eta^Tg(y_t^i,x^i)]]\\
&amp;=\mathop{argmax}_\theta L(\lambda,\eta,x^i)
\end{align}
\]</span><br />
最优化 <span class="math inline">\(L\)</span> 函数，使用梯度上升法，求
<span class="math inline">\(\nabla_\lambda L\)</span> 和 <span
class="math inline">\(\nabla_\eta L\)</span>，下面以求 <span
class="math inline">\(\nabla_\lambda L\)</span> 为例：<br />
<span class="math display">\[
\begin{align}
\nabla_\lambda L= \sum\limits_{i=1}^N[-\nabla_\lambda \log
Z(x^i,\lambda,\eta)+\sum\limits_{t=1}^T f(y_{t-1}^i,y_t^i,x^i)]
\end{align}
\]</span><br />
上面的式子中，第一项是对数配分函数，根据指数族分布(log-partition
function)的结论：<br />
<span class="math display">\[
\begin{align}
\nabla_\lambda(\log
Z(x^i,\lambda,\eta))=\mathbb{E}_{p(y^i|x^i)}[\sum\limits_{t=1}^Tf(y_{t-1},y_t,x^i)]
\end{align}
\]</span><br />
其中，和 <span class="math inline">\(\eta\)</span>
相关的项相当于一个常数。求解这个期望值：<br />
<span class="math display">\[
\begin{align}
\mathbb{E}_{p(y^i|x^i)}[\sum\limits_{t=1}^Tf(y_{t-1},y_t,x^i)]=\sum\limits_{y}p(y|x^i)\sum\limits_{t=1}^Tf(y_{t-1},y_t,x^i)
\end{align}
\]</span><br />
第一个求和号的复杂度为 <span
class="math inline">\(O(S^T)\)</span>，重新排列求和符号：<br />
<span class="math display">\[
\begin{align}
\mathbb{E}_{p(y^i|x^i)}[\sum\limits_{t=1}^Tf(y_{t-1},y_t,x^i)]&amp;=\sum\limits_{t=1}^T\sum\limits_{y_{1:t-2}}\sum\limits_{y_{t-1}}\sum\limits_{y_t}\sum\limits_{y_{t+1:T}}p(y|x^i)f(y_{t-1},y_t,x^i)\nonumber\\
&amp;=\sum\limits_{t=1}^T\sum\limits_{y_{t-1}}\sum\limits_{y_t}(\sum\limits_{y_{1:t-2}}\sum\limits_{y_{t+1:T}}p(y|x^i)f(y_{t-1},y_t,x^i))\nonumber\\
&amp;=\sum\limits_{t=1}^T\sum\limits_{y_{t-1}}\sum\limits_{y_t}p(y_{t-1},y_t|x^i)f(y_{t-1},y_t,x^i)
\end{align}
\]</span><br />
和上面的求边缘概率 <span class="math inline">\(p(y_t|x)\)</span>
类似，这里 <span class="math inline">\(p(y_{t-1},y_t|x^i)\)</span> 的
<span class="math inline">\(y\)</span>
是两个，也可以通过前向后向算法得到上面式子中的边缘概率。</p>
<p>于是：<br />
<span class="math display">\[
\begin{align}
\nabla_\lambda
L=\sum\limits_{i=1}^N\sum\limits_{t=1}^T[f(y_{t-1},y_t,x^i)-\sum\limits_{y_{t-1}}\sum\limits_{y_t}p(y_{t-1},y_t|x^i)f(y_{t-1},y_t,x^i)]
\end{align}
\]</span><br />
利用梯度上升算法可以求解。对于 <span class="math inline">\(\eta\)</span>
也是类似的过程。</p>
<h2 id="译码">译码</h2>
<p>译码问题和 HMM 中的 Viterbi
算法类似，同样采样动态规划的思想一层一层求解最大值。</p>
<h2 id="参考文献">参考文献</h2>
<p><span class="exturl" data-url="aHR0cHM6Ly93d3cuY25ibG9ncy5jb20vcGluYXJkL3AvNzA0ODMzMy5odG1s">条件随机场CRF<i class="fa fa-external-link-alt"></i></span></p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://soundmemories.github.io/2020/10/08/ML/30.particleFilter/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avstar.png">
      <meta itemprop="name" content="SoundMemories">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="SoundMemories">
      <meta itemprop="description" content="今日事，今日毕">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | SoundMemories">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2020/10/08/ML/30.particleFilter/" class="post-title-link" itemprop="url">particleFilter</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2020-10-08 00:00:00" itemprop="dateCreated datePublished" datetime="2020-10-08T00:00:00+08:00">2020-10-08</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/ML/" itemprop="url" rel="index"><span itemprop="name">ML</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>1k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>4 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="粒子滤波">粒子滤波</h1>
<p>Kalman
滤波根据线性高斯模型可以求得解析解，但是在非线性，非高斯的情况，是无法得到解析解的，对这类一般的情况，我们叫做粒子滤波，我们需要求得概率分布，需要采用采样的方式。</p>
<p>我们希望应用 Monte Carlo
方法来进行采样，对于一个概率分布，如果我们希望计算依这个分布的某个函数
<span class="math inline">\(f(z)\)</span>
的期望，可以利用某种抽样方法，在这个概率分布中抽取 <span
class="math inline">\(N\)</span> 个样本，则 <span
class="math inline">\(\mathbb{E}[f(z)]\simeq\frac{1}{N}\sum\limits_{i=1}^Nf(z_i)\)</span>。但是如果这个概率十分复杂，那么采样比较困难。对于复杂的概率分布，我们可以通过一个简单的概率分布
<span class="math inline">\(q(z)\)</span> 作为桥梁（重要值采样）:<br />
<span class="math display">\[
\mathbb{E}[f(z)]=\int_zf(z)p(z)dz=\int_zf(z)\frac{p(z)}{q(z)}q(z)dz=\sum\limits_{i=1}^Nf(z_i)\frac{p(z_i)}{q(z_i)}
\]</span><br />
于是直接通过对 <span class="math inline">\(q(z)\)</span>
采样，然后对每一个采样的样本应用权重就得到了期望的近似，当然为了概率分布的特性，我们需要对权重进行归一化。</p>
<p>在滤波问题中，需要求解 <span
class="math inline">\(p(z_t|x_{1:t})\)</span>，其权重为：<br />
<span class="math display">\[
w_t^i=\frac{p(z_t^i|x_{1:t})}{q(z_t^i|x_{1:t})},i=1,2,\cdots,N
\]</span><br />
于是在每一个时刻 <span class="math inline">\(t\)</span>，都需要采样
<span class="math inline">\(N\)</span>
个点，但是即使采样了这么多点，分子上面的那一项也十分难求，于是希望找到一个关于权重的递推公式。为了解决这个问题，引入序列重要性采样（SIS）。</p>
<h2 id="sis">SIS</h2>
<p>在 SIS 中，解决的问题是 <span
class="math inline">\(p(z_{1:t}|x_{1:t})\)</span>。<br />
<span class="math display">\[
w_t^i\propto\frac{p(z_{1:t}|x_{1:t})}{q(z_{1:t}|x_{1:t})}
\]</span><br />
根据 LDS 中的推导：<br />
<span class="math display">\[
\begin{align}p(z_{1:t}|x_{1:t})\propto
p(x_{1:t},z_{1:t})&amp;=p(x_t|z_{1:t},x_{1:t-1})p(z_{1:t},x_{1:t-1})\nonumber\\
&amp;=p(x_t|z_t)p(z_t|x_{1:t-1},z_{1:t-1})p(x_{1:t-1},z_{1:t-1})\nonumber\\
&amp;=p(x_t|z_t)p(z_t|z_{t-1})p(x_{1:t-1},z_{1:t-1})\nonumber\\
&amp;\propto p(x_t|z_t)p(z_t|z_{t-1})p(z_{1:t-1}|x_{1:t-1})
\end{align}
\]</span><br />
于是分子的递推式就得到了。对于提议分布的分母，可以取：<br />
<span class="math display">\[
q(z_{1:t}|x_{1:t})=q(z_t|z_{1:t-1},x_{1:t})q(z_{1:t-1}|x_{1:t-1})
\]</span><br />
所以有：<br />
<span class="math display">\[
w_t^i\propto\frac{p(z_{1:t}|x_{1:t})}{q(z_{1:t}|x_{1:t})}\propto
\frac{p(x_t|z_t)p(z_t|z_{t-1})p(z_{1:t-1}|x_{1:t-1})}{q(z_t|z_{1:t-1},x_{1:t})q(z_{1:t-1}|x_{1:t-1})}=\frac{p(x_t|z_t)p(z_t|z_{t-1})}{q(z_t|z_{1:t-1},x_{1:t})}w_{t-1}^i
\]</span><br />
我们得到的对权重的算法为：</p>
<ol type="1">
<li><span class="math inline">\(t-1\)</span>
时刻，采样完成并计算得到权重</li>
<li>t 时刻，根据 <span
class="math inline">\(q(z_t|z_{1:t-1},x_{1:t})\)</span> 进行采样得到
<span class="math inline">\(z_t^i\)</span>。然后计算得到 <span
class="math inline">\(N\)</span> 个权重。</li>
<li>最后对权重归一化。</li>
</ol>
<p>SIS
算法会出现权值退化的情况，在一定时间后，可能会出现大部分权重都逼近0的情况，这是由于空间维度越来越高，需要的样本也越来越多。解决这个问题的方法有：</p>
<ol type="1">
<li>重采样，以权重作为概率分布，重新在已经采样的样本中采样，然后所有样本的权重相同，这个方法的思路是将权重作为概率分布，然后得到累积密度函数，在累积密度上取点（阶梯函数）。</li>
<li>选择一个合适的提议分布，<span
class="math inline">\(q(z_t|z_{1:t-1},x_{1:t})=p(z_t|z_{t-1})\)</span>，于是就消掉了一项，并且采样的概率就是
<span
class="math inline">\(p(z_t|z_{t-1})\)</span>，这就叫做生成与测试方法。</li>
</ol>
<p>采用重采样的 SIS
算法就是基本的粒子滤波算法。如果像上面那样选择提议分布，这个算法叫做 SIR
算法。</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://soundmemories.github.io/2020/10/07/ML/29.LDS/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avstar.png">
      <meta itemprop="name" content="SoundMemories">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="SoundMemories">
      <meta itemprop="description" content="今日事，今日毕">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | SoundMemories">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2020/10/07/ML/29.LDS/" class="post-title-link" itemprop="url">LDS</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2020-10-07 00:00:00" itemprop="dateCreated datePublished" datetime="2020-10-07T00:00:00+08:00">2020-10-07</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/ML/" itemprop="url" rel="index"><span itemprop="name">ML</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>712</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>3 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="线性动态系统">线性动态系统</h1>
<p>HMM 模型适用于隐变量是离散的值的时候，对于连续隐变量的
HMM，常用线性动态系统描述线性高斯模型的态变量，使用粒子滤波来表述非高斯非线性的态变量。</p>
<p>LDS
又叫卡尔曼滤波，其中，线性体现在上一时刻和这一时刻的隐变量以及隐变量和观测之间：<br />
<span class="math display">\[
\begin{align}
z_t&amp;=A\cdot z_{t-1}+B+\varepsilon\\
x_t&amp;=C\cdot z_t+D+\delta\\
\varepsilon&amp;\sim\mathcal{N}(0,Q)\\
\delta&amp;\sim\mathcal{N}(0,R)
\end{align}
\]</span><br />
类比 HMM 中的几个参数：<br />
<span class="math display">\[
\begin{align}
p(z_t|z_{t-1})&amp;\sim\mathcal{N}(A\cdot z_{t-1}+B,Q)\\
p(x_t|z_t)&amp;\sim\mathcal{N}(C\cdot z_t+D,R)\\
z_1&amp;\sim\mathcal{N}(\mu_1,\Sigma_1)
\end{align}
\]</span><br />
在含时的概率图中，除了对参数估计的学习问题外，在推断任务中，包括译码，证据概率，滤波，平滑，预测问题，LDS
更关心滤波这个问题：<span
class="math inline">\(p(z_t|x_1,x_2,\cdots,x_t)\)</span>。类似 HMM
中的前向算法，我们需要找到一个递推关系。<br />
<span class="math display">\[
p(z_t|x_{1:t})=p(x_{1:t},z_t)/p(x_{1:t})=Cp(x_{1:t},z_t)
\]</span><br />
对于 <span class="math inline">\(p(x_{1:t},z_t)\)</span>：<br />
<span class="math display">\[
\begin{align}p(x_{1:t},z_t)&amp;=p(x_t|x_{1:t-1},z_t)p(x_{1:t-1},z_t)=p(x_t|z_t)p(x_{1:t-1},z_t)\nonumber\\
&amp;=p(x_t|z_t)p(z_t|x_{1:t-1})p(x_{1:t-1})=Cp(x_t|z_t)p(z_t|x_{1:t-1})\\
\end{align}
\]</span><br />
我们看到，右边除了只和观测相关的常数项，还有一项是预测任务需要的概率。对这个值：<br />
<span class="math display">\[
\begin{align}
p(z_t|x_{1:t-1})&amp;=\int_{z_{t-1}}p(z_t,z_{t-1}|x_{1:t-1})dz_{t-1}\nonumber\\
&amp;=\int_{z_{t-1}}p(z_t|z_{t-1},x_{1:t-1})p(z_{t-1}|x_{1:t-1})dz_{t-1}\nonumber\\
&amp;=\int_{z_{t-1}}p(z_t|z_{t-1})p(z_{t-1}|x_{1:t-1})dz_{t-1}
\end{align}
\]</span><br />
我们看到，这又化成了一个滤波问题。于是我们得到了一个递推公式：</p>
<ol type="1">
<li><span class="math inline">\(t=1\)</span>，<span
class="math inline">\(p(z_1|x_1)\)</span>，称为 update 过程，然后计算
<span
class="math inline">\(p(z_2|x_1)\)</span>，通过上面的积分进行，称为
prediction 过程。</li>
<li><span class="math inline">\(t=2\)</span>，<span
class="math inline">\(p(z_2|x_2,x_1)\)</span> 和 <span
class="math inline">\(p(z_3|x_1,x_2)\)</span></li>
</ol>
<p>我们看到，这个过程是一个 Online
的过程，对于我们的线性高斯假设，这个计算过程都可以得到解析解。</p>
<ol type="1">
<li><p>Prediction：<br />
<span class="math display">\[
p(z_t|x_{1:t-1})=\int_{z_{t-1}}p(z_t|z_{t-1})p(z_{t-1}|x_{1:t-1})dz_{t-1}=\int_{z_{t-1}}\mathcal{N}(Az_{t-1}+B,Q)\mathcal{N}(\mu_{t-1},\Sigma_{t-1})dz_{t-1}
\]</span><br />
其中第二个高斯分布是上一步的 Update
过程，所以根据线性高斯模型，直接可以写出这个积分：<br />
<span class="math display">\[
p(z_t|x_{1:t-1})=\mathcal{N}(A\mu_{t-1}+B,Q+A\Sigma_{t-1}A^T)
\]</span></p></li>
<li><p>Update:<br />
<span class="math display">\[
p(z_t|x_{1:t})\propto p(x_t|z_t)p(z_t|x_{1:t-1})
\]</span><br />
同样利用线性高斯模型，也可以直接写出这个高斯分布。</p></li>
</ol>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://soundmemories.github.io/2020/10/06/ML/28.HMM/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avstar.png">
      <meta itemprop="name" content="SoundMemories">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="SoundMemories">
      <meta itemprop="description" content="今日事，今日毕">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | SoundMemories">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2020/10/06/ML/28.HMM/" class="post-title-link" itemprop="url">HMM</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2020-10-06 00:00:00" itemprop="dateCreated datePublished" datetime="2020-10-06T00:00:00+08:00">2020-10-06</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/ML/" itemprop="url" rel="index"><span itemprop="name">ML</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>2.8k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>10 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="隐马尔可夫模型">隐马尔可夫模型</h1>
<p>隐马尔可夫模型是一种概率图模型。我们知道，机器学习模型可以从频率派和贝叶斯派两个方向考虑，<strong>在频率派的方法中的核心是优化问题</strong>；<strong>而在贝叶斯派的方法中，核心是积分问题（求后验概率）</strong>，贝叶斯派也发展出来了一系列的积分方法如变分推断，MCMC
等。</p>
<p>概率图模型最基本的模型可以分为有向图（贝叶斯网络）和无向图（马尔可夫随机场）两个方面，例如
GMM。如果在这些基本的模型上，样本之间存在关联，可以认为样本中附带了时序信息，从而样本之间不独立全同分布的，这种模型就叫做动态模型，<strong>隐变量随着时间发生变化，于是观测变量也发生变化（z为隐状态，x为观测值）</strong>：</p>
<pre class="mermaid">
graph LR
z1--&gt;z2--&gt;z3;
   x1--&gt;x2--&gt;x3;
</pre>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">|-概率图</span><br><span class="line">    |-有向：Bayesiam Network</span><br><span class="line">    |-无向：Markov Random Field(Markov Network)</span><br><span class="line">    </span><br><span class="line">|-概率图+时间time：Dynamic Model</span><br><span class="line">    |-HMM(状态变量（隐变量）是离散的)</span><br><span class="line">    |-Kalman Filter(状态变量是连续的，线性的)</span><br><span class="line">    |-Particle Filter(状态变量是连续的，非线性的)</span><br></pre></td></tr></table></figure>
<h1 id="hmm">HMM</h1>
<p>HMM 用概率图表示为：</p>
<pre class="mermaid">
graph TD
z1--&gt;z2;
subgraph four
	z4--&gt;x4((x4))
end
subgraph three
	z3--&gt;x3((x3))
end
subgraph two
	z2--&gt;x2((x2))
end
subgraph one
	z1--&gt;x1((x1))
end

z2--&gt;z3;
z3--&gt;z4;
</pre>
<p>上图表示了四个时刻的隐变量变化。用参数 <span
class="math inline">\(\lambda=(\pi,A,B)\)</span> 来表示，其中 <span
class="math inline">\(\pi\)</span> 是开始的概率分布，<span
class="math inline">\(A\)</span> 为状态转移矩阵，<span
class="math inline">\(B\)</span> 为发射矩阵（观测矩阵）。</p>
<p><span class="math inline">\(o_t\)</span> 表示观测变量，<span
class="math inline">\(V=\{v_1,v_2,\cdots,v_M\}\)</span>
表示观测变量的值域，<span class="math inline">\(O\)</span>
为观测序列。<br />
<span class="math inline">\(i_t\)</span> 表示状态变量，<span
class="math inline">\(Q=\{q_1,q_2,\cdots,q_N\}\)</span>
表示状态变量的值域，<span class="math inline">\(I\)</span>
为状态序列。<br />
定义 <span
class="math inline">\(A=(a_{ij}=p(i_{t+1}=q_j|i_t=q_i))\)</span>
表示状态转移矩阵，<span
class="math inline">\(B=(b_j(k)=p(o_t=v_k|i_t=q_j))\)</span>
表示发射矩阵。</p>
<p>在 HMM 中，有两个基本假设：</p>
<ol type="1">
<li><p>齐次 Markov 假设（未来只依赖于当前）：<br />
<span class="math display">\[
p(i_{t+1}|i_t,i_{t-1},\cdots,i_1,o_t,o_{t-1},\cdots,o_1)=p(i_{t+1}|i_t)
\]</span></p></li>
<li><p>观测独立假设：<br />
<span class="math display">\[
p(o_t|i_t,i_{t-1},\cdots,i_1,o_{t-1},\cdots,o_1)=p(o_t|i_t)
\]</span></p></li>
</ol>
<p>HMM 要解决三个问题：</p>
<ol type="1">
<li>Evaluation：已知模型和观测序列，计算在该模型下观测序列出现的概率。<span
class="math inline">\(p(O|\lambda)\)</span> -&gt; Forward-Backward
算法</li>
<li>Learning：已知观测序列，估计模型参数，使得该模型下观测序列概率最大。<span
class="math inline">\(\lambda=\mathop{argmax}\limits_{\lambda}p(O|\lambda)\)</span>
<ol type="1">
<li>有监督学习（训练数据包含状态序列）-&gt; 最大似然估计</li>
<li>无监督学习（训练数据不包含状态序列）-&gt; EM 算法（Baum-Welch）</li>
</ol></li>
<li>Decoding：已知模型参数和观测序列，求最有可能对应的状态序列。<span
class="math inline">\(I=\mathop{argmax}\limits_{I}p(I|O,\lambda)\)</span>
-&gt; Vierbi 算法
<ol type="1">
<li>预测问题：<span
class="math inline">\(p(i_{t+1}|o_1,o_2,\cdots,o_t)\)</span>，已知 <span
class="math inline">\(t\)</span> 时刻的观测序列，求下一时刻的状态。</li>
<li>滤波问题：<span
class="math inline">\(p(i_t|o_1,o_2,\cdots,o_t)\)</span>，已知 <span
class="math inline">\(t\)</span> 时刻的观测序列，求 <span
class="math inline">\(t\)</span> 时刻的状态。</li>
</ol></li>
</ol>
<h2 id="evaluation">Evaluation</h2>
<h3 id="直接计算法">直接计算法</h3>
<p><span class="math display">\[
p(O|\lambda)=\sum\limits_{I}p(I,O|\lambda)=\sum\limits_{I}p(O|I,\lambda)p(I|\lambda)
\]</span></p>
<p><span class="math display">\[
p(I|\lambda)=p(i_1,i_2,\cdots,i_t|\lambda)=p(i_t|i_1,i_2,\cdots,i_{t-1},\lambda)p(i_1,i_2,\cdots,i_{t-1}|\lambda)
\]</span></p>
<p>根据齐次 Markov 假设：<br />
<span class="math display">\[
p(i_t|i_1,i_2,\cdots,i_{t-1},\lambda)=p(i_t|i_{t-1})=a_{i_{t-1}i_t}
\]</span><br />
所以：<br />
<span class="math display">\[
p(I|\lambda)=\pi_1\prod\limits_{t=2}^Ta_{i_{t-1},i_t}
\]</span><br />
又由于：<br />
<span class="math display">\[
p(O|I,\lambda)=\prod\limits_{t=1}^Tb_{i_t}(o_t)
\]</span><br />
于是：<br />
<span class="math display">\[
p(O|\lambda)=\sum\limits_{I}\pi_{i_1}\prod\limits_{t=2}^Ta_{i_{t-1},i_t}\prod\limits_{t=1}^Tb_{i_t}(o_t)
\]</span><br />
我们看到，上面的式子中的求和符号是对状态序列求和，于是复杂度为 <span
class="math inline">\(O(TN^T)\)</span>，这种算法不可行。</p>
<h3 id="前向算法">前向算法</h3>
<p>下面，记 <span
class="math inline">\(\alpha_t(i)=p(o_1,o_2,\cdots,o_t,i_t=q_i|\lambda)\)</span>
为（<span class="math inline">\(t\)</span> 时刻状态序列+ <span
class="math inline">\(t\)</span> 时刻前所有观测序列）的联合概率。<br />
所以整个序列最后一项可写为：<span
class="math inline">\(\alpha_T(i)=p(O,i_T=q_i|\lambda)\)</span>。那么对所有状态序列求和可得：<br />
<span class="math display">\[
p(O|\lambda)=\sum\limits_{i=1}^Np(O,i_T=q_i|\lambda)=\sum\limits_{i=1}^N\alpha_T(i)
\]</span><br />
对 <span class="math inline">\(\alpha_{t+1}(j)\)</span>：<br />
<span class="math display">\[
\begin{align}\alpha_{t}(i) \to
\alpha_{t+1}(j)&amp;=p(o_1,o_2,\cdots,o_{t+1},i_{t+1}=q_j|\lambda)\nonumber\\
&amp;=\sum\limits_{i=1}^Np(o_1,o_2,\cdots,o_{t},o_{t+1},i_t=q_i,i_{t+1}=q_j|\lambda)\nonumber\\
&amp;=\sum\limits_{i=1}^Np(o_{t+1}|o_1,o_2,\cdots,o_{t},i_t=q_i,i_{t+1}=q_j,\lambda)p(o_1,\cdots,o_t,i_t=q_i,i_{t+1}=q_j|\lambda)
\end{align}
\]</span><br />
利用观测独立假设+齐次 Markov 假设：<br />
<span class="math display">\[
\begin{align}\alpha_{t}(i) \to
\alpha_{t+1}(j)&amp;=\sum\limits_{i=1}^Np(o_{t+1}|i_{t+1}=q_j)p(o_1,\cdots,o_t,i_t=q_i,i_{t+1}=q_j|\lambda)\nonumber\\
&amp;=\sum\limits_{i=1}^Np(o_{t+1}|i_{t+1}=q_j)p(i_{t+1}=q_j|o_1,\cdots,o_t,i_t=q_i,\lambda)p(o_1,\cdots,o_t,i_t=q_i|\lambda)\nonumber\\
&amp;=\sum\limits_{i=1}^Np(o_{t+1}|i_{t+1}=q_j)p(i_{t+1}=q_j|i_{t}=q_i)\alpha_t(i)\nonumber\\
&amp;=\sum\limits_{i=1}^Nb_{j}(o_{t+1})a_{ij}\alpha_t(i)
\end{align}
\]</span><br />
上面利用了齐次 Markov 假设得到了一个递推公式，这个算法叫做前向算法。</p>
<h3 id="后向算法">后向算法</h3>
<p>下面，记 <span
class="math inline">\(\beta_t(i)=p(o_{t+1},o_{t+2},\cdots，o_T|i_t=q_i,\lambda)\)</span>
为（<span class="math inline">\(t\)</span> 时刻状态序列+ <span
class="math inline">\(t\)</span> 时刻后所有观测序列）的联合概率。<br />
进一步可写为：<span
class="math inline">\(\beta_t(i)=\sum\limits_{j=1}^Nb_j(o_{t+1})a_{ij}\beta_{t+1}(j)\)</span>。那么对所有状态序列求和可得：：<br />
<span class="math display">\[
\begin{align}p(O|\lambda)&amp;=p(o_1,\cdots,o_T|\lambda)\nonumber\\
&amp;=\sum\limits_{i=1}^Np(o_1,o_2,\cdots,o_T,i_1=q_i|\lambda)\nonumber\\
&amp;=\sum\limits_{i=1}^Np(o_1,o_2,\cdots,o_T|i_1=q_i,\lambda)p(i_1=q_i,|\lambda)\nonumber\\
&amp;=\sum\limits_{i=1}^Np(o_1,o_2,\cdots,o_T|i_1=q_i,\lambda)\pi_i\nonumber\\
&amp;=\sum\limits_{i=1}^Np(o_1|o_2,\cdots,o_T,i_1=q_i,\lambda)p(o_2,\cdots,o_T|i_1=q_i,\lambda)\pi_i\nonumber\\
&amp;=\sum\limits_{i=1}^Nb_i(o_1)\beta_1(i)\pi_i
\end{align}
\]</span><br />
对于这个 <span class="math inline">\(\beta_1(i)\)</span> 可通过 <span
class="math inline">\(\beta_t(i)\)</span> 得到，利用观测序列只与 <span
class="math inline">\(i_{t+1}\)</span>有关，化简<span
class="math inline">\(p(O|i_{t+1},i_t)\)</span>：<br />
<span class="math display">\[
\begin{align}\beta_{t+1}(j) \to
\beta_t(i)&amp;=p(o_{t+1},\cdots,o_T|i_t=q_i,\lambda)\nonumber\\
&amp;=\sum\limits_{j=1}^Np(o_{t+1},\cdots,o_T,i_{t+1}=q_j|i_t=q_i,\lambda)\nonumber\\
&amp;=\sum\limits_{j=1}^Np(o_{t+1},\cdots,o_T|i_{t+1}=q_j,i_t=q_i,\lambda)p(i_{t+1}=q_j|i_t=q_i,\lambda)\nonumber\\
&amp;=\sum\limits_{j=1}^Np(o_{t+1},\cdots,o_T|i_{t+1}=q_j,\lambda)a_{ij}\nonumber\\
&amp;=\sum\limits_{j=1}^Np(o_{t+1}|o_{t+2},\cdots,o_T,i_{t+1}=q_j,\lambda)p(o_{t+2},\cdots,o_T|i_{t+1}=q_j,\lambda)a_{ij}\nonumber\\
&amp;=\sum\limits_{j=1}^Nb_j(o_{t+1})\beta_{t+1}(j)a_{ij}
\end{align}
\]</span><br />
于是后向地得到了第一项。</p>
<h2 id="learning">Learning</h2>
<p>为了学习得到参数的最优值，在 MLE 中：<br />
<span class="math display">\[
\lambda_{MLE}=\mathop{argmax}_\lambda p(O|\lambda)
\]</span><br />
我们采用 EM 算法（在这里也叫 Baum Welch
算法），用上标表示迭代（时刻）：<br />
<span class="math display">\[
\theta^{t+1}=\mathop{argmax}_{\theta}\int_z\log
p(X,Z|\theta)p(Z|X,\theta^t)dz
\]</span><br />
其中，<span class="math inline">\(X\)</span> 是观测变量，<span
class="math inline">\(Z\)</span> 是隐变量序列。对应替换成HMM变量：<br />
<span class="math display">\[
\lambda^{t+1}=\mathop{argmax}_\lambda\sum\limits_I\log
p(O,I|\lambda)p(I|O,\lambda^t)\\
=\mathop{argmax}_\lambda\sum\limits_I\log p(O,I|\lambda)p(O,I|\lambda^t)
\]</span><br />
上式 <span
class="math inline">\(p(I|O,\lambda^t)=p(O,I|\lambda^t)/p(O|\lambda^t)\)</span>
利用了 <span class="math inline">\(p(O|\lambda^t)\)</span> 和<span
class="math inline">\(\lambda\)</span> 无关为常量。</p>
<p>定义 <span class="math inline">\(Q(\lambda,\lambda^t)\)</span>
函数：<br />
<span class="math display">\[
Q(\lambda,\lambda^t)=\sum\limits_I\log p(O,I|\lambda)p(O,I|\lambda^t)
\]</span><br />
将 Evaluation 中的直接计算式子代入：<br />
<span class="math display">\[
\sum\limits_I\log p(O,I|\lambda)p(O,I|\lambda^t)=\sum\limits_I[(\log
\pi_{i_1}+\sum\limits_{t=2}^T\log
a_{i_{t-1},i_t}+\sum\limits_{t=1}^T\log b_{i_t}(o_t))p(O,I|\lambda^t)]
\]</span></p>
<p>因为求参数 <span
class="math inline">\(\lambda^t=(\pi^t,A^t,B^t)\)</span>，有三个参数但求解方法一样这里以对求
<span class="math inline">\(\pi^{t+1}\)</span> 求解为例，它与后面 <span
class="math inline">\(a_{i_{t-1},i_t}\)</span> 和 <span
class="math inline">\(b_{i_t}(o_t)\)</span> 不相关：<br />
<span class="math display">\[
\begin{align}\pi^{t+1}&amp;=\mathop{argmax}_\pi Q(\lambda,\lambda^t)
\nonumber\\
&amp;=\mathop{argmax}_\pi\sum\limits_I[\log
\pi_{i_1}p(O,I|\lambda^t)]\nonumber\\
&amp;=\mathop{argmax}_\pi\sum\limits_{i_1}\cdots \sum\limits_{i_T}[\log
\pi_{i_1}\cdot p(O,i_1,i_2,\cdots,i_T|\lambda^t)]
\end{align}
\]</span><br />
上面的式子中，对 <span class="math inline">\(i_2,\cdots,i_T\)</span>
求和可以将这些参数消掉：<br />
<span class="math display">\[
\begin{aligned}
\pi^{t+1}&amp;=\mathop{argmax}_\pi\sum\limits_{i_1}[\log \pi_{i_1}\cdot
p(O,i_1|\lambda^t)]\\
&amp;=\mathop{argmax}_\pi \sum\limits_{i=1}^N[\log \pi_{i}\cdot
p(O,i_1=q_i|\lambda^t)]
\end{aligned}
\]</span><br />
上面的式子还有对 <span class="math inline">\(\pi\)</span> 的约束 <span
class="math inline">\(\sum\limits_i\pi_i=1\)</span>。使用拉格朗日乘子法，定义
Lagrange 函数：<br />
<span class="math display">\[
L(\pi,\eta)=\sum\limits_{i=1}^N\log \pi_i\cdot
p(O,i_1=q_i|\lambda^t)+\eta(\sum\limits_{i=1}^N\pi_i-1)
\]</span><br />
于是：<br />
<span class="math display">\[
\begin{aligned}
\frac{\partial
L}{\partial\pi_i}&amp;=\frac{1}{\pi_i}p(O,i_1=q_i|\lambda^t)+\eta=0\\
&amp;\Rightarrow p(O,i_1=q_i|\lambda^t)+\pi_i\eta=0
\end{aligned}
\]</span><br />
对上式 <span class="math inline">\(i\)</span> 求和消去 <span
class="math inline">\(q_i\)</span> 和 <span
class="math inline">\(\pi_i\)</span>：<br />
<span class="math display">\[
\begin{aligned}
&amp;\sum\limits_{i=1}^N[p(O,i_1=q_i|\lambda^t)+\pi_i\eta]=0\\
&amp;\Rightarrow p(O|\lambda^t)+\eta=0\\
&amp;\Rightarrow\eta=-p(O|\lambda^t)
\end{aligned}
\]</span><br />
所以：<br />
<span class="math display">\[
\pi_i^{t+1}=\frac{p(O,i_1=q_i|\lambda^t)}{p(O|\lambda^t)}
\]</span></p>
<h2 id="decoding">Decoding</h2>
<p>Decoding 问题表述为：<br />
<span class="math display">\[
I=\mathop{argmax}\limits_{I}p(I|O,\lambda)
\]</span><br />
我们需要找到一个状态序列，其概率最大，这个序列就是在参数空间中的一个路径，可以采用动态规划的思想。</p>
<p>定义在 <span class="math inline">\(t\)</span> 时刻状态为 <span
class="math inline">\(i\)</span> 的所有单个路径中概率最大值：<br />
<span class="math display">\[
\delta_{t}(i)=\max\limits_{i_1,\cdots,i_{t-1}}p(o_1,\cdots,o_t,i_1,\cdots,i_{t-1},i_t=q_i)
\]</span><br />
由定义可得其递推公式：<br />
<span class="math display">\[
\delta_{t+1}(j)=\max\limits_{1\le i\le N}\delta_t(i)a_{ij}b_j(o_{t+1})
\]</span><br />
从上一步到下一步的概率再求最大值，记这个路径为：<br />
<span class="math display">\[
\psi_{t+1}(j)=\mathop{argmax}\limits_{1\le i\le N}\delta_t(i)a_{ij}
\]</span></p>
<h1 id="小结">小结</h1>
<p>HMM 是一种动态模型，是由混合树形模型和时序结合起来的一种模型（类似
GMM + Time）。对于类似 HMM 的这种状态空间模型，普遍的除了学习任务（采用
EM ）外，还有推断任务，推断任务包括：</p>
<ol type="1">
<li><p>译码 Decoding：<span
class="math inline">\(p(z_1,z_2,\cdots,z_t|x_1,x_2,\cdots,x_t)\)</span></p></li>
<li><p>似然 Prob of evidence：<span
class="math inline">\(p(x_1,x_2,\cdots,x_t|\theta)\)</span></p></li>
<li><p>滤波 Filtering：$ p(z_t|x_1,,x_t)$，Online<br />
<span class="math display">\[
p(z_t|x_{1:t})=\frac{p(x_{1:t},z_t)}{p(x_{1:t})}\propto
p(x_{1:t},z_t)=\alpha_t (前向算法)
\]</span><br />
记 <span
class="math inline">\(\alpha_t=p(x_1,\cdots,x_t,z_t)\)</span>，<span
class="math inline">\(\beta_t=p(x_{t+1},\cdots,x_T|z_t)\)</span>。</p></li>
<li><p>平滑 Smoothing：<span
class="math inline">\(p(z_t|x_1,\cdots,x_T)\)</span>，Offline<br />
<span class="math display">\[
p(z_t|x_{1:T})=\frac{p(x_{1:T},z_t)}{p(x_{1:T})}=\frac{p(x_{1:t},x_{t+1:T},z_t)}{p(x_{1:T})}=\frac{p(x_{1:t},z_t)p(x_{t+1:T}|x_{1:t},z_t)}{p(x_{1:T})}=\frac{\alpha_t
p(x_{t+1:T}|x_{1:t},z_t)}{p(x_{1:T})}
\]</span><br />
根据概率图的条件独立性 <span class="math inline">\(x_{t+1:T}\)</span> 和
<span class="math inline">\(x_{1:t}\)</span> 相互独立，有 <span
class="math inline">\(p(x_{t+1:T}|x_{1:t},z_t)=p(x_{t+1:T}|z_t)\)</span>
：<br />
<span class="math display">\[
p(z_t|x_{1:T})=\frac{\alpha_t p(x_{t+1:T}|z_t)}{p(x_{1:T})}\propto
\alpha_t\beta_t
\]</span><br />
这个算法叫做前向后向算法。</p></li>
<li><p>预测 Prediction：预测下一时刻隐状态<span
class="math inline">\(p(z_{t+1},z_{t+2}|x_1,\cdots,x_t)\)</span>，预测下一时刻观测状态<span
class="math inline">\(p(x_{t+1},x_{t+2}|x_1,\cdots,x_t)\)</span><br />
<span class="math display">\[
p(z_{t+1}|x_{1:t})=\sum_{z_t}p(z_{t+1},z_t|x_{1:t})=\sum\limits_{z_t}p(z_{t+1}|z_t,x_{1:t})p(z_t|x_{1:t})=\sum\limits_{z_t}p(z_{t+1}|z_t)p(z_t|x_{1:t})
\]</span><br />
其中 <span class="math inline">\(p(z_t|x_{1:t})\)</span>
是Filtering问题。<br />
<span class="math display">\[
p(x_{t+1}|x_{1:t})=\sum\limits_{z_{t+1}}p(x_{t+1},z_{t+1}|x_{1:t})=\sum\limits_{z_{t+1}}p(x_{t+1}|z_{t+1},x_{1:t})p(z_{t+1}|x_{1:t})=\sum\limits_{z_{t+1}}p(x_{t+1}|z_{t+1})p(z_{t+1}|x_{1:t})
\]</span><br />
其中 <span class="math inline">\(p(z_{t+1}|x_{1:t})\)</span>
是上一个问题。</p></li>
</ol>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




  <nav class="pagination">
    <a class="extend prev" rel="prev" title="上一页" aria-label="上一页" href="/page/3/"><i class="fa fa-angle-left"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/3/">3</a><span class="page-number current">4</span><a class="page-number" href="/page/5/">5</a><span class="space">&hellip;</span><a class="page-number" href="/page/13/">13</a><a class="extend next" rel="next" title="下一页" aria-label="下一页" href="/page/5/"><i class="fa fa-angle-right"></i></a>
  </nav>

</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="copyright">
    &copy; 2019 – 
    <span itemprop="copyrightYear">2023</span>
    <span class="with-love">
      <i class="fa fa-heart"></i>
    </span>
    <span class="author" itemprop="copyrightHolder">SoundMemories</span>
  </div>
  <div class="powered-by">由 <span class="exturl" data-url="aHR0cHM6Ly9oZXhvLmlv">Hexo</span> & <span class="exturl" data-url="aHR0cHM6Ly90aGVtZS1uZXh0LmpzLm9yZy9tdXNlLw==">NexT.Muse</span> 强力驱动
  </div>

    </div>
  </footer>

  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>
  <div class="sidebar-dimmer"></div>
  <div class="back-to-top" role="button" aria-label="返回顶部">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>
  <a role="button" class="book-mark-link book-mark-link-fixed"></a>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/next-theme-pjax/0.6.0/pjax.min.js" integrity="sha256-vxLn1tSKWD4dqbMRyv940UYw4sXgMtYcK6reefzZrao=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.7.0/jquery.min.js" integrity="sha256-2Pmvv0kuTBOenSvLm6bvfBSSHrUJ+3A7x6P5Ebd07/g=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.js" integrity="sha256-yt2kYMy0w8AbtF89WXb2P1rfjcP/HTHLT7097U8Y5b8=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/medium-zoom/1.0.8/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/muse.js"></script><script src="/js/next-boot.js"></script><script src="/js/bookmark.js"></script><script src="/js/pjax.js"></script>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-generator-searchdb/1.4.1/search.js" integrity="sha256-1kfA5uHPf65M5cphT2dvymhkuyHPQp5A53EGZOnOLmc=" crossorigin="anonymous"></script>
<script src="/js/third-party/search/local-search.js"></script>

  <script class="next-config" data-name="pdf" type="application/json">{"object_url":{"url":"https://cdnjs.cloudflare.com/ajax/libs/pdfobject/2.2.12/pdfobject.min.js","integrity":"sha256-g2xji1rlE3KsGVClvuxTbcR0Kn2+wtQADSff2Tbb4zA="},"url":"/lib/pdf/web/viewer.html"}</script>
  <script src="/js/third-party/tags/pdf.js"></script>

  <script class="next-config" data-name="mermaid" type="application/json">{"enable":true,"theme":{"light":"neutral","dark":"neutral"},"js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mermaid/10.2.3/mermaid.min.js","integrity":"sha256-JFptYy4KzJ5OQP+Q9fubNf3cxpPPmZKqUOovyEONKrQ="}}</script>
  <script src="/js/third-party/tags/mermaid.js"></script>


  <script src="/js/third-party/fancybox.js"></script>



  




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"none","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>


  <script src="https://cdnjs.cloudflare.com/ajax/libs/quicklink/2.3.0/quicklink.umd.js" integrity="sha256-yvJQOINiH9fWemHn0vCA5lsHWJaHs6/ZmO+1Ft04SvM=" crossorigin="anonymous"></script>
  <script class="next-config" data-name="quicklink" type="application/json">{"enable":true,"home":true,"archive":true,"delay":true,"timeout":3000,"priority":true,"url":"https://soundmemories.github.io/page/4/"}</script>
  <script src="/js/third-party/quicklink.js"></script>

</body>
</html>
