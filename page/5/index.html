<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222" media="(prefers-color-scheme: light)">
<meta name="theme-color" content="#222" media="(prefers-color-scheme: dark)"><meta name="generator" content="Hexo 6.3.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="https://fonts.loli.net/css?family=Monda:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">

<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css" integrity="sha256-HtsXJanqjKTc8vVQjO4YMhiqFoXkfBsjBWcX91T1jr8=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.css" integrity="sha256-Vzbj7sDDS/woiFS3uNKo8eIuni59rjyNGtXfstRzStA=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"soundmemories.github.io","root":"/","images":"/images","scheme":"Muse","darkmode":true,"version":"8.17.1","exturl":true,"sidebar":{"position":"left","display":"always","padding":18,"offset":12},"copycode":{"enable":true,"style":null},"bookmark":{"enable":true,"color":"#222","save":"auto"},"mediumzoom":true,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"},"path":"/search.xml","localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false}}</script><script src="/js/config.js"></script>

    <meta name="description" content="今日事，今日毕">
<meta property="og:type" content="website">
<meta property="og:title" content="SoundMemories">
<meta property="og:url" content="https://soundmemories.github.io/page/5/index.html">
<meta property="og:site_name" content="SoundMemories">
<meta property="og:description" content="今日事，今日毕">
<meta property="og:locale" content="zh_CN">
<meta property="article:author" content="SoundMemories">
<meta name="twitter:card" content="summary">


<link rel="canonical" href="https://soundmemories.github.io/page/5/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":true,"isPost":false,"lang":"zh-CN","comments":"","permalink":"","path":"page/5/index.html","title":""}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>SoundMemories</title>
  








  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <h1 class="site-title">SoundMemories</h1>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="搜索" role="button">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签<span class="badge">8</span></a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类<span class="badge">8</span></a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档<span class="badge">122</span></a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup"><div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" maxlength="80"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close" role="button">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="search-result-container no-result">
  <div class="search-result-icon">
    <i class="fa fa-spinner fa-pulse fa-5x"></i>
  </div>
</div>

    </div>
  </div>

</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-overview-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="SoundMemories"
      src="/images/avstar.png">
  <p class="site-author-name" itemprop="name">SoundMemories</p>
  <div class="site-description" itemprop="description">今日事，今日毕</div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">122</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">8</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">8</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author animated">
      <span class="links-of-author-item">
        <span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL3NvdW5kbWVtb3JpZXM=" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;soundmemories"><i class="fab fa-github fa-fw"></i>GitHub</span>
      </span>
      <span class="links-of-author-item">
        <span class="exturl" data-url="bWFpbHRvOnNvdW5kbWVtb3JpZXNAMTYzLmNvbQ==" title="E-Mail → mailto:soundmemories@163.com"><i class="fa fa-envelope fa-fw"></i>E-Mail</span>
      </span>
  </div>
  <div class="cc-license animated" itemprop="license">
    <span class="exturl cc-opacity" data-url="aHR0cHM6Ly9jcmVhdGl2ZWNvbW1vbnMub3JnL2xpY2Vuc2VzL2J5LW5jLXNhLzQuMC8="><img src="https://cdnjs.cloudflare.com/ajax/libs/creativecommons-vocabulary/2020.11.3/assets/license_badges/small/by_nc_sa.svg" alt="Creative Commons"></span>
  </div>

        </div>
      </div>
    </div>

    
        <div class="pjax">
        </div>
  </aside>


    </div>

    <div class="main-inner index posts-expand">

    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://soundmemories.github.io/2020/10/01/Machine%20Learning/23.PGMIntro/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avstar.png">
      <meta itemprop="name" content="SoundMemories">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="SoundMemories">
      <meta itemprop="description" content="今日事，今日毕">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | SoundMemories">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2020/10/01/Machine%20Learning/23.PGMIntro/" class="post-title-link" itemprop="url">PGMIntro</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2020-10-01 00:00:00" itemprop="dateCreated datePublished" datetime="2020-10-01T00:00:00+08:00">2020-10-01</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Machine-Learning/" itemprop="url" rel="index"><span itemprop="name">Machine Learning</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>3k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>11 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="概率图模型">概率图模型</h1>
<p>概率图模型使用图的方式表示概率分布。为了在图中添加各种概率，首先总结一下随机变量分布的一些规则：<br />
<span class="math display">\[
\begin{align}
&amp;Sum\ Rule:p(x_1)=\int p(x_1,x_2)dx_2\\
&amp;Product\ Rule:p(x_1,x_2)=p(x_1|x_2)p(x_2)\\
&amp;Chain\
Rule:p(x_1,x_2,\cdots,x_p)=\prod\limits_{i=1}^pp(x_i|x_{i+1,x_{i+2}
\cdots}x_p)\\
&amp;Bayesian\ Rule:p(x_1|x_2)=\frac{p(x_2|x_1)p(x_1)}{p(x_2)}
\end{align}
\]</span><br />
可以看到，在链式法则中，如果数据维度特别高，那么的采样和计算非常困难，我们需要在一定程度上作出简化，在朴素贝叶斯中，作出了条件独立性假设。在
Markov
假设中，给定数据的维度是以时间顺序出现的，给定当前时间的维度，那么下一个维度与之前的维度独立。在
HMM 中，采用了齐次 Markov 假设。在 Markov
假设之上，更一般的，加入条件独立性假设，对维度划分集合 <span
class="math inline">\(A,B,C\)</span>，使得 <span
class="math inline">\(X_A\perp X_B|X_C\)</span>。</p>
<p>概率图模型采用图的特点表示上述的条件独立性假设，节点表示随机变量，边表示条件概率。概率图模型可以分为三大理论部分：</p>
<ol type="1">
<li>表示：
<ol type="1">
<li>有向图（离散）：贝叶斯网络</li>
<li>高斯图（连续）：高斯贝叶斯和高斯马尔可夫网路</li>
<li>无向图（离散）：马尔可夫网络</li>
</ol></li>
<li>推断
<ol type="1">
<li>精确推断</li>
<li>近似推断
<ol type="1">
<li>确定性近似（如变分推断）</li>
<li>随机近似（如 MCMC）</li>
</ol></li>
</ol></li>
<li>学习
<ol type="1">
<li>参数学习
<ol type="1">
<li>完备数据</li>
<li>隐变量：E-M 算法</li>
</ol></li>
<li>结构学习</li>
</ol></li>
</ol>
<h2 id="有向图-贝叶斯网络">有向图-贝叶斯网络</h2>
<p>已知联合分布中，各个随机变量之间的依赖关系，那么可以通过拓扑排序（根据依赖关系）可以获得一个有向图。而如果已知一个图，也可以直接得到联合概率分布的因子分解：<br />
<span class="math display">\[
p(x_1,x_2,\cdots,x_p)=\prod\limits_{i=1}^pp(x_i|x_{parent(i)})
\]</span><br />
那么实际的图中条件独立性是如何体现的呢？在局部任何三个节点，可以有三种结构：</p>
<ol type="1">
<li><pre class="mermaid">
graph TB
A((A))--&gt;B((B));
B--&gt;C((C));
</pre>
<p><span class="math display">\[
\begin{aligned}
p(A,B,C)&amp;=p(A)p(B|A)p(C|B)=p(A)p(B|A)p(C|B,A)\\
&amp;\Longrightarrow p(C|B)=p(C|B,A)\\
&amp;\Leftrightarrow p(C|B)p(A|B)=p(C|A,B)p(A|B)=p(C,A|B)\\
&amp;\Longrightarrow C\perp A|B
\end{aligned}
\]</span></p></li>
<li><pre class="mermaid">
graph TB
B((B))--&gt;A((A));
B--&gt;C((C));
    
</pre>
<p><span class="math display">\[
\begin{aligned}
p(A,B,C)&amp;=p(A|B)p(B)p(C|B)=p(B)p(A|B)p(C|A,B)\\
&amp;\Longrightarrow p(C|B)=p(C|B,A)\\
&amp;\Leftrightarrow p(C|B)p(A|B)=p(C|A,B)p(A|B)=p(C,A|B)\\
&amp;\Longrightarrow C\perp A|B
\end{aligned}
\]</span></p></li>
<li><pre class="mermaid">
graph TB
A((A))--&gt;B((B));
C((C))--&gt;B
    
</pre>
<p><span class="math display">\[
\begin{aligned}
p(A,B,C)&amp;=p(A)p(C)p(B|C,A)=p(A)p(C|A)p(B|C,A)\\
&amp;\Longrightarrow p(C)=p(C|A)\\
&amp;\Leftrightarrow C\perp A\\
\end{aligned}
\]</span></p>
<p>对这种结构，<span class="math inline">\(A,C\)</span> 不与 <span
class="math inline">\(B\)</span> 条件独立。</p></li>
</ol>
<p>从整体的图来看，可以引入 D 划分的概念。对于类似上面图 1和图
2的关系，引入集合A，B，那么满足 <span class="math inline">\(A\perp
B|C\)</span> 的 <span class="math inline">\(C\)</span> 集合中的点与
<span class="math inline">\(A,B\)</span> 中的点的关系都满足图
1，2，满足图3 关系的点都不在 <span class="math inline">\(C\)</span>
中。D 划分应用在贝叶斯定理中：<br />
<span class="math display">\[
p(x_i|x_{-i})=\frac{p(x)}{\int
p(x)dx_{i}}=\frac{\prod\limits_{j=1}^pp(x_j|x_{parents(j)})}{\int\prod\limits_{j=1}^pp(x_j|x_{parents(j)})dx_i}
\]</span><br />
可以发现，上下部分可以分为两部分，一部分是和 <span
class="math inline">\(x_i\)</span> 相关的，另一部分是和 <span
class="math inline">\(x_i\)</span>
无关的，而这个无关的部分可以相互约掉。于是计算只涉及和 <span
class="math inline">\(x_i\)</span> 相关的部分。</p>
<p>与 <span class="math inline">\(x_i\)</span>
相关的部分可以写成：<br />
<span class="math display">\[
p(x_i|x_{parents(i)})p(x_{child(i)}|x_i)
\]</span><br />
这些相关的部分又叫做 Markov 毯。</p>
<p>实际应用的模型中，对这些条件独立性作出了假设，从单一到混合，从有限到无限（时间，空间）可以分为：</p>
<ol type="1">
<li>朴素贝叶斯，单一的条件独立性假设 <span
class="math inline">\(p(x|y)=\prod\limits_{i=1}^pp(x_i|y)\)</span>，在 D
划分后，所有条件依赖的集合就是单个元素。</li>
<li>高斯混合模型：混合的条件独立。引入多类别的隐变量 <span
class="math inline">\(z_1, z_2,\cdots,z_k\)</span>， <span
class="math inline">\(p(x|z)=\mathcal{N}(\mu,\Sigma)\)</span>，条件依赖集合为多个元素。</li>
<li>与时间相关的条件依赖
<ol type="1">
<li>Markov 链</li>
<li>高斯过程（无限维高斯分布）</li>
</ol></li>
<li>连续：高斯贝叶斯网络</li>
<li>组合上面的分类
<ul>
<li>GMM 与时序结合：动态模型
<ul>
<li>HMM（离散）</li>
<li>线性动态系统 LDS（Kalman 滤波）</li>
<li>粒子滤波（非高斯，非线性）</li>
</ul></li>
</ul></li>
</ol>
<h2
id="无向图-马尔可夫网络马尔可夫随机场">无向图-马尔可夫网络（马尔可夫随机场）</h2>
<p>无向图没有了类似有向图的局部不同结构，在马尔可夫网络中，也存在 D
划分的概念。直接将条件独立的集合 <span class="math inline">\(x_A\perp
x_B|x_C\)</span> 划分为三个集合。这个也叫全局
Markov。对局部的节点，<span class="math inline">\(x\perp
(X-Neighbour(\mathcal{x}))|Neighbour(x)\)</span>。这也叫局部
Markov。对于成对的节点：<span class="math inline">\(x_i\perp
x_j|x_{-i-j}\)</span>，其中 <span class="math inline">\(i,j\)</span>
不能相邻。这也叫成对
Markov。事实上上面三个点局部全局成对是相互等价的。</p>
<p>有了这个条件独立性的划分，还需要因子分解来实际计算。引入团的概念：</p>
<blockquote>
<p>团，最大团：图中节点的集合，集合中的节点之间相互都是连接的叫做团，如果不能再添加节点，那么叫最大团。</p>
</blockquote>
<p>利用这个定义进行的 <span class="math inline">\(x\)</span>
所有维度的联合概率分布的因子分解为，假设有 <span
class="math inline">\(K\)</span> 个团，<span
class="math inline">\(Z\)</span> 就是对所有可能取值求和：<br />
<span class="math display">\[
\begin{align}p(x)=\frac{1}{Z}\prod\limits_{i=1}^{K}\phi(x_{ci})\\
Z=\sum\limits_{x\in\mathcal{X}}\prod\limits_{i=1}^{K}\phi(x_{ci})
\end{align}
\]</span><br />
其中 <span class="math inline">\(\phi(x_{ci})\)</span>
叫做势函数，它必须是一个正值，可以记为：<br />
<span class="math display">\[
\phi(x_{ci})=\exp(-E(x_{ci}))
\]</span><br />
这个分布叫做 Gibbs 分布（玻尔兹曼分布）。于是也可以记为：<span
class="math inline">\(p(x)=\frac{1}{Z}\exp(-\sum\limits_{i=1}^KE(x_{ci}))\)</span>。这个分解和条件独立性等价（Hammesley-Clifford
定理），这个分布的形式也和指数族分布形式上相同，于是满足最大熵原理。</p>
<h2 id="两种图的转换-道德图">两种图的转换-道德图</h2>
<p>我们常常想将有向图转为无向图，从而应用更一般的表达式。</p>
<ol type="1">
<li><p>链式：</p>
<pre class="mermaid">
graph TB
A((A))--&gt;B((B));
B--&gt;C((C));
    
</pre>
<p>直接去掉箭头，<span
class="math inline">\(p(a,b,c)=p(a)p(b|a)p(c|b)=\phi(a,b)\phi(b,c)\)</span>：</p>
<pre class="mermaid">
graph TB
A((A))---B((B));
B---C((C));
    
</pre></li>
<li><p>V 形：</p>
<pre class="mermaid">
graph TB
B((B))--&gt;A((A));
B--&gt;C((C));
    
</pre>
<p>由于 <span
class="math inline">\(p(a,b,c)=p(b)p(a|b)p(c|b)=\phi(a,b)\phi(b,c)\)</span>，直接去掉箭头：</p>
<pre class="mermaid">
graph TB
B((B))---A((A));
B---C((C));
    
</pre></li>
<li><p>倒 V 形：</p>
<pre class="mermaid">
graph TB
A((A))--&gt;B((B));
C((C))--&gt;B
    
</pre>
<p>由于 <span
class="math inline">\(p(a,b,c)=p(a)p(c)p(b|a,c)=\phi(a,b,c)\)</span>，于是在
<span class="math inline">\(a,c\)</span> 之间添加线：</p>
<pre class="mermaid">
graph TD
a((a))---b((b));
b---c((c));
a---c;
    
</pre>
<p>观察着三种情况可以概括为：</p>
<ol type="1">
<li>将每个节点的父节点两两相连</li>
<li>将有向边替换为无向边</li>
</ol></li>
</ol>
<h2 id="更精细的分解-因子图">更精细的分解-因子图</h2>
<p>对于一个有向图，可以通过引入环的方式，可以将其转换为无向图（Tree-like
graph），这个图就叫做道德图。但是我们上面的 BP
算法只对无环图有效，通过因子图可以变为无环图。</p>
<p>考虑一个无向图：</p>
<pre class="mermaid">
graph TD
a((a))---b((b));
b---c((c));
a---c;
</pre>
<p>可以将其转为：</p>
<pre class="mermaid">
graph TD
a((a))---f;
f---b((b));
f---c((c))
</pre>
<p>其中 <span
class="math inline">\(f=f(a,b,c)\)</span>。因子图不是唯一的，这是由于因式分解本身就对应一个特殊的因子图，将因式分解：<span
class="math inline">\(p(x)=\prod\limits_{s}f_s(x_s)\)</span>
可以进一步分解得到因子图。</p>
<h2 id="推断">推断</h2>
<p>推断的主要目的是求各种概率分布，包括边缘概率，条件概率，以及使用 MAP
来求得参数。通常推断可以分为：</p>
<ol type="1">
<li>精确推断
<ol type="1">
<li>Variable Elimination(VE)</li>
<li>Belief Propagation(BP, Sum-Product Algo)，从 VE 发展而来</li>
<li>Junction Tree，上面两种在树结构上应用，Junction Tree
在图结构上应用</li>
</ol></li>
<li>近似推断
<ol type="1">
<li>Loop Belief Propagation（针对有环图）</li>
<li>Mente Carlo Interference：例如 Importance Sampling，MCMC</li>
<li>Variational Inference</li>
</ol></li>
</ol>
<h3 id="推断-变量消除ve">推断-变量消除（VE）</h3>
<p>变量消除的方法是在求解概率分布的时候，将相关的条件概率先行求和或积分，从而一步步地消除变量，例如在马尔可夫链中：</p>
<pre class="mermaid">
graph LR
a((a))--&gt;b((b));
b--&gt;c((c));
c--&gt;d((d))
</pre>
<p><span class="math display">\[
p(d)=\sum\limits_{a,b,c}p(a,b,c,d)=\sum\limits_cp(d|c)\sum\limits_bp(c|b)\sum\limits_ap(b|a)p(a)
\]</span></p>
<p>变量消除的缺点很明显：</p>
<ol type="1">
<li>计算步骤无法存储</li>
<li>消除的最优次序是一个 NP-hard 问题</li>
</ol>
<h3 id="推断-信念传播bp">推断-信念传播（BP）</h3>
<p>为了克服 VE
的第一个缺陷-计算步骤无法存储。我们进一步地对上面的马尔可夫链进行观察：</p>
<pre class="mermaid">
graph LR
a((a))--&gt;b((b));
b--&gt;c((c));
c--&gt;d((d));
d--&gt;e((e));
</pre>
<p>要求 <span class="math inline">\(p(e)\)</span>，当然使用 VE，从 <span
class="math inline">\(a\)</span> 一直消除到 <span
class="math inline">\(d\)</span>，记 <span
class="math inline">\(\sum\limits_ap(a)p(b|a)=m_{a\to
b(b)}\)</span>，表示这是消除 <span class="math inline">\(a\)</span>
后的关于 <span class="math inline">\(b\)</span> 的概率，类似地，记 <span
class="math inline">\(\sum\limits_bp(c|b)m_{a\to b}(b)=m_{b\to
c}(c)\)</span>。于是 <span
class="math inline">\(p(e)=\sum\limits_dp(e|d)m_{b\to
c}(c)\)</span>。进一步观察，对 <span
class="math inline">\(p(c)\)</span>：<br />
<span class="math display">\[
p(c)=[\sum\limits_bp(c|b)\sum\limits_ap(b|a)p(a)]\cdot[\sum\limits_dp(d|c)\sum\limits_ep(e)p(e|d)]
\]</span><br />
我们发现了和上面计算 <span class="math inline">\(p(e)\)</span>
类似的结构，这个式子可以分成两个部分，一部分是从 <span
class="math inline">\(a\)</span> 传播过来的概率，第二部分是从 $ e$
传播过来的概率。</p>
<p>一般地，对于图（只对树形状的图）：</p>
<pre class="mermaid">
graph TD
a((a))---b((b));
b---c((c));
b---d((d));
</pre>
<p>这四个团（对于无向图是团，对于有向图就是概率为除了根的节点为1），有四个节点，三个边：<br />
<span class="math display">\[
p(a,b,c,d)=\frac{1}{Z}\phi_a(a)\phi_b(b)\phi_c(c)\phi_d(d)\cdot\phi_{ab}(a,b)\phi_{bc}(c,b)\phi_{bd}(d,b)
\]</span><br />
套用上面关于有向图的观察，如果求解边缘概率 <span
class="math inline">\(p(a)\)</span>，定义 <span
class="math inline">\(m_{c\to
b}(b)=\sum\limits_c\phi_c(c)\phi_{bc}(bc)\)</span>，<span
class="math inline">\(m_{d\to
b}(b)=\sum\limits_d\phi_d(d)\phi_{bd}(bd)\)</span>，<span
class="math inline">\(m_{b\to
a}(a)=\sum\limits_b\phi_{ba}(ba)\phi_b(b)m_{c\to b}(b)_{d\to
b}m(b)\)</span>，这样概率就一步步地传播到了 <span
class="math inline">\(a\)</span>：<br />
<span class="math display">\[
p(a)=\phi_a(a)m_{b\to a}(a)
\]</span><br />
写成一般的形式，对于相邻节点 <span
class="math inline">\(i,j\)</span>：<br />
<span class="math display">\[
m_{j\to i}(i)=\sum\limits_j\phi_j(j)\phi_{ij}(ij)\prod\limits_{k\in
Neighbour(j)-i}m_{k\to j}(j)
\]</span><br />
这个表达式，就可以保存计算过程了，只要对每条边的传播分别计算，对于一个无向树形图可以递归并行实现：</p>
<ol type="1">
<li>任取一个节点 <span class="math inline">\(a\)</span> 作为根节点</li>
<li>对这个根节点的邻居中的每一个节点，收集信息（计算入信息）</li>
<li>对根节点的邻居，分发信息（计算出信息）</li>
</ol>
<h3 id="推断-max-product-算法">推断-Max-Product 算法</h3>
<p>在推断任务中，MAP 也是常常需要的，MAP 的目的是寻找最佳参数：<br />
<span class="math display">\[
(\hat{a},\hat{b},\hat{c},\hat{d})=\mathop{argmax}_{a,b,c,d}p(a,b,c,d|E)
\]</span><br />
类似
BP，我们采用信息传递的方式来求得最优参数，不同的是，我们在所有信息传递中，传递的是最大化参数的概率，而不是将所有可能求和：<br />
<span class="math display">\[
m_{j\to i}=\max\limits_{j}\phi_j\phi_{ij}\prod\limits_{k\in
Neighbour(j)-i}m_{k\to j}
\]</span><br />
于是对于上面的图：<br />
<span class="math display">\[
\max_a p(a,b,c,d)=\max_a\phi_a\phi_{ab}m_{c\to b}m_{d\to b}
\]</span><br />
这个算法是 Sum-Product 算法的改进，也是在 HMM 中应用给的 Viterbi
算法的推广。</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://soundmemories.github.io/2020/09/30/Machine%20Learning/22.Exponentialfamily/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avstar.png">
      <meta itemprop="name" content="SoundMemories">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="SoundMemories">
      <meta itemprop="description" content="今日事，今日毕">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | SoundMemories">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2020/09/30/Machine%20Learning/22.Exponentialfamily/" class="post-title-link" itemprop="url">Exponentialfamily</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2020-09-30 00:00:00" itemprop="dateCreated datePublished" datetime="2020-09-30T00:00:00+08:00">2020-09-30</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Machine-Learning/" itemprop="url" rel="index"><span itemprop="name">Machine Learning</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>1.2k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>4 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="指数族分布">指数族分布</h1>
<p>指数族是一类分布，包括高斯分布、伯努利分布、二项分布、泊松分布、Beta
分布、Dirichlet 分布、Gamma
分布等一系列分布。指数族分布可以写为统一的形式：<br />
<span class="math display">\[
p(x|\eta)=h(x)\exp(\eta^T\phi(x)-A(\eta))=\frac{1}{\exp(A(\eta))}h(x)\exp(\eta^T\phi(x))
\]</span><br />
其中，<span class="math inline">\(\eta\)</span> 是参数向量，<span
class="math inline">\(A(\eta)\)</span>
是对数配分函数（归一化因子）。</p>
<p>在这个式子中，$ (x)$
叫做充分统计量，包含样本集合所有的信息，例如高斯分布中的均值和方差。充分统计量在在线学习中有应用，对于一个数据集，只需要记录样本的充分统计量即可。</p>
<p>对于一个模型分布假设（似然），那么我们在求解中，常常需要寻找一个共轭先验，使得先验与后验的形式相同，例如选取似然是二项分布，可取先验是
Beta 分布，那么后验也是 Beta
分布。指数族分布常常具有共轭的性质，于是我们在模型选择以及推断具有很大的便利。</p>
<p>共轭先验的性质便于计算，同时，指数族分布满足最大熵的思想（无信息先验），也就是说对于经验分布利用最大熵原理导出的分布就是指数族分布。</p>
<p>观察到指数族分布的表达式类似线性模型，事实上，指数族分布很自然地导出广义线性模型：<br />
<span class="math display">\[
y=f(w^Tx)\\
y|x\sim Exp Family
\]</span><br />
在更复杂的概率图模型中，例如在无向图模型中如受限玻尔兹曼机中，指数族分布也扮演着重要作用。</p>
<p>在推断的算法中，例如变分推断中，指数族分布也会大大简化计算。</p>
<h2 id="一维高斯分布">一维高斯分布</h2>
<p>一维高斯分布可以写成：<br />
<span class="math display">\[
p(x|\theta)=\frac{1}{\sqrt{2\pi}\sigma}\exp(-\frac{(x-\mu)^2}{2\sigma^2})
\]</span><br />
将这个式子改写：<br />
<span class="math display">\[
\frac{1}{\sqrt{2\pi\sigma^2}}\exp(-\frac{1}{2\sigma^2}(x^2-2\mu
x+\mu^2))\\
=\exp(\log(2\pi\sigma^2)^{-1/2})\exp(-\frac{1}{2\sigma^2}\begin{pmatrix}-2\mu&amp;1\end{pmatrix}\begin{pmatrix}x\\x^2\end{pmatrix}-\frac{\mu^2}{2\sigma^2})
\]</span><br />
所以：<br />
<span class="math display">\[
\eta=\begin{pmatrix}\frac{\mu}{\sigma^2}\\-\frac{1}{2\sigma^2}\end{pmatrix}=\begin{pmatrix}\eta_1\\\eta_2\end{pmatrix}
\]</span><br />
于是 <span class="math inline">\(A(\eta)\)</span>：<br />
<span class="math display">\[
A(\eta)=-\frac{\eta_1^2}{4\eta_2}+\frac{1}{2}\log(-\frac{\pi}{\eta_2})
\]</span></p>
<h2
id="充分统计量和对数配分函数的关系">充分统计量和对数配分函数的关系</h2>
<p>对概率密度函数求积分：<br />
<span class="math display">\[
\begin{align}
\exp(A(\eta))&amp;=\int h(x)\exp(\eta^T\phi(x))dx\nonumber
\end{align}
\]</span><br />
两边对参数求导：<br />
<span class="math display">\[
\exp(A(\eta))A&#39;(\eta)=\int h(x)\exp(\eta^T\phi(x))\phi(x)dx\\
\Longrightarrow A&#39;(\eta)=\mathbb{E}_{p(x|\eta)}[\phi(x)]
\]</span><br />
类似的：<br />
<span class="math display">\[
A&#39;&#39;(\eta)=Var_{p(x|\eta)}[\phi(x)]
\]</span><br />
由于方差为正，于是 <span class="math inline">\(A(\eta)\)</span>
一定是凸函数。</p>
<h2 id="充分统计量和极大似然估计">充分统计量和极大似然估计</h2>
<p>对于独立全同采样得到的数据集 <span
class="math inline">\(\mathcal{D}=\{x_1,x_2,\cdots,x_N\}\)</span>。<br />
$$<br />
<span
class="math display">\[\begin{align}\eta_{MLE}&amp;=\mathop{argmax}_\eta\sum\limits_{i=1}^N\log
p(x_i|\eta)\nonumber\\
&amp;=\mathop{argmax}_\eta\sum\limits_{i=1}^N(\eta^T\phi(x_i)-A(\eta))\nonumber\\
&amp;\Longrightarrow
A&#39;(\eta_{MLE})=\frac{1}{N}\sum\limits_{i=1}^N\phi(x_i)

\end{align}\]</span><br />
$$<br />
由此可以看到，为了估算参数，只需要知道充分统计量就可以了。</p>
<h2 id="最大熵">最大熵</h2>
<p>信息熵记为：<br />
<span class="math display">\[
Entropy=\int-p(x)\log(p(x))dx
\]</span></p>
<blockquote>
<p>一般地，对于完全随机的变量（等可能），信息熵最大。</p>
<p>我们的假设为最大熵原则，假设数据是离散分布的，<span
class="math inline">\(k\)</span> 个特征的概率分别为 <span
class="math inline">\(p_k\)</span>，最大熵原理可以表述为：<br />
<span class="math display">\[
  \max\{H(p)\}=\min\{\sum\limits_{k=1}^Kp_k\log p_k\}\ s.t.\
\sum\limits_{k=1}^Kp_k=1
  \]</span><br />
利用 Lagrange 乘子法：<br />
<span class="math display">\[
  L(p,\lambda)=\sum\limits_{k=1}^Kp_k\log
p_k+\lambda(1-\sum\limits_{k=1}^Kp_k)
  \]</span><br />
于是可得：<br />
<span class="math display">\[
  p_1=p_2=\cdots=p_K=\frac{1}{K}
  \]</span><br />
因此等可能的情况熵最大。</p>
</blockquote>
<p>一个数据集 <span
class="math inline">\(\mathcal{D}\)</span>，在这个数据集上的经验分布为
<span
class="math inline">\(\hat{p}(x)=\frac{Count(x)}{N}\)</span>，实际不可能满足所有的经验概率相同，于是在上面的最大熵原理中还需要加入这个经验分布的约束。</p>
<p>对任意一个函数，经验分布的经验期望可以求得为：<br />
<span class="math display">\[
\mathbb{E}_\hat{p}[f(x)]=\Delta
\]</span><br />
于是：<br />
<span class="math display">\[
\max\{H(p)\}=\min\{\sum\limits_{k=1}^Np_k\log p_k\}\ s.t.\
\sum\limits_{k=1}^Np_k=1,\mathbb{E}_p[f(x)]=\Delta
\]</span><br />
Lagrange 函数为：<br />
<span class="math display">\[
L(p,\lambda_0,\lambda)=\sum\limits_{k=1}^Np_k\log
p_k+\lambda_0(1-\sum\limits_{k=1}^Np_k)+\lambda^T(\Delta-\mathbb{E}_p[f(x)])
\]</span><br />
求导得到：<br />
<span class="math display">\[
\frac{\partial}{\partial p(x)}L=\sum\limits_{k=1}^N(\log
p(x)+1)-\sum\limits_{k=1}^N\lambda_0-\sum\limits_{k=1}^N\lambda^Tf(x)\\
\Longrightarrow\sum\limits_{k=1}^N\log p(x)+1-\lambda_0-\lambda^Tf(x)=0
\]</span><br />
由于数据集是任意的，对数据集求和也意味着求和项里面的每一项都是0：<br />
<span class="math display">\[
p(x)=\exp(\lambda^Tf(x)+\lambda_0-1)
\]</span><br />
这就是指数族分布。</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://soundmemories.github.io/2020/09/29/Machine%20Learning/21.SVM/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avstar.png">
      <meta itemprop="name" content="SoundMemories">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="SoundMemories">
      <meta itemprop="description" content="今日事，今日毕">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | SoundMemories">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2020/09/29/Machine%20Learning/21.SVM/" class="post-title-link" itemprop="url">SVM</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2020-09-29 00:00:00" itemprop="dateCreated datePublished" datetime="2020-09-29T00:00:00+08:00">2020-09-29</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Machine-Learning/" itemprop="url" rel="index"><span itemprop="name">Machine Learning</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>3.1k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>11 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="支撑向量机">支撑向量机</h1>
<p>支撑向量机（SVM）算法在分类问题中有着重要地位，其主要思想是最大化两类之间的间隔。按照数据集的特点：</p>
<ol type="1">
<li>线性可分问题，如之前的感知机算法处理的问题</li>
<li>线性可分，只有一点点错误点，如感知机算法发展出来的 Pocket
算法处理的问题</li>
<li>非线性问题，完全不可分，如在感知机问题发展出来的多层感知机和深度学习</li>
</ol>
<p>这三种情况对于 SVM 分别有下面三种处理手段：</p>
<ol type="1">
<li>hard-margin SVM</li>
<li>soft-margin SVM</li>
<li>kernel Method</li>
</ol>
<p>SVM 的求解中，大量用到了 Lagrange
乘子法，首先对这种方法进行介绍。</p>
<h2 id="约束优化问题">约束优化问题</h2>
<p>一般地，约束优化问题（原问题）可以写成：<br />
$$<br />
<span class="math display">\[\begin{align}

&amp;\min_{x\in\mathbb{R^p}}f(x)\\
&amp;s.t.\ m_i(x)\le0,i=1,2,\cdots,M\\
&amp;\ \ \ \ \ \ \ \ n_j(x)=0,j=1,2,\cdots,N

\end{align}\]</span><br />
<span class="math display">\[
定义 Lagrange 函数：
\]</span><br />
L(x,,)=f(x)+<em>{i=1}<sup>M<em>im_i(x)+</em>{i=1}</sup>N<em>in_i(x)<br />
<span class="math display">\[
那么原问题可以等价于无约束形式：
\]</span><br />
</em>{x^p}</em>{,}L(x,,) s.t. _i<br />
$$<br />
这是由于，当满足原问题的不等式约束的时候，<span
class="math inline">\(\lambda_i=0\)</span>
才能取得最大值，直接等价于原问题，如果不满足原问题的不等式约束，那么最大值就为
<span
class="math inline">\(+\infty\)</span>，由于需要取最小值，于是不会取到这个情况。</p>
<p>这个问题的对偶形式：<br />
<span class="math display">\[
\max_{\lambda,\eta}\min_{x\in\mathbb{R}^p}L(x,\lambda,\eta)\ s.t.\
\lambda_i\ge0
\]</span><br />
对偶问题是关于 $ , $ 的最大化问题。</p>
<p>由于：<br />
<span class="math display">\[
\max_{\lambda_i,\eta_j}\min_{x}L(x,\lambda_i,\eta_j)\le\min_{x}\max_{\lambda_i,\eta_j}L(x,\lambda_i,\eta_j)
\]</span></p>
<blockquote>
<p>证明：显然有 <span class="math inline">\(\min\limits_{x}L\le
L\le\max\limits_{\lambda,\eta}L\)</span>，于是显然有 <span
class="math inline">\(\max\limits_{\lambda,\eta}\min\limits_{x}L\le
L\)</span>，且 <span
class="math inline">\(\min\limits_{x}\max\limits_{\lambda,\eta}L\ge
L\)</span>。</p>
</blockquote>
<p>对偶问题的解小于原问题，有两种情况：</p>
<ol type="1">
<li>强对偶：可以取等于号</li>
<li>弱对偶：不可以取等于号</li>
</ol>
<p>其实这一点也可以通过一张图来说明：</p>
<p><img src="/images/SVM/SVM.png" width="90%" height="90%"></p>
<p>对于一个凸优化问题，有如下定理：</p>
<blockquote>
<p>如果凸优化问题满足某些条件如 Slater
条件，那么它和其对偶问题满足强对偶关系。记问题的定义域为：<span
class="math inline">\(\mathcal{D}=domf(x)\cap dom m_i(x)\cap
domn_j(x)\)</span>。于是 Slater 条件为：<br />
<span class="math display">\[
  \exists\hat{x}\in Relint\mathcal{D}\ s.t.\ \forall
i=1,2,\cdots,M,m_i(x)\lt0
  \]</span><br />
其中 Relint 表示相对内部（不包含边界的内部）。</p>
</blockquote>
<ol type="1">
<li>对于大多数凸优化问题，Slater 条件成立。</li>
<li>松弛 Slater 条件，如果 M 个不等式约束中，有 K
个函数为仿射函数，那么只要其余的函数满足 Slater 条件即可。</li>
</ol>
<p>上面介绍了原问题和对偶问题的对偶关系，但是实际还需要对参数进行求解，求解方法使用
KKT 条件进行：</p>
<blockquote>
<p>KKT 条件和强对偶关系是等价关系。KKT 条件对最优解的条件为：</p>
<ol type="1">
<li><p>可行域：<br />
<span class="math display">\[
\begin{align}
m_i(x^*)\le0\\
n_j(x^*)=0\\
\lambda^*\ge0
\end{align}
\]</span></p></li>
<li><p>互补松弛 <span class="math inline">\(\lambda^*m_i(x^*)=0,\forall
m_i\)</span>，对偶问题的最佳值为 <span
class="math inline">\(d^*\)</span>，原问题为 <span
class="math inline">\(p^*\)</span><br />
<span class="math display">\[
\begin{align}
d^*&amp;=\max_{\lambda,\eta}g(\lambda,\eta)=g(\lambda^*,\eta^*)\nonumber\\
&amp;=\min_{x}L(x,\lambda^*,\eta^*)\nonumber\\
&amp;\le L(x^*,\lambda^*,\eta^*)\nonumber\\
&amp;=f(x^*)+\sum\limits_{i=1}^M\lambda^*m_i(x^*)\nonumber\\
&amp;\le f(x^*)=p^*
\end{align}
\]</span><br />
为了满足相等，两个不等式必须成立，于是，对于第一个不等于号，需要有梯度为0条件，对于第二个不等于号需要满足互补松弛条件。</p></li>
<li><p>梯度为0：<span class="math inline">\(\frac{\partial
L(x,\lambda^*,\eta^*)}{\partial x}|_{x=x^*}=0\)</span></p></li>
</ol>
</blockquote>
<h2 id="hard-margin-svm">Hard-margin SVM</h2>
<p>支撑向量机也是一种硬分类模型，在之前的感知机模型中，我们在线性模型的基础上叠加了符号函数，在几何直观上，可以看到，如果两类分的很开的话，那么其实会存在无穷多条线可以将两类分开。在
SVM
中，我们引入最大化间隔这个概念，间隔指的是数据和直线的距离的最小值，因此最大化这个值反映了我们的模型倾向。</p>
<p>分割的超平面可以写为：<br />
<span class="math display">\[
0=w^Tx+b
\]</span><br />
那么最大化间隔（约束为分类任务的要求）：<br />
<span class="math display">\[
\mathop{argmax}_{w,b}[\min_i\frac{|w^Tx_i+b|}{||w||}]\ s.t.\
y_i(w^Tx_i+b)&gt;0\\
\Longrightarrow\mathop{argmax}_{w,b}[\min_i\frac{y_i(w^Tx_i+b)}{||w||}]\
s.t.\ y_i(w^Tx_i+b)&gt;0
\]</span><br />
对于这个约束 <span
class="math inline">\(y_i(w^Tx_i+b)&gt;0\)</span>，不妨固定 <span
class="math inline">\(\min
y_i(w^Tx_i+b)=1&gt;0\)</span>，这是由于分开两类的超平面的系数经过比例放缩不会改变这个平面，这也相当于给超平面的系数作出了约束。化简后的式子可以表示为：<br />
<span class="math display">\[
\mathop{argmin}_{w,b}\frac{1}{2}w^Tw\ s.t.\ \min_iy_i(w^Tx_i+b)=1\\
\Rightarrow\mathop{argmin}_{w,b}\frac{1}{2}w^Tw\ s.t.\
y_i(w^Tx_i+b)\ge1,i=1,2,\cdots,N
\]</span><br />
这就是一个包含 <span class="math inline">\(N\)</span>
个约束的凸优化问题，有很多求解这种问题的软件。</p>
<p>但是，如果样本数量或维度非常高，直接求解困难甚至不可解，于是需要对这个问题进一步处理。引入
Lagrange 函数：<br />
<span class="math display">\[
L(w,b,\lambda)=\frac{1}{2}w^Tw+\sum\limits_{i=1}^N\lambda_i(1-y_i(w^Tx_i+b))
\]</span><br />
我们有原问题就等价于：<br />
<span class="math display">\[
\mathop{argmin}_{w,b}\max_{\lambda}L(w,b,\lambda_i)\ s.t.\ \lambda_i\ge0
\]</span><br />
我们交换最小和最大值的符号得到对偶问题：<br />
<span class="math display">\[
\max_{\lambda_i}\min_{w,b}L(w,b,\lambda_i)\ s.t.\ \lambda_i\ge0
\]</span><br />
由于不等式约束是仿射函数，对偶问题和原问题等价：</p>
<ul>
<li><p><span class="math inline">\(b\)</span>：<span
class="math inline">\(\frac{\partial}{\partial
b}L=0\Rightarrow\sum\limits_{i=1}^N\lambda_iy_i=0\)</span></p></li>
<li><p><span class="math inline">\(w\)</span>：首先将 <span
class="math inline">\(b\)</span> 代入：<br />
<span class="math display">\[
L(w,b,\lambda_i)=\frac{1}{2}w^Tw+\sum\limits_{i=1}^N\lambda_i(1-y_iw^Tx_i-y_ib)=\frac{1}{2}w^Tw+\sum\limits_{i=1}^N\lambda_i-\sum\limits_{i=1}^N\lambda_iy_iw^Tx_i
\]</span><br />
所以：<br />
<span class="math display">\[
\frac{\partial}{\partial w}L=0\Rightarrow
w=\sum\limits_{i=1}^N\lambda_iy_ix_i
\]</span></p></li>
<li><p>将上面两个参数代入：<br />
<span class="math display">\[
L(w,b,\lambda_i)=-\frac{1}{2}\sum\limits_{i=1}^N\sum\limits_{j=1}^N\lambda_i\lambda_jy_iy_jx_i^Tx_j+\sum\limits_{i=1}^N\lambda_i
\]</span></p></li>
</ul>
<p>因此，对偶问题就是：<br />
<span class="math display">\[
\max_{\lambda}-\frac{1}{2}\sum\limits_{i=1}^N\sum\limits_{j=1}^N\lambda_i\lambda_jy_iy_jx_i^Tx_j+\sum\limits_{i=1}^N\lambda_i,\
s.t.\ \lambda_i\ge0
\]</span><br />
从 KKT 条件得到超平面的参数：</p>
<blockquote>
<p>原问题和对偶问题满足强对偶关系的充要条件为其满足 KKT 条件：<br />
<span class="math display">\[
  \begin{align}
  &amp;\frac{\partial L}{\partial w}=0,\frac{\partial L}{\partial b}=0
  \\&amp;\lambda_k(1-y_k(w^Tx_k+b))=0(slackness\ complementary)\\
  &amp;\lambda_i\ge0\\
  &amp;1-y_i(w^Tx_i+b)\le0
  \end{align}
  \]</span></p>
</blockquote>
<p>根据这个条件就得到了对应的最佳参数：<br />
<span class="math display">\[
\hat{w}=\sum\limits_{i=1}^N\lambda_iy_ix_i\\
\hat{b}=y_k-w^Tx_k=y_k-\sum\limits_{i=1}^N\lambda_iy_ix_i^Tx_k,\exists
k,1-y_k(w^Tx_k+b)=0
\]</span><br />
于是这个超平面的参数 <span class="math inline">\(w\)</span>
就是数据点的线性组合，最终的参数值就是部分满足 <span
class="math inline">\(y_i(w^Tx_i+b)=1\)</span>向量的线性组合（互补松弛条件给出），这些向量也叫支撑向量。</p>
<h2 id="soft-margin-svm">Soft-margin SVM</h2>
<p>Hard-margin 的 SVM
只对可分数据可解，如果不可分的情况，我们的基本想法是在损失函数中加入错误分类的可能性。错误分类的个数可以写成：<br />
<span class="math display">\[
error=\sum\limits_{i=1}^N\mathbb{I}\{y_i(w^Tx_i+b)\lt1\}
\]</span><br />
这个函数不连续，可以将其改写为：<br />
<span class="math display">\[
error=\sum\limits_{i=1}^N\max\{0,1-y_i(w^Tx_i+b)\}
\]</span><br />
求和符号中的式子又叫做 Hinge Function。</p>
<p>将这个错误加入 Hard-margin SVM 中，于是：<br />
<span class="math display">\[
\mathop{argmin}_{w,b}\frac{1}{2}w^Tw+C\sum\limits_{i=1}^N\max\{0,1-y_i(w^Tx_i+b)\}\
s.t.\ y_i(w^Tx_i+b)\ge1-\xi_i,i=1,2,\cdots,N
\]</span><br />
这个式子中，常数 <span class="math inline">\(C\)</span>
可以看作允许的错误水平，同时上式为了进一步消除 <span
class="math inline">\(\max\)</span>
符号，对数据集中的每一个观测，我们可以认为其大部分满足约束，但是其中部分违反约束，因此这部分约束变成
<span class="math inline">\(y_i(w^Tx+b)\ge1-\xi_i\)</span>，其中 <span
class="math inline">\(\xi_i=1-y_i(w^Tx_i+b)\)</span>，进一步的化简：<br />
<span class="math display">\[
\mathop{argmin}_{w,b}\frac{1}{2}w^Tw+C\sum\limits_{i=1}^N\xi_i\ s.t.\
y_i(w^Tx_i+b)\ge1-\xi_i,\xi_i\ge0,i=1,2,\cdots,N
\]</span></p>
<h2 id="kernel-method">Kernel Method</h2>
<p>核方法可以应用在很多问题上，在分类问题中，对于严格不可分问题，我们引入一个特征转换函数将原来的不可分的数据集变为可分的数据集，然后再来应用已有的模型。往往将低维空间的数据集变为高维空间的数据集后，数据会变得可分（数据变得更为稀疏）：</p>
<blockquote>
<p>Cover TH：高维空间比低维空间更易线性可分。</p>
</blockquote>
<p>应用在 SVM 中时，观察上面的 SVM 对偶问题：<br />
<span class="math display">\[
\max_{\lambda}-\frac{1}{2}\sum\limits_{i=1}^N\sum\limits_{j=1}^N\lambda_i\lambda_jy_iy_jx_i^Tx_j+\sum\limits_{i=1}^N\lambda_i,\
s.t.\ \lambda_i\ge0
\]</span><br />
在求解的时候需要求得内积，于是不可分数据在通过特征变换后，需要求得变换后的内积。我们常常很难求得变换函数的内积。于是直接引入内积的变换函数：<br />
<span class="math display">\[
\forall x,x&#39;\in\mathcal{X},\exists\phi\in\mathcal{H}:x\rightarrow z\
s.t.\ k(x,x&#39;)=\phi(x)^T\phi(x)
\]</span><br />
称 <span class="math inline">\(k(x,x&#39;)\)</span>
为一个正定核函数，其中<span class="math inline">\(\mathcal{H}\)</span>
是 Hilbert
空间（完备的线性内积空间），如果去掉内积这个条件我们简单地称为核函数。</p>
<blockquote>
<p><span
class="math inline">\(k(x,x&#39;)=\exp(-\frac{(x-x&#39;)^2}{2\sigma^2})\)</span>
是一个核函数。</p>
<p>证明：<br />
<span class="math display">\[
  \begin{align}
  \exp(-\frac{(x-x&#39;)^2}{2\sigma^2})&amp;=\exp(-\frac{x^2}{2\sigma^2})\exp(\frac{xx&#39;}{\sigma^2})\exp(-\frac{x&#39;^2}{2\sigma^2})\nonumber\\
  &amp;=\exp(-\frac{x^2}{2\sigma^2})\sum\limits_{n=0}^{+\infty}\frac{x^nx&#39;^n}{\sigma^{2n}n!}\exp(-\frac{x&#39;^2}{2\sigma^2})\nonumber\\
  &amp;=\exp(-\frac{x^2}{2\sigma^2})\varphi(x)\varphi(x&#39;)\exp(-\frac{x&#39;^2}{2\sigma^2})\nonumber\\
  &amp;=\phi(x)\phi(x&#39;)
  \end{align}
  \]</span></p>
</blockquote>
<p>正定核函数有下面的等价定义：</p>
<blockquote>
<p>如果核函数满足：</p>
<ol type="1">
<li>对称性</li>
<li>正定性</li>
</ol>
<p>那么这个核函数时正定核函数。</p>
<p>证明：</p>
<ol type="1">
<li>对称性 <span class="math inline">\(\Leftrightarrow\)</span> <span
class="math inline">\(k(x,z)=k(z,x)\)</span>，显然满足内积的定义</li>
<li>正定性 <span class="math inline">\(\Leftrightarrow\)</span> <span
class="math inline">\(\forall
N,x_1,x_2,\cdots,x_N\in\mathcal{X}\)</span>，对应的 Gram Matrix <span
class="math inline">\(K=[k(x_i,x_j)]\)</span> 是半正定的。</li>
</ol>
<p>要证：<span
class="math inline">\(k(x,z)=\phi(x)^T\phi(z)\Leftrightarrow K\)</span>
半正定+对称性。</p>
<ol type="1">
<li><p><span
class="math inline">\(\Rightarrow\)</span>：首先，对称性是显然的，对于正定性：<br />
<span class="math display">\[
K=\begin{pmatrix}k(x_1,x_2)&amp;\cdots&amp;k(x_1,x_N)\\\vdots&amp;\vdots&amp;\vdots\\k(x_N,x_1)&amp;\cdots&amp;k(x_N,x_N)\end{pmatrix}
\]</span><br />
任意取 <span
class="math inline">\(\alpha\in\mathbb{R}^N\)</span>，即需要证明 <span
class="math inline">\(\alpha^TK\alpha\ge0\)</span>：<br />
<span class="math display">\[
\alpha^TK\alpha=\sum\limits_{i,j}\alpha_i\alpha_jK_{ij}=\sum\limits_{i,j}\alpha_i\phi^T(x_i)\phi(x_j)\alpha_j=\sum\limits_{i}\alpha_i\phi^T(x_i)\sum\limits_{j}\alpha_j\phi(x_j)
\]</span><br />
这个式子就是内积的形式，Hilbert
空间满足线性性，于是正定性的证。</p></li>
<li><p><span class="math inline">\(\Leftarrow\)</span>：对于 <span
class="math inline">\(K\)</span> 进行分解，对于对称矩阵 <span
class="math inline">\(K=V\Lambda V^T\)</span>，那么令 <span
class="math inline">\(\phi(x_i)=\sqrt{\lambda_i}V_i\)</span>，其中 <span
class="math inline">\(V_i\)</span>是特征向量，于是就构造了 <span
class="math inline">\(k(x,z)=\sqrt{\lambda_i\lambda_j}V_i^TV_j\)</span></p></li>
</ol>
</blockquote>
<h2 id="小结">小结</h2>
<p>分类问题在很长一段时间都依赖 SVM，对于严格可分的数据集，Hard-margin
SVM
选定一个超平面，保证所有数据到这个超平面的距离最大，对这个平面施加约束，固定
<span
class="math inline">\(y_i(w^Tx_i+b)=1\)</span>，得到了一个凸优化问题并且所有的约束条件都是仿射函数，于是满足
Slater
条件，将这个问题变换成为对偶的问题，可以得到等价的解，并求出约束参数：<br />
<span class="math display">\[
\max_{\lambda}-\frac{1}{2}\sum\limits_{i=1}^N\sum\limits_{j=1}^N\lambda_i\lambda_jy_iy_jx_i^Tx_j+\sum\limits_{i=1}^N\lambda_i,\
s.t.\ \lambda_i\ge0
\]</span><br />
对需要的超平面参数的求解采用强对偶问题的 KKT 条件进行。<br />
<span class="math display">\[
\begin{align}
&amp;\frac{\partial L}{\partial w}=0,\frac{\partial L}{\partial b}=0
\\&amp;\lambda_k(1-y_k(w^Tx_k+b))=0(slackness\ complementary)\\
&amp;\lambda_i\ge0\\
&amp;1-y_i(w^Tx_i+b)\le0
\end{align}
\]</span><br />
解就是：<br />
<span class="math display">\[
\hat{w}=\sum\limits_{i=1}^N\lambda_iy_ix_i\\
\hat{b}=y_k-w^Tx_k=y_k-\sum\limits_{i=1}^N\lambda_iy_ix_i^Tx_k,\exists
k,1-y_k(w^Tx_k+b)=0
\]</span><br />
当允许一点错误的时候，可以在 Hard-margin SVM 中加入错误项。用 Hinge
Function 表示错误项的大小，得到：<br />
<span class="math display">\[
\mathop{argmin}_{w,b}\frac{1}{2}w^Tw+C\sum\limits_{i=1}^N\xi_i\ s.t.\
y_i(w^Tx_i+b)\ge1-\xi_i,\xi_i\ge0,i=1,2,\cdots,N
\]</span><br />
对于完全不可分的问题，我们采用特征转换的方式，在 SVM
中，我们引入正定核函数来直接对内积进行变换，只要这个变换满足对称性和正定性，那么就可以用做核函数。</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://soundmemories.github.io/2020/09/28/Machine%20Learning/20.DimentionReduction/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avstar.png">
      <meta itemprop="name" content="SoundMemories">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="SoundMemories">
      <meta itemprop="description" content="今日事，今日毕">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | SoundMemories">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2020/09/28/Machine%20Learning/20.DimentionReduction/" class="post-title-link" itemprop="url">DimentionReduction</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2020-09-28 00:00:00" itemprop="dateCreated datePublished" datetime="2020-09-28T00:00:00+08:00">2020-09-28</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Machine-Learning/" itemprop="url" rel="index"><span itemprop="name">Machine Learning</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>1.5k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>5 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="降维">降维</h1>
<p>我们知道，解决过拟合的问题除了正则化和添加数据之外，降维就是最好的方法。降维的思路来源于维度灾难的问题，我们知道
<span class="math inline">\(n\)</span> 维球的体积为：<br />
<span class="math display">\[
CR^n
\]</span><br />
那么在球体积与边长为 <span class="math inline">\(2R\)</span>
的超立方体比值为：<br />
<span class="math display">\[
\lim\limits_{n\rightarrow0}\frac{CR^n}{2^nR^n}=0
\]</span></p>
<p>这就是所谓的维度灾难，在高维数据中，主要样本都分布在立方体的边缘，所以数据集更加稀疏。</p>
<p>降维的算法分为：</p>
<ol type="1">
<li>直接降维，特征选择</li>
<li>线性降维，PCA，MDS等</li>
<li>分线性，流形包括 Isomap，LLE 等</li>
</ol>
<p>为了方便，我们首先将协方差矩阵（数据集）写成中心化的形式：<br />
<span class="math display">\[
\begin{align}S&amp;=\frac{1}{N}\sum\limits_{i=1}^N(x_i-\overline{x})(x_i-\overline{x})^T\nonumber\\
&amp;=\frac{1}{N}(x_1-\overline{x},x_2-\overline{x},\cdots,x_N-\overline{x})(x_1-\overline{x},x_2-\overline{x},\cdots,x_N-\overline{x})^T\nonumber\\
&amp;=\frac{1}{N}(X^T-\frac{1}{N}X^T\mathbb{I}_{N1}\mathbb{I}_{N1}^T)(X^T-\frac{1}{N}X^T\mathbb{I}_{N1}\mathbb{I}_{N1}^T)^T\nonumber\\
&amp;=\frac{1}{N}X^T(E_N-\frac{1}{N}\mathbb{I}_{N1}\mathbb{I}_{1N})(E_N-\frac{1}{N}\mathbb{I}_{N1}\mathbb{I}_{1N})^TX\nonumber\\
&amp;=\frac{1}{N}X^TH_NH_N^TX\nonumber\\
&amp;=\frac{1}{N}X^TH_NH_NX=\frac{1}{N}X^THX
\end{align}
\]</span><br />
这个式子利用了中心矩阵 $ H$的对称性，这也是一个投影矩阵。</p>
<h2 id="线性降维-主成分分析-pca">线性降维-主成分分析 PCA</h2>
<h3 id="损失函数">损失函数</h3>
<p>主成分分析中，我们的基本想法是将所有数据投影到一个字空间中，从而达到降维的目标，为了寻找这个子空间，我们基本想法是：</p>
<ol type="1">
<li>所有数据在子空间中更为分散</li>
<li>损失的信息最小，即：在补空间的分量少</li>
</ol>
<p>原来的数据很有可能各个维度之间是相关的，于是我们希望找到一组 <span
class="math inline">\(p\)</span> 个新的线性无关的单位基 <span
class="math inline">\(u_i\)</span>，降维就是取其中的 <span
class="math inline">\(q\)</span> 个基。于是对于一个样本 <span
class="math inline">\(x_i\)</span>，经过这个坐标变换后：<br />
<span class="math display">\[
\hat{x_i}=\sum\limits_{i=1}^p(u_i^Tx_i)u_i=\sum\limits_{i=1}^q(u_i^Tx_i)u_i+\sum\limits_{i=q+1}^p(u_i^Tx_i)u_i
\]</span><br />
对于数据集来说，我们首先将其中心化然后再去上面的式子的第一项，并使用其系数的平方平均作为损失函数并最大化：<br />
<span class="math display">\[
\begin{align}J&amp;=\frac{1}{N}\sum\limits_{i=1}^N\sum\limits_{j=1}^q((x_i-\overline{x})^Tu_j)^2\nonumber\\
&amp;=\sum\limits_{j=1}^qu_j^TSu_j\ ,\ s.t.\ u_j^Tu_j=1
\end{align}
\]</span><br />
由于每个基都是线性无关的，于是每一个 <span
class="math inline">\(u_j\)</span>
的求解可以分别进行，使用拉格朗日乘子法：<br />
<span class="math display">\[
\mathop{argmax}_{u_j}L(u_j,\lambda)=\mathop{argmax}_{u_j}u_j^TSu_j+\lambda(1-u_j^Tu_j)
\]</span><br />
于是：<br />
<span class="math display">\[
Su_j=\lambda u_j
\]</span><br />
可见，我们需要的基就是协方差矩阵的本征矢。损失函数最大取在本征值前 <span
class="math inline">\(q\)</span> 个最大值。</p>
<p>下面看其损失的信息最少这个条件，同样适用系数的平方平均作为损失函数，并最小化：<br />
<span class="math display">\[
\begin{align}J&amp;=\frac{1}{N}\sum\limits_{i=1}^N\sum\limits_{j=q+1}^p((x_i-\overline{x})^Tu_j)^2\nonumber\\
&amp;=\sum\limits_{j=q+1}^pu_j^TSu_j\ ,\ s.t.\ u_j^Tu_j=1
\end{align}
\]</span><br />
同样的：<br />
<span class="math display">\[
\mathop{argmin}_{u_j}L(u_j,\lambda)=\mathop{argmin}_{u_j}u_j^TSu_j+\lambda(1-u_j^Tu_j)
\]</span><br />
损失函数最小取在本征值剩下的个最小的几个值。数据集的协方差矩阵可以写成
<span class="math inline">\(S=U\Lambda
U^T\)</span>，直接对这个表达式当然可以得到本征矢。</p>
<h3 id="svd-与-pcoa">SVD 与 PCoA</h3>
<p>下面使用实际训练时常常使用的 SVD 直接求得这个 <span
class="math inline">\(q\)</span> 个本征矢。</p>
<p>对中心化后的数据集进行奇异值分解：<br />
<span class="math display">\[
HX=U\Sigma V^T,U^TU=E_N,V^TV=E_p,\Sigma:N\times p
\]</span></p>
<p>于是：<br />
<span class="math display">\[
S=\frac{1}{N}X^THX=\frac{1}{N}X^TH^THX=\frac{1}{N}V\Sigma^T\Sigma V^T
\]</span><br />
因此，我们直接对中心化后的数据集进行 SVD，就可以得到特征值和特征向量
<span class="math inline">\(V\)</span>，在新坐标系中的坐标就是：<br />
<span class="math display">\[
HX\cdot V
\]</span><br />
由上面的推导，我们也可以得到另一种方法 PCoA
主坐标分析，定义并进行特征值分解：<br />
<span class="math display">\[
T=HXX^TH=U\Sigma\Sigma^TU^T
\]</span><br />
由于：<br />
<span class="math display">\[
TU\Sigma=U\Sigma(\Sigma^T\Sigma)
\]</span><br />
于是可以直接得到坐标。这两种方法都可以得到主成分，但是由于方差矩阵是
<span class="math inline">\(p\times p\)</span> 的，而 <span
class="math inline">\(T\)</span> 是 <span class="math inline">\(N\times
N\)</span> 的，所以对样本量较少的时候可以采用 PCoA的方法。</p>
<h3 id="p-pca">p-PCA</h3>
<p>下面从概率的角度对 PCA 进行分析，概率方法也叫
p-PCA。我们使用线性模型，类似之前 LDA，我们选定一个方向，对原数据 <span
class="math inline">\(x\in\mathbb{R}^p\)</span> ，降维后的数据为 <span
class="math inline">\(z\in\mathbb{R}^q,q&lt;p\)</span>。降维通过一个矩阵变换（投影）进行：<br />
<span class="math display">\[
\begin{align}
z&amp;\sim\mathcal{N}(\mathbb{O}_{q1},\mathbb{I}_{qq})\\
x&amp;=Wz+\mu+\varepsilon\\
\varepsilon&amp;\sim\mathcal{N}(0,\sigma^2\mathbb{I}_{pp})
\end{align}
\]</span><br />
对于这个模型，我么可以使用期望-最大（EM）的算法进行学习，在进行推断的时候需要求得
<span
class="math inline">\(p(z|x)\)</span>，推断的求解过程和线性高斯模型类似。<br />
<span class="math display">\[
\begin{align}
&amp;p(z|x)=\frac{p(x|z)p(z)}{p(x)}\\
&amp;\mathbb{E}[x]=\mathbb{E}[Wz+\mu+\varepsilon]=\mu\\
&amp;Var[x]=WW^T+\sigma^2\mathbb{I}_{pp}\\
\Longrightarrow
p(z|x)=\mathcal{N}(W^T(WW^T+&amp;\sigma^2\mathbb{I})^{-1}(x-\mu),\mathbb{I}-W^T(WW^T+\sigma^2\mathbb{I})^{-1}W)
\end{align}
\]</span></p>
<h2 id="小结">小结</h2>
<p>降维是解决维度灾难和过拟合的重要方法，除了直接的特征选择外，我们还可以采用算法的途径对特征进行筛选，线性的降维方法以
PCA 为代表，在 PCA
中，我们只要直接对数据矩阵进行中心化然后求奇异值分解或者对数据的协方差矩阵进行分解就可以得到其主要维度。非线性学习的方法如流形学习将投影面从平面改为超曲面。</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://soundmemories.github.io/2020/09/27/Machine%20Learning/19.LinearClassification/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avstar.png">
      <meta itemprop="name" content="SoundMemories">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="SoundMemories">
      <meta itemprop="description" content="今日事，今日毕">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | SoundMemories">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2020/09/27/Machine%20Learning/19.LinearClassification/" class="post-title-link" itemprop="url">LinearClassification</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2020-09-27 00:00:00" itemprop="dateCreated datePublished" datetime="2020-09-27T00:00:00+08:00">2020-09-27</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Machine-Learning/" itemprop="url" rel="index"><span itemprop="name">Machine Learning</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>3.7k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>13 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="线性分类">线性分类</h1>
<p>对于分类任务，线性回归模型就无能为力了，但是我们可以在线性模型的函数进行后再加入一层激活函数，这个函数是非线性的，激活函数的反函数叫做链接函数。我们有两种线性分类的方式：</p>
<ol type="1">
<li>硬分类，我们直接需要输出观测对应的分类。这类模型的代表为：
<ol type="1">
<li>线性判别分析（Fisher 判别）</li>
<li>感知机</li>
</ol></li>
<li>软分类，产生不同类别的概率，这类算法根据概率方法的不同分为两种
<ol type="1">
<li>生成式（根据贝叶斯定理先计算参数后验，再进行推断）：高斯判别分析（GDA）和朴素贝叶斯等为代表
<ol type="1">
<li>GDA</li>
<li>Naive Bayes</li>
</ol></li>
<li>判别式（直接对条件概率进行建模）：Logistic 回归</li>
</ol></li>
</ol>
<h2 id="两分类-硬分类-感知机算法">两分类-硬分类-感知机算法</h2>
<p>我们选取激活函数为：<br />
<span class="math display">\[
sign(a)=\left\{\begin{matrix}+1,a\ge0\\-1,a\lt0\end{matrix}\right.
\]</span><br />
这样就可以将线性回归的结果映射到两分类的结果上了。</p>
<p>定义损失函数为错误分类的数目，比较直观的方式是使用指示函数，但是指示函数不可导，因此可以定义：<br />
<span class="math display">\[
L(w)=\sum\limits_{x_i\in\mathcal{D}_{wrong}}-y_iw^Tx_i
\]</span><br />
其中，<span
class="math inline">\(\mathcal{D}_{wrong}\)</span>是错误分类集合，实际在每一次训练的时候，我们采用梯度下降的算法。损失函数对
<span class="math inline">\(w\)</span> 的偏导为：<br />
<span class="math display">\[
\frac{\partial}{\partial
w}L(w)=\sum\limits_{x_i\in\mathcal{D}_{wrong}}-y_ix_i
\]</span><br />
但是如果样本非常多的情况下，计算复杂度较高，但是，实际上我们并不需要绝对的损失函数下降的方向，我们只需要损失函数的期望值下降，但是计算期望需要知道真实的概率分布，我们实际只能根据训练数据抽样来估算这个概率分布（经验风险）：<br />
<span class="math display">\[
\mathbb{E}_{\mathcal
D}[\mathbb{E}_\hat{p}[\nabla_wL(w)]]=\mathbb{E}_{\mathcal
D}[\frac{1}{N}\sum\limits_{i=1}^N\nabla_wL(w)]
\]</span><br />
我们知道， <span class="math inline">\(N\)</span>
越大，样本近似真实分布越准确，但是对于一个标准差为 <span
class="math inline">\(\sigma\)</span> 的数据，可以确定的标准差仅和 <span
class="math inline">\(\sqrt{N}\)</span> 成反比，而计算速度却和 <span
class="math inline">\(N\)</span>
成正比。因此可以每次使用较少样本，则在数学期望的意义上损失降低的同时，有可以提高计算速度，如果每次只使用一个错误样本，我们有下面的更新策略（根据泰勒公式，在负方向）：<br />
<span class="math display">\[
w^{t+1}\leftarrow w^{t}+\lambda y_ix_i
\]</span><br />
是可以收敛的，同时使用单个观测更新也可以在一定程度上增加不确定度，从而减轻陷入局部最小的可能。在更大规模的数据上，常用的是小批量随机梯度下降法。</p>
<h2 id="两分类-硬分类-线性判别分析-lda">两分类-硬分类-线性判别分析
LDA</h2>
<p>在 LDA
中，我们的基本想法是选定一个方向，将试验样本顺着这个方向投影，投影后的数据需要满足两个条件，从而可以更好地分类：</p>
<ol type="1">
<li>相同类内部的试验样本距离接近。</li>
<li>不同类别之间的距离较大。</li>
</ol>
<p>首先是投影，我们假定原来的数据是向量 <span
class="math inline">\(x\)</span>，那么顺着 $ w$
方向的投影就是标量：<br />
<span class="math display">\[
z=w^T\cdot x(=|w|\cdot|x|\cos\theta)
\]</span><br />
对第一点，相同类内部的样本更为接近，我们假设属于两类的试验样本数量分别是
<span class="math inline">\(N_1\)</span>和 <span
class="math inline">\(N_2\)</span>，那么我们采用方差矩阵来表征每一个类内的总体分布，这里我们使用了协方差的定义，用
<span class="math inline">\(S\)</span> 表示原数据的协方差：<br />
<span class="math display">\[
\begin{align}
C_1:Var_z[C_1]&amp;=\frac{1}{N_1}\sum\limits_{i=1}^{N_1}(z_i-\overline{z_{c1}})(z_i-\overline{z_{c1}})^T\nonumber\\
&amp;=\frac{1}{N_1}\sum\limits_{i=1}^{N_1}(w^Tx_i-\frac{1}{N_1}\sum\limits_{j=1}^{N_1}w^Tx_j)(w^Tx_i-\frac{1}{N_1}\sum\limits_{j=1}^{N_1}w^Tx_j)^T\nonumber\\
&amp;=w^T\frac{1}{N_1}\sum\limits_{i=1}^{N_1}(x_i-\overline{x_{c1}})(x_i-\overline{x_{c1}})^Tw\nonumber\\
&amp;=w^TS_1w\\
C_2:Var_z[C_2]&amp;=\frac{1}{N_2}\sum\limits_{i=1}^{N_2}(z_i-\overline{z_{c2}})(z_i-\overline{z_{c2}})^T\nonumber\\
&amp;=w^TS_2w
\end{align}
\]</span><br />
所以类内距离可以记为：<br />
<span class="math display">\[
\begin{align}
Var_z[C_1]+Var_z[C_2]=w^T(S_1+S_2)w
\end{align}
\]</span><br />
对于第二点，我们可以用两类的均值表示这个距离：<br />
<span class="math display">\[
\begin{align}
(\overline{z_{c1}}-\overline{z_{c2}})^2&amp;=(\frac{1}{N_1}\sum\limits_{i=1}^{N_1}w^Tx_i-\frac{1}{N_2}\sum\limits_{i=1}^{N_2}w^Tx_i)^2\nonumber\\
&amp;=(w^T(\overline{x_{c1}}-\overline{x_{c2}}))^2\nonumber\\
&amp;=w^T(\overline{x_{c1}}-\overline{x_{c2}})(\overline{x_{c1}}-\overline{x_{c2}})^Tw
\end{align}
\]</span><br />
综合这两点，由于协方差是一个矩阵，于是我们用将这两个值相除来得到我们的损失函数，并最大化这个值：<br />
<span class="math display">\[
\begin{align}
\hat{w}=\mathop{argmax}\limits_wJ(w)&amp;=\mathop{argmax}\limits_w\frac{(\overline{z_{c1}}-\overline{z_{c2}})^2}{Var_z[C_1]+Var_z[C_2]}\nonumber\\
&amp;=\mathop{argmax}\limits_w\frac{w^T(\overline{x_{c1}}-\overline{x_{c2}})(\overline{x_{c1}}-\overline{x_{c2}})^Tw}{w^T(S_1+S_2)w}\nonumber\\
&amp;=\mathop{argmax}\limits_w\frac{w^TS_bw}{w^TS_ww}
\end{align}
\]</span><br />
这样，我们就把损失函数和原数据集以及参数结合起来了。下面对这个损失函数求偏导，注意我们其实对
<span class="math inline">\(w\)</span>
的绝对值没有任何要求，只对方向有要求，因此只要一个方程就可以求解了：<br />
<span class="math display">\[
\begin{align}
&amp;\frac{\partial}{\partial
w}J(w)=2S_bw(w^TS_ww)^{-1}-2w^TS_bw(w^TS_ww)^{-2}S_ww=0\nonumber\\
&amp;\Longrightarrow S_bw(w^TS_ww)=(w^TS_bw)S_ww\nonumber\\
&amp;\Longrightarrow w\propto
S_w^{-1}S_bw=S_w^{-1}(\overline{x_{c1}}-\overline{x_{c2}})(\overline{x_{c1}}-\overline{x_{c2}})^Tw\propto
S_w^{-1}(\overline{x_{c1}}-\overline{x_{c2}})
\end{align}
\]</span><br />
于是 $ S_w^{-1}(-)$ 就是我们需要寻找的方向。最后可以归一化求得单位的
<span class="math inline">\(w\)</span> 值。</p>
<h2
id="两分类-软分类-概率判别模型-logistic-回归">两分类-软分类-概率判别模型-Logistic
回归</h2>
<p>有时候我们只要得到一个类别的概率，那么我们需要一种能输出 <span
class="math inline">\([0,1]\)</span>
区间的值的函数。考虑两分类模型，我们利用判别模型，希望对 <span
class="math inline">\(p(C|x)\)</span> 建模，利用贝叶斯定理：<br />
<span class="math display">\[
p(C_1|x)=\frac{p(x|C_1)p(C_1)}{p(x|C_1)p(C_1)+p(x|C_2)p(C_2)}
\]</span><br />
取 <span
class="math inline">\(a=\ln\frac{p(x|C_1)p(C_1)}{p(x|C_2)p(C_2)}\)</span>，于是：<br />
<span class="math display">\[
p(C_1|x)=\frac{1}{1+\exp(-a)}
\]</span><br />
上面的式子叫 Logistic Sigmoid
函数，其参数表示了两类联合概率比值的对数。在判别式中，不关心这个参数的具体值，模型假设直接对
<span class="math inline">\(a\)</span> 进行。</p>
<p>Logistic 回归的模型假设是：<br />
<span class="math display">\[
a=w^Tx
\]</span><br />
于是，通过寻找 $  w$
的最佳值可以得到在这个模型假设下的最佳模型。概率判别模型常用最大似然估计的方式来确定参数。</p>
<p>对于一次观测，获得分类 <span class="math inline">\(y\)</span>
的概率为（假定<span class="math inline">\(C_1=1,C_2=0\)</span>）：<br />
<span class="math display">\[
p(y|x)=p_1^yp_0^{1-y}
\]</span></p>
<p>那么对于 <span class="math inline">\(N\)</span> 次独立全同的观测
MLE为：<br />
<span class="math display">\[
\hat{w}=\mathop{argmax}_wJ(w)=\mathop{argmax}_w\sum\limits_{i=1}^N(y_i\log
p_1+(1-y_i)\log p_0)
\]</span><br />
注意到，这个表达式是交叉熵表达式的相反数乘 <span
class="math inline">\(N\)</span>，MLE
中的对数也保证了可以和指数函数相匹配，从而在大的区间汇总获取稳定的梯度。</p>
<p>对这个函数求导数，注意到：<br />
<span class="math display">\[
p_1&#39;=(\frac{1}{1+\exp(-a)})&#39;=p_1(1-p_1)
\]</span><br />
则：<br />
<span class="math display">\[
J&#39;(w)=\sum\limits_{i=1}^Ny_i(1-p_1)x_i-p_1x_i+y_ip_1x_i=\sum\limits_{i=1}^N(y_i-p_1)x_i
\]</span><br />
由于概率值的非线性，放在求和符号中时，这个式子无法直接求解。于是在实际训练的时候，和感知机类似，也可以使用不同大小的批量随机梯度上升（对于最小化就是梯度下降）来获得这个函数的极大值。</p>
<h2
id="两分类-软分类-概率生成模型-高斯判别分析-gda">两分类-软分类-概率生成模型-高斯判别分析
GDA</h2>
<p>生成模型中，我们对联合概率分布进行建模，然后采用 MAP
来获得参数的最佳值。两分类的情况，我们采用的假设：</p>
<ol type="1">
<li><span class="math inline">\(y\sim Bernoulli(\phi)\)</span></li>
<li><span
class="math inline">\(x|y=1\sim\mathcal{N}(\mu_1,\Sigma)\)</span></li>
<li><span
class="math inline">\(x|y=0\sim\mathcal{N}(\mu_0,\Sigma)\)</span></li>
</ol>
<p>那么独立全同的数据集最大后验概率可以表示为：<br />
<span class="math display">\[
\begin{align}
\mathop{argmax}_{\phi,\mu_0,\mu_1,\Sigma}\log
p(X|Y)p(Y)=\mathop{argmax}_{\phi,\mu_0,\mu_1,\Sigma}\sum\limits_{i=1}^N
(\log p(x_i|y_i)+\log p(y_i))\nonumber\\
=\mathop{argmax}_{\phi,\mu_0,\mu_1,\Sigma}\sum\limits_{i=1}^N((1-y_i)\log\mathcal{N}(\mu_0,\Sigma)+y_i\log
\mathcal{N}(\mu_1,\Sigma)+y_i\log\phi+(1-y_i)\log(1-\phi))
\end{align}
\]</span></p>
<ul>
<li><p>首先对 <span class="math inline">\(\phi\)</span>
进行求解，将式子对 <span class="math inline">\(\phi\)</span>
求偏导：<br />
<span class="math display">\[
\begin{align}\sum\limits_{i=1}^N\frac{y_i}{\phi}+\frac{y_i-1}{1-\phi}=0\nonumber\\
\Longrightarrow\phi=\frac{\sum\limits_{i=1}^Ny_i}{N}=\frac{N_1}{N}
\end{align}
\]</span></p></li>
<li><p>然后求解 <span class="math inline">\(\mu_1\)</span>：<br />
<span class="math display">\[
\begin{align}\hat{\mu_1}&amp;=\mathop{argmax}_{\mu_1}\sum\limits_{i=1}^Ny_i\log\mathcal{N}(\mu_1,\Sigma)\nonumber\\
&amp;=\mathop{argmin}_{\mu_1}\sum\limits_{i=1}^Ny_i(x_i-\mu_1)^T\Sigma^{-1}(x_i-\mu_1)
\end{align}
\]</span><br />
由于：<br />
<span class="math display">\[
\sum\limits_{i=1}^Ny_i(x_i-\mu_1)^T\Sigma^{-1}(x_i-\mu_1)=\sum\limits_{i=1}^Ny_ix_i^T\Sigma^{-1}x_i-2y_i\mu_1^T\Sigma^{-1}x_i+y_i\mu_1^T\Sigma^{-1}\mu_1
\]</span><br />
求微分左边乘以 <span class="math inline">\(\Sigma\)</span>
可以得到：<br />
<span class="math display">\[
\begin{align}
\sum\limits_{i=1}^N-2y_i\Sigma^{-1}x_i+2y_i\Sigma^{-1}\mu_1=0\nonumber\\
\Longrightarrow\mu_1=\frac{\sum\limits_{i=1}^Ny_ix_i}{\sum\limits_{i=1}^Ny_i}=\frac{\sum\limits_{i=1}^Ny_ix_i}{N_1}
\end{align}
\]</span></p></li>
<li><p>求解 <span
class="math inline">\(\mu_0\)</span>，由于正反例是对称的，所以：<br />
<span class="math display">\[
\mu_0=\frac{\sum\limits_{i=1}^N(1-y_i)x_i}{N_0}
\]</span></p></li>
<li><p>最为困难的是求解 <span
class="math inline">\(\Sigma\)</span>，我们的模型假设对正反例采用相同的协方差矩阵，当然从上面的求解中我们可以看到，即使采用不同的矩阵也不会影响之前的三个参数。首先我们有：<br />
<span class="math display">\[
\begin{align}
\sum\limits_{i=1}^N\log\mathcal{N}(\mu,\Sigma)&amp;=\sum\limits_{i=1}^N\log(\frac{1}{(2\pi)^{p/2}|\Sigma|^{1/2}})+(-\frac{1}{2}(x_i-\mu)^T\Sigma^{-1}(x_i-\mu))\nonumber\\
&amp;=Const-\frac{1}{2}N\log|\Sigma|-\frac{1}{2}Trace((x_i-\mu)^T\Sigma^{-1}(x_i-\mu))\nonumber\\
&amp;=Const-\frac{1}{2}N\log|\Sigma|-\frac{1}{2}Trace((x_i-\mu)(x_i-\mu)^T\Sigma^{-1})\nonumber\\
&amp;=Const-\frac{1}{2}N\log|\Sigma|-\frac{1}{2}NTrace(S\Sigma^{-1})
\end{align}
\]</span><br />
在这个表达式中，我们在标量上加入迹从而可以交换矩阵的顺序，对于包含绝对值和迹的表达式的导数，我们有：<br />
<span class="math display">\[
\begin{align}
\frac{\partial}{\partial A}(|A|)&amp;=|A|A^{-1}\\
\frac{\partial}{\partial A}Trace(AB)&amp;=B^T
\end{align}
\]</span><br />
因此：<br />
<span class="math display">\[
\begin{align}[\sum\limits_{i=1}^N((1-y_i)\log\mathcal{N}(\mu_0,\Sigma)+y_i\log
\mathcal{N}(\mu_1,\Sigma)]&#39;
\nonumber\\=Const-\frac{1}{2}N\log|\Sigma|-\frac{1}{2}N_1Trace(S_1\Sigma^{-1})-\frac{1}{2}N_2Trace(S_2\Sigma^{-1})
\end{align}
\]</span><br />
其中，<span class="math inline">\(S_1,S_2\)</span>
分别为两个类数据内部的协方差矩阵，于是：<br />
<span class="math display">\[
\begin{align}N\Sigma^{-1}-N_1S_1^T\Sigma^{-2}-N_2S_2^T\Sigma^{-2}=0\nonumber
\\\Longrightarrow\Sigma=\frac{N_1S_1+N_2S_2}{N}
\end{align}
\]</span><br />
这里应用了类协方差矩阵的对称性。</p></li>
</ul>
<p>于是我们就利用最大后验的方法求得了我们模型假设里面的所有参数，根据模型，可以得到联合分布，也就可以得到用于推断的条件分布了。</p>
<h2
id="两分类-软分类-概率生成模型-朴素贝叶斯">两分类-软分类-概率生成模型-朴素贝叶斯</h2>
<p>上面的高斯判别分析的是对数据集的分布作出了高斯分布的假设，同时引入伯努利分布作为类先验，从而利用最大后验求得这些假设中的参数。</p>
<p>朴素贝叶斯队数据的属性之间的关系作出了假设，一般地，我们有需要得到
<span class="math inline">\(p(x|y)\)</span> 这个概率值，由于 <span
class="math inline">\(x\)</span> 有 <span
class="math inline">\(p\)</span>
个维度，因此需要对这么多的维度的联合概率进行采样，但是我们知道这么高维度的空间中采样需要的样本数量非常大才能获得较为准确的概率近似。</p>
<p>在一般的有向概率图模型中，对各个属性维度之间的条件独立关系作出了不同的假设，其中最为简单的一个假设就是在朴素贝叶斯模型描述中的条件独立性假设。<br />
<span class="math display">\[
p(x|y)=\prod\limits_{i=1}^pp(x_i|y)
\]</span><br />
即：<br />
<span class="math display">\[
x_i\perp x_j|y,\forall\  i\ne j
\]</span><br />
于是利用贝叶斯定理，对于单次观测：<br />
<span class="math display">\[
p(y|x)=\frac{p(x|y)p(y)}{p(x)}=\frac{\prod\limits_{i=1}^pp(x_i|y)p(y)}{p(x)}
\]</span><br />
对于单个维度的条件概率以及类先验作出进一步的假设：</p>
<ol type="1">
<li><span class="math inline">\(x_i\)</span> 为连续变量：<span
class="math inline">\(p(x_i|y)=\mathcal{N}(\mu_i,\sigma_i^2)\)</span></li>
<li><span class="math inline">\(x_i\)</span>
为离散变量：类别分布（Categorical）：<span
class="math inline">\(p(x_i=i|y)=\theta_i,\sum\limits_{i=1}^K\theta_i=1\)</span></li>
<li><span class="math inline">\(p(y)=\phi^y(1-\phi)^{1-y}\)</span></li>
</ol>
<p>对这些参数的估计，常用 MLE
的方法直接在数据集上估计，由于不需要知道各个维度之间的关系，因此，所需数据量大大减少了。估算完这些参数，再代入贝叶斯定理中得到类别的后验分布。</p>
<h2 id="小结">小结</h2>
<p>分类任务分为两类，对于需要直接输出类别的任务，感知机算法中我们在线性模型的基础上加入符号函数作为激活函数，那么就能得到这个类别，但是符号函数不光滑，于是我们采用错误驱动的方式，引入
<span
class="math inline">\(\sum\limits_{x_i\in\mathcal{D}_{wrong}}-y_iw^Tx_i\)</span>
作为损失函数，然后最小化这个误差，采用批量随机梯度下降的方法来获取最佳的参数值。而在线性判别分析中，我们将线性模型看作是数据点在某一个方向的投影，采用类内小，类间大的思路来定义损失函数，其中类内小定义为两类数据的方差之和，类间大定义为两类数据中心点的间距，对损失函数求导得到参数的方向，这个方向就是
<span class="math inline">\(S_w^{-1}(\overline x_{c1}-\overline
x_{c2})\)</span>，其中 <span class="math inline">\(S_w\)</span>
为原数据集两类的方差之和。</p>
<p>另一种任务是输出分类的概率，对于概率模型，我们有两种方案，第一种是判别模型，也就是直接对类别的条件概率建模，将线性模型套入
Logistic 函数中，我们就得到了 Logistic
回归模型，这里的概率解释是两类的联合概率比值的对数是线性的，我们定义的损失函数是交叉熵（等价于
MLE），对这个函数求导得到 <span
class="math inline">\(\frac{1}{N}\sum\limits_{i=1}^N(y_i-p_1)x_i\)</span>，同样利用批量随机梯度（上升）的方法进行优化。第二种是生成模型，生成模型引入了类别的先验，在高斯判别分析中，我们对数据集的数据分布作出了假设，其中类先验是二项分布，而每一类的似然是高斯分布，对这个联合分布的对数似然进行最大化就得到了参数，
<span
class="math inline">\(\frac{\sum\limits_{i=1}^Ny_ix_i}{N_1},\frac{\sum\limits_{i=1}^N(1-y_i)x_i}{N_0},\frac{N_1S_1+N_2S_2}{N},\frac{N_1}{N}\)</span>。在朴素贝叶斯中，我们进一步对属性的各个维度之间的依赖关系作出假设，条件独立性假设大大减少了数据量的需求。</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://soundmemories.github.io/2020/09/25/Machine%20Learning/18.LinearRegression/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avstar.png">
      <meta itemprop="name" content="SoundMemories">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="SoundMemories">
      <meta itemprop="description" content="今日事，今日毕">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | SoundMemories">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2020/09/25/Machine%20Learning/18.LinearRegression/" class="post-title-link" itemprop="url">LinearRegression</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2020-09-25 00:00:00" itemprop="dateCreated datePublished" datetime="2020-09-25T00:00:00+08:00">2020-09-25</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Machine-Learning/" itemprop="url" rel="index"><span itemprop="name">Machine Learning</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>1.5k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>6 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="线性回归">线性回归</h1>
<p>假设数据集为：<br />
<span class="math display">\[
\mathcal{D}=\{(x_1, y_1),(x_2, y_2),\cdots,(x_N, y_N)\}
\]</span><br />
后面我们记：<br />
<span class="math display">\[
X=(x_1,x_2,\cdots,x_N)^T,Y=(y_1,y_2,\cdots,y_N)^T
\]</span><br />
线性回归假设：<br />
<span class="math display">\[
f(w)=w^Tx
\]</span></p>
<h2 id="最小二乘法">最小二乘法</h2>
<p>对这个问题，采用二范数定义的平方误差来定义损失函数：<br />
<span class="math display">\[
L(w)=\sum\limits_{i=1}^N||w^Tx_i-y_i||^2_2
\]</span><br />
展开得到：<br />
<span class="math display">\[
\begin{aligned}
L(w)&amp;=(w^Tx_1-y_1,\cdots,w^Tx_N-y_N)\cdot(w^Tx_1-y_1,\cdots,w^Tx_N-y_N)^T\\
&amp;=(w^TX^T-Y^T)\cdot(Xw-Y)=w^TX^TXw-Y^TXw-w^TX^TY+Y^TY\\
&amp;=w^TX^TXw-2w^TX^TY+Y^TY
\end{aligned}
\]</span><br />
最小化这个值的 <span class="math inline">\(\hat{w}\)</span> ：<br />
<span class="math display">\[
\begin{aligned}
\hat{w}=\mathop{argmin}\limits_wL(w)&amp;\longrightarrow\frac{\partial}{\partial
w}L(w)=0\\
&amp;\longrightarrow2X^TX\hat{w}-2X^TY=0\\
&amp;\longrightarrow \hat{w}=(X^TX)^{-1}X^TY=X^+Y
\end{aligned}
\]</span><br />
这个式子中 <span class="math inline">\((X^TX)^{-1}X^T\)</span>
又被称为伪逆。对于行满秩或者列满秩的 <span
class="math inline">\(X\)</span>，可以直接求解，但是对于非满秩的样本集合，需要使用奇异值分解（SVD）的方法，对
<span class="math inline">\(X\)</span> 求奇异值分解，得到<br />
<span class="math display">\[
X=U\Sigma V^T
\]</span><br />
于是：<br />
<span class="math display">\[
X^+=V\Sigma^{-1}U^T
\]</span><br />
在几何上，最小二乘法相当于模型（这里就是直线）和试验值的距离的平方求和，假设我们的试验样本张成一个
<span class="math inline">\(p\)</span> 维空间（满秩的情况）：<span
class="math inline">\(X=Span(x_1,\cdots,x_N)\)</span>，而模型可以写成
<span class="math inline">\(f(w)=X\beta\)</span>，也就是 <span
class="math inline">\(x_1,\cdots,x_N\)</span>
的某种组合，而最小二乘法就是说希望 <span
class="math inline">\(Y\)</span>
和这个模型距离越小越好，于是它们的差应该与这个张成的空间垂直：<br />
<span class="math display">\[
X^T\cdot(Y-X\beta)=0\longrightarrow\beta=(X^TX)^{-1}X^TY
\]</span></p>
<h2 id="噪声为高斯分布的-mle">噪声为高斯分布的 MLE</h2>
<p>对于一维的情况，记 <span
class="math inline">\(y=w^Tx+\epsilon,\epsilon\sim\mathcal{N}(0,\sigma^2)\)</span>，那么
<span
class="math inline">\(y\sim\mathcal{N}(w^Tx,\sigma^2)\)</span>。代入极大似然估计中：<br />
<span class="math display">\[
\begin{aligned}
L(w)=\log p(Y|X,w)&amp;=\log\prod\limits_{i=1}^Np(y_i|x_i,w)\\
&amp;=\sum\limits_{i=1}^N\log(\frac{1}{\sqrt{2\pi\sigma}}e^{-\frac{(y_i-w^Tx_i)^2}{2\sigma^2}})\\
\mathop{argmax}\limits_wL(w)&amp;=\mathop{argmin}\limits_w\sum\limits_{i=1^N}(y_i-w^Tx_i)^2
\end{aligned}
\]</span><br />
这个表达式和最小二乘估计得到的结果一样。</p>
<h2 id="权重先验也为高斯分布的-map">权重先验也为高斯分布的 MAP</h2>
<p>取先验分布 <span
class="math inline">\(w\sim\mathcal{N}(0,\sigma_0^2)\)</span>。于是： <br />
<span class="math display">\[
\begin{aligned}
\hat{w}=\mathop{argmax}\limits_wp(w|Y)&amp;=\mathop{argmax}\limits_wp(Y|w)p(w)\\
&amp;=\mathop{argmax}\limits_w\log p(Y|w)p(w)\\
&amp;=\mathop{argmax}\limits_w(\log p(Y|w)+\log p(w))\\
&amp;=\mathop{argmin}\limits_w[(y-w^Tx)^2+\frac{\sigma^2}{\sigma_0^2}w^Tw]
\end{aligned}
\]</span><br />
这里省略了 <span class="math inline">\(X\)</span>，<span
class="math inline">\(p(Y)\)</span>和 <span
class="math inline">\(w\)</span> 没有关系，同时也利用了上面高斯分布的
MLE的结果。</p>
<p>我们将会看到，超参数 <span
class="math inline">\(\sigma_0\)</span>的存在和下面会介绍的 Ridge
正则项可以对应，同样的如果将先验分布取为 Laplace 分布，那么就会得到和 L1
正则类似的结果。</p>
<h2 id="正则化">正则化</h2>
<p>在实际应用时，如果样本容量不远远大于样本的特征维度，很可能造成过拟合，对这种情况，我们有下面三个解决方式：</p>
<ol type="1">
<li>加数据</li>
<li>特征选择（降低特征维度）如 PCA 算法。</li>
<li>正则化</li>
</ol>
<p>正则化一般是在损失函数（如上面介绍的最小二乘损失）上加入正则化项（表示模型的复杂度对模型的惩罚），下面我们介绍一般情况下的两种正则化框架。<br />
<span class="math display">\[
\begin{aligned}
L1&amp;:\mathop{argmin}\limits_wL(w)+\lambda||w||_1,\lambda\gt0\\
L2&amp;:\mathop{argmin}\limits_wL(w)+\lambda||w||^2_2,\lambda \gt 0
\end{aligned}
\]</span><br />
下面对最小二乘误差分别分析这两者的区别。</p>
<h3 id="l1-lasso">L1 Lasso</h3>
<p>L1正则化可以引起稀疏解。</p>
<p>从最小化损失的角度看，由于 L1
项求导在0附近的左右导数都不是0，因此更容易取到0解。</p>
<p>从另一个方面看，L1 正则化相当于：<br />
<span class="math display">\[
\mathop{argmin}\limits_wL(w)\\
s.t. ||w||_1\lt C
\]</span><br />
我们已经看到平方误差损失函数在 <span class="math inline">\(w\)</span>
空间是一个椭球，因此上式求解就是椭球和 <span
class="math inline">\(||w||_1=C\)</span>的切点，因此更容易相切在坐标轴上。</p>
<h3 id="l2-ridge">L2 Ridge</h3>
<p><span class="math display">\[
\begin{aligned}
\hat{w}=\mathop{argmin}\limits_wL(w)+\lambda
w^Tw&amp;\longrightarrow\frac{\partial}{\partial w}L(w)+2\lambda w=0\\
&amp;\longrightarrow2X^TX\hat{w}-2X^TY+2\lambda \hat w=0\\
&amp;\longrightarrow \hat{w}=(X^TX+\lambda \mathbb{I})^{-1}X^TY
\end{aligned}
\]</span></p>
<p>可以看到，这个正则化参数和前面的 MAP
结果不谋而合。利用2范数进行正则化不仅可以是模型选择 <span
class="math inline">\(w\)</span> 较小的参数，同时也避免
$ X^TX$不可逆的问题。</p>
<h2 id="小结">小结</h2>
<p>线性回归模型是最简单的模型，但是麻雀虽小，五脏俱全，在这里，我们利用最小二乘误差得到了闭式解。同时也发现，在噪声为高斯分布的时候，MLE
的解等价于最小二乘误差，而增加了正则项后，最小二乘误差加上 L2
正则项等价于高斯噪声先验下的 MAP解，加上 L1 正则项后，等价于 Laplace
噪声先验。</p>
<p>传统的机器学习方法或多或少都有线性回归模型的影子：</p>
<ol type="1">
<li>线性模型往往不能很好地拟合数据，因此有三种方案克服这一劣势：
<ol type="1">
<li>对特征的维数进行变换，例如多项式回归模型就是在线性特征的基础上加入高次项。</li>
<li>在线性方程后面加入一个非线性变换，即引入一个非线性的激活函数，典型的有线性分类模型如感知机。</li>
<li>对于一致的线性系数，我们进行多次变换，这样同一个特征不仅仅被单个系数影响，例如多层感知机（深度前馈网络）。</li>
</ol></li>
<li>线性回归在整个样本空间都是线性的，我们修改这个限制，在不同区域引入不同的线性或非线性，例如线性样条回归和决策树模型。</li>
<li>线性回归中使用了所有的样本，但是对数据预先进行加工学习的效果可能更好（所谓的维数灾难，高维度数据更难学习），例如
PCA 算法和流形学习。</li>
</ol>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://soundmemories.github.io/2020/09/21/Machine%20Learning/17.MathBasics/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avstar.png">
      <meta itemprop="name" content="SoundMemories">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="SoundMemories">
      <meta itemprop="description" content="今日事，今日毕">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | SoundMemories">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2020/09/21/Machine%20Learning/17.MathBasics/" class="post-title-link" itemprop="url">MathBasics</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2020-09-21 00:00:00" itemprop="dateCreated datePublished" datetime="2020-09-21T00:00:00+08:00">2020-09-21</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Machine-Learning/" itemprop="url" rel="index"><span itemprop="name">Machine Learning</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>1.9k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>7 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2 id="高斯分布">高斯分布</h2>
<h3 id="一维情况-mle">一维情况 MLE</h3>
<p>高斯分布在机器学习中占有举足轻重的作用。在 MLE 方法中：</p>
<p><span class="math display">\[
\theta=(\mu,\Sigma)=(\mu,\sigma^{2}),\theta_{MLE}=\mathop{argmax}\limits
_{\theta}\log p(X|\theta)\mathop{=}\limits _{iid}\mathop{argmax}\limits
_{\theta}\sum\limits _{i=1}^{N}\log p(x_{i}|\theta)
\]</span><br />
一般地，高斯分布的概率密度函数PDF写为：</p>
<p><span class="math display">\[
p(x|\mu,\Sigma)=\frac{1}{(2\pi)^{p/2}|\Sigma|^{1/2}}e^{-\frac{1}{2}(x-\mu)^{T}\Sigma^{-1}(x-\mu)}
\]</span><br />
带入 MLE 中我们考虑一维的情况</p>
<p><span class="math display">\[
\log p(X|\theta)=\sum\limits _{i=1}^{N}\log p(x_{i}|\theta)=\sum\limits
_{i=1}^{N}\log\frac{1}{\sqrt{2\pi}\sigma}\exp(-(x_{i}-\mu)^{2}/2\sigma^{2})
\]</span><br />
首先对 <span class="math inline">\(\mu\)</span> 的极值可以得到 ：<br />
<span class="math display">\[
\mu_{MLE}=\mathop{argmax}\limits _{\mu}\log
p(X|\theta)=\mathop{argmax}\limits _{\mu}\sum\limits
_{i=1}^{N}(x_{i}-\mu)^{2}
\]</span><br />
于是：<br />
<span class="math display">\[
\frac{\partial}{\partial\mu}\sum\limits
_{i=1}^{N}(x_{i}-\mu)^{2}=0\longrightarrow\mu_{MLE}=\frac{1}{N}\sum\limits
_{i=1}^{N}x_{i}
\]</span><br />
其次对 <span class="math inline">\(\theta\)</span> 中的另一个参数 <span
class="math inline">\(\sigma\)</span> ，有：<br />
<span class="math display">\[
\begin{aligned}
\sigma_{MLE}=\mathop{argmax}\limits _{\sigma}\log
p(X|\theta)&amp;=\mathop{argmax}\limits _{\sigma}\sum\limits
_{i=1}^{N}[-\log\sigma-\frac{1}{2\sigma^{2}}(x_{i}-\mu)^{2}]\\&amp;=\mathop{argmin}\limits
_{\sigma}\sum\limits
_{i=1}^{N}[\log\sigma+\frac{1}{2\sigma^{2}}(x_{i}-\mu)^{2}]
\end{aligned}
\]</span><br />
于是：<br />
<span class="math display">\[
\frac{\partial}{\partial\sigma}\sum\limits
_{i=1}^{N}[\log\sigma+\frac{1}{2\sigma^{2}}(x_{i}-\mu)^{2}]=0\longrightarrow\sigma_{MLE}^{2}=\frac{1}{N}\sum\limits
_{i=1}^{N}(x_{i}-\mu)^{2}
\]</span><br />
值得注意的是，上面的推导中，首先对 <span
class="math inline">\(\mu\)</span> 求 MLE， 然后利用这个结果求 <span
class="math inline">\(\sigma_{MLE}\)</span>
，因此可以预期的是对数据集求期望时 <span
class="math inline">\(\mathbb{E}_{\mathcal{D}}[\mu_{MLE}]\)</span>
是无偏差的：<br />
<span class="math display">\[
\mathbb{E}_{\mathcal{D}}[\mu_{MLE}]=\mathbb{E}_{\mathcal{D}}[\frac{1}{N}\sum\limits
_{i=1}^{N}x_{i}]=\frac{1}{N}\sum\limits
_{i=1}^{N}\mathbb{E}_{\mathcal{D}}[x_{i}]=\mu
\]</span><br />
但是当对 <span class="math inline">\(\sigma_{MLE}\)</span> 求
期望的时候由于使用了单个数据集的 <span
class="math inline">\(\mu_{MLE}\)</span>，因此对所有数据集求期望的时候我们会发现
<span class="math inline">\(\sigma_{MLE}\)</span> 是 有偏的：</p>
<p><span class="math display">\[
\begin{aligned}
\mathbb{E}_{\mathcal{D}}[\sigma_{MLE}^{2}]&amp;=\mathbb{E}_{\mathcal{D}}[\frac{1}{N}\sum\limits
_{i=1}^{N}(x_{i}-\mu_{MLE})^{2}]=\mathbb{E}_{\mathcal{D}}[\frac{1}{N}\sum\limits
_{i=1}^{N}(x_{i}^{2}-2x_{i}\mu_{MLE}+\mu_{MLE}^{2})
\\&amp;=\mathbb{E}_{\mathcal{D}}[\frac{1}{N}\sum\limits
_{i=1}^{N}x_{i}^{2}-\mu_{MLE}^{2}]=\mathbb{E}_{\mathcal{D}}[\frac{1}{N}\sum\limits
_{i=1}^{N}x_{i}^{2}-\mu^{2}+\mu^{2}-\mu_{MLE}^{2}]\\
&amp;= \mathbb{E}_{\mathcal{D}}[\frac{1}{N}\sum\limits
_{i=1}^{N}x_{i}^{2}-\mu^{2}]-\mathbb{E}_{\mathcal{D}}[\mu_{MLE}^{2}-\mu^{2}]=\sigma^{2}-(\mathbb{E}_{\mathcal{D}}[\mu_{MLE}^{2}]-\mu^{2})\\&amp;=\sigma^{2}-(\mathbb{E}_{\mathcal{D}}[\mu_{MLE}^{2}]-\mathbb{E}_{\mathcal{D}}^{2}[\mu_{MLE}])=\sigma^{2}-Var[\mu_{MLE}]\\&amp;=\sigma^{2}-Var[\frac{1}{N}\sum\limits
_{i=1}^{N}x_{i}]=\sigma^{2}-\frac{1}{N^{2}}\sum\limits
_{i=1}^{N}Var[x_{i}]=\frac{N-1}{N}\sigma^{2}
\end{aligned}
\]</span><br />
所以：<br />
<span class="math display">\[
\hat{\sigma}^{2}=\frac{1}{N-1}\sum\limits _{i=1}^{N}(x_{i}-\mu)^{2}
\]</span></p>
<h3 id="多维情况">多维情况</h3>
<p>多维高斯分布表达式为：<br />
<span class="math display">\[
p(x|\mu,\Sigma)=\frac{1}{(2\pi)^{p/2}|\Sigma|^{1/2}}e^{-\frac{1}{2}(x-\mu)^{T}\Sigma^{-1}(x-\mu)}
\]</span><br />
其中 <span
class="math inline">\(x,\mu\in\mathbb{R}^{p},\Sigma\in\mathbb{R}^{p\times
p}\)</span> ，<span class="math inline">\(\Sigma\)</span>
为协方差矩阵，一般而言也是半正定矩阵。这里我们只考虑正定矩阵。首先我们处理指数上的数字，指数上的数字可以记为
<span class="math inline">\(x\)</span> 和 <span
class="math inline">\(\mu\)</span>
之间的马氏距离。对于对称的协方差矩阵可进行特征值分解，<span
class="math inline">\(\Sigma=U\Lambda
U^{T}=(u_{1},u_{2},\cdots,u_{p})diag(\lambda_{i})(u_{1},u_{2},\cdots,u_{p})^{T}=\sum\limits
_{i=1}^{p}u_{i}\lambda_{i}u_{i}^{T}\)</span> ，于是：</p>
<p><span class="math display">\[
\Sigma^{-1}=\sum\limits _{i=1}^{p}u_{i}\frac{1}{\lambda_{i}}u_{i}^{T}
\]</span></p>
<p><span class="math display">\[
\Delta=(x-\mu)^{T}\Sigma^{-1}(x-\mu)=\sum\limits
_{i=1}^{p}(x-\mu)^{T}u_{i}\frac{1}{\lambda_{i}}u_{i}^{T}(x-\mu)=\sum\limits
_{i=1}^{p}\frac{y_{i}^{2}}{\lambda_{i}}
\]</span></p>
<p>我们注意到 <span class="math inline">\(y_{i}\)</span> 是 <span
class="math inline">\(x-\mu\)</span> 在特征向量 <span
class="math inline">\(u_{i}\)</span> 上的投影长度，因此上式子就是 <span
class="math inline">\(\Delta\)</span> 取不同值时的同心椭圆。</p>
<p>下面我们看多维高斯模型在实际应用时的两个问题</p>
<ol type="1">
<li><p>参数 <span class="math inline">\(\Sigma,\mu\)</span> 的自由度为
<span class="math inline">\(O(p^{2})\)</span>
对于维度很高的数据其自由度太高。解决方案：高自由度的来源是 <span
class="math inline">\(\Sigma\)</span> 有 <span
class="math inline">\(\frac{p(p+1)}{2}\)</span>
个自由参数，可以假设其是对角矩阵，甚至在各向同性假设中假设其对角线上的元素都相同。前一种的算法有
Factor Analysis，后一种有概率 PCA(p-PCA) 。</p></li>
<li><p>第二个问题是单个高斯分布是单峰的，对有多个峰的数据分布不能得到好的结果。解决方案：高斯混合GMM
模型。</p></li>
</ol>
<p>下面对多维高斯分布的常用定理进行介绍。</p>
<p>我们记 <span class="math inline">\(x=(x_1,
x_2,\cdots,x_p)^T=(x_{a,m\times 1},
x_{b,n\times1})^T,\mu=(\mu_{a,m\times1},
\mu_{b,n\times1}),\Sigma=\begin{pmatrix}\Sigma_{aa}&amp;\Sigma_{ab}\\\Sigma_{ba}&amp;\Sigma_{bb}\end{pmatrix}\)</span>，已知
<span class="math inline">\(x\sim\mathcal{N}(\mu,\Sigma)\)</span>。</p>
<p>首先是一个高斯分布的定理：</p>
<blockquote>
<p>定理：已知 <span class="math inline">\(x\sim\mathcal{N}(\mu,\Sigma),
y\sim Ax+b\)</span>，那么 <span
class="math inline">\(y\sim\mathcal{N}(A\mu+b, A\Sigma
A^T)\)</span>。</p>
<p>证明：<span
class="math inline">\(\mathbb{E}[y]=\mathbb{E}[Ax+b]=A\mathbb{E}[x]+b=A\mu+b\)</span>，<span
class="math inline">\(Var[y]=Var[Ax+b]=Var[Ax]=A\cdot Var[x]\cdot
A^T\)</span>。</p>
</blockquote>
<p>下面利用这个定理得到 <span
class="math inline">\(p(x_a),p(x_b),p(x_a|x_b),p(x_b|x_a)\)</span>
这四个量。</p>
<ol type="1">
<li><p><span
class="math inline">\(x_a=\begin{pmatrix}\mathbb{I}_{m\times
m}&amp;\mathbb{O}_{m\times
n}\end{pmatrix}\begin{pmatrix}x_a\\x_b\end{pmatrix}\)</span>，代入定理中得到：<br />
<span class="math display">\[
\mathbb{E}[x_a]=\begin{pmatrix}\mathbb{I}&amp;\mathbb{O}\end{pmatrix}\begin{pmatrix}\mu_a\\\mu_b\end{pmatrix}=\mu_a\\
Var[x_a]=\begin{pmatrix}\mathbb{I}&amp;\mathbb{O}\end{pmatrix}\begin{pmatrix}\Sigma_{aa}&amp;\Sigma_{ab}\\\Sigma_{ba}&amp;\Sigma_{bb}\end{pmatrix}\begin{pmatrix}\mathbb{I}\\\mathbb{O}\end{pmatrix}=\Sigma_{aa}
\]</span><br />
所以 <span
class="math inline">\(x_a\sim\mathcal{N}(\mu_a,\Sigma_{aa})\)</span>。</p></li>
<li><p>同样的，<span
class="math inline">\(x_b\sim\mathcal{N}(\mu_b,\Sigma_{bb})\)</span>。</p></li>
<li><p>对于两个条件概率，我们引入三个量：<br />
<span class="math display">\[
x_{b\cdot a}=x_b-\Sigma_{ba}\Sigma_{aa}^{-1}x_a\\
\mu_{b\cdot a}=\mu_b-\Sigma_{ba}\Sigma_{aa}^{-1}\mu_a\\
\Sigma_{bb\cdot a}=\Sigma_{bb}-\Sigma_{ba}\Sigma_{aa}^{-1}\Sigma_{ab}
\]</span><br />
特别的，最后一个式子叫做 <span
class="math inline">\(\Sigma_{bb}\)</span> 的 Schur
Complementary。可以看到：<br />
<span class="math display">\[
x_{b\cdot
a}=\begin{pmatrix}-\Sigma_{ba}\Sigma_{aa}^{-1}&amp;\mathbb{I}_{n\times
n}\end{pmatrix}\begin{pmatrix}x_a\\x_b\end{pmatrix}
\]</span><br />
所以：<br />
<span class="math display">\[
\mathbb{E}[x_{b\cdot
a}]=\begin{pmatrix}-\Sigma_{ba}\Sigma_{aa}^{-1}&amp;\mathbb{I}_{n\times
n}\end{pmatrix}\begin{pmatrix}\mu_a\\\mu_b\end{pmatrix}=\mu_{b\cdot a}\\
Var[x_{b\cdot
a}]=\begin{pmatrix}-\Sigma_{ba}\Sigma_{aa}^{-1}&amp;\mathbb{I}_{n\times
n}\end{pmatrix}\begin{pmatrix}\Sigma_{aa}&amp;\Sigma_{ab}\\\Sigma_{ba}&amp;\Sigma_{bb}\end{pmatrix}\begin{pmatrix}-\Sigma_{aa}^{-1}\Sigma_{ba}^T\\\mathbb{I}_{n\times
n}\end{pmatrix}=\Sigma_{bb\cdot a}
\]</span><br />
利用这三个量可以得到 <span class="math inline">\(x_b=x_{b\cdot
a}+\Sigma_{ba}\Sigma_{aa}^{-1}x_a\)</span>。因此：<br />
<span class="math display">\[
\mathbb{E}[x_b|x_a]=\mu_{b\cdot a}+\Sigma_{ba}\Sigma_{aa}^{-1}x_a
\]</span></p>
<p><span class="math display">\[
Var[x_b|x_a]=\Sigma_{bb\cdot a}
\]</span></p>
<p>这里同样用到了定理。</p></li>
<li><p>同样：<br />
<span class="math display">\[
x_{a\cdot b}=x_a-\Sigma_{ab}\Sigma_{bb}^{-1}x_b\\
\mu_{a\cdot b}=\mu_a-\Sigma_{ab}\Sigma_{bb}^{-1}\mu_b\\
\Sigma_{aa\cdot b}=\Sigma_{aa}-\Sigma_{ab}\Sigma_{bb}^{-1}\Sigma_{ba}
\]</span><br />
所以：<br />
<span class="math display">\[
\mathbb{E}[x_a|x_b]=\mu_{a\cdot b}+\Sigma_{ab}\Sigma_{bb}^{-1}x_b
\]</span></p>
<p><span class="math display">\[
Var[x_a|x_b]=\Sigma_{aa\cdot b}
\]</span></p></li>
</ol>
<p>下面利用上边四个量，求解线性模型：</p>
<blockquote>
<p>已知：<span
class="math inline">\(p(x)=\mathcal{N}(\mu,\Lambda^{-1}),p(y|x)=\mathcal{N}(Ax+b,L^{-1})\)</span>，求解：<span
class="math inline">\(p(y),p(x|y)\)</span>。</p>
<p>解：令 <span
class="math inline">\(y=Ax+b+\epsilon,\epsilon\sim\mathcal{N}(0,L^{-1})\)</span>，所以
<span
class="math inline">\(\mathbb{E}[y]=\mathbb{E}[Ax+b+\epsilon]=A\mu+b\)</span>，<span
class="math inline">\(Var[y]=A
\Lambda^{-1}A^T+L^{-1}\)</span>，因此：<br />
<span class="math display">\[
  p(y)=\mathcal{N}(A\mu+b,L^{-1}+A\Lambda^{-1}A^T)
  \]</span><br />
引入 <span
class="math inline">\(z=\begin{pmatrix}x\\y\end{pmatrix}\)</span>，我们可以得到
<span
class="math inline">\(Cov[x,y]=\mathbb{E}[(x-\mathbb{E}[x])(y-\mathbb{E}[y])^T]\)</span>。对于这个协方差可以直接计算：<br />
<span class="math display">\[
  \begin{aligned}Cov(x,y)&amp;=\mathbb{E}[(x-\mu)(Ax-A\mu+\epsilon)^T]=\mathbb{E}[(x-\mu)(x-\mu)^TA^T]=Var[x]A^T=\Lambda^{-1}A^T\end{aligned}
\]</span><br />
注意到协方差矩阵的对称性，所以 <span
class="math inline">\(p(z)=\mathcal{N}\begin{pmatrix}\mu\\A\mu+b\end{pmatrix},\begin{pmatrix}\Lambda^{-1}&amp;\Lambda^{-1}A^T\\A\Lambda^{-1}&amp;L^{-1}+A\Lambda^{-1}A^T\end{pmatrix})\)</span>。根据之前的公式，我们可以得到：<br />
<span class="math display">\[
  \mathbb{E}[x|y]=\mu+\Lambda^{-1}A^T(L^{-1}+A\Lambda^{-1}A^T)^{-1}(y-A\mu-b)
  \]</span></p>
<p><span class="math display">\[
  Var[x|y]=\Lambda^{-1}-\Lambda^{-1}A^T(L^{-1}+A\Lambda^{-1}A^T)^{-1}A\Lambda^{-1}
  \]</span></p>
</blockquote>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://soundmemories.github.io/2020/09/20/Machine%20Learning/16.Introduction/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avstar.png">
      <meta itemprop="name" content="SoundMemories">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="SoundMemories">
      <meta itemprop="description" content="今日事，今日毕">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | SoundMemories">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2020/09/20/Machine%20Learning/16.Introduction/" class="post-title-link" itemprop="url">Introduction</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2020-09-20 00:00:00" itemprop="dateCreated datePublished" datetime="2020-09-20T00:00:00+08:00">2020-09-20</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Machine-Learning/" itemprop="url" rel="index"><span itemprop="name">Machine Learning</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>657</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>2 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>对概率的诠释有两大学派，一种是频率派另一种是贝叶斯派。后面我们对观测集采用下面记号：<br />
<span class="math display">\[
X_{N\times
p}=(x_{1},x_{2},\cdots,x_{N})^{T},x_{i}=(x_{i1},x_{i2},\cdots,x_{ip})^{T}
\]</span><br />
这个记号表示有 <span class="math inline">\(N\)</span>
个样本，每个样本都是 <span class="math inline">\(p\)</span>
维向量。其中每个观测都是由 <span
class="math inline">\(p(x|\theta)\)</span> 生成的。</p>
<h1 id="频率派的观点">频率派的观点</h1>
<p>频率派认为 <span class="math inline">\(p(x|\theta)\)</span> 中的
<span class="math inline">\(\theta\)</span> 是一个常量。对于 <span
class="math inline">\(N\)</span> 个观测来说观测集的概率为 <span
class="math inline">\(p(X|\theta)\mathop{=}\limits _{iid}\prod\limits
_{i=1}^{N}p(x_{i}|\theta))\)</span> 。为了求 <span
class="math inline">\(\theta\)</span>
的大小，我们采用最大对数似然MLE的方法：</p>
<p><span class="math display">\[
\theta_{MLE}=\mathop{argmax}\limits _{\theta}\log
p(X|\theta)\mathop{=}\limits _{iid}\mathop{argmax}\limits
_{\theta}\sum\limits _{i=1}^{N}\log p(x_{i}|\theta)
\]</span></p>
<h1 id="贝叶斯派的观点">贝叶斯派的观点</h1>
<p>贝叶斯派认为 <span class="math inline">\(p(x|\theta)\)</span> 中的
<span class="math inline">\(\theta\)</span> 不是一个常量，而是和 <span
class="math inline">\(x\)</span> 一样为一个变量。这个 <span
class="math inline">\(\theta\)</span> 满足一个预设的先验的分布 <span
class="math inline">\(\theta\sim p(\theta)\)</span>
。于是根据贝叶斯定理依赖观测集参数的后验可以写成：</p>
<p><span class="math display">\[
p(\theta|X)=\frac{p(X|\theta)\cdot
p(\theta)}{p(X)}=\frac{p(X|\theta)\cdot p(\theta)}{\int\limits
_{\theta}p(X|\theta)\cdot p(\theta)d\theta}
\]</span><br />
为了求 <span class="math inline">\(\theta\)</span>
的值，我们要最大化这个参数后验MAP：</p>
<p><span class="math display">\[
\theta_{MAP}=\mathop{argmax}\limits
_{\theta}p(\theta|X)=\mathop{argmax}\limits _{\theta}p(X|\theta)\cdot
p(\theta)
\]</span><br />
其中第二个等号是由于分母和 <span class="math inline">\(\theta\)</span>
没有关系。求解这个 <span class="math inline">\(\theta\)</span>
值后计算<span class="math inline">\(\frac{p(X|\theta)\cdot
p(\theta)}{\int\limits _{\theta}p(X|\theta)\cdot
p(\theta)d\theta}\)</span> ，就得到了参数的后验概率。其中 <span
class="math inline">\(p(X|\theta)\)</span>
叫似然，是我们的模型分布。得到了参数的后验分布后，我们可以将这个分布用于预测贝叶斯预测：<br />
<span class="math display">\[
p(x_{new}|X)=\int\limits _{\theta}p(x_{new}|\theta)\cdot
p(\theta|X)d\theta
\]</span><br />
其中积分中的被乘数是模型，乘数是后验分布。</p>
<h1 id="小结">小结</h1>
<p>频率派和贝叶斯派分别给出了一系列的机器学习算法。频率派的观点导出了一系列的统计机器学习算法，而贝叶斯派导出了概率图理论。在应用频率派的
MLE
方法时最优化理论占有重要地位。而贝叶斯派的算法无论是后验概率的建模还是应用这个后验进行推断时积分占有重要地位。因此采样积分方法如
MCMC 有很多应用。</p>
<p>如果对此有疑问的，可以参考知乎这篇文章：<span class="exturl" data-url="aHR0cHM6Ly96aHVhbmxhbi56aGlodS5jb20vcC8zMjQ4MDgxMA==">聊一聊机器学习的MLE和MAP：最大似然估计和最大后验估计<i class="fa fa-external-link-alt"></i></span>。</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://soundmemories.github.io/2020/06/20/Machine%20Learning/13.ML-%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avstar.png">
      <meta itemprop="name" content="SoundMemories">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="SoundMemories">
      <meta itemprop="description" content="今日事，今日毕">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | SoundMemories">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2020/06/20/Machine%20Learning/13.ML-%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/" class="post-title-link" itemprop="url">ML-模型评估</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2020-06-20 00:00:00" itemprop="dateCreated datePublished" datetime="2020-06-20T00:00:00+08:00">2020-06-20</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Machine-Learning/" itemprop="url" rel="index"><span itemprop="name">Machine Learning</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>4.9k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>18 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="常用词">常用词</h1>
<p><strong>错误率</strong> ：分类错误的样本数占总样本数的比例。<br />
<strong>精度</strong>：1-错误率。</p>
<p><strong>训练误差/经验误差</strong>
：机器学习器在训练集上的误差。<br />
<strong>泛化误差</strong> ：机器学习器在新样本上的误差。</p>
<p><strong>过拟合</strong>：我们希望学习器在新样本下也能有好的表现，因此应该从训练样本中尽可能学出适用于所有潜在潜在样本的“普遍规律”，然而有时候学习器把训练样本学得“太好了”，很可能把训练样本自身的一些特点当作了所有潜在样本都具有的一般性质，这样就会导致泛化能力下降，这就称为过拟合。<br />
<strong>欠拟合</strong>：和过拟合相对，即对训练样本的一般性质尚未学好。</p>
<p><strong>生成模型与判别模型</strong>：<br />
<strong>生成模型</strong>：学习得到联合概率分布<span
class="math inline">\(P(x,y)\)</span>，即特征x和标记y共同出现的概率，然后求条件概率分布。能够学习到数据生成的机制。通过求输入与输出的<strong>联合概率分布</strong>，再求解类别归类的概率。比如，朴素贝叶斯生成的模型。生成模型，对数据要求较高（比如朴素贝叶斯要求数据是离散的），速度会快些。<br />
<strong>判别模型</strong>：学习得到条件概率分布<span
class="math inline">\(P(y|x)\)</span>，即在特征x出现的情况下标记y出现的概率。直接获得输出对应最大分类的概率。比如，KNN。判别模型，要求较低，有很好的容忍度，但速度会慢些。</p>
<h1 id="数据划分">数据划分</h1>
<p><strong>训练集</strong>：训练和拟合模型。提高训练集是好的，但比例不能变。<br />
<strong>验证集</strong>：当通过训练集训练出多个模型后，使用验证集纠偏或比较预测。<br />
<strong>测试集</strong>：模型的泛化能力考量。<br />
<strong>测试集</strong>应该尽可能与<strong>训练集</strong>互斥，即测试样本尽量不在训练集中出现，未在训练过程中使用过。</p>
<p>一般情况下：<strong>训练集:测试集:验证集=6：2：2</strong>。<br />
有时候会忽略验证集，而通过 训练集-测试集
不断的尝试来达到目的，此时训练集、测试集比例一般为8：2。</p>
<h2 id="留出法hold-out">留出法（hold-out）</h2>
<p>直接将数据集划分为两个互斥集合，一个作为训练，一个作为测试。</p>
<p>注意点：<br />
1. <strong>保持数据分布一致性</strong>
：如保留类别比例一致的分层采样。<br />
2. <strong>多次重复划分</strong>
：单次使用留出法得到的估计结果往往不够稳定可靠，一般要采用若干次随机划分、重复进行实验评估后取平均值作为留出法的结果。<br />
3.
<strong>训练集和测试集比例问题</strong>：在划分时，如果训练集包含绝大多数样本，则训练出来的模型更接近于未划分的数据集训练出的模型，由于测试集比较小，评估的结果可能不够准确；若测试集包含的样本多些，则训练集和未划分的数据集差别更大，这样训练出的模型偏离初衷，而且降低评估结果的保真性。常见的作法是2/3<sub>4/5的样本用于训练，1/3</sub>1/5的样本用于测试。</p>
<h2 id="k折交叉验证k-cross-validation">K折交叉验证（k-cross
validation）</h2>
<p><img src="/images/模型评估/K折交叉验证.png" width="80%"></p>
<p>每个子集<span
class="math inline">\(D_i\)</span>都尽可能的保持数据分布的一致性，即从<span
class="math inline">\(D\)</span>中通过分层采样得到。<br />
最终返回的是这k个测试结果的均值，所以交叉验证评估结果的稳定性和保真性很大程度取决于k的取值。</p>
<p><strong>常用的k值为5，10。</strong><br />
<strong>为减少因样本划分不同而引入的差别，<span
class="math inline">\(k\)</span>折交叉验证通常要随机使用不同的划分重复<span
class="math inline">\(p\)</span>次，最终的评估结果是这<span
class="math inline">\(p\)</span>次<span
class="math inline">\(k\)</span>折交叉验证结果的均值，例如常见的有10次10折交叉验证。</strong></p>
<p><strong>当数据集<span class="math inline">\(D\)</span>中包含<span
class="math inline">\(m\)</span>个样本，若令<span
class="math inline">\(k=m\)</span>，则得到了交叉验证法的一个特例，留一法（Leave-One-Out）。</strong><br />
即<span
class="math inline">\(m\)</span>个样本只有唯一的方式划分为m个子集——每个子集包含一个样本；留一法使用的训练集与初始数据集相比只少了一个样本，这就使得在绝大多数情况下，留一法被实际评估的模型与期望评估的用<span
class="math inline">\(D\)</span>训练出的模型很相似，因此留一法的评估结果往往被认为比较准确。<br />
然而，留一法也有其缺陷：在数据集比较大的时候，计算开销非常大。</p>
<h2 id="自助法boostrapping">自助法（boostrapping）</h2>
<p>不管是留出法还是交叉验证法，都保留了一部分样本用于测试，因此实际评估的模型所使用的训练集比原始数据集<span
class="math inline">\(D\)</span>小，这必然会引入一些<strong>因训练样本规模不同而导致的估计偏差</strong>。留一法受训练样本规模变化的影响较小，但计算复杂度太高。<strong>有没有办法可以减少训练样本规模不同造成的影响，同时还能比较高效地进行实验估计呢？</strong>————<strong>自助法（boostrapping）</strong>。</p>
<p><strong>自助法（boostrapping）直接以自助采样法为基础。给定包含<span
class="math inline">\(m\)</span>个样本的数据集<span
class="math inline">\(D\)</span>，对它进行采样产生数据集<span
class="math inline">\(D^\prime\)</span>：每次随机从<span
class="math inline">\(D\)</span>中挑选一个样本，将其拷贝放入<span
class="math inline">\(D^\prime\)</span>，然后再将该样本放回初始数据集<span
class="math inline">\(D\)</span>中，使得该样本在下次采样时仍可能被采到；这个过程重复执行<span
class="math inline">\(m\)</span>次后，我们就得到了包含<span
class="math inline">\(m\)</span>个样本的数据集<span
class="math inline">\(D^\prime\)</span>（和原数据集<span
class="math inline">\(D\)</span>同规模），这就是自助采样的结果，显然<span
class="math inline">\(D\)</span>中有一部分样本会在<span
class="math inline">\(D^\prime\)</span>中多次出现，而另一部分样本不出现，可以做一个简单估计，样本<span
class="math inline">\(m\)</span>次采样中始种不被采到的概率是<span
class="math inline">\((1-\dfrac{1}{m})^{m}\)</span>，取极限得到：<span
class="math inline">\(\lim\limits_{m \to \infty}(1-\dfrac{1}{m})^{m} \to
\dfrac{1}{e} \approx 0.368\)</span>，即初始数据集<span
class="math inline">\(D\)</span>中约有<span
class="math inline">\(36.8\%\)</span> 的样本未出现在采样数据集<span
class="math inline">\(D^\prime\)</span>中，可以使用这些样本作为测试集（约为总量1/3），<span
class="math inline">\(D^\prime\)</span>作为训练集，这样的测试结果亦称为包外估计（out-of-bagestimate）</strong></p>
<p><strong>优点：自助法在数据集较小、难以有效划分训练/测试集时很有效。</strong><br />
<strong>缺点：自助法产生的数据集改变了初始数据集的分布，这会引入估计偏差，因此在数据量足够时，使用留出法/交叉验证更好</strong></p>
<h1 id="模型评估">模型评估</h1>
<p>主要为：分类模型、回归模型、聚类模型、关联模型的评估。</p>
<h2 id="分类模型">分类模型</h2>
<h3 id="混淆矩阵">混淆矩阵</h3>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<table>
<tr>
<td rowspan="2">
真实情况
</td>
<td colspan="2">
预测结果　　　　　
</td>
</tr>
<tr>
<td>
正例　　
</td>
<td>
反例　　
</td>
</tr>
<tr>
<td>
正例
</td>
<td>
TP（真正例）
</td>
<td>
FN（假反例）
</td>
</tr>
<tr>
<td>
反例
</td>
<td>
FP（假正例）
</td>
<td>
TN（真反例）
</td>
</tr>
</table>
<p><strong>准确率（Accuracy）</strong>：<span
class="math inline">\(\dfrac{TP+TN}{TP+FP+FN+TN}\)</span>，注意只用准确率不行，比如样本极度不平衡标签0：1的比例为99：1，而模型只要是输入就判别为0，准确率还是能达到99%，所以只用准确率判别是不好的。</p>
<p><strong>精确率（Precision）</strong>：<span
class="math inline">\(P=\dfrac{TP}{TP+FP}\)</span>，预测结果正类中，正确的程度。</p>
<p><strong>召回率/真正率（Recall/TPR）</strong>：<span
class="math inline">\(R=\dfrac{TP}{TP+FN}\)</span>，真实情况正类中，预测出的占比。</p>
<p><strong>假正率（False Positive Rate/FPR）：</strong><span
class="math inline">\(\dfrac{FP}{FP+TN}\)</span>，真实负类中，被预测为正类的比例。</p>
<p><strong>错误拒绝率（False Rejection Rate/FRR）：</strong><span
class="math inline">\(\dfrac{FN}{TP+FN}\)</span>，真实正类中，被预测为负类的比例。</p>
<p><strong><span class="math inline">\(F1\)</span>-score</strong>：<span
class="math inline">\(\dfrac{2\ast P\ast R}{P + R}=\dfrac{2\ast
TP}{\text{样例总数}+TP-TN}\)</span>，F1充分考量了精确率和召回率，比单独使用准确率更好。</p>
<p><strong><span
class="math inline">\(F_\beta\)</span>-score</strong>：<span
class="math inline">\(\dfrac{(1+\beta^2)\ast P\ast R}{\beta^2\ast
P+R}\)</span>，F1的一般形式，<span class="math inline">\(\beta
&gt;1\)</span>时召回率有更大的影响；<span
class="math inline">\(\beta&lt;1\)</span>时精确率有更大影响。</p>
<h3 id="多元混淆矩阵">多元混淆矩阵</h3>
<p>在多次训练/测试后会有多个二分类混淆矩阵，或者在多个数据集上训练/测试后希望估计算法全局性能，或者执行多分类任务，每两两类别的组合都能对应一个混淆矩阵，总之我们希望在n个二分类混淆矩阵上综合考察精确率和召回率。</p>
<p>一种直接的做法是先在各混淆矩阵上分别计算出精确率和召回率，记为<span
class="math inline">\((P_1,R_1),(P_2,R_2),...,(P_n,R_n)\)</span>，再计算平均值，这样就得到<strong>宏精确率</strong>（macro-P）和<strong>宏召回率</strong>（macro-R），以及相应的<strong>宏F1</strong>（macro-F1）。<br />
<strong>macro-P</strong>：<span
class="math inline">\(\dfrac{1}{n}\sum\limits_{i=1}^NP_i\)</span></p>
<p><strong>macro-R</strong>：<span
class="math inline">\(\dfrac{1}{n}\sum\limits_{i=1}^NR_i\)</span></p>
<p><strong>macro-F1</strong>：<span class="math inline">\(\dfrac{2\times
\text{macro-P}\times\text{macro-R}}{\text{macro-P}+\text{macro-R}}\)</span></p>
<p>另一种是，先将混淆矩阵的对应元素进行平均，得到<span
class="math inline">\(TP\)</span>、<span
class="math inline">\(FP\)</span>、<span
class="math inline">\(TN\)</span>、<span
class="math inline">\(FN\)</span>的平均值，分别记为<span
class="math inline">\(\overline{TP}\)</span>、<span
class="math inline">\(\overline{FP}\)</span>、<span
class="math inline">\(\overline{TN}\)</span>、<span
class="math inline">\(\overline{FN}\)</span>，再基于这些平均值计算出<strong>微精确率</strong>（micro-P）和<strong>微召回率</strong>（micro-R），以及相应的<strong>微F1</strong>（micro-F1）。<br />
<strong>micro-P</strong>：<span
class="math inline">\(\dfrac{\overline{TP}}{\overline{TP}+\overline{FP}}\)</span></p>
<p><strong>micro-R</strong>：<span
class="math inline">\(\dfrac{\overline{TP}}{\overline{TP}+\overline{FN}}\)</span></p>
<p><strong>micro-F1</strong>：<span class="math inline">\(\dfrac{2\times
\text{macro-P}\times\text{macro-R}}{\text{macro-P}+\text{macro-R}}\)</span></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> recall_score</span><br><span class="line"></span><br><span class="line">recall_score(</span><br><span class="line">    y_true,</span><br><span class="line">    y_pred,</span><br><span class="line">    labels=<span class="literal">None</span>,</span><br><span class="line">    pos_label=<span class="number">1</span>,</span><br><span class="line">    average=<span class="string">&#x27;binary&#x27;</span>,<span class="comment">#默认是二分类;&#x27;micro&#x27;就是1的计算方式;&#x27;macro&#x27;就是2的不加权计算方式;&#x27;weighted&#x27;就是2的加权计算方式</span></span><br><span class="line">    sample_weight=<span class="literal">None</span>,</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<h3 id="pr曲线">PR曲线</h3>
<p>根据学习器的预测结果，对样例进行排序，排在最前面的是学习器认为最可能是正例的样本，排在最后面的是学习器认为最不可能是正例的样本。按此顺序逐个把样本作为正例进行预测，则每次可以计算出当前的召回率（查全率）、精确率（查准率）。以精确率为纵轴，召回率为横轴，作图就得到了精确率-召回率曲线，简称P-R曲线。<br />
<img src="/images/模型评估/PR.png" width="40%"></p>
<p>P-R曲线能直观的显示出学习器在样本总体上的精确率和、召回率。在比较时，若一个学习器的P-R曲线被另一个学习器的曲线完全包住，则可断言后者性能优于前者。</p>
<h3 id="roc曲线与auc">ROC曲线与AUC</h3>
<p><strong>ROC</strong>（Receiver Operating characteristic
Curve）根据学习器的结果对样例进行排序，按此顺序逐个把样本作为正例进行预测，每次可以计算出当前的<strong>召回率TPR</strong>和<strong>错误接收率FPR</strong>，作为纵轴和横轴。我们希望TPR越大越好，FPR越小越好。ROC曲线能很容易的查出任意阈值时，对性能的识别能力。一般我们取拐点处的阈值最佳，这兼顾了TPR和FPR。</p>
<p><strong>TPR</strong>：<span
class="math inline">\(\dfrac{TP}{TP+FN}\)</span><br />
<strong>FPR</strong>：<span
class="math inline">\(\dfrac{FP}{TN+FP}\)</span></p>
<p><img src="/images/模型评估/ROC和AUC.png" width="80%"></p>
<p>若一个学习器的ROC曲线被另一个学习器的曲线完全包住，则可断言后者性能优于前者。若两个学习器的ROC曲线发生交叉，则难以断定孰优孰劣，此时看AUC面积。<br />
<strong>AUC</strong>（Area Under
Curve）就是ROC曲线下方的面积，它反映了曲线向左上方靠近程度。越大越好。</p>
<p><strong>ROC和PR区别</strong>：<br />
<img src="/images/模型评估/ROC和PR.png" width="60%"></p>
<p><span class="exturl" data-url="aHR0cHM6Ly93d3cubWF0aC51Y2RhdmlzLmVkdS9+c2FpdG8vZGF0YS9yb2MvZmF3Y2V0dC1yb2MucGRm">An
introduction to ROC analysis<i class="fa fa-external-link-alt"></i></span><br />
根据作者原文来看，负例增加了10倍，ROC曲线没有改变，而PR曲线则变了很多。作者认为这是ROC曲线的优点，即具有鲁棒性，在类别分布发生明显改变的情况下依然能客观地识别出较好的分类器。<br />
不过在类别不平衡的背景下，负例的数目众多致使FPR的增长不明显，导致ROC曲线呈现一个过分的效果估计。</p>
<p><strong>ROC和PR应用场景</strong>：<br />
ROC曲线由于兼容正例与负例，所以适合评估分类器的整体性能，相比而言PR曲线完全聚焦于正例。</p>
<p>如果有<strong>多份数据且存在不同的类别分布</strong>，比如信用卡欺诈问题中，每个月正例和负例的比例可能都不相同，这时候如果只想单纯地比较分类器的性能且剔除类别分布改变的影响，则ROC曲线比较适合，因为类别分布的改变可能使得PR曲线发生变化时好时坏，这种时候难以进行模型比较。反之，如果想测试不同类别分布下，对分类器的性能的影响，则PR曲线比较合适。</p>
<p>如果想要评估在<strong>相同的类别分布</strong>下正例的预测情况，则宜选PR曲线。</p>
<p>类别不平衡问题中，ROC曲线通常会给一个乐观的效果估计，所以大部分时候还是PR曲线更好。</p>
<h3 id="增益图与ks图">增益图与KS图</h3>
<p><img src="/images/模型评估/增益与KS图.png" width="60%"></p>
<p>左图是<strong>增益图</strong>，虚线是1。比如挖取游戏作弊玩家，100个玩家，10个作弊的：计算得分，取出得分前10的用户，发现有9个作弊的，那么正样本比例是9/10=0.9，测试集取样比例为10/100=0.1，正样本比例/平均比例为0.9/0.1=9。增益图最好情况是在取样比例为0.5为转折点，前部分全是最高值，后部分全是最低值。增益图宏观上反映了分类器的分类效果。</p>
<p>右图是<strong>KS图</strong>，我们关心的是两条曲线的最大差值，横坐标是阈值，纵坐标是差值<span
class="math inline">\(TPR-FPR\)</span>，这个差值反映了对正类样本的区分度。一般选择差值最大的阈值作为分类阈值。</p>
<h2 id="回归模型">回归模型</h2>
<p><strong>MSE（Mean Square Error）均方误差:</strong> <span
class="math inline">\(\dfrac{1}{n}\sum\limits_{i=1}^n (f_i -
y_i)^2\)</span>，真实值和预测值差值的平方和。</p>
<p><strong>RMSE（Root MSE）均方根误差:</strong> <span
class="math inline">\(\sqrt{MSE}\)</span>，对MSE开根号，如果MSE都是比较小的，可以放大它们之间尺度。</p>
<p><strong>MAE（Mean Absolute Error）平均绝对误差:</strong> <span
class="math inline">\(\dfrac{1}{n}\sum\limits_{i=1}^n |f_i -
y_i|\)</span>，真实值和预测值差值的绝对值和。用此为指标，求导非常麻烦，常用MSE。</p>
<p><strong>MAPE（Mean Absolute Percentage
Error）平均绝对百分比误差：</strong> <span
class="math inline">\(\dfrac{100\%}{n}\sum\limits_{i=1}^n |\dfrac{f_i -
y_i}{y_i}|\)</span>，范围<span
class="math inline">\([0,+\infty)\)</span>，MAPE 为0%表示完美模型，MAPE
大于 100 %则表示劣质模型。<span class="exturl" data-url="aHR0cHM6Ly96aHVhbmxhbi56aGlodS5jb20vcC8yNTk2NjI4NjQ=">mape和smape<i class="fa fa-external-link-alt"></i></span>。</p>
<p><strong>r2_score（决定系数）:</strong> <span
class="math inline">\(R^2=\dfrac{\sum\limits_{i=1}^n ( y_i -
f_i)^2}{\sum\limits_{i=1}^n ( y_i - \bar y)^2}\)</span>，<span
class="math inline">\(f_i\)</span>是预测值，<span
class="math inline">\(y_i\)</span>是实际值，<span
class="math inline">\(\bar
y\)</span>是实际值的平均值。分子代表预测值对实际值的离散程度，分母代表真实值的离散程度。R平方为回归平方和与总离差平方和的比值，表示总离差平方和中可以由回归平方和解释的比例，这一比例越大越好，模型越精确，回归效果越显著。R平方介于0~1之间，越接近1，回归拟合效果越好，一般认为超过0.8的模型拟合优度比较高。</p>
<h2 id="聚类模型">聚类模型</h2>
<p><strong>RMS（Root Mean Square）：</strong> <span
class="math inline">\(\dfrac{1}{n}\sqrt{\sum\limits_{i=1}^n (x_i-\bar
x)^2}\)</span>，聚类的值减去每个类的平均值，然后平方和开根号，除以n。越小越好，越大表示每个类和它的中心比较远，效果差。</p>
<p><strong>轮廓系数：</strong> <span
class="math inline">\(s(i)=\dfrac{b(i)-a(i)}{max\{a(i),b(i)\}}\)</span>，<span
class="math inline">\(a(i)\)</span>为样本i与簇内其他样本的平均距离，也叫内聚度；<span
class="math inline">\(b(i)\)</span>为样本i与其他某簇样本的平均距离，多个簇<span
class="math inline">\(b(i)\)</span>取最小的，也叫分离度。它结合了内聚度和分离度来评价，可以在相同数据的基础上评价不同的算法，或者算法的不同运行方式对聚类结果产生的影响。这个值越趋近于1，越好；越接近于-1，越差。</p>
<h2 id="关联模型">关联模型</h2>
<p>支持度、置信度、提升度即可。</p>
<h1 id="代码">代码</h1>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> MinMaxScaler,StandardScaler</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> LabelEncoder,OneHotEncoder</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> Normalizer</span><br><span class="line"><span class="keyword">from</span> sklearn.discriminant_analysis <span class="keyword">import</span> LinearDiscriminantAnalysis</span><br><span class="line"><span class="keyword">from</span> sklearn.decomposition <span class="keyword">import</span> PCA</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> pydotplus</span><br><span class="line">os.environ[<span class="string">&quot;PATH&quot;</span>]+=os.pathsep+<span class="string">&quot;D:/Program/Graphviz/bin/&quot;</span></span><br><span class="line"><span class="comment">#sl:satisfaction_level---False:MinMaxScaler;True:StandardScaler</span></span><br><span class="line"><span class="comment">#le:last_evaluation---False:MinMaxScaler;True:StandardScaler</span></span><br><span class="line"><span class="comment">#npr:number_project---False:MinMaxScaler;True:StandardScaler</span></span><br><span class="line"><span class="comment">#amh:average_monthly_hours--False:MinMaxScaler;True:StandardScaler</span></span><br><span class="line"><span class="comment">#tsc:time_spend_company--False:MinMaxScaler;True:StandardScaler</span></span><br><span class="line"><span class="comment">#wa:Work_accident--False:MinMaxScaler;True:StandardScaler</span></span><br><span class="line"><span class="comment">#pl5:promotion_last_5years--False:MinMaxScaler;True:StandardScaler</span></span><br><span class="line"><span class="comment">#dp:department--False:LabelEncoding;True:OneHotEncoding</span></span><br><span class="line"><span class="comment">#slr:salary--False:LabelEncoding;True:OneHotEncoding</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">hr_preprocessing</span>(<span class="params">sl=<span class="literal">False</span>,le=<span class="literal">False</span>,npr=<span class="literal">False</span>,amh=<span class="literal">False</span>,tsc=<span class="literal">False</span>,wa=<span class="literal">False</span>,pl5=<span class="literal">False</span>,dp=<span class="literal">False</span>,slr=<span class="literal">False</span>,lower_d=<span class="literal">False</span>,ld_n=<span class="number">1</span></span>):</span><br><span class="line">    df=pd.read_csv(<span class="string">&quot;./data/HR.csv&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">#1、清洗数据</span></span><br><span class="line">    df=df.dropna(subset=[<span class="string">&quot;satisfaction_level&quot;</span>,<span class="string">&quot;last_evaluation&quot;</span>])</span><br><span class="line">    df=df[df[<span class="string">&quot;satisfaction_level&quot;</span>]&lt;=<span class="number">1</span>][df[<span class="string">&quot;salary&quot;</span>]!=<span class="string">&quot;nme&quot;</span>]</span><br><span class="line">    <span class="comment">#2、得到标注</span></span><br><span class="line">    label = df[<span class="string">&quot;left&quot;</span>]</span><br><span class="line">    df = df.drop(<span class="string">&quot;left&quot;</span>, axis=<span class="number">1</span>)</span><br><span class="line">    <span class="comment">#3、特征选择</span></span><br><span class="line">    <span class="comment">#4、特征处理</span></span><br><span class="line">    scaler_lst=[sl,le,npr,amh,tsc,wa,pl5]</span><br><span class="line">    column_lst=[<span class="string">&quot;satisfaction_level&quot;</span>,<span class="string">&quot;last_evaluation&quot;</span>,<span class="string">&quot;number_project&quot;</span>,\</span><br><span class="line">                <span class="string">&quot;average_monthly_hours&quot;</span>,<span class="string">&quot;time_spend_company&quot;</span>,<span class="string">&quot;Work_accident&quot;</span>,\</span><br><span class="line">                <span class="string">&quot;promotion_last_5years&quot;</span>]</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(scaler_lst)):</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> scaler_lst[i]:</span><br><span class="line">            df[column_lst[i]]=\</span><br><span class="line">                MinMaxScaler().fit_transform(df[column_lst[i]].values.reshape(-<span class="number">1</span>,<span class="number">1</span>)).reshape(<span class="number">1</span>,-<span class="number">1</span>)[<span class="number">0</span>]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            df[column_lst[i]]=\</span><br><span class="line">                StandardScaler().fit_transform(df[column_lst[i]].values.reshape(-<span class="number">1</span>,<span class="number">1</span>)).reshape(<span class="number">1</span>,-<span class="number">1</span>)[<span class="number">0</span>]</span><br><span class="line">    scaler_lst=[slr,dp]</span><br><span class="line">    column_lst=[<span class="string">&quot;salary&quot;</span>,<span class="string">&quot;department&quot;</span>]</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(scaler_lst)):</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> scaler_lst[i]:</span><br><span class="line">            <span class="keyword">if</span> column_lst[i]==<span class="string">&quot;salary&quot;</span>:</span><br><span class="line">                df[column_lst[i]]=[map_salary(s) <span class="keyword">for</span> s <span class="keyword">in</span> df[<span class="string">&quot;salary&quot;</span>].values]</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                df[column_lst[i]]=LabelEncoder().fit_transform(df[column_lst[i]])</span><br><span class="line">            df[column_lst[i]]=MinMaxScaler().fit_transform(df[column_lst[i]].values.reshape(-<span class="number">1</span>,<span class="number">1</span>)).reshape(<span class="number">1</span>,-<span class="number">1</span>)[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            df=pd.get_dummies(df,columns=[column_lst[i]])</span><br><span class="line">    <span class="keyword">if</span> lower_d:</span><br><span class="line">        <span class="keyword">return</span> PCA(n_components=ld_n).fit_transform(df.values),label</span><br><span class="line">    <span class="keyword">return</span> df,label</span><br><span class="line">d=<span class="built_in">dict</span>([(<span class="string">&quot;low&quot;</span>,<span class="number">0</span>),(<span class="string">&quot;medium&quot;</span>,<span class="number">1</span>),(<span class="string">&quot;high&quot;</span>,<span class="number">2</span>)])</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">map_salary</span>(<span class="params">s</span>):</span><br><span class="line">    <span class="keyword">return</span> d.get(s,<span class="number">0</span>)</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">hr_modeling_nn</span>(<span class="params">features,label</span>):</span><br><span class="line">    <span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line">    f_v = features.values</span><br><span class="line">    f_names = features.columns.values</span><br><span class="line">    l_v = label.values</span><br><span class="line">    X_tt, X_validation, Y_tt, Y_validation = train_test_split(f_v, l_v, test_size=<span class="number">0.2</span>)</span><br><span class="line">    X_train, X_test, Y_train, Y_test = train_test_split(X_tt, Y_tt, test_size=<span class="number">0.25</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">from</span> keras.models <span class="keyword">import</span> Sequential</span><br><span class="line">    <span class="keyword">from</span> keras.layers.core <span class="keyword">import</span> Dense, Activation</span><br><span class="line">    <span class="keyword">from</span> keras.optimizers <span class="keyword">import</span> SGD</span><br><span class="line">    mdl = Sequential()</span><br><span class="line">    mdl.add(Dense(<span class="number">50</span>, input_dim=<span class="built_in">len</span>(f_v[<span class="number">0</span>])))</span><br><span class="line">    mdl.add(Activation(<span class="string">&quot;sigmoid&quot;</span>))</span><br><span class="line">    mdl.add(Dense(<span class="number">2</span>))</span><br><span class="line">    mdl.add(Activation(<span class="string">&quot;softmax&quot;</span>))</span><br><span class="line">    sgd = SGD(lr=<span class="number">0.1</span>)</span><br><span class="line">    mdl.<span class="built_in">compile</span>(loss=<span class="string">&quot;mean_squared_error&quot;</span>, optimizer=<span class="string">&quot;adam&quot;</span>)</span><br><span class="line">    mdl.fit(X_train, np.array([[<span class="number">0</span>, <span class="number">1</span>] <span class="keyword">if</span> i == <span class="number">1</span> <span class="keyword">else</span> [<span class="number">1</span>, <span class="number">0</span>] <span class="keyword">for</span> i <span class="keyword">in</span> Y_train]), nb_epoch=<span class="number">1000</span>, batch_size=<span class="number">8999</span>)</span><br><span class="line">    xy_lst = [(X_train, Y_train), (X_validation, Y_validation), (X_test, Y_test)]</span><br><span class="line">    <span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 评价指标</span></span><br><span class="line">    <span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> roc_curve, auc, roc_auc_score</span><br><span class="line">    f = plt.figure()</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(xy_lst)):</span><br><span class="line">        X_part = xy_lst[i][<span class="number">0</span>]</span><br><span class="line">        Y_part = xy_lst[i][<span class="number">1</span>]</span><br><span class="line">        Y_pred = mdl.predict(X_part)</span><br><span class="line">        <span class="built_in">print</span>(Y_pred)</span><br><span class="line">        Y_pred = np.array(Y_pred[:, <span class="number">1</span>]).reshape((<span class="number">1</span>, -<span class="number">1</span>))[<span class="number">0</span>]</span><br><span class="line">        <span class="comment"># print(i)</span></span><br><span class="line">        <span class="comment"># print(&quot;NN&quot;, &quot;-ACC:&quot;, accuracy_score(Y_part, Y_pred))</span></span><br><span class="line">        <span class="comment"># print(&quot;NN&quot;, &quot;-REC:&quot;, recall_score(Y_part, Y_pred))</span></span><br><span class="line">        <span class="comment"># print(&quot;NN&quot;, &quot;-F1:&quot;, f1_score(Y_part, Y_pred))</span></span><br><span class="line">        f.add_subplot(<span class="number">1</span>, <span class="number">3</span>, i + <span class="number">1</span>)</span><br><span class="line">        <span class="comment"># 使用roc_curve绘制ROC曲线，返回三部分</span></span><br><span class="line">        fpr, tpr, threshold = roc_curve(Y_part, Y_pred)</span><br><span class="line">        plt.plot(fpr, tpr)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;NN&quot;</span>, <span class="string">&quot;AUC&quot;</span>, auc(fpr, tpr))</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;NN&quot;</span>, <span class="string">&quot;AUC_Score&quot;</span>, roc_auc_score(Y_part, Y_pred))</span><br><span class="line">    plt.show()</span><br><span class="line">    <span class="keyword">return</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">hr_modeling</span>(<span class="params">features,label</span>):</span><br><span class="line">    <span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line">    f_v=features.values</span><br><span class="line">    f_names=features.columns.values</span><br><span class="line">    l_v=label.values</span><br><span class="line">    X_tt,X_validation,Y_tt,Y_validation=train_test_split(f_v,l_v,test_size=<span class="number">0.2</span>)</span><br><span class="line">    X_train,X_test,Y_train,Y_test=train_test_split(X_tt,Y_tt,test_size=<span class="number">0.25</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 评价指标</span></span><br><span class="line">    <span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score, recall_score, f1_score</span><br><span class="line">    <span class="keyword">from</span> sklearn.neighbors <span class="keyword">import</span> NearestNeighbors,KNeighborsClassifier</span><br><span class="line">    <span class="keyword">from</span> sklearn.naive_bayes <span class="keyword">import</span> GaussianNB,BernoulliNB</span><br><span class="line">    <span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> DecisionTreeClassifier,export_graphviz</span><br><span class="line">    <span class="keyword">from</span> sklearn.externals.six <span class="keyword">import</span> StringIO</span><br><span class="line">    <span class="keyword">from</span> sklearn.svm <span class="keyword">import</span> SVC</span><br><span class="line">    <span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> RandomForestClassifier</span><br><span class="line">    <span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> AdaBoostClassifier</span><br><span class="line">    <span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</span><br><span class="line">    <span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> GradientBoostingClassifier</span><br><span class="line">    models=[]</span><br><span class="line">    models.append((<span class="string">&quot;KNN&quot;</span>,KNeighborsClassifier(n_neighbors=<span class="number">3</span>)))</span><br><span class="line">    models.append((<span class="string">&quot;GaussianNB&quot;</span>,GaussianNB()))</span><br><span class="line">    models.append((<span class="string">&quot;BernoulliNB&quot;</span>,BernoulliNB()))</span><br><span class="line">    models.append((<span class="string">&quot;DecisionTreeGini&quot;</span>,DecisionTreeClassifier()))</span><br><span class="line">    models.append((<span class="string">&quot;DecisionTreeEntropy&quot;</span>,DecisionTreeClassifier(criterion=<span class="string">&quot;entropy&quot;</span>)))</span><br><span class="line">    models.append((<span class="string">&quot;SVM Classifier&quot;</span>,SVC(C=<span class="number">1000</span>)))</span><br><span class="line">    models.append((<span class="string">&quot;OriginalRandomForest&quot;</span>,RandomForestClassifier()))</span><br><span class="line">    models.append((<span class="string">&quot;RandomForest&quot;</span>,RandomForestClassifier(n_estimators=<span class="number">11</span>,max_features=<span class="literal">None</span>)))</span><br><span class="line">    models.append((<span class="string">&quot;Adaboost&quot;</span>,AdaBoostClassifier(n_estimators=<span class="number">100</span>)))</span><br><span class="line">    models.append((<span class="string">&quot;LogisticRegression&quot;</span>,LogisticRegression(C=<span class="number">1000</span>,tol=<span class="number">1e-10</span>,solver=<span class="string">&quot;sag&quot;</span>,max_iter=<span class="number">10000</span>)))</span><br><span class="line">    models.append((<span class="string">&quot;GBDT&quot;</span>,GradientBoostingClassifier(max_depth=<span class="number">6</span>,n_estimators=<span class="number">100</span>)))</span><br><span class="line">    <span class="keyword">for</span> clf_name,clf <span class="keyword">in</span> models:</span><br><span class="line">        clf.fit(X_train,Y_train)</span><br><span class="line">        xy_lst=[(X_train,Y_train),(X_validation,Y_validation),(X_test,Y_test)]</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(xy_lst)):</span><br><span class="line">            X_part=xy_lst[i][<span class="number">0</span>]</span><br><span class="line">            Y_part=xy_lst[i][<span class="number">1</span>]</span><br><span class="line">            Y_pred=clf.predict(X_part)</span><br><span class="line">            <span class="built_in">print</span>(i)</span><br><span class="line">            <span class="built_in">print</span>(clf_name,<span class="string">&quot;-ACC:&quot;</span>,accuracy_score(Y_part,Y_pred))</span><br><span class="line">            <span class="built_in">print</span>(clf_name,<span class="string">&quot;-REC:&quot;</span>,recall_score(Y_part,Y_pred))</span><br><span class="line">            <span class="built_in">print</span>(clf_name,<span class="string">&quot;-F1:&quot;</span>,f1_score(Y_part,Y_pred))</span><br><span class="line">            <span class="comment"># dot_data=StringIO()</span></span><br><span class="line">            <span class="comment"># export_graphviz(clf,out_file=dot_data,</span></span><br><span class="line">            <span class="comment">#                          feature_names=f_names,</span></span><br><span class="line">            <span class="comment">#                          class_names=[&quot;NL&quot;,&quot;L&quot;],</span></span><br><span class="line">            <span class="comment">#                          filled=True,</span></span><br><span class="line">            <span class="comment">#                          rounded=True,</span></span><br><span class="line">            <span class="comment">#                          special_characters=True)</span></span><br><span class="line">            <span class="comment"># graph=pydotplus.graph_from_dot_data(dot_data.getvalue())</span></span><br><span class="line">            <span class="comment"># graph.write_pdf(&quot;dt_tree_2.pdf&quot;)</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">regr_test</span>(<span class="params">features,label</span>):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;X&quot;</span>,features)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Y&quot;</span>,label)</span><br><span class="line">    <span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LinearRegression,Ridge,Lasso</span><br><span class="line">    <span class="comment">#regr=LinearRegression()</span></span><br><span class="line">    regr=Ridge(alpha=<span class="number">1</span>)</span><br><span class="line">    regr.fit(features.values,label.values)</span><br><span class="line">    Y_pred=regr.predict(features.values)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Coef:&quot;</span>,regr.coef_)</span><br><span class="line">    <span class="comment"># 回归模型评价指标</span></span><br><span class="line">    <span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> mean_squared_error,mean_absolute_error,r2_score</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;MSE:&quot;</span>,mean_squared_error(label.values,Y_pred))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;MAE:&quot;</span>,mean_absolute_error(label.values,Y_pred))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;R2:&quot;</span>,r2_score(label.values,Y_pred))</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">main</span>():</span><br><span class="line">    features,label=hr_preprocessing()</span><br><span class="line">    regr_test(features[[<span class="string">&quot;number_project&quot;</span>,<span class="string">&quot;average_monthly_hours&quot;</span>]],features[<span class="string">&quot;last_evaluation&quot;</span>])</span><br><span class="line">    <span class="comment">#hr_modeling(features,label)</span></span><br><span class="line"><span class="keyword">if</span> __name__==<span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    main()</span><br><span class="line"></span><br></pre></td></tr></table></figure>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://soundmemories.github.io/2020/06/17/Machine%20Learning/12.ML-%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avstar.png">
      <meta itemprop="name" content="SoundMemories">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="SoundMemories">
      <meta itemprop="description" content="今日事，今日毕">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | SoundMemories">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2020/06/17/Machine%20Learning/12.ML-%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/" class="post-title-link" itemprop="url">ML-特征工程</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2020-06-17 00:00:00" itemprop="dateCreated datePublished" datetime="2020-06-17T00:00:00+08:00">2020-06-17</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Machine-Learning/" itemprop="url" rel="index"><span itemprop="name">Machine Learning</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>12k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>43 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="特征分析">特征分析</h1>
<h2 id="单特征分析">单特征分析</h2>
<ul>
<li>异常值分析：离散异常值、连续异常值、常识异常值。</li>
<li>对比分析：绝对数与相对数；时间、空间、理论维度比较。</li>
<li>结构分析：各组成部分的分布与规律。</li>
</ul>
<p><strong>1、异常值分析</strong><br />
<img src="/images/数据分析/异常值分析.png" width="74%"></p>
<p>（1）连续异常值可以删除，也可以使用边界值代替。<br />
（2）离散异常值可以删除，也可以把这些异常值都归为一类，做个标记。<br />
（3）常识异常值删除，都是不符合常理的。</p>
<p><strong>2、对比分析</strong><br />
- 对比分析比什么？<br />
- 绝对值<br />
- 相对值：结构、比例、比较、动态、强度。<br />
- 对比分析怎么比？<br />
- 时间：同比（今年和去年同期比较）、环比（这期和上期）<br />
- 空间：不同城市、地域。公司的不同部门。<br />
- 经验与计划</p>
<p><strong>3、结构分析</strong><br />
- 部分和总体<br />
- 静态<br />
- 动态：随着时间的变化，能看出变化趋势。</p>
<p><strong>4、分布分析</strong><br />
- 直接获得概率分布<br />
- 是不是正态分布</p>
<h2 id="多特征分析">多特征分析</h2>
<h3 id="假设检验">假设检验</h3>
<ul>
<li>根据假设条件，从样本推断总体，或者推断样本与样本之间关系的一种方法。</li>
<li>做出一个假设，根据已知的分布或性质，来推断这个假设成立的概率有多大。</li>
</ul>
<ol type="1">
<li>建立原假设<span class="math inline">\(H_0\)</span>(包含等号)，<span
class="math inline">\(H_0\)</span>的反命题为<span
class="math inline">\(H_1\)</span>，也叫备择假设。</li>
<li>选择检验统计量，检验统计量就是根据均值、方差等性质构造的转换函数，构造函数的目的是让这个数据符合一个已知的分布。</li>
<li>根据显著性水平（一般为0.05），确定拒绝域。显著性水平用<span
class="math inline">\(\alpha\)</span>表示，指我们接受假设失真程度的最大限度，显著性水平和相似度和为1，显著水平是认为设定的，设定的值越低，表示要求与设定的分布相似度要求越高。比如：确定某数据有95%的可能为某一分布，那么它的显著性水平就是5%。</li>
<li>计算p值或样本统计值，做出判断，一般有以下两种方法：
<ul>
<li>根据上面的例子，可以画出一个与假设分布相似度95%的区域，这个就是接受域，其余的就是拒绝域，回头看数据，计算其样本统计值，若其落在拒绝域上，那么这个假设就是不成立的。</li>
<li>计算p值，直接和显著性水平进行比较。如果p值小于<span
class="math inline">\(\alpha\)</span>，就可以认为假设是不成立的。</li>
</ul></li>
</ol>
<ul>
<li>检验统计量：
<ul>
<li>知到<span class="math inline">\(\mu_0\)</span>、<span
class="math inline">\(\sigma_0\)</span>：<span class="math inline">\(Z =
\dfrac{\bar X-\mu_0}{\sigma_0/\sqrt{n}} \sim N(0,1)\)</span></li>
</ul></li>
</ul>
<p><strong>卡方检验</strong>，就是统计样本的实际值与预测值之间的偏离程度，实际值与预测值之间的偏离程度就决定卡方值的大小，如果卡方值越大，二者偏差程度越大；反之，二者偏差越小；若两个值完全相等时，卡方值就为0，表明理论值完全符合。<br />
卡方检验公式：<span class="math inline">\(\chi^2 = \sum
\limits_{i=1}^{k}\dfrac{(f_i-npi)^2}{npi}\)</span>，<span
class="math inline">\(f_i\)</span>表示实际值，<span
class="math inline">\(npi\)</span>理论分布值。</p>
<p><strong>t检验</strong>，检验两独立样本均数差异是否能推论至总体、比较两个样本均数的差异、可以使用t检验。<br />
t检验公式：<span class="math inline">\(t = \dfrac{\bar X_1 - \bar
X_2}{\sqrt{\dfrac{(n_1-1)S^2_1+(n_2-1)S^2_2}{n_1+n_2-2}(1/n_1+1/n_2)}}\)</span>，两个样本长度可以不一样。</p>
<p><strong>F检验</strong>，F检验又叫方差齐性检验。比较两个样本方差的差异，在两样本t检验中要用到F检验。<br />
F检验公式：<span class="math inline">\(F =
\dfrac{SSM/(m-1)}{SSE/(n-m)}\)</span>，F满足自由度<span
class="math inline">\((m-1,n-m)\)</span>的F分布。<br />
<span class="math inline">\(SST = \sum \limits_{i=1}^m \sum
\limits_{j=1}^{n_i}(x_{ij}-\bar x)^2\)</span><br />
<span class="math inline">\(SSM = \sum \limits_{i=1}^m \sum
\limits_{j=1}^{n_i}(\bar x_{i}-\bar x)^2\)</span><br />
<span class="math inline">\(SSE = \sum \limits_{i=1}^m \sum
\limits_{j=1}^{n_i}(x_{ij}-\bar x_i)^2\)</span><br />
<span class="math inline">\(m\)</span>指多少组，<span
class="math inline">\(n_i\)</span>指每组数据数量，<span
class="math inline">\(\bar x_i\)</span>指每组的均值，<span
class="math inline">\(\bar x\)</span>指总体的均值。</p>
<p><strong>两样本t检验</strong>：从两研究总体中随机抽取样本，要对这两个样本进行比较的时候，首先要判断两总体方差是否相同，即方差齐性。若两总体方差相等，则直接用t检验，若不等，可采用t'检验或变量变换或秩和检验等方法。其中要判断两总体方差是否相等，就可以用F检验。</p>
<h3 id="相关系数">相关系数</h3>
<p>相关系数是衡量两组变量变化趋势一致性程度。有正相关、负相关、不相关之分。<br />
相关系数范围是[-1,1]，大于0，表示正相关，就是一个变大另一个也变大；小于0，表示负相关，就是一个变大，另一变小；等于0，认为二者不相关（无关联性）。</p>
<p><strong><em>基础知识：</em></strong><br />
<span class="math inline">\(EX=\sum \limits_{i=1}^n x_i
p_i\)</span><br />
<span
class="math inline">\(DX=Var(x)=E[(X-EX)^2]=E(X^2)-(EX)^2\)</span><br />
<span
class="math inline">\(Cov(X,Y)=E[(X-EX)(Y-EY)]=E(XY)-EX*EY\)</span></p>
<p><strong><em>Pearson相关系数</em></strong><br />
<span
class="math inline">\(\rho_{XY}=\dfrac{Cov(X,Y)}{\sqrt{DX}\sqrt{DY}}\)</span><br />
皮尔森相关系数，体现了两个变量间的相关联性，越接近1，表示正相关，就是一个变大另一个也变大；越接近-1，表示负相关，就是一个变大，另一变小。</p>
<p><strong><em>Spearman相关系数</em></strong><br />
<span class="math inline">\(\rho_S=1-\dfrac{6 \sum
d^2_i}{n(n^2-1)}\)</span><br />
<span class="math inline">\(d_i\)</span>是X和Y值排序的序号的差值，<span
class="math inline">\(n\)</span>是X样本数（X和Y样本数量一致）<br />
斯皮尔曼相关系数，只和排序名次差有关，和具体数值无关，所以用在相对比较的情况下比较适合（不考虑具体变量值）。</p>
<h3 id="分组与钻取">分组与钻取</h3>
<p>分组就是把数据按照某一属性分组。连续属性分组前需要离散化。<br />
钻取是改变维的层次，变换分析的粒度。分为向上钻取和向下钻取。</p>
<p>连续属性分组：<br />
（1）分隔（一阶差分）、拐点（二阶差分）；<br />
（2）使用聚类算法；<br />
（3）使用不纯度（Gini）：Gini系数越小，纯度越高，不纯度越低，数据分布越均匀；Gini系数越大，纯度越低，不纯度越高，数据分布越不均匀。</p>
<h1 id="特征工程">特征工程</h1>
<h2 id="特征清洗">特征清洗</h2>
<p>我们在实际项目中拿到的数据往往有不少异常数据，有时候不筛选出这些异常数据很可能让我们后面的数据分析模型有很大的偏差。那么如果我们没有专业知识，如何筛选出这些异常特征样本呢？常用的方法有两种。</p>
<p>第一种是<strong>聚类</strong>，比如我们可以用KMeans聚类将训练样本分成若干个簇，如果某一个簇里的样本数很少，而且簇质心和其他所有的簇都很远，那么这个簇里面的样本极有可能是异常特征样本了。我们可以将其从训练集过滤掉。</p>
<p>第二种是<strong>异常点检测</strong>方法，主要是使用iForest或者one
class SVM，使用异常点检测的机器学习算法来过滤所有的异常点。</p>
<p>当然，某些筛选出来的异常样本是否真的是不需要的异常特征样本，最好找懂业务的再确认一下，防止我们将正常的样本过滤掉了。</p>
<h2 id="特征选择">特征选择</h2>
<p><img src="/images/特征工程/特征选择.png" width="70%"></p>
<p><strong>特征选择：剔除与label不相关或冗余的特征，可以防止过拟合</strong>。<br />
征选择方法有很多，一般分为三类：<br />
第一类<strong>过滤法</strong>，它按照特征的发散性或者相关性指标对各个特征进行评分，设定评分阈值或者待选择阈值的个数，选择合适特征。上面我们提到的方差筛选就是过滤法的一种。<br />
第二类<strong>包装法</strong>，根据目标函数，通常是预测效果评分，每次选择部分特征，或者排除部分特征。<br />
第三类<strong>嵌入法</strong>，它先使用某些机器学习的算法和模型进行训练，得到各个特征的权值系数，根据权值系数从大到小来选择特征。类似于过滤法，但是它是通过机器学习训练来确定特征的优劣，而不是直接从特征的一些统计学指标来确定特征的优劣。</p>
<h3 id="过滤法">过滤法</h3>
<p>（1）特征选择表。计算和label的关联性，设定阈值，剔除冗余特征。<br />
（2）评估单个特征和结果之间的相关程度，排序留下Top相关的特征部分。<br />
主要是使用<strong>卡方检验、信息熵/信息增益率</strong>进行过滤，信息增益率越大越需要保留。<br />
缺点：没有考虑到特征之间的关联作用，可能把有用的关联特征误剔除。</p>
<p>最简单的方法就是<strong>方差</strong>筛选。方差越大的特征，那么我们可以认为它是比较有用的。如果方差较小，比如小于1，那么这个特征可能对我们的算法作用没有那么大。最极端的，如果某个特征方差为0，即所有的样本该特征的取值都是一样的，那么它对我们的模型训练没有任何作用，可以直接舍弃。在实际应用中，我们会指定一个方差的阈值，当方差小于这个阈值的特征会被我们筛掉。sklearn中的VarianceThreshold类可以很方便的完成这个工作。</p>
<p>第二个可以使用的是<strong>相关系数</strong>。这个主要用于输出连续值的监督学习算法中。我们分别计算所有训练集中各个特征与输出值之间的相关系数，设定一个阈值，选择相关系数较大的部分特征。</p>
<p>第三个可以使用的是<strong>假设检验</strong>，比如卡方检验。卡方检验可以检验某个特征分布和输出值分布之间的相关性。个人觉得它比比粗暴的方差法好用。如果大家对卡方检验不熟悉，可以参看这篇<span class="exturl" data-url="aHR0cHM6Ly9zZWdtZW50ZmF1bHQuY29tL2EvMTE5MDAwMDAwMzcxOTcxMg==">卡方检验原理及应用<i class="fa fa-external-link-alt"></i></span>。在sklearn中，可以使用chi2这个类来做卡方检验得到所有特征的卡方值与显著性水平P临界值，我们可以给定卡方值阈值，
选择卡方值较大的部分特征。<br />
除了卡方检验，我们还可以使用F检验和t检验，它们都是使用假设检验的方法，只是使用的统计分布不是卡方分布，而是F分布和t分布而已。在sklearn中，有F检验的函数f_classif和f_regression，分别在分类和回归特征选择时使用。</p>
<p>第四个是<strong>互信息</strong>，即从信息熵的角度分析各个特征和输出值之间的关系评分。在决策树算法中我们讲到过互信息（信息增益）。互信息值越大，说明该特征和输出值之间的相关性越大，越需要保留。在sklearn中，可以使用mutual_info_classif(分类)和mutual_info_regression(回归)来计算各个输入特征和输出值之间的互信息。</p>
<h3 id="包裹法">包裹法</h3>
<p>包装法的解决思路没有过滤法这么直接，它会选择一个目标函数来一步步的筛选特征。<br />
（1）选择一个目标函数，设定一个评价指标，找到这个指标下最佳的特征子集，再在这个子集下迭代减少特征，直到特征量低于阈值。<br />
（2）把特征选择看作一个特征子集搜索问题，筛选各种特征子集，用模型评估效果。</p>
<p>比较常见的是RFE（recursive feature elimination algorithm
递归特征删除算法）方法：<br />
（1）列出特征集合，用全量的特征跑一个模型。<br />
（2）根据线性模型系数（体现相关性），去掉5%-10%的弱特征，观察准确率/auc的变化。<br />
（3）余下的特征重复过程，直到评价指标（准确率/auc）下降较大，或者低于阈值，停止。</p>
<p>以经典的SVM-RFE算法来讨论这个特征选择的思路。这个算法以支持向量机来做RFE的机器学习模型选择特征。它在第一轮训练的时候，会选择所有的特征来训练，得到了分类的超平面<span
class="math inline">\(wx+b=0\)</span>后，如果有n个特征，那么RFE-SVM会选择出w中分量的平方值<span
class="math inline">\(w_i^2\)</span>最小的那个序号i对应的特征，将其排除，在第二类的时候，特征数就剩下n-1个了，我们继续用这n-1个特征和输出值来训练SVM，同样的，去掉<span
class="math inline">\(w_i^2\)</span>最小的那个序号i对应的特征。以此类推，直到剩下的特征数满足我们的需求为止。</p>
<h3 id="嵌入法">嵌入法</h3>
<p>（1）特征嵌入一个简单的模型，根据模型来分析特征重要性。<br />
（2）最常见的使用正则化的方式来做特征选择。<br />
风险：模型选择不当，会使重要特征丢失。<br />
建议：使用嵌入法的模型，最好和最终使用的模型有比较强的关联。比如都用同一种模型、都用同一种分布的，函数图形一致的非线性函数。</p>
<p>嵌入法也是用机器学习的方法来选择特征，但是它和RFE的区别是它不是通过不停的筛掉特征来进行训练，而是使用的都是特征全集。在sklearn中，使用SelectFromModel函数来选择特征。<br />
比如：用一个回归模型训练，最后得到一些W系数，然后对这些W系数做正则化(0~1之间的数)。正则化后的值反映了这些特征的重要程度，越小的就是越可能是不重要特征，根据经验选择。</p>
<p>最常用的是使用<strong>L1正则化</strong>和<strong>L2正则化</strong>来选择特征。在Ridge回归中，正则化惩罚项越大，那么模型的系数就会越小。当正则化惩罚项大到一定的程度的时候，部分特征系数会变成0，当正则化惩罚项继续增大到一定程度时，所有的特征系数都会趋于0.
但是我们会发现一部分特征系数会更容易先变成0，这部分系数就是可以筛掉的。也就是说，<strong>我们选择特征系数较大的特征</strong>。常用的L1正则化和L2正则化来选择特征的基学习器是逻辑回归。<br />
<img src="/images/特征工程/线性回归正则图.png" width="60%"></p>
<p>此外也可以使用决策树或者GBDT。那么是不是所有的机器学习方法都可以作为嵌入法的基学习器呢？也不是，一般来说，可以得到特征系数coef或者可以得到特征重要度(feature
importances)的算法才可以做为嵌入法的基学习器。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.feature_selection <span class="keyword">import</span> SelectKBest <span class="comment"># 过滤</span></span><br><span class="line"><span class="keyword">from</span> sklearn.feature_selection <span class="keyword">import</span> RFE <span class="comment"># 包裹</span></span><br><span class="line"><span class="keyword">from</span> sklearn feature_selection <span class="keyword">import</span> SelectFromModel <span class="comment"># 嵌入</span></span><br></pre></td></tr></table></figure>
<p>问：有些人会奇怪，既然包裹和嵌入都使用了模型，为什么不直接省去特征选择，用这个模型训练？<br />
答：因为我们做特征选择的时候用的不是全部数据，是数据的“一部分”，主要是选择重要特征减少计算量的，所以在特征选择时候为了快速选取特征，不会使用全部数据，也就不能直接做模型训练了。</p>
<h2 id="特征变换">特征变换</h2>
<p><strong>特征变换：根据特征的特性进行一定的转换，使特征能发挥出它的特点。</strong><br />
主要分为：时间型、数值型、类别型、文本型等。</p>
<h3 id="时间型">时间型</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># pd.to_datetime转成时间型</span></span><br><span class="line">.dt.month</span><br><span class="line">.dt.dayofweek</span><br><span class="line">.dt.dayofyear</span><br><span class="line">...</span><br></pre></td></tr></table></figure>
<h3 id="数值型">数值型</h3>
<p>主要有：指数化、对数化、归一化、标准化、正则化、离散化、统计量。特征衍生（四则运算）；数值种类少的转成label
encode。</p>
<h4 id="指数化">指数化</h4>
<p><strong>指数化</strong>：对数据进行指数化<span
class="math inline">\(e^x\)</span>。<br />
一般取e为底数，在大于0的时候，可以放大数据值、放大正半轴数据之间的差值。适合比较小的数据放大，也放大了它们之间的关系。单调性保持了大小关系。对数据进行指数化，指数化然后再归一化的过程，就是Softmax函数。</p>
<h4 id="对数化">对数化</h4>
<p><strong>对数化</strong>：对数据进行对数化<span
class="math inline">\(log(x)\)</span>。<br />
可以取自然底数e，也可以是2或者10。可以把大于1的数据缩小、间隔拉小。适合比较大的数据放小，也缩小了它们之间的关系。单调性保持了大小关系。</p>
<h4 id="归一化">归一化</h4>
<p><strong>归一化</strong>：也称为离差标准化，预处理后使特征值映射到[0,1]之间，可以消除量纲影响。</p>
<p>具体的方法是求出样本特征x的最大值max和最小值min，然后用(x-min)/(max-min)来代替原特征：<br />
min-max归一化：<span class="math inline">\(x&#39;=
\dfrac{x-min}{max-min}\)</span>，所有的值被映射到[0,1]范围。</p>
<p>如果我们希望将数据映射到任意一个区间[a,b]，而不是[0,1]，那么也很简单：<br />
min-max归一化：<span class="math inline">\(x&#39; =
\dfrac{(x-min)(b-a)}{(max-min)+a}\)</span>，所有的值被映射到[a,b]范围。</p>
<p>在sklearn中，我们可以用MinMaxScaler来做max-min标准化。这种方法的问题就是如果测试集或者预测数据里的特征有小于min，或者大于max的数据，会导致max和min发生变化，需要重新计算。所以实际算法中，
除非你对特征的取值区间有需求，否则max-min标准化没有z-score标准化好用。</p>
<h4 id="标准化">标准化</h4>
<p><strong>标准化</strong>：也叫z-score标准化。数据映射成均值为0，方差为1的分布。</p>
<p>具体的方法是求出样本特征x的均值<span
class="math inline">\(\mu\)</span>和标准差<span
class="math inline">\(\sigma\)</span>：<br />
z-score标准化：<span class="math inline">\(x&#39; =
\dfrac{x-\mu}{\sigma}\)</span>，符合均值为0，标准差为1的正态分布。</p>
<p>这是最常见的特征预处理方式，基本所有的线性模型在拟合的时候都会做
z-score标准化。<br />
在sklearn中，我们可以用StandardScaler来做z-score标准化。当然，如果我们是用pandas做数据预处理，可以自己在数据框里面减去均值，再除以方差，自己做z-score标准化。</p>
<h4 id="正则化">正则化</h4>
<p><strong>正则化</strong>：也叫正规化、规范化。将特征向量的长度正规到单位1，就是各个维度到中心距离为单位1，距离尺度的衡量用L1就是L1正规化，用L2就是L2正规化（欧氏距离）。<br />
L1：<span class="math inline">\(x&#39; = \dfrac{x_i}{\sum
\limits_{j=1}^{n}|x_j|}\)</span><br />
L2：<span class="math inline">\(x&#39; = \dfrac{x_i}{\sqrt{\sum
\limits_{j=1}^{n}|x_j|^2}}\)</span><br />
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[1,1,3,-1,2]-&gt;[1/8,1/8,3/8,-1/8,2/8]（L1正则）</span><br><span class="line">[1,1,3,-1,2]-&gt;[1/4,1/4,3/4,-1/4,2/4]（L2正则）</span><br></pre></td></tr></table></figure><br />
使用方式：<br />
（1）直接用在特征上（用的很少），如果我们只是为了统一量纲，那么通过L2范数整体标准化也是可以的。<br />
（2）用在每个对象的各个特征的表示（特征矩阵的行），可以体现一个对象特征之间的相对关系特点（用的较多）。<br />
（3）用在模型的参数上，使其更平滑（回归模型使用较多）。<br />
<strong>逻辑回归、SVM、神经网络</strong>最好都做正则化处理。</p>
<p>在sklearn中，我们可以用Normalizer来做L1/L2范数标准化。</p>
<p>此外，经常我们还会用到中心化，主要是在PCA降维的时候，此时我们求出特征x的平均值mean后，用x-mean代替原特征，也就是特征的均值变成了0,
但是方差并不改变。这个很好理解，因为PCA就是依赖方差来降维的。<br />
虽然大部分机器学习模型都需要做标准化和归一化，也有不少模型可以不做做标准化和归一化，主要是基于概率分布的模型，比如决策树大家族的CART，随机森林等。当然此时使用标准化也是可以的，大多数情况下对模型的泛化能力也有改进。</p>
<h4 id="离散化">离散化</h4>
<p><strong>离散化</strong>：把连续数据变成离散的。<strong>通过离散化，可以把一个单调相关的连续值，变成可以同时照顾的离散值。</strong><br />
离散化的原因：<br />
（1）克服数据缺陷：比如连续数据中有噪声影响，离散化后分组对比，会更有说服力。<br />
（2）某些算法要求：某些算法要求数据是离散值，比如朴素贝叶斯。<br />
（3）非线性数据映射：比如连续数据有明显的拐点，那么以拐点的离散分组，就可以代表不同的含义。<br />
离散化方法：<br />
（1）等频分箱：也叫等深分箱，先排序，深度就是数据的个数，等深就是每个箱的个数相等。<br />
（2）等距分箱：也叫等宽分箱，先排序，宽度就是数据的区间，等宽就是每个箱的区间相等。<br />
（3）自因变量优化：根据自变量因变量变化，找到拐点，进行离散化。比如：分隔（一阶差分）、拐点（二阶差分）；使用聚类算法；使用不纯度（Gini）。<br />
<strong>等频分箱</strong>：<br />
<img src="/images/特征工程/等深分箱.png" width="70%"></p>
<p><strong>等距分箱</strong>：<br />
<img src="/images/特征工程/等宽分箱.png" width="70%"></p>
<p>Pandas的等深分箱：pd.qcut(q=每个箱内数据的个数)<br />
Pandas的等宽分箱：pd.cut(bins=分成几段)</p>
<h4 id="非线性编码">非线性编码</h4>
<p>使用多项式、高斯核等方法编码。</p>
<h4 id="统计量">统计量</h4>
<p>正负值个数、均值、方差、最大值、最小值、偏度、峰度。</p>
<h3 id="类别型">类别型</h3>
<p>把非数值特征转换成数值特征。<br />
（1）自然编码（label encode）；<br />
（2）独热编码（one-hot encode）；<br />
（3）分层编码；<br />
（4）hash与聚类处理。</p>
<p><img src="/images/特征工程/数值化.png" width="70%"></p>
<h4 id="自然编码">自然编码</h4>
<p>定序：有顺序关系，大小关系，使用Label Encode。<br />
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">down-&gt;0</span><br><span class="line">up  -&gt;1</span><br><span class="line"></span><br><span class="line">low   -&gt;0</span><br><span class="line">medium-&gt;1</span><br><span class="line">high  -&gt;2</span><br></pre></td></tr></table></figure></p>
<h4 id="独热编码">独热编码</h4>
<p>定类：无大小关系，只有类别关系，使用One-Hot Encode。<br />
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">red   -&gt;[1,0,0,0]</span><br><span class="line">yellow-&gt;[0,1,0,0]</span><br><span class="line">red   -&gt;[0,0,1,0]</span><br><span class="line">red   -&gt;[1,0,0,1]</span><br></pre></td></tr></table></figure></p>
<h4 id="分层编码">分层编码</h4>
<p>按类别分层编码。比如，例统计类别比例转成数值型；对邮政编码、身份证等进行数据划分编码。</p>
<h3 id="文本型">文本型</h3>
<p>1、词性标注<br />
2、文本统计特征：文本长度、单词个数、数字个数、数字占比、名词数、动词数。<br />
3、N-gram：将文本表示成连续的序列。<br />
4、词袋模型：分量重的取值为文档中的词频。<br />
5、TF-IDF<br />
6、word2vec</p>
<h2 id="特征降维">特征降维</h2>
<h3 id="pca">PCA</h3>
<p>步骤：<br />
（1）求特征协方差矩阵；<br />
（2）求协方差的特征值和特征向量；<br />
（3）将特征值按照从大到小的顺序排序，选择其中最大的k个；<br />
（4）将样本点投影到选取的特征向量上；<br />
缺点：不管是PCA还是SVD都只考虑用特征间相关性强弱来决定降维后的形态，是一种无监督的方法（没有考虑到Label）。</p>
<h3 id="lda">LDA</h3>
<p>LDA（Linear Discriminant
Analysis）线性判别式分析。注意，不是文本分析的Latent Dirichlet
Allocation隐含狄利克雷分布。</p>
<p><strong>LDA核心思想：投影变换后，同一标注间距离尽可能小；不用标注间距离尽可能大。</strong><br />
将带上标签数据(点)，通过投影(变换)的方法，投影更低维的空间。在这个低维空间中，同类样本尽可能接近，异类样本尽可能远离。<br />
<img src="/images/特征工程/LDA图.png" width="70%"></p>
<p>具体原理可参考<span class="exturl" data-url="aHR0cHM6Ly93d3cuY25ibG9ncy5jb20vcGluYXJkL3AvNjI0NDI2NS5odG1s">线性判别分析LDA原理总结<i class="fa fa-external-link-alt"></i></span>这篇文章。<br />
下图给出了二分类LDA过程：<br />
<img src="/images/特征工程/LDA_1.png" width="70%"><br />
<img src="/images/特征工程/LDA_2.png" width="70%"><br />
<img src="/images/特征工程/LDA_3.png" width="70%"><br />
<img src="/images/特征工程/LDA_4.png" width="70%"><br />
<img src="/images/特征工程/LDA_5.png" width="70%"><br />
<img src="/images/特征工程/LDA_6.png" width="70%"></p>
<h2 id="特征衍生">特征衍生</h2>
<p>特征衍生：利用现有特征，进行某些组合，生成新的具有某些含义的特征。<br />
通常我们采集到的数据维度不是很大，而且采集到的特征不一定能反映全部数据的信息，需要通过已有的数据组合发现新的含义特征。<br />
比如，有车的路程特征和时间间隔特征，我们就可以得到车的平均速度这个二级特征。根据车的速度特征，我们就可以得到车的加速度这个三级特征，根据车的加速度特征，我们就可以得到车的加加速度这个四级特征。。。也就是说，高级特征可以一直寻找下去。</p>
<p>在Kaggle之类的算法竞赛中，高分团队主要使用的方法除了集成学习算法，剩下的主要就是在高级特征上面做文章。所以寻找高级特征是模型优化的必要步骤之一。当然，在第一次建立模型的时候，我们可以先不寻找高级特征，得到以后基准模型后，再寻找高级特征进行优化。</p>
<p>常用方法：<br />
（1）四则运算；<br />
若干项特征加和：假设你希望根据每日销售额得到一周销售额的特征。你可以将最近的7天的销售额相加得到。<br />
若干项特征之差：假设你已经拥有每周销售额以及每月销售额两项特征，可以求一周前一月内的销售额。<br />
若干项特征乘积：假设你有商品价格和商品销量的特征，那么就可以得到销售额的特征。<br />
若干项特征除商：假设你有每个用户的销售额和购买的商品件数，那么就是得到该用户平均每件商品的销售额。<br />
（2）求导与高阶求导；<br />
比如，有速度特征和时间特征，能得到加速度特征。<br />
（3）人工归纳。<br />
当然，寻找高级特征的方法远不止于此，它需要你根据你的业务和模型需要而得，而不是随便的两两组合形成高级特征，这样容易导致特征爆炸，反而没有办法得到较好的模型。<br />
个人经验是，聚类的时候高级特征尽量少一点，分类回归的时候高级特征适度的多一点。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> PolynomialFeatures</span><br></pre></td></tr></table></figure>
<h1 id="过拟合和欠拟合">过拟合和欠拟合</h1>
<p><strong>过拟合</strong>：模型在训练集上表现好，在测试集和新数据上表现差。<br />
<strong>欠拟合</strong>：模型在训练集和测试集上表现都差。</p>
<p><strong>降低过拟合风险</strong>：<br />
1. 增加数据。<br />
2. 降低模型复杂度，如减少神经元数量、网络层数，降低树深度，剪枝。<br />
3. 增加正则化。<br />
4. 使用集成学习方法，降低单一模型过拟合风险。</p>
<p><strong>降低欠拟合风险</strong>：<br />
1. 增加新特征。<br />
2. 增加模型复杂度。<br />
3. 减小正则化系数。</p>
<h1 id="样本不平衡处理">样本不平衡处理</h1>
<p>所谓不平衡是指不同类别的样本量差异非常大，或者少数样本代表了业务关键数据，需要对少量样本的模式有很好的学习。<br />
样本不平衡问题主要在<strong>分类问题</strong>上常见。</p>
<p>在工程上，应对样本均衡问题常从以下三方面入手：<br />
1、数据相关处理：<br />
（1）欠采样：在少量样本数量<strong>不影响模型训练</strong>的情况下，可以通过<strong>对多数样本欠采样</strong>，实现少数样本和多数样本的均衡。<br />
（2）过采样：在少量样本数量<strong>不支撑模型训练</strong>的情况下，可以通过<strong>对少量样本过采样</strong>，实现少数样本和多数样本的均衡。<br />
2、模型相关处理：<br />
（1）模型算法：通过引入有倚重的模型算法，针对少量样本着重拟合，以提升对少量样本特征的学习。<br />
3、评价指标相关处理<br />
（1）设计相关指标，使指标偏向样本少的那类。</p>
<p>想了解更多<strong>数据相关</strong>方法，可以参考<span class="exturl" data-url="aHR0cHM6Ly9pbWJhbGFuY2VkLWxlYXJuLm9yZy9zdGFibGUvcmVmZXJlbmNlcy9pbmRleC5odG1s">imbalanced-learn<i class="fa fa-external-link-alt"></i></span>。</p>
<h2 id="欠采样">欠采样</h2>
<p><strong>欠采样</strong>，也叫下采样（under-sampling，us）。通过<strong>减少分类中多数类样本的数量</strong>来实现样本均衡。通过欠采样，在保留少量样本的同时，<strong>会丢失多数样本中的一些信息</strong>。经过欠采样，严格不能总量在减少。<br />
<img src="/images/数据分析/欠采样.jpg" width="50%"></p>
<p>欠采样有多种方法，常见的有：<br />
<strong>1、随机删除</strong><br />
（1）分别确定样本集中多量样本数<span
class="math inline">\(N_{max}\)</span>和少量样本数<span
class="math inline">\(N_{min}\)</span>；<br />
（2）确定采样样本集中多量样本与少量样本比值<span
class="math inline">\(N_{ratio}\)</span>；<br />
（3）以少量样本为基准，确定多量样本采样总数<span
class="math inline">\(N_{sample}=N_{ratio}*N_{min}\)</span><br />
（4）以<span
class="math inline">\(N_{sample}\)</span>为限，对多量样本进行随机抽样。<br />
缺点：不确定性很大，一般不采用这种方法。</p>
<p>算法中，可以以样本序号为种子，利用python自带的ranom.sample()函数进行行采样。<br />
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"></span><br><span class="line">seed_idx = np.arange(<span class="number">0</span>,<span class="number">100</span>,<span class="number">1</span>)</span><br><span class="line">sample_idx = random.sample(seed_idx,<span class="number">40</span>)</span><br></pre></td></tr></table></figure><br />
<strong>2、原型生成</strong>（prototype generation）<br />
原型生成主要是<strong>在多量样本的基础上，生成新的子集样本</strong>来实现样本均衡，<strong>能保证样本分布和原先相同</strong>。<br />
（1）以少量样本总数出发，确定均衡后多量样本总数<span
class="math inline">\(N_{max}\)</span>；<br />
（2）多量样本出发，利用K-means算法随机的计算K个多量样本的中心；<br />
（3）认为K-means的中心点可以代表样本簇的特性，以该中心点<span
class="math inline">\(s^\prime\)</span>代表样本簇；<br />
（4）重复2、3两步<span
class="math inline">\(N_{max}\)</span>次，生成新的多量样本集合<span
class="math inline">\(S^\prime=s_1^\prime,s_2^\prime,...,s_{N_{max}}^\prime\)</span>，且<span
class="math inline">\(S^\prime \notin S\)</span>。</p>
<p>下图所示，为三类共2000个样本点的集合，每个样本维度为2，label1个数为1861个，label2个数为108个，label3个数为31个。利用原型生成算法完成数据均衡后，样本整体分布没有变化。<br />
<img src="/images/数据分析/Cluster Centroids.png" width="70%"></p>
<p><strong>3、原型选择</strong>（prototype selection）<br />
NearMiss<strong>从多数类样本中选取最具代表性的样本</strong>用于训练，主要是为了缓解随机欠采样中的信息丢失问题。<br />
NearMiss采用一些启发式的规则来选择样本，根据规则的不同可分为3类：<br />
（1）NearMiss-1：在多数类样本中选择与最近的K个少数类样本平均距离最近的多数类样本。<br />
（2）NearMiss-2：在多数类样本中选择与最远的K个少数类样本平均距离最近的多数类样本。<br />
（3）NearMiss-3：对于每个少数类样本选择K个最近的多数类样本，目的是保证每个少数类样本都被多数类样本包围。</p>
<p>NearMiss-1考虑的是与最近的3个少数类样本的平均距离，是局部的；NearMiss-2考虑的是与最远的3个少数类样本的平均距离，是全局的。NearMiss-3方法则会使得每一个少数类样本附近都有足够多的多数类样本，显然这会使得模型的精确度高、召回率低。论文中有对这几种方法的比较，得到的结论是NearMiss-2的效果最好，不过这也是需要综合考虑数据集和采样比例的不同造成的影响。<br />
NearMiss-1和NearMiss-2的计算开销很大，因为需要计算每个多类别样本的K近邻点。另外，NearMiss-1易受离群点的影响，如下面第二幅图中合理的情况是处于边界附近的多数类样本会被选中，然而由于右下方一些少数类离群点的存在，其附近的多数类样本就被选择了。相比之下NearMiss-2和NearMiss-3不易产生这方面的问题。<br />
<img src="/images/数据分析/NearMiss.png" width="70%"></p>
<p><strong>4、利用模型融合的方法（Ensemble）</strong>：<br />
<strong>EasyEnsemble</strong>：多次欠采样（放回采样，这样产生的训练集才相互独立）产生多个不同的训练集，进而训练多个不同的分类器，通过组合多个分类器的结果得到最终的结果。</p>
<p><strong>5、利用增量训练的思想（Boosting）</strong>：<br />
<strong>BalanceCascade</strong>：先通过欠采样产生训练集，训练一个分类器，剔除初始样本中被此分类器分类正确的多数类样本，经过多次迭代，初始样本中多数类样本数量和少数类样本数量平衡，最终组合所有分类器的结果得到最终结果。</p>
<h2 id="过采样">过采样</h2>
<p>过采样，也叫上采样（over-sampling）方法，通过增加分类中少数样本数量来实现样本均衡。缺点就是<strong>样本特征少时，会导致过拟合</strong>。<br />
<img src="/images/数据分析/过采样.jpg" width="50%"></p>
<p>过采样有多种方法，常见的有：<br />
<strong>1、随机重复</strong><br />
这个和前面讲的随机删除有异曲同工之妙，通过对少数样本随机复制，达到样本均衡。<br />
最直接的方法是随机复制，但这样<strong>很容易过拟合</strong>。经过改进的抽样方法是通过在少数类中加入随机的噪声、干扰数据或通过一定规则产生新的合成样本。</p>
<p>下图所示，在进行复制前，Linear
SVC只找到一个超平面，即认为样本集中仅有两类样本。随机复制后，Linear
SVC找到了另外两个超平面。<br />
<img src="/images/数据分析/RandomOverSampler.png" width="75%"></p>
<p><strong>2、样本构建</strong><br />
样本构建方法有很多，常见的有 <strong>SMOTE</strong>（Synthetic Minority
Oversampling TEchnique）和 <strong>ADASYN</strong>（Adaptive Synthetic）
。</p>
<p><strong>SMOTE</strong>：对少数类样本集中的每个样本 <span
class="math inline">\(x\)</span>，从它在的少数类样本集中的 K近邻
中随机选一个样本 <span class="math inline">\(y\)</span>，然后在 <span
class="math inline">\(x,y\)</span>
的连线上随机选取一点作为新合成的样本。<br />
SMOTE缺点：为每个少数类样本合成相同数量的新样本，可能会<strong>增大类间重叠度</strong>，并且会生成一些不能提供有益信息的样本。而且<strong>维度过高效果不好</strong>，所以实际不常用。</p>
<p>针对SMOTE缺点，衍生出SMOTEBoost、Borderline-SMOTE、Kmeans-SMOTE等。<br />
<strong>SMOTEBoost</strong>：把SMOTE算法和Boost算法结合，在每一轮分类学习过程中增加对少数类的样本的权重，使得基学习器能过更好得关注到少数类样本。<br />
<strong>Borderline-SMOTE</strong>：在构造样本时考虑少量样本周围的样本分布，选择少量样本集合（DANGER集合）————其邻居节点既有多量样本也有少量样本，且多量样本数不大于少量样本的点来构造新样本。<br />
<strong>Kmeans-SMOTE</strong>：包括聚类、过滤和过采样三步。利用Kmeans算法完成聚类后，进行样本过滤，在每个样本簇内利用SMOTE算法构建新样本。<br />
<img src="/images/数据分析/SMOTE.png" width="100%"></p>
<p><strong>ADASYN</strong>：根据数据分布情况为少数类样本生成新样本。</p>
<p>通过比较比较不同算法得到的样本构造，可得到如下结论：<br />
不同算法构建的新样本在<strong>数量</strong>和<strong>分布</strong>上不同：<br />
（1）其中利用<strong>SMOTE算法</strong>构建的新样本，没有考虑原始样本分布情况，构建的新样本受到“噪声”点的影响。<br />
（2）同样<strong>ADASYN算法</strong>只考虑了分布密度而未考虑样本分布，构建的新样本也会受到“噪声”点的影响。<br />
（3）<strong>Borderline-SMOTE算法</strong>考虑了样本的分布，构建的新样本能够比较好的避免“噪声”点的影响。<br />
（4）<strong>Kmeans-SMOTE算法</strong>由于要去寻找簇后再构建新样本，可构建的新样本数量受限。<br />
注意：“噪声”点对应类别上属于少量样本，但是分布上比较靠近边界或与多量样本混为一起。</p>
<h2 id="模型相关">模型相关</h2>
<p>上面的过采样和欠采样都是从样本的层面去克服样本的不平衡。从算法层面来说，不同类型的错误造成的后果不同。</p>
<p>比如，医疗诊断中，把患者诊断为健康人、把健康人诊断为患者，这看起来都是“错了一次”，但是代价不同，前者可能会丧失拯救声明的机会，后者可能增加进一步检查的麻烦。<br />
比如，门禁系统中，把可通行的人拦截在外，把陌生人放进门，前者使用户体验不佳，后者可能造成严重的安全事故。<br />
比如，信用卡盗用检查中，把正常使用误认为盗用，把盗用误认为正常使用，前者使用户体验不佳，后者将使用户承受巨大损失。</p>
<p>为了权衡不同类型错误所造成的不同损失，可为错误赋予<strong>非均等代价</strong>。<br />
常见的方法有：<br />
（1）损失函数计算时，对不同类别的样本赋予不同的权重。使用Focal
loss损失函数。<br />
（2）使用模型集成方法，比如随机森林等。<br />
（3）使用异常点检测算法。<br />
（4）小样本学习。</p>
<p><strong>1、Cost Sensitive算法</strong><br />
可根据任务设定一个代价矩阵，比如二分类任务，<span
class="math inline">\(cost_{ij}\)</span>表示将第<span
class="math inline">\(i\)</span>类样本预测为第<span
class="math inline">\(j\)</span>类样本的代价。一般来说，<span
class="math inline">\(cost_{ii}=0\)</span>，若将第0类判别为第1类所造成的损失更大，则<span
class="math inline">\(cost_{01}&gt;cost_{10}\)</span>，损失程度相差越大，<span
class="math inline">\(cost_{01}\)</span>和<span
class="math inline">\(cost_{10}\)</span>值的差别越大。<br />
<img src="/images/数据分析/代价矩阵.png" width="30%"></p>
<p>从学习模型出发，对某一具体学习方法的改造，使之能适应不平衡数据下的学习，研究者们针对不同的学习模型，如感知机、支持向量机、决策树、神经网络等分别提出了其代价敏感的版本。以代价敏感的决策树为例，可从三个方面对其进行改造以适应不平衡数据的学习：<strong>决策阈值的选择、分裂标准的选择、剪枝</strong>。这三个方面都可以将代价矩阵引入。</p>
<p>从贝叶斯风险理论出发，把代价敏感学习看成是分类结果的一种后处理，按照传统方法学习到一个模型，以实现损失最小为目标对结果进行调整。此方法优点在于它可以不依赖所用的具体分类器，但是缺点也很明显，它要求分类器输出值为概率。</p>
<p>从预处理的角度出发，将代价用于权重调整，使得分类器满足代价敏感的特性。</p>
<p><strong>2、Meta Cost算法</strong><br />
<img src="/images/数据分析/Cost Sensitive.png" width="40%"></p>
<p>（1）在训练集中多次采样，生成多个模型。<br />
（2）根据多个模型，得到训练集中每条记录属于每个类别的概率。<br />
（3）计算训练集中每条记录的属于每个类别的代价，根据最小代价，修改标签。<br />
（4）训练修改过的数据集，得到新的模型。</p>
<p><strong>3、Focal loss算法</strong><br />
Focal
loss是在标准交叉熵损失基础上修改得到的，通过减少易分类样本的权重，使得模型在训练时更专注于难分类的样本。了解细节可参考<span class="exturl" data-url="aHR0cHM6Ly96aHVhbmxhbi56aGlodS5jb20vcC80OTk4MTIzNA==">Focal
loss论文详解<i class="fa fa-external-link-alt"></i></span>。<br />
<img src="/images/数据分析/Focal loss.png" width="80%"></p>
<h2 id="评价指标">评价指标</h2>
<p>样本不平衡的情况下，由于少量样本占比较小，如果仅考虑Error
Rate或Accuracy，即使模型全部把少量样本分错，其整体的Error
Rate和Accuracy还是比较高，因此，对于样本不平衡情况下，引入另外一个评价指标————<strong>G-mean</strong>。</p>
<p><strong>G-mean</strong>：<span
class="math inline">\(\sqrt{\dfrac{TP}{TP+FN}\times\dfrac{TN}{TN+FP}}\)</span></p>
<p>在下一篇模型评估中，详细讲解了评价指标。</p>
<h2 id="数据增强">数据增强</h2>
<p>常见的数据增强：UDA，EDA，MixText，LOTClass</p>
<p><strong>1、UDA</strong>（Unsupervised Data Augmentation）<br />
原文：<span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzE5MDQuMTI4NDgucGRmP3JlZj1oYWNrZXJub29uLmNvbQ==">Unsupervised
Data Augmentation for Consistency Training<i class="fa fa-external-link-alt"></i></span>。<br />
Github：<span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL2dvb2dsZS1yZXNlYXJjaC91ZGE=">uda<i class="fa fa-external-link-alt"></i></span>。</p>
<p><img src="/images/数据分析/UDA.png" width="90%"><br />
<img src="/images/数据分析/UDA_loss.png" width="90%"></p>
<p><strong>2、EDA</strong>（Easy Data Augmentation）<br />
原文：<span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzE5MDEuMTExOTYucGRm">EDA: Easy Data
Augmentation Techniques for Boosting Performance on Text Classification
Tasks<i class="fa fa-external-link-alt"></i></span>。<br />
Github：<span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL3poYW5sYW9iYW4vRURBX05MUF9mb3JfQ2hpbmVzZQ==">EDA_NLP_for_Chinese<i class="fa fa-external-link-alt"></i></span>。</p>
<h1 id="代码">代码</h1>
<p>特征分析：<br />
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">main</span>():</span><br><span class="line">    <span class="keyword">import</span> scipy.stats <span class="keyword">as</span> ss</span><br><span class="line">    <span class="built_in">print</span>(ss.normaltest(ss.norm.rvs(size=<span class="number">10</span>)))<span class="comment">#正态检验</span></span><br><span class="line">    <span class="built_in">print</span>(ss.chi2_contingency([[<span class="number">15</span>, <span class="number">95</span>], [<span class="number">85</span>, <span class="number">5</span>]], <span class="literal">False</span>))<span class="comment">#卡方四格表</span></span><br><span class="line">    <span class="built_in">print</span>(ss.ttest_ind(ss.norm.rvs(size=<span class="number">10</span>), ss.norm.rvs(size=<span class="number">20</span>)))<span class="comment">#t独立分布检验</span></span><br><span class="line">    <span class="built_in">print</span>(ss.f_oneway([<span class="number">49</span>, <span class="number">50</span>, <span class="number">39</span>,<span class="number">40</span>,<span class="number">43</span>], [<span class="number">28</span>, <span class="number">32</span>, <span class="number">30</span>,<span class="number">26</span>,<span class="number">34</span>], [<span class="number">38</span>,<span class="number">40</span>,<span class="number">45</span>,<span class="number">42</span>,<span class="number">48</span>]))<span class="comment">#F分布检验</span></span><br><span class="line">    <span class="keyword">from</span> statsmodels.graphics.api <span class="keyword">import</span> qqplot</span><br><span class="line">    <span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line">    qqplot(ss.norm.rvs(size=<span class="number">100</span>))<span class="comment">#QQ图，横轴正态分布四分位数，纵轴你的数据分布四分位数，如果画出来是x=y的一条直线就是符合正态分布。</span></span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line">    s = pd.Series([<span class="number">0.1</span>, <span class="number">0.2</span>, <span class="number">1.1</span>, <span class="number">2.4</span>, <span class="number">1.3</span>, <span class="number">0.3</span>, <span class="number">0.5</span>])</span><br><span class="line">    df = pd.DataFrame([[<span class="number">0.1</span>, <span class="number">0.2</span>, <span class="number">1.1</span>, <span class="number">2.4</span>, <span class="number">1.3</span>, <span class="number">0.3</span>, <span class="number">0.5</span>], [<span class="number">0.5</span>, <span class="number">0.4</span>, <span class="number">1.2</span>, <span class="number">2.5</span>, <span class="number">1.1</span>, <span class="number">0.7</span>, <span class="number">0.1</span>]])</span><br><span class="line">    <span class="comment">#相关分析</span></span><br><span class="line">    <span class="built_in">print</span>(s.corr(pd.Series([<span class="number">0.5</span>, <span class="number">0.4</span>, <span class="number">1.2</span>, <span class="number">2.5</span>, <span class="number">1.1</span>, <span class="number">0.7</span>, <span class="number">0.1</span>])))</span><br><span class="line">    <span class="built_in">print</span>(df.corr())</span><br><span class="line"></span><br><span class="line">    <span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">    <span class="comment">#回归分析</span></span><br><span class="line">    x = np.arange(<span class="number">10</span>).astype(np.<span class="built_in">float</span>).reshape((<span class="number">10</span>, <span class="number">1</span>))</span><br><span class="line">    y = x * <span class="number">3</span> + <span class="number">4</span> + np.random.random((<span class="number">10</span>, <span class="number">1</span>))</span><br><span class="line">    <span class="built_in">print</span>(x)</span><br><span class="line">    <span class="built_in">print</span>(y)</span><br><span class="line">    <span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LinearRegression</span><br><span class="line">    linear_reg = LinearRegression()</span><br><span class="line">    reg = linear_reg.fit(x, y)</span><br><span class="line">    y_pred = reg.predict(x)</span><br><span class="line">    <span class="built_in">print</span>(reg.coef_)</span><br><span class="line">    <span class="built_in">print</span>(reg.intercept_)</span><br><span class="line">    <span class="built_in">print</span>(y.reshape(<span class="number">1</span>, <span class="number">10</span>))</span><br><span class="line">    <span class="built_in">print</span>(y_pred.reshape(<span class="number">1</span>, <span class="number">10</span>))</span><br><span class="line">    plt.figure()</span><br><span class="line">    plt.plot(x.reshape(<span class="number">1</span>, <span class="number">10</span>)[<span class="number">0</span>], y.reshape(<span class="number">1</span>, <span class="number">10</span>)[<span class="number">0</span>], <span class="string">&quot;r*&quot;</span>)</span><br><span class="line">    plt.plot(x.reshape(<span class="number">1</span>, <span class="number">10</span>)[<span class="number">0</span>], y_pred.reshape(<span class="number">1</span>, <span class="number">10</span>)[<span class="number">0</span>])</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line">    <span class="comment">#PCA降维，sklearn的PCA使用的是SVD的方法</span></span><br><span class="line">    df = pd.DataFrame(np.array([np.array([<span class="number">2.5</span>, <span class="number">0.5</span>, <span class="number">2.2</span>, <span class="number">1.9</span>, <span class="number">3.1</span>, <span class="number">2.3</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">1.5</span>, <span class="number">1.1</span>]),</span><br><span class="line">                                np.array([<span class="number">2.4</span>, <span class="number">0.7</span>, <span class="number">2.9</span>, <span class="number">2.2</span>, <span class="number">3</span>, <span class="number">2.7</span>, <span class="number">1.6</span>, <span class="number">1.1</span>, <span class="number">1.6</span>, <span class="number">0.9</span>])]).T)</span><br><span class="line">    <span class="keyword">from</span> sklearn.decomposition <span class="keyword">import</span> PCA</span><br><span class="line">    lower_dim = PCA(n_components=<span class="number">1</span>)</span><br><span class="line">    lower_dim.fit(df.values)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;PCA&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(lower_dim.explained_variance_ratio_) <span class="comment"># 它代表降维后的各主成分的方差值占总方差值的比例，这个比例越大，则越是重要的主成分。</span></span><br><span class="line">    <span class="built_in">print</span>(lower_dim.explained_variance_) <span class="comment"># 它代表降维后的各主成分的方差值。方差值越大，则说明越是重要的主成分。</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> scipy <span class="keyword">import</span> linalg</span><br><span class="line"><span class="comment">#一般线性PCA函数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">pca</span>(<span class="params">data_mat, topNfeat=<span class="number">1000000</span></span>):</span><br><span class="line">    mean_vals = np.mean(data_mat, axis=<span class="number">0</span>) <span class="comment"># 两个属性的均值</span></span><br><span class="line">    mid_mat = data_mat - mean_vals <span class="comment"># 与均值的差值</span></span><br><span class="line">    cov_mat = np.cov(mid_mat, rowvar=<span class="literal">False</span>) <span class="comment"># 两个属性的协方差，rowvar默认True是计算行的协方差</span></span><br><span class="line">    eig_vals, eig_vects = linalg.eig(np.mat(cov_mat)) <span class="comment"># 求协方差矩阵的特征值和特征向量</span></span><br><span class="line">    eig_val_index = np.argsort(eig_vals) <span class="comment"># 特征值排序</span></span><br><span class="line">    eig_val_index = eig_val_index[:-(topNfeat + <span class="number">1</span>):-<span class="number">1</span>] <span class="comment"># 特征值从大到小取topNfeat个 </span></span><br><span class="line">    eig_vects = eig_vects[:, eig_val_index] <span class="comment"># 取出对应的特征向量</span></span><br><span class="line">    low_dim_mat = np.dot(mid_mat, eig_vects) <span class="comment"># mid_mat与特征向量点乘</span></span><br><span class="line">    <span class="comment"># ret_mat = np.dot(low_dim_mat,eig_vects.T)</span></span><br><span class="line">    <span class="keyword">return</span> low_dim_mat, eig_vals</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__==<span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure><br />
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> scipy.stats <span class="keyword">as</span> ss</span><br><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br><span class="line">sns.set_context(context=<span class="string">&quot;poster&quot;</span>,font_scale=<span class="number">1.2</span>)</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">main</span>():</span><br><span class="line">    df=pd.read_csv(<span class="string">&quot;./data/HR.csv&quot;</span>)</span><br><span class="line">    <span class="comment">#Left与部分属性的t独立分布检验</span></span><br><span class="line">    dp_indices=df.groupby(by=<span class="string">&quot;department&quot;</span>).indices</span><br><span class="line">    sales_values=df[<span class="string">&quot;left&quot;</span>].iloc[dp_indices[<span class="string">&quot;sales&quot;</span>]].values</span><br><span class="line">    <span class="built_in">print</span>(sales_values)</span><br><span class="line">    technical_values=df[<span class="string">&quot;left&quot;</span>].iloc[dp_indices[<span class="string">&quot;technical&quot;</span>]].values</span><br><span class="line">    <span class="built_in">print</span>(technical_values)</span><br><span class="line">    <span class="built_in">print</span>(ss.ttest_ind(sales_values,technical_values))</span><br><span class="line">    dp_keys=dp_indices.keys()</span><br><span class="line">    dp_t_mat=np.zeros((<span class="built_in">len</span>(dp_keys),<span class="built_in">len</span>(dp_keys)))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(dp_keys)):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(dp_keys)):</span><br><span class="line">            p_value=ss.ttest_ind(df[<span class="string">&quot;left&quot;</span>].iloc[dp_indices[dp_keys[i]]].values,\</span><br><span class="line">                                        df[<span class="string">&quot;left&quot;</span>].iloc[dp_indices[dp_keys[j]]].values)[<span class="number">1</span>]</span><br><span class="line">            <span class="keyword">if</span> p_value&lt;<span class="number">0.05</span>:</span><br><span class="line">                dp_t_mat[i][j]=-<span class="number">1</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                dp_t_mat[i][j]=p_value</span><br><span class="line">    sns.heatmap(dp_t_mat,xticklabels=dp_keys,yticklabels=dp_keys)</span><br><span class="line">    plt.show()</span><br><span class="line">    <span class="comment">#交叉分析left</span></span><br><span class="line">    piv_tb=pd.pivot_table(df, values=<span class="string">&quot;left&quot;</span>, index=[<span class="string">&quot;department&quot;</span>, <span class="string">&quot;salary&quot;</span>], columns=[<span class="string">&quot;time_spend_company&quot;</span>],aggfunc=np.mean)</span><br><span class="line">    piv_tb=pd.pivot_table(df, values=<span class="string">&quot;left&quot;</span>,index=[<span class="string">&quot;department&quot;</span>,<span class="string">&quot;salary&quot;</span>],columns=[<span class="string">&quot;number_project&quot;</span>],aggfunc=np.mean)</span><br><span class="line">    piv_tb = pd.pivot_table(df, values=<span class="string">&quot;left&quot;</span>, index=[<span class="string">&quot;promotion_last_5years&quot;</span>, <span class="string">&quot;salary&quot;</span>], columns=[<span class="string">&quot;Work_accident&quot;</span>],aggfunc=np.mean)</span><br><span class="line">    <span class="built_in">print</span>(piv_tb.index)</span><br><span class="line">    sns.heatmap(piv_tb,vmax=<span class="number">1</span>,vmin=<span class="number">0</span>)</span><br><span class="line">    plt.show()</span><br><span class="line">    sns.barplot(x=<span class="string">&quot;salary&quot;</span>,y=<span class="string">&quot;left&quot;</span>,hue=<span class="string">&quot;department&quot;</span>,data=df)</span><br><span class="line">    plt.show()</span><br><span class="line">    sl_s=df[<span class="string">&quot;satisfaction_level&quot;</span>]</span><br><span class="line">    sns.barplot(<span class="built_in">range</span>(<span class="built_in">len</span>(sl_s)),sl_s.sort_values())</span><br><span class="line">    <span class="comment">#sns.barplot(range(len(sl_s)),sl_s.sort_values().diff().diff())</span></span><br><span class="line">    plt.show()</span><br><span class="line"><span class="keyword">if</span> __name__==<span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure><br />
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> scipy.stats <span class="keyword">as</span> ss</span><br><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br><span class="line">sns.set_context(context=<span class="string">&quot;poster&quot;</span>,font_scale=<span class="number">1.2</span>)</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">from</span> sklearn.decomposition <span class="keyword">import</span> PCA</span><br><span class="line"><span class="comment">#Gini</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">getGini</span>(<span class="params">a1, a2</span>):</span><br><span class="line">    <span class="keyword">assert</span> (<span class="built_in">len</span>(a1) == <span class="built_in">len</span>(a2))</span><br><span class="line">    d = <span class="built_in">dict</span>()</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">list</span>(<span class="built_in">range</span>(<span class="built_in">len</span>(a1))):</span><br><span class="line">        d[a1[i]] = d.get(a1[i], []) + [a2[i]]</span><br><span class="line">    <span class="keyword">return</span> <span class="number">1</span> - <span class="built_in">sum</span>([getProbSS(d[k]) * <span class="built_in">len</span>(d[k]) / <span class="built_in">float</span>(<span class="built_in">len</span>(a1)) <span class="keyword">for</span> k <span class="keyword">in</span> d])</span><br><span class="line"><span class="comment">#可能性平方和</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">getProbSS</span>(<span class="params">s</span>):</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> <span class="built_in">isinstance</span>(s,pd.core.series.Series):</span><br><span class="line">        s = pd.Series(s)</span><br><span class="line">    prt_ary = np.array(pd.groupby(s, by=s).count().values / <span class="built_in">float</span>(<span class="built_in">len</span>(s)))</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">sum</span>(prt_ary ** <span class="number">2</span>)</span><br><span class="line"><span class="comment">#熵</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">getEntropy</span>(<span class="params">s</span>):</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> <span class="built_in">isinstance</span>(s, pd.core.series.Series):</span><br><span class="line">        s = pd.Series(s)</span><br><span class="line">    prt_ary = np.array(s.value_counts()./ <span class="built_in">float</span>(<span class="built_in">len</span>(s)))</span><br><span class="line">    <span class="keyword">return</span> -(np.log2(prt_ary) * prt_ary).<span class="built_in">sum</span>()</span><br><span class="line"><span class="comment">#条件熵</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">getCondEntropy</span>(<span class="params">a1, a2</span>): <span class="comment"># 条件a1下a2的熵</span></span><br><span class="line">    <span class="keyword">assert</span> (<span class="built_in">len</span>(a1) == <span class="built_in">len</span>(a2))</span><br><span class="line">    d = <span class="built_in">dict</span>()</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">list</span>(<span class="built_in">range</span>(<span class="built_in">len</span>(a1))):</span><br><span class="line">        d[a1[i]] = d.get(a1[i], []) + [a2[i]]</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">sum</span>([getEntropy(d[k]) * <span class="built_in">len</span>(d[k]) / <span class="built_in">float</span>(<span class="built_in">len</span>(a1)) <span class="keyword">for</span> k <span class="keyword">in</span> d]) <span class="comment"># len(d[k])表示k值出现的次数</span></span><br><span class="line"><span class="comment">#熵增益</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">getEntropyGain</span>(<span class="params">a1, a2</span>):</span><br><span class="line">    <span class="keyword">return</span> getEntropy(a2) - getCondEntropy(a1, a2)</span><br><span class="line"><span class="comment">#熵增益率</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">getEntropyGainRatio</span>(<span class="params">a1, a2</span>):</span><br><span class="line">    <span class="keyword">return</span> getEntropyGain(a1, a2) / getEntropy(a2)</span><br><span class="line"><span class="comment">#相关度</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">getDiscreteRelation</span>(<span class="params">a1, a2</span>):</span><br><span class="line">    <span class="keyword">return</span> getEntropyGain(a1, a2) / math.sqrt(getEntropy(a1) * getEntropy(a2))</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">main</span>():</span><br><span class="line">    df=pd.read_csv(<span class="string">&quot;./data/HR.csv&quot;</span>)</span><br><span class="line">    <span class="comment">#相关图</span></span><br><span class="line">    sns.heatmap(df.corr())</span><br><span class="line">    sns.heatmap(df.corr(), vmax=<span class="number">1</span>, vmin=-<span class="number">1</span>)</span><br><span class="line">    plt.show()</span><br><span class="line">    <span class="comment">#PCA降维</span></span><br><span class="line">    my_pca=PCA(n_components=<span class="number">7</span>)</span><br><span class="line">    lower_mat=my_pca.fit_transform(df.drop(labels=[<span class="string">&quot;salary&quot;</span>,<span class="string">&quot;department&quot;</span>,<span class="string">&quot;left&quot;</span>],axis=<span class="number">1</span>).values)</span><br><span class="line">    <span class="built_in">print</span>(my_pca.explained_variance_ratio_)</span><br><span class="line">    sns.heatmap(pd.DataFrame(lower_mat).corr())</span><br><span class="line">    plt.show()</span><br><span class="line">    <span class="comment">#离散相关性度量</span></span><br><span class="line">    s1 = pd.Series([<span class="string">&quot;X1&quot;</span>, <span class="string">&quot;X1&quot;</span>, <span class="string">&quot;X2&quot;</span>, <span class="string">&quot;X2&quot;</span>, <span class="string">&quot;X2&quot;</span>, <span class="string">&quot;X2&quot;</span>])</span><br><span class="line">    s2 = pd.Series([<span class="string">&quot;Y1&quot;</span>, <span class="string">&quot;Y1&quot;</span>, <span class="string">&quot;Y1&quot;</span>, <span class="string">&quot;Y2&quot;</span>, <span class="string">&quot;Y2&quot;</span>, <span class="string">&quot;Y2&quot;</span>])</span><br><span class="line">    <span class="built_in">print</span>(getEntropy(s1))</span><br><span class="line">    <span class="built_in">print</span>(getEntropy(s2))</span><br><span class="line">    <span class="built_in">print</span>(getCondEntropy(s1, s2))</span><br><span class="line">    <span class="built_in">print</span>(getCondEntropy(s2, s1))</span><br><span class="line">    <span class="built_in">print</span>(getEntropyGain(s1, s2))</span><br><span class="line">    <span class="built_in">print</span>(getEntropyGain(s2, s1))</span><br><span class="line">    <span class="built_in">print</span>(getEntropyGainRatio(s1, s2))</span><br><span class="line">    <span class="built_in">print</span>(getEntropyGainRatio(s2, s1))</span><br><span class="line">    <span class="built_in">print</span>(getDiscreteRelation(s1, s2))</span><br><span class="line">    <span class="built_in">print</span>(getDiscreteRelation(s2, s1))</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__==<span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure></p>
<p>特征选择：<br />
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> scipy.stats <span class="keyword">as</span> ss</span><br><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br><span class="line">sns.set_context(context=<span class="string">&quot;poster&quot;</span>,font_scale=<span class="number">1.2</span>)</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.feature_selection <span class="keyword">import</span> SelectKBest,RFE,SelectFromModel</span><br><span class="line"><span class="keyword">from</span> sklearn.svm <span class="keyword">import</span> SVR</span><br><span class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> DecisionTreeRegressor</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">main</span>():</span><br><span class="line">    <span class="comment">#特征选择</span></span><br><span class="line">    df=pd.DataFrame(&#123;<span class="string">&quot;A&quot;</span>:ss.norm.rvs(size=<span class="number">10</span>),<span class="string">&quot;B&quot;</span>:ss.norm.rvs(size=<span class="number">10</span>),\</span><br><span class="line">                     <span class="string">&quot;C&quot;</span>:ss.norm.rvs(size=<span class="number">10</span>),<span class="string">&quot;D&quot;</span>:np.random.randint(low=<span class="number">0</span>,high=<span class="number">2</span>,size=<span class="number">10</span>)&#125;)</span><br><span class="line">    X=df.loc[:,[<span class="string">&quot;A&quot;</span>,<span class="string">&quot;B&quot;</span>,<span class="string">&quot;C&quot;</span>]]</span><br><span class="line">    Y=df.loc[:,<span class="string">&quot;D&quot;</span>]</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;X&quot;</span>,X)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Y&quot;</span>,Y)</span><br><span class="line">    skb=SelectKBest(k=<span class="number">2</span>)</span><br><span class="line">    skb.fit(X.values,Y.values)</span><br><span class="line">    <span class="built_in">print</span>(skb.transform(X.values))</span><br><span class="line"></span><br><span class="line">    rfe=RFE(estimator=SVR(kernel=<span class="string">&quot;linear&quot;</span>),n_features_to_select=<span class="number">2</span>,step=<span class="number">1</span>)</span><br><span class="line">    <span class="built_in">print</span>(rfe.fit_transform(X,Y))</span><br><span class="line"></span><br><span class="line">    sfm=SelectFromModel(estimator=DecisionTreeRegressor(),threshold=<span class="number">0.01</span>)</span><br><span class="line">    <span class="built_in">print</span>(sfm.fit_transform(X,Y))</span><br><span class="line"><span class="keyword">if</span> __name__==<span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure><br />
特征变换、降维：<br />
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> scipy.stats <span class="keyword">as</span> ss</span><br><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br><span class="line">sns.set_context(context=<span class="string">&quot;poster&quot;</span>,font_scale=<span class="number">1.2</span>)</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">main</span>():</span><br><span class="line">    lst=[<span class="number">6</span>,<span class="number">8</span>,<span class="number">10</span>,<span class="number">15</span>,<span class="number">16</span>,<span class="number">24</span>,<span class="number">25</span>,<span class="number">40</span>,<span class="number">67</span>]</span><br><span class="line">    <span class="comment">#离散化</span></span><br><span class="line">    binings,bins=pd.qcut(lst,q=<span class="number">3</span>,retbins=<span class="literal">True</span>) <span class="comment">#等深分箱</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="built_in">list</span>(bins))</span><br><span class="line">    <span class="built_in">print</span>(pd.cut(lst,bins=<span class="number">3</span>)) <span class="comment"># 等宽分箱</span></span><br><span class="line">    <span class="built_in">print</span>(pd.cut(lst,bins=<span class="number">4</span>,labels=[<span class="string">&quot;low&quot;</span>,<span class="string">&quot;medium&quot;</span>,<span class="string">&quot;high&quot;</span>,<span class="string">&quot;very high&quot;</span>]))</span><br><span class="line"></span><br><span class="line">    <span class="comment">#归一化与标准化</span></span><br><span class="line">    <span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> MinMaxScaler,StandardScaler</span><br><span class="line">    <span class="built_in">print</span>(MinMaxScaler().fit_transform(np.array([<span class="number">1</span>,<span class="number">4</span>,<span class="number">10</span>,<span class="number">15</span>,<span class="number">21</span>]).reshape(-<span class="number">1</span>,<span class="number">1</span>))) <span class="comment">#归一化</span></span><br><span class="line">    <span class="built_in">print</span>(StandardScaler().fit_transform(np.array([<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>]).reshape(-<span class="number">1</span>,<span class="number">1</span>)))<span class="comment"># 标准化</span></span><br><span class="line">    <span class="built_in">print</span>(StandardScaler().fit_transform(np.array([<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>]).reshape(-<span class="number">1</span>, <span class="number">1</span>)))</span><br><span class="line"></span><br><span class="line">    <span class="comment">#标签化与独热编码</span></span><br><span class="line">    <span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> LabelEncoder,OneHotEncoder</span><br><span class="line">    <span class="built_in">print</span>(LabelEncoder().fit_transform(np.array([<span class="string">&quot;Down&quot;</span>,<span class="string">&quot;Down&quot;</span>,<span class="string">&quot;Up&quot;</span>,<span class="string">&quot;Down&quot;</span>,<span class="string">&quot;Up&quot;</span>]).reshape(-<span class="number">1</span>,<span class="number">1</span>)))</span><br><span class="line">    <span class="built_in">print</span>(LabelEncoder().fit_transform(np.array([<span class="string">&quot;Low&quot;</span>,<span class="string">&quot;Medium&quot;</span>,<span class="string">&quot;Low&quot;</span>,<span class="string">&quot;High&quot;</span>,<span class="string">&quot;Medium&quot;</span>]).reshape(-<span class="number">1</span>,<span class="number">1</span>))) <span class="comment">#LabelEncoder按照字母升序处理，所以High，Medium，Low会被映射成0，1，2。如果想映射成2，1，0，请自己写函数转换。</span></span><br><span class="line">    </span><br><span class="line">    lb_encoder=LabelEncoder()</span><br><span class="line">    lb_encoder=lb_encoder.fit(np.array([<span class="string">&quot;Red&quot;</span>,<span class="string">&quot;Yellow&quot;</span>,<span class="string">&quot;Blue&quot;</span>,<span class="string">&quot;Green&quot;</span>]))</span><br><span class="line">    lb_trans_f=lb_encoder.transform(np.array([<span class="string">&quot;Red&quot;</span>,<span class="string">&quot;Yellow&quot;</span>,<span class="string">&quot;Blue&quot;</span>,<span class="string">&quot;Green&quot;</span>]))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># OneHotEncoder处理DataFrame数据比较费劲，可以使用get_dummies进行OneHot编码</span></span><br><span class="line">    oht_enoder=OneHotEncoder().fit(lb_trans_f.reshape(-<span class="number">1</span>,<span class="number">1</span>))</span><br><span class="line">    <span class="built_in">print</span>(oht_enoder.transform(lb_encoder.transform(np.array([<span class="string">&quot;Red&quot;</span>,<span class="string">&quot;Blue&quot;</span>])).reshape(-<span class="number">1</span>,<span class="number">1</span>)).toarray())</span><br><span class="line"></span><br><span class="line">    <span class="comment">#规范化</span></span><br><span class="line">    <span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> Normalizer</span><br><span class="line">    <span class="built_in">print</span>(Normalizer(norm=<span class="string">&quot;l1&quot;</span>).fit_transform([[<span class="number">1</span>,<span class="number">1</span>,<span class="number">3</span>,-<span class="number">1</span>,<span class="number">2</span>]]))<span class="comment">#默认对行进行正规化</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">#LDA降维</span></span><br><span class="line">    <span class="keyword">from</span> sklearn.discriminant_analysis <span class="keyword">import</span> LinearDiscriminantAnalysis</span><br><span class="line">    X = np.array([[-<span class="number">1</span>, -<span class="number">1</span>], [-<span class="number">2</span>, -<span class="number">1</span>], [-<span class="number">3</span>, -<span class="number">2</span>], [<span class="number">1</span>, <span class="number">1</span>], [<span class="number">2</span>, <span class="number">1</span>], [<span class="number">3</span>, <span class="number">2</span>]])</span><br><span class="line">    y = np.array([<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>])</span><br><span class="line">    clf = LinearDiscriminantAnalysis()<span class="comment">#参数n_components不能超过标签类别数，如果2分类， n_components最多是2，此时可以考虑使用PCA降维，不受限制。</span></span><br><span class="line">    clf.fit(X, y)</span><br><span class="line">    <span class="built_in">print</span>(clf.predict([[-<span class="number">0.8</span>, -<span class="number">1</span>]]))</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line"><span class="comment"># 自定义LabelEncoder   </span></span><br><span class="line">d=<span class="built_in">dict</span>([(<span class="string">&quot;low&quot;</span>,<span class="number">0</span>),(<span class="string">&quot;medium&quot;</span>,<span class="number">1</span>),(<span class="string">&quot;high&quot;</span>,<span class="number">2</span>)])</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">map_salary</span>(<span class="params">s</span>):</span><br><span class="line">    <span class="keyword">return</span> d.get(s,<span class="number">0</span>)  </span><br><span class="line"><span class="comment"># 调用</span></span><br><span class="line">[map_salary(s) <span class="keyword">for</span> s <span class="keyword">in</span> df[<span class="string">&quot;salary&quot;</span>].values]</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__==<span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure></p>
<h1 id="参考文献">参考文献</h1>
<p><span class="exturl" data-url="aHR0cHM6Ly93d3cuY25ibG9ncy5jb20vcGluYXJkL3AvOTA5Mzg5MC5odG1s">特征工程之特征预处理<i class="fa fa-external-link-alt"></i></span><br />
<span class="exturl" data-url="aHR0cHM6Ly93d3cuY25ibG9ncy5jb20vcGluYXJkL3AvOTAzMjc1OS5odG1s">特征工程之特征选择<i class="fa fa-external-link-alt"></i></span><br />
<span class="exturl" data-url="aHR0cHM6Ly93d3cuY25ibG9ncy5jb20vcGluYXJkL3AvOTA2MTU0OS5odG1s">特征工程之特征表达<i class="fa fa-external-link-alt"></i></span></p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




  <nav class="pagination">
    <a class="extend prev" rel="prev" title="上一页" aria-label="上一页" href="/page/4/"><i class="fa fa-angle-left"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/4/">4</a><span class="page-number current">5</span><a class="page-number" href="/page/6/">6</a><span class="space">&hellip;</span><a class="page-number" href="/page/13/">13</a><a class="extend next" rel="next" title="下一页" aria-label="下一页" href="/page/6/"><i class="fa fa-angle-right"></i></a>
  </nav>

</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="copyright">
    &copy; 2019 – 
    <span itemprop="copyrightYear">2023</span>
    <span class="with-love">
      <i class="fa fa-heart"></i>
    </span>
    <span class="author" itemprop="copyrightHolder">SoundMemories</span>
  </div>
  <div class="powered-by">由 <span class="exturl" data-url="aHR0cHM6Ly9oZXhvLmlv">Hexo</span> & <span class="exturl" data-url="aHR0cHM6Ly90aGVtZS1uZXh0LmpzLm9yZy9tdXNlLw==">NexT.Muse</span> 强力驱动
  </div>

    </div>
  </footer>

  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>
  <div class="sidebar-dimmer"></div>
  <div class="back-to-top" role="button" aria-label="返回顶部">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>
  <a role="button" class="book-mark-link book-mark-link-fixed"></a>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/next-theme-pjax/0.6.0/pjax.min.js" integrity="sha256-vxLn1tSKWD4dqbMRyv940UYw4sXgMtYcK6reefzZrao=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.7.0/jquery.min.js" integrity="sha256-2Pmvv0kuTBOenSvLm6bvfBSSHrUJ+3A7x6P5Ebd07/g=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.js" integrity="sha256-yt2kYMy0w8AbtF89WXb2P1rfjcP/HTHLT7097U8Y5b8=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/medium-zoom/1.0.8/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/muse.js"></script><script src="/js/next-boot.js"></script><script src="/js/bookmark.js"></script><script src="/js/pjax.js"></script>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-generator-searchdb/1.4.1/search.js" integrity="sha256-1kfA5uHPf65M5cphT2dvymhkuyHPQp5A53EGZOnOLmc=" crossorigin="anonymous"></script>
<script src="/js/third-party/search/local-search.js"></script>

  <script class="next-config" data-name="pdf" type="application/json">{"object_url":{"url":"https://cdnjs.cloudflare.com/ajax/libs/pdfobject/2.2.12/pdfobject.min.js","integrity":"sha256-g2xji1rlE3KsGVClvuxTbcR0Kn2+wtQADSff2Tbb4zA="},"url":"/lib/pdf/web/viewer.html"}</script>
  <script src="/js/third-party/tags/pdf.js"></script>

  <script class="next-config" data-name="mermaid" type="application/json">{"enable":true,"theme":{"light":"neutral","dark":"neutral"},"js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mermaid/10.2.3/mermaid.min.js","integrity":"sha256-JFptYy4KzJ5OQP+Q9fubNf3cxpPPmZKqUOovyEONKrQ="}}</script>
  <script src="/js/third-party/tags/mermaid.js"></script>


  <script src="/js/third-party/fancybox.js"></script>



  




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"none","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>


  <script src="https://cdnjs.cloudflare.com/ajax/libs/quicklink/2.3.0/quicklink.umd.js" integrity="sha256-yvJQOINiH9fWemHn0vCA5lsHWJaHs6/ZmO+1Ft04SvM=" crossorigin="anonymous"></script>
  <script class="next-config" data-name="quicklink" type="application/json">{"enable":true,"home":true,"archive":true,"delay":true,"timeout":3000,"priority":true,"url":"https://soundmemories.github.io/page/5/"}</script>
  <script src="/js/third-party/quicklink.js"></script>

</body>
</html>
