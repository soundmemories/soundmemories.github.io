<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222" media="(prefers-color-scheme: light)">
<meta name="theme-color" content="#222" media="(prefers-color-scheme: dark)"><meta name="generator" content="Hexo 6.3.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="https://fonts.loli.net/css?family=Monda:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">

<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css" integrity="sha256-HtsXJanqjKTc8vVQjO4YMhiqFoXkfBsjBWcX91T1jr8=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.css" integrity="sha256-Vzbj7sDDS/woiFS3uNKo8eIuni59rjyNGtXfstRzStA=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"soundmemories.github.io","root":"/","images":"/images","scheme":"Muse","darkmode":true,"version":"8.17.1","exturl":true,"sidebar":{"position":"left","display":"always","padding":18,"offset":12},"copycode":{"enable":true,"style":null},"bookmark":{"enable":true,"color":"#222","save":"auto"},"mediumzoom":true,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"},"path":"/search.xml","localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false}}</script><script src="/js/config.js"></script>

    <meta name="description" content="今日事，今日毕">
<meta property="og:type" content="website">
<meta property="og:title" content="SoundMemories">
<meta property="og:url" content="https://soundmemories.github.io/page/5/index.html">
<meta property="og:site_name" content="SoundMemories">
<meta property="og:description" content="今日事，今日毕">
<meta property="og:locale" content="zh_CN">
<meta property="article:author" content="SoundMemories">
<meta name="twitter:card" content="summary">


<link rel="canonical" href="https://soundmemories.github.io/page/5/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":true,"isPost":false,"lang":"zh-CN","comments":"","permalink":"","path":"page/5/index.html","title":""}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>SoundMemories</title>
  








  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <h1 class="site-title">SoundMemories</h1>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="搜索" role="button">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签<span class="badge">10</span></a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类<span class="badge">10</span></a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档<span class="badge">122</span></a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup"><div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" maxlength="80"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close" role="button">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="search-result-container no-result">
  <div class="search-result-icon">
    <i class="fa fa-spinner fa-pulse fa-5x"></i>
  </div>
</div>

    </div>
  </div>

</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-overview-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="SoundMemories"
      src="/images/avstar.png">
  <p class="site-author-name" itemprop="name">SoundMemories</p>
  <div class="site-description" itemprop="description">今日事，今日毕</div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">122</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">10</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">10</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author animated">
      <span class="links-of-author-item">
        <span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL3NvdW5kbWVtb3JpZXM=" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;soundmemories"><i class="fab fa-github fa-fw"></i>GitHub</span>
      </span>
      <span class="links-of-author-item">
        <span class="exturl" data-url="bWFpbHRvOnNvdW5kbWVtb3JpZXNAMTYzLmNvbQ==" title="E-Mail → mailto:soundmemories@163.com"><i class="fa fa-envelope fa-fw"></i>E-Mail</span>
      </span>
  </div>
  <div class="cc-license animated" itemprop="license">
    <span class="exturl cc-opacity" data-url="aHR0cHM6Ly9jcmVhdGl2ZWNvbW1vbnMub3JnL2xpY2Vuc2VzL2J5LW5jLXNhLzQuMC8="><img src="https://cdnjs.cloudflare.com/ajax/libs/creativecommons-vocabulary/2020.11.3/assets/license_badges/small/by_nc_sa.svg" alt="Creative Commons"></span>
  </div>

        </div>
      </div>
    </div>

    
        <div class="pjax">
        </div>
  </aside>


    </div>

    <div class="main-inner index posts-expand">

    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://soundmemories.github.io/2020/10/07/ML/29.LDS/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avstar.png">
      <meta itemprop="name" content="SoundMemories">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="SoundMemories">
      <meta itemprop="description" content="今日事，今日毕">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | SoundMemories">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2020/10/07/ML/29.LDS/" class="post-title-link" itemprop="url">LDS</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2020-10-07 00:00:00" itemprop="dateCreated datePublished" datetime="2020-10-07T00:00:00+08:00">2020-10-07</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/ML/" itemprop="url" rel="index"><span itemprop="name">ML</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>712</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>3 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="线性动态系统">线性动态系统</h1>
<p>HMM 模型适用于隐变量是离散的值的时候，对于连续隐变量的
HMM，常用线性动态系统描述线性高斯模型的态变量，使用粒子滤波来表述非高斯非线性的态变量。</p>
<p>LDS
又叫卡尔曼滤波，其中，线性体现在上一时刻和这一时刻的隐变量以及隐变量和观测之间：<br />
<span class="math display">\[
\begin{align}
z_t&amp;=A\cdot z_{t-1}+B+\varepsilon\\
x_t&amp;=C\cdot z_t+D+\delta\\
\varepsilon&amp;\sim\mathcal{N}(0,Q)\\
\delta&amp;\sim\mathcal{N}(0,R)
\end{align}
\]</span><br />
类比 HMM 中的几个参数：<br />
<span class="math display">\[
\begin{align}
p(z_t|z_{t-1})&amp;\sim\mathcal{N}(A\cdot z_{t-1}+B,Q)\\
p(x_t|z_t)&amp;\sim\mathcal{N}(C\cdot z_t+D,R)\\
z_1&amp;\sim\mathcal{N}(\mu_1,\Sigma_1)
\end{align}
\]</span><br />
在含时的概率图中，除了对参数估计的学习问题外，在推断任务中，包括译码，证据概率，滤波，平滑，预测问题，LDS
更关心滤波这个问题：<span
class="math inline">\(p(z_t|x_1,x_2,\cdots,x_t)\)</span>。类似 HMM
中的前向算法，我们需要找到一个递推关系。<br />
<span class="math display">\[
p(z_t|x_{1:t})=p(x_{1:t},z_t)/p(x_{1:t})=Cp(x_{1:t},z_t)
\]</span><br />
对于 <span class="math inline">\(p(x_{1:t},z_t)\)</span>：<br />
<span class="math display">\[
\begin{align}p(x_{1:t},z_t)&amp;=p(x_t|x_{1:t-1},z_t)p(x_{1:t-1},z_t)=p(x_t|z_t)p(x_{1:t-1},z_t)\nonumber\\
&amp;=p(x_t|z_t)p(z_t|x_{1:t-1})p(x_{1:t-1})=Cp(x_t|z_t)p(z_t|x_{1:t-1})\\
\end{align}
\]</span><br />
我们看到，右边除了只和观测相关的常数项，还有一项是预测任务需要的概率。对这个值：<br />
<span class="math display">\[
\begin{align}
p(z_t|x_{1:t-1})&amp;=\int_{z_{t-1}}p(z_t,z_{t-1}|x_{1:t-1})dz_{t-1}\nonumber\\
&amp;=\int_{z_{t-1}}p(z_t|z_{t-1},x_{1:t-1})p(z_{t-1}|x_{1:t-1})dz_{t-1}\nonumber\\
&amp;=\int_{z_{t-1}}p(z_t|z_{t-1})p(z_{t-1}|x_{1:t-1})dz_{t-1}
\end{align}
\]</span><br />
我们看到，这又化成了一个滤波问题。于是我们得到了一个递推公式：</p>
<ol type="1">
<li><span class="math inline">\(t=1\)</span>，<span
class="math inline">\(p(z_1|x_1)\)</span>，称为 update 过程，然后计算
<span
class="math inline">\(p(z_2|x_1)\)</span>，通过上面的积分进行，称为
prediction 过程。</li>
<li><span class="math inline">\(t=2\)</span>，<span
class="math inline">\(p(z_2|x_2,x_1)\)</span> 和 <span
class="math inline">\(p(z_3|x_1,x_2)\)</span></li>
</ol>
<p>我们看到，这个过程是一个 Online
的过程，对于我们的线性高斯假设，这个计算过程都可以得到解析解。</p>
<ol type="1">
<li><p>Prediction：<br />
<span class="math display">\[
p(z_t|x_{1:t-1})=\int_{z_{t-1}}p(z_t|z_{t-1})p(z_{t-1}|x_{1:t-1})dz_{t-1}=\int_{z_{t-1}}\mathcal{N}(Az_{t-1}+B,Q)\mathcal{N}(\mu_{t-1},\Sigma_{t-1})dz_{t-1}
\]</span><br />
其中第二个高斯分布是上一步的 Update
过程，所以根据线性高斯模型，直接可以写出这个积分：<br />
<span class="math display">\[
p(z_t|x_{1:t-1})=\mathcal{N}(A\mu_{t-1}+B,Q+A\Sigma_{t-1}A^T)
\]</span></p></li>
<li><p>Update:<br />
<span class="math display">\[
p(z_t|x_{1:t})\propto p(x_t|z_t)p(z_t|x_{1:t-1})
\]</span><br />
同样利用线性高斯模型，也可以直接写出这个高斯分布。</p></li>
</ol>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://soundmemories.github.io/2020/10/06/ML/28.HMM/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avstar.png">
      <meta itemprop="name" content="SoundMemories">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="SoundMemories">
      <meta itemprop="description" content="今日事，今日毕">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | SoundMemories">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2020/10/06/ML/28.HMM/" class="post-title-link" itemprop="url">HMM</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2020-10-06 00:00:00" itemprop="dateCreated datePublished" datetime="2020-10-06T00:00:00+08:00">2020-10-06</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/ML/" itemprop="url" rel="index"><span itemprop="name">ML</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>2.8k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>10 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="隐马尔可夫模型">隐马尔可夫模型</h1>
<p>隐马尔可夫模型是一种概率图模型。我们知道，机器学习模型可以从频率派和贝叶斯派两个方向考虑，<strong>在频率派的方法中的核心是优化问题</strong>；<strong>而在贝叶斯派的方法中，核心是积分问题（求后验概率）</strong>，贝叶斯派也发展出来了一系列的积分方法如变分推断，MCMC
等。</p>
<p>概率图模型最基本的模型可以分为有向图（贝叶斯网络）和无向图（马尔可夫随机场）两个方面，例如
GMM。如果在这些基本的模型上，样本之间存在关联，可以认为样本中附带了时序信息，从而样本之间不独立全同分布的，这种模型就叫做动态模型，<strong>隐变量随着时间发生变化，于是观测变量也发生变化（z为隐状态，x为观测值）</strong>：</p>
<pre class="mermaid">
graph LR
z1--&gt;z2--&gt;z3;
   x1--&gt;x2--&gt;x3;
</pre>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">|-概率图</span><br><span class="line">    |-有向：Bayesiam Network</span><br><span class="line">    |-无向：Markov Random Field(Markov Network)</span><br><span class="line">    </span><br><span class="line">|-概率图+时间time：Dynamic Model</span><br><span class="line">    |-HMM(状态变量（隐变量）是离散的)</span><br><span class="line">    |-Kalman Filter(状态变量是连续的，线性的)</span><br><span class="line">    |-Particle Filter(状态变量是连续的，非线性的)</span><br></pre></td></tr></table></figure>
<h1 id="hmm">HMM</h1>
<p>HMM 用概率图表示为：</p>
<pre class="mermaid">
graph TD
z1--&gt;z2;
subgraph four
	z4--&gt;x4((x4))
end
subgraph three
	z3--&gt;x3((x3))
end
subgraph two
	z2--&gt;x2((x2))
end
subgraph one
	z1--&gt;x1((x1))
end

z2--&gt;z3;
z3--&gt;z4;
</pre>
<p>上图表示了四个时刻的隐变量变化。用参数 <span
class="math inline">\(\lambda=(\pi,A,B)\)</span> 来表示，其中 <span
class="math inline">\(\pi\)</span> 是开始的概率分布，<span
class="math inline">\(A\)</span> 为状态转移矩阵，<span
class="math inline">\(B\)</span> 为发射矩阵（观测矩阵）。</p>
<p><span class="math inline">\(o_t\)</span> 表示观测变量，<span
class="math inline">\(V=\{v_1,v_2,\cdots,v_M\}\)</span>
表示观测变量的值域，<span class="math inline">\(O\)</span>
为观测序列。<br />
<span class="math inline">\(i_t\)</span> 表示状态变量，<span
class="math inline">\(Q=\{q_1,q_2,\cdots,q_N\}\)</span>
表示状态变量的值域，<span class="math inline">\(I\)</span>
为状态序列。<br />
定义 <span
class="math inline">\(A=(a_{ij}=p(i_{t+1}=q_j|i_t=q_i))\)</span>
表示状态转移矩阵，<span
class="math inline">\(B=(b_j(k)=p(o_t=v_k|i_t=q_j))\)</span>
表示发射矩阵。</p>
<p>在 HMM 中，有两个基本假设：</p>
<ol type="1">
<li><p>齐次 Markov 假设（未来只依赖于当前）：<br />
<span class="math display">\[
p(i_{t+1}|i_t,i_{t-1},\cdots,i_1,o_t,o_{t-1},\cdots,o_1)=p(i_{t+1}|i_t)
\]</span></p></li>
<li><p>观测独立假设：<br />
<span class="math display">\[
p(o_t|i_t,i_{t-1},\cdots,i_1,o_{t-1},\cdots,o_1)=p(o_t|i_t)
\]</span></p></li>
</ol>
<p>HMM 要解决三个问题：</p>
<ol type="1">
<li>Evaluation：已知模型和观测序列，计算在该模型下观测序列出现的概率。<span
class="math inline">\(p(O|\lambda)\)</span> -&gt; Forward-Backward
算法</li>
<li>Learning：已知观测序列，估计模型参数，使得该模型下观测序列概率最大。<span
class="math inline">\(\lambda=\mathop{argmax}\limits_{\lambda}p(O|\lambda)\)</span>
<ol type="1">
<li>有监督学习（训练数据包含状态序列）-&gt; 最大似然估计</li>
<li>无监督学习（训练数据不包含状态序列）-&gt; EM 算法（Baum-Welch）</li>
</ol></li>
<li>Decoding：已知模型参数和观测序列，求最有可能对应的状态序列。<span
class="math inline">\(I=\mathop{argmax}\limits_{I}p(I|O,\lambda)\)</span>
-&gt; Vierbi 算法
<ol type="1">
<li>预测问题：<span
class="math inline">\(p(i_{t+1}|o_1,o_2,\cdots,o_t)\)</span>，已知 <span
class="math inline">\(t\)</span> 时刻的观测序列，求下一时刻的状态。</li>
<li>滤波问题：<span
class="math inline">\(p(i_t|o_1,o_2,\cdots,o_t)\)</span>，已知 <span
class="math inline">\(t\)</span> 时刻的观测序列，求 <span
class="math inline">\(t\)</span> 时刻的状态。</li>
</ol></li>
</ol>
<h2 id="evaluation">Evaluation</h2>
<h3 id="直接计算法">直接计算法</h3>
<p><span class="math display">\[
p(O|\lambda)=\sum\limits_{I}p(I,O|\lambda)=\sum\limits_{I}p(O|I,\lambda)p(I|\lambda)
\]</span></p>
<p><span class="math display">\[
p(I|\lambda)=p(i_1,i_2,\cdots,i_t|\lambda)=p(i_t|i_1,i_2,\cdots,i_{t-1},\lambda)p(i_1,i_2,\cdots,i_{t-1}|\lambda)
\]</span></p>
<p>根据齐次 Markov 假设：<br />
<span class="math display">\[
p(i_t|i_1,i_2,\cdots,i_{t-1},\lambda)=p(i_t|i_{t-1})=a_{i_{t-1}i_t}
\]</span><br />
所以：<br />
<span class="math display">\[
p(I|\lambda)=\pi_1\prod\limits_{t=2}^Ta_{i_{t-1},i_t}
\]</span><br />
又由于：<br />
<span class="math display">\[
p(O|I,\lambda)=\prod\limits_{t=1}^Tb_{i_t}(o_t)
\]</span><br />
于是：<br />
<span class="math display">\[
p(O|\lambda)=\sum\limits_{I}\pi_{i_1}\prod\limits_{t=2}^Ta_{i_{t-1},i_t}\prod\limits_{t=1}^Tb_{i_t}(o_t)
\]</span><br />
我们看到，上面的式子中的求和符号是对状态序列求和，于是复杂度为 <span
class="math inline">\(O(TN^T)\)</span>，这种算法不可行。</p>
<h3 id="前向算法">前向算法</h3>
<p>下面，记 <span
class="math inline">\(\alpha_t(i)=p(o_1,o_2,\cdots,o_t,i_t=q_i|\lambda)\)</span>
为（<span class="math inline">\(t\)</span> 时刻状态序列+ <span
class="math inline">\(t\)</span> 时刻前所有观测序列）的联合概率。<br />
所以整个序列最后一项可写为：<span
class="math inline">\(\alpha_T(i)=p(O,i_T=q_i|\lambda)\)</span>。那么对所有状态序列求和可得：<br />
<span class="math display">\[
p(O|\lambda)=\sum\limits_{i=1}^Np(O,i_T=q_i|\lambda)=\sum\limits_{i=1}^N\alpha_T(i)
\]</span><br />
对 <span class="math inline">\(\alpha_{t+1}(j)\)</span>：<br />
<span class="math display">\[
\begin{align}\alpha_{t}(i) \to
\alpha_{t+1}(j)&amp;=p(o_1,o_2,\cdots,o_{t+1},i_{t+1}=q_j|\lambda)\nonumber\\
&amp;=\sum\limits_{i=1}^Np(o_1,o_2,\cdots,o_{t},o_{t+1},i_t=q_i,i_{t+1}=q_j|\lambda)\nonumber\\
&amp;=\sum\limits_{i=1}^Np(o_{t+1}|o_1,o_2,\cdots,o_{t},i_t=q_i,i_{t+1}=q_j,\lambda)p(o_1,\cdots,o_t,i_t=q_i,i_{t+1}=q_j|\lambda)
\end{align}
\]</span><br />
利用观测独立假设+齐次 Markov 假设：<br />
<span class="math display">\[
\begin{align}\alpha_{t}(i) \to
\alpha_{t+1}(j)&amp;=\sum\limits_{i=1}^Np(o_{t+1}|i_{t+1}=q_j)p(o_1,\cdots,o_t,i_t=q_i,i_{t+1}=q_j|\lambda)\nonumber\\
&amp;=\sum\limits_{i=1}^Np(o_{t+1}|i_{t+1}=q_j)p(i_{t+1}=q_j|o_1,\cdots,o_t,i_t=q_i,\lambda)p(o_1,\cdots,o_t,i_t=q_i|\lambda)\nonumber\\
&amp;=\sum\limits_{i=1}^Np(o_{t+1}|i_{t+1}=q_j)p(i_{t+1}=q_j|i_{t}=q_i)\alpha_t(i)\nonumber\\
&amp;=\sum\limits_{i=1}^Nb_{j}(o_{t+1})a_{ij}\alpha_t(i)
\end{align}
\]</span><br />
上面利用了齐次 Markov 假设得到了一个递推公式，这个算法叫做前向算法。</p>
<h3 id="后向算法">后向算法</h3>
<p>下面，记 <span
class="math inline">\(\beta_t(i)=p(o_{t+1},o_{t+2},\cdots，o_T|i_t=q_i,\lambda)\)</span>
为（<span class="math inline">\(t\)</span> 时刻状态序列+ <span
class="math inline">\(t\)</span> 时刻后所有观测序列）的联合概率。<br />
进一步可写为：<span
class="math inline">\(\beta_t(i)=\sum\limits_{j=1}^Nb_j(o_{t+1})a_{ij}\beta_{t+1}(j)\)</span>。那么对所有状态序列求和可得：：<br />
<span class="math display">\[
\begin{align}p(O|\lambda)&amp;=p(o_1,\cdots,o_T|\lambda)\nonumber\\
&amp;=\sum\limits_{i=1}^Np(o_1,o_2,\cdots,o_T,i_1=q_i|\lambda)\nonumber\\
&amp;=\sum\limits_{i=1}^Np(o_1,o_2,\cdots,o_T|i_1=q_i,\lambda)p(i_1=q_i,|\lambda)\nonumber\\
&amp;=\sum\limits_{i=1}^Np(o_1,o_2,\cdots,o_T|i_1=q_i,\lambda)\pi_i\nonumber\\
&amp;=\sum\limits_{i=1}^Np(o_1|o_2,\cdots,o_T,i_1=q_i,\lambda)p(o_2,\cdots,o_T|i_1=q_i,\lambda)\pi_i\nonumber\\
&amp;=\sum\limits_{i=1}^Nb_i(o_1)\beta_1(i)\pi_i
\end{align}
\]</span><br />
对于这个 <span class="math inline">\(\beta_1(i)\)</span> 可通过 <span
class="math inline">\(\beta_t(i)\)</span> 得到，利用观测序列只与 <span
class="math inline">\(i_{t+1}\)</span>有关，化简<span
class="math inline">\(p(O|i_{t+1},i_t)\)</span>：<br />
<span class="math display">\[
\begin{align}\beta_{t+1}(j) \to
\beta_t(i)&amp;=p(o_{t+1},\cdots,o_T|i_t=q_i,\lambda)\nonumber\\
&amp;=\sum\limits_{j=1}^Np(o_{t+1},\cdots,o_T,i_{t+1}=q_j|i_t=q_i,\lambda)\nonumber\\
&amp;=\sum\limits_{j=1}^Np(o_{t+1},\cdots,o_T|i_{t+1}=q_j,i_t=q_i,\lambda)p(i_{t+1}=q_j|i_t=q_i,\lambda)\nonumber\\
&amp;=\sum\limits_{j=1}^Np(o_{t+1},\cdots,o_T|i_{t+1}=q_j,\lambda)a_{ij}\nonumber\\
&amp;=\sum\limits_{j=1}^Np(o_{t+1}|o_{t+2},\cdots,o_T,i_{t+1}=q_j,\lambda)p(o_{t+2},\cdots,o_T|i_{t+1}=q_j,\lambda)a_{ij}\nonumber\\
&amp;=\sum\limits_{j=1}^Nb_j(o_{t+1})\beta_{t+1}(j)a_{ij}
\end{align}
\]</span><br />
于是后向地得到了第一项。</p>
<h2 id="learning">Learning</h2>
<p>为了学习得到参数的最优值，在 MLE 中：<br />
<span class="math display">\[
\lambda_{MLE}=\mathop{argmax}_\lambda p(O|\lambda)
\]</span><br />
我们采用 EM 算法（在这里也叫 Baum Welch
算法），用上标表示迭代（时刻）：<br />
<span class="math display">\[
\theta^{t+1}=\mathop{argmax}_{\theta}\int_z\log
p(X,Z|\theta)p(Z|X,\theta^t)dz
\]</span><br />
其中，<span class="math inline">\(X\)</span> 是观测变量，<span
class="math inline">\(Z\)</span> 是隐变量序列。对应替换成HMM变量：<br />
<span class="math display">\[
\lambda^{t+1}=\mathop{argmax}_\lambda\sum\limits_I\log
p(O,I|\lambda)p(I|O,\lambda^t)\\
=\mathop{argmax}_\lambda\sum\limits_I\log p(O,I|\lambda)p(O,I|\lambda^t)
\]</span><br />
上式 <span
class="math inline">\(p(I|O,\lambda^t)=p(O,I|\lambda^t)/p(O|\lambda^t)\)</span>
利用了 <span class="math inline">\(p(O|\lambda^t)\)</span> 和<span
class="math inline">\(\lambda\)</span> 无关为常量。</p>
<p>定义 <span class="math inline">\(Q(\lambda,\lambda^t)\)</span>
函数：<br />
<span class="math display">\[
Q(\lambda,\lambda^t)=\sum\limits_I\log p(O,I|\lambda)p(O,I|\lambda^t)
\]</span><br />
将 Evaluation 中的直接计算式子代入：<br />
<span class="math display">\[
\sum\limits_I\log p(O,I|\lambda)p(O,I|\lambda^t)=\sum\limits_I[(\log
\pi_{i_1}+\sum\limits_{t=2}^T\log
a_{i_{t-1},i_t}+\sum\limits_{t=1}^T\log b_{i_t}(o_t))p(O,I|\lambda^t)]
\]</span></p>
<p>因为求参数 <span
class="math inline">\(\lambda^t=(\pi^t,A^t,B^t)\)</span>，有三个参数但求解方法一样这里以对求
<span class="math inline">\(\pi^{t+1}\)</span> 求解为例，它与后面 <span
class="math inline">\(a_{i_{t-1},i_t}\)</span> 和 <span
class="math inline">\(b_{i_t}(o_t)\)</span> 不相关：<br />
<span class="math display">\[
\begin{align}\pi^{t+1}&amp;=\mathop{argmax}_\pi Q(\lambda,\lambda^t)
\nonumber\\
&amp;=\mathop{argmax}_\pi\sum\limits_I[\log
\pi_{i_1}p(O,I|\lambda^t)]\nonumber\\
&amp;=\mathop{argmax}_\pi\sum\limits_{i_1}\cdots \sum\limits_{i_T}[\log
\pi_{i_1}\cdot p(O,i_1,i_2,\cdots,i_T|\lambda^t)]
\end{align}
\]</span><br />
上面的式子中，对 <span class="math inline">\(i_2,\cdots,i_T\)</span>
求和可以将这些参数消掉：<br />
<span class="math display">\[
\begin{aligned}
\pi^{t+1}&amp;=\mathop{argmax}_\pi\sum\limits_{i_1}[\log \pi_{i_1}\cdot
p(O,i_1|\lambda^t)]\\
&amp;=\mathop{argmax}_\pi \sum\limits_{i=1}^N[\log \pi_{i}\cdot
p(O,i_1=q_i|\lambda^t)]
\end{aligned}
\]</span><br />
上面的式子还有对 <span class="math inline">\(\pi\)</span> 的约束 <span
class="math inline">\(\sum\limits_i\pi_i=1\)</span>。使用拉格朗日乘子法，定义
Lagrange 函数：<br />
<span class="math display">\[
L(\pi,\eta)=\sum\limits_{i=1}^N\log \pi_i\cdot
p(O,i_1=q_i|\lambda^t)+\eta(\sum\limits_{i=1}^N\pi_i-1)
\]</span><br />
于是：<br />
<span class="math display">\[
\begin{aligned}
\frac{\partial
L}{\partial\pi_i}&amp;=\frac{1}{\pi_i}p(O,i_1=q_i|\lambda^t)+\eta=0\\
&amp;\Rightarrow p(O,i_1=q_i|\lambda^t)+\pi_i\eta=0
\end{aligned}
\]</span><br />
对上式 <span class="math inline">\(i\)</span> 求和消去 <span
class="math inline">\(q_i\)</span> 和 <span
class="math inline">\(\pi_i\)</span>：<br />
<span class="math display">\[
\begin{aligned}
&amp;\sum\limits_{i=1}^N[p(O,i_1=q_i|\lambda^t)+\pi_i\eta]=0\\
&amp;\Rightarrow p(O|\lambda^t)+\eta=0\\
&amp;\Rightarrow\eta=-p(O|\lambda^t)
\end{aligned}
\]</span><br />
所以：<br />
<span class="math display">\[
\pi_i^{t+1}=\frac{p(O,i_1=q_i|\lambda^t)}{p(O|\lambda^t)}
\]</span></p>
<h2 id="decoding">Decoding</h2>
<p>Decoding 问题表述为：<br />
<span class="math display">\[
I=\mathop{argmax}\limits_{I}p(I|O,\lambda)
\]</span><br />
我们需要找到一个状态序列，其概率最大，这个序列就是在参数空间中的一个路径，可以采用动态规划的思想。</p>
<p>定义在 <span class="math inline">\(t\)</span> 时刻状态为 <span
class="math inline">\(i\)</span> 的所有单个路径中概率最大值：<br />
<span class="math display">\[
\delta_{t}(i)=\max\limits_{i_1,\cdots,i_{t-1}}p(o_1,\cdots,o_t,i_1,\cdots,i_{t-1},i_t=q_i)
\]</span><br />
由定义可得其递推公式：<br />
<span class="math display">\[
\delta_{t+1}(j)=\max\limits_{1\le i\le N}\delta_t(i)a_{ij}b_j(o_{t+1})
\]</span><br />
从上一步到下一步的概率再求最大值，记这个路径为：<br />
<span class="math display">\[
\psi_{t+1}(j)=\mathop{argmax}\limits_{1\le i\le N}\delta_t(i)a_{ij}
\]</span></p>
<h1 id="小结">小结</h1>
<p>HMM 是一种动态模型，是由混合树形模型和时序结合起来的一种模型（类似
GMM + Time）。对于类似 HMM 的这种状态空间模型，普遍的除了学习任务（采用
EM ）外，还有推断任务，推断任务包括：</p>
<ol type="1">
<li><p>译码 Decoding：<span
class="math inline">\(p(z_1,z_2,\cdots,z_t|x_1,x_2,\cdots,x_t)\)</span></p></li>
<li><p>似然 Prob of evidence：<span
class="math inline">\(p(x_1,x_2,\cdots,x_t|\theta)\)</span></p></li>
<li><p>滤波 Filtering：$ p(z_t|x_1,,x_t)$，Online<br />
<span class="math display">\[
p(z_t|x_{1:t})=\frac{p(x_{1:t},z_t)}{p(x_{1:t})}\propto
p(x_{1:t},z_t)=\alpha_t (前向算法)
\]</span><br />
记 <span
class="math inline">\(\alpha_t=p(x_1,\cdots,x_t,z_t)\)</span>，<span
class="math inline">\(\beta_t=p(x_{t+1},\cdots,x_T|z_t)\)</span>。</p></li>
<li><p>平滑 Smoothing：<span
class="math inline">\(p(z_t|x_1,\cdots,x_T)\)</span>，Offline<br />
<span class="math display">\[
p(z_t|x_{1:T})=\frac{p(x_{1:T},z_t)}{p(x_{1:T})}=\frac{p(x_{1:t},x_{t+1:T},z_t)}{p(x_{1:T})}=\frac{p(x_{1:t},z_t)p(x_{t+1:T}|x_{1:t},z_t)}{p(x_{1:T})}=\frac{\alpha_t
p(x_{t+1:T}|x_{1:t},z_t)}{p(x_{1:T})}
\]</span><br />
根据概率图的条件独立性 <span class="math inline">\(x_{t+1:T}\)</span> 和
<span class="math inline">\(x_{1:t}\)</span> 相互独立，有 <span
class="math inline">\(p(x_{t+1:T}|x_{1:t},z_t)=p(x_{t+1:T}|z_t)\)</span>
：<br />
<span class="math display">\[
p(z_t|x_{1:T})=\frac{\alpha_t p(x_{t+1:T}|z_t)}{p(x_{1:T})}\propto
\alpha_t\beta_t
\]</span><br />
这个算法叫做前向后向算法。</p></li>
<li><p>预测 Prediction：预测下一时刻隐状态<span
class="math inline">\(p(z_{t+1},z_{t+2}|x_1,\cdots,x_t)\)</span>，预测下一时刻观测状态<span
class="math inline">\(p(x_{t+1},x_{t+2}|x_1,\cdots,x_t)\)</span><br />
<span class="math display">\[
p(z_{t+1}|x_{1:t})=\sum_{z_t}p(z_{t+1},z_t|x_{1:t})=\sum\limits_{z_t}p(z_{t+1}|z_t,x_{1:t})p(z_t|x_{1:t})=\sum\limits_{z_t}p(z_{t+1}|z_t)p(z_t|x_{1:t})
\]</span><br />
其中 <span class="math inline">\(p(z_t|x_{1:t})\)</span>
是Filtering问题。<br />
<span class="math display">\[
p(x_{t+1}|x_{1:t})=\sum\limits_{z_{t+1}}p(x_{t+1},z_{t+1}|x_{1:t})=\sum\limits_{z_{t+1}}p(x_{t+1}|z_{t+1},x_{1:t})p(z_{t+1}|x_{1:t})=\sum\limits_{z_{t+1}}p(x_{t+1}|z_{t+1})p(z_{t+1}|x_{1:t})
\]</span><br />
其中 <span class="math inline">\(p(z_{t+1}|x_{1:t})\)</span>
是上一个问题。</p></li>
</ol>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://soundmemories.github.io/2020/10/05/ML/27.MCMC/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avstar.png">
      <meta itemprop="name" content="SoundMemories">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="SoundMemories">
      <meta itemprop="description" content="今日事，今日毕">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | SoundMemories">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2020/10/05/ML/27.MCMC/" class="post-title-link" itemprop="url">MCMC</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2020-10-05 00:00:00" itemprop="dateCreated datePublished" datetime="2020-10-05T00:00:00+08:00">2020-10-05</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/ML/" itemprop="url" rel="index"><span itemprop="name">ML</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>2k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>7 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="马尔可夫链蒙特卡洛">马尔可夫链蒙特卡洛</h1>
<p>MCMC
是一种<strong>随机的近似推断</strong>，其核心就是基于采样的随机近似方法蒙特卡洛方法。对于采样任务来说，有下面一些常用的场景：</p>
<ol type="1">
<li>采样作为任务，用于生成新的样本</li>
<li>求和/求积分/求期望</li>
</ol>
<p>采样结束后，我们需要评价采样出来的样本点是不是好的样本集：</p>
<ol type="1">
<li>样本趋向于高概率的区域</li>
<li>样本之间必须独立</li>
</ol>
<p>具体采样中，采样是一个困难的过程：</p>
<ol type="1">
<li>无法采样得到归一化因子，即无法直接对概率 $ p(x)=(x)$
采样，常常需要对 CDF 采样，但复杂的情况不行</li>
<li>如果归一化因子可以求得，但是对高维数据依然不能均匀采样（维度灾难），这是由于对
<span class="math inline">\(p\)</span> 维空间，总的状态空间是 <span
class="math inline">\(K^p\)</span>
这么大，于是在这种情况下，直接采样也不行</li>
</ol>
<p>因此需要借助其他手段，如蒙特卡洛方法中的拒绝采样，重要性采样和
MCMC。</p>
<h2 id="蒙特卡洛方法">蒙特卡洛方法</h2>
<p>蒙特卡洛方法旨在求得复杂概率分布下的期望值：<br />
<span class="math display">\[
\mathbb{E}_{p(z|x)}[f(z)]=\int
p(z|x)f(z)dz\simeq\frac{1}{N}\sum\limits_{i=1}^Nf(z_i)
\]</span><br />
也就是说，从概率分布 <span class="math inline">\(f(z)\)</span> 中取
<span class="math inline">\(N\)</span>
个点，从而近似计算这个积分。采样方法有：</p>
<p>1、概率分布采样（也叫直接采样），首先求得概率密度（PDF）的累积密度函数（CDF），然后求得
CDF
的反函数，在0到1之间均匀采样，代入反函数，就得到了采样点。但是实际大部分概率分布不能得到
CDF。</p>
<p>2、 Rejection Sampling （接受-拒绝采样）：对于概率分布 <span
class="math inline">\(p(z)\)</span>，引入简单的提议分布 <span
class="math inline">\(q(z)\)</span>，使得 <span
class="math inline">\(\forall z_i,Mq(z_i)\ge p(z_i)\)</span>。我们先在
$ q(z)$ 中采样，定义接受率：<span
class="math inline">\(\alpha=\frac{p(z^i)}{Mq(z^i)}\le1\)</span>。算法描述为：<br />
1. 取 <span class="math inline">\(z^i\sim q(z)\)</span>。<br />
2. 在均匀分布中选取 <span class="math inline">\(u\)</span>。<br />
3. 如果 <span class="math inline">\(u\le\alpha\)</span>，则接受 <span
class="math inline">\(z^i\)</span>，否则，拒绝这个值。</p>
<p>3、Importance Sampling （重要性采样）：直接对期望：<span
class="math inline">\(\mathbb{E}_{p(z)}[f(z)]\)</span> 进行采样。<br />
<span class="math display">\[
\mathbb{E}_{p(z)}[f(z)]=\int p(z)f(z)dz=\int
\frac{p(z)}{q(z)}f(z)q(z)dz\simeq\frac{1}{N}\sum\limits_{i=1}^Nf(z_i)\frac{p(z_i)}{q(z_i)}
\]</span><br />
于是采样在 <span class="math inline">\(q(z)\)</span>
中采样，并通过权重计算和。重要值采样对于权重非常小的时候，效率非常低。<br />
重要性采样有一个变种
Sampling-Importance-Resampling，这种方法，首先和上面一样进行采样，然后在采样出来的
<span class="math inline">\(N\)</span>
个样本中，重新采样，这个重新采样，使用每个样本点的权重作为概率分布进行采样。</p>
<h2 id="mcmc">MCMC</h2>
<p>马尔可夫链式一种时间状态都是离散的随机变量序列。我们关注的主要是齐次的一阶马尔可夫链。马尔可夫链满足：<span
class="math inline">\(p(X_{t+1}|X_1,X_2,\cdots,X_t)=p(X_{t+1}|X_t)\)</span>。这个式子可以写成转移矩阵的形式
<span
class="math inline">\(p_{ij}=p(X_{t+1}=j|X_t=i)\)</span>。我们有：<br />
<span class="math display">\[
\pi_{t+1}(x^*)=\int\pi_i(x)p_{x\to x^*}dx
\]</span><br />
如果存在 <span
class="math inline">\(\pi=(\pi(1),\pi(2),\cdots),\sum\limits_{i=1}^{+\infty}\pi(i)=1\)</span>，有上式成立，这个序列就叫马尔可夫链
<span class="math inline">\(X_t\)</span>
的平稳分布，平稳分布就是表示在某一个时刻后，分布不再改变。MCMC
就是通过构建马尔可夫链概率序列，使其收敛到平稳分布 <span
class="math inline">\(p(z)\)</span>。引入细致平衡：<span
class="math inline">\(\pi(x)p_{x\to x^{\ast}}=\pi(x^{\ast})p_{x^* \to
x}\)</span>。如果一个分布满足细致平衡，那么一定满足平稳分布（反之不成立）：<br />
<span class="math display">\[
\int\pi(x)p_{x\to x^*}dx=\int\pi(x^*)p_{x^*\to x}dx=\pi(x^*)
\]</span><br />
细致平衡条件将平稳分布的序列和马尔可夫链的转移矩阵联系在一起了，通过转移矩阵可以不断生成样本点。假定随机取一个转移矩阵
<span
class="math inline">\((Q=Q_{ij})\)</span>，作为一个提议矩阵。我们有：<br />
<span class="math display">\[
p(z)\cdot Q_{z\to z^*}\alpha(z,z^*)=p(z^*)\cdot Q_{z^*\to
z}\alpha(z^*,z)
\]</span><br />
<span class="math inline">\(\alpha(z,z^*)\)</span>
可看作接受率，当取值为：<br />
<span class="math display">\[
\alpha(z,z^*)=\min\{1,\frac{p(z^*)Q_{z^*\to z}}{p(z)Q_{z\to z^*}}\}
\]</span><br />
则<br />
<span class="math display">\[
p(z)\cdot Q_{z\to z^*}\alpha(z,z^*)=\min\{p(z)Q_{z\to
z^*},p(z^*)Q_{z^*\to z}\}=p(z^*)\cdot Q_{z^*\to z}\alpha(z^*,z)
\]</span><br />
于是，迭代就得到了序列，这个算法叫做 Metropolis-Hastings 算法：</p>
<ol type="1">
<li>通过在0，1之间均匀分布取点 <span
class="math inline">\(u\)</span>。</li>
<li>生成 <span class="math inline">\(z^{\ast}\sim
Q(z^*|z^{i-1})\)</span></li>
<li>计算 <span class="math inline">\(\alpha\)</span> 值</li>
<li>如果 <span class="math inline">\(\alpha\ge
u\)</span>，就接受这个样本，即 <span
class="math inline">\(z^i=z^*\)</span>，否则 <span
class="math inline">\(z^{i}=z^{i-1}\)</span></li>
</ol>
<p>这样取的样本就服从 <span
class="math inline">\(p(z)=\dfrac{\hat{p}(z)}{z_p}\sim
\hat{p}(z)\)</span>。</p>
<p>下面介绍另一种采样方式 Gibbs 采样，如果 <span
class="math inline">\(z\)</span>
的维度非常高，那么通过固定被采样的维度其余的维度来简化采样过程：<span
class="math inline">\(z_i\sim p(z_i|z_{-i})\)</span>：</p>
<ol type="1">
<li>给定初始值 <span
class="math inline">\(z_1^0,z_2^0,\cdots\)</span></li>
<li>在 <span class="math inline">\(t+1\)</span> 时刻，采样 <span
class="math inline">\(z_i^{t+1}\sim
p(z_i|z_{1}^{t+1},...z_{i-1}^{t+1},z_{i+1}^{t},...z_{k}^{t})\)</span>，简写
<span class="math inline">\(z_i^{t+1}\sim
p(z_i|z_{-i})\)</span>，从第一个维度开始，每次采样一个维度时，固定其余维度（之前采样过的留下，之后未采样的用上一时刻的值）。</li>
</ol>
<p>Gibbs 采样方法是一种特殊的 MH 采样，可以计算 Gibbs
采样的接受率：<br />
<span class="math display">\[
\frac{p(z^*)Q_{z^*\to z}}{p(z)Q_{z\to
z^*}}=\frac{p(z_i^*|z^*_{-i})p(z^*_{-i})p(z_i|z_{-i}^*)}{p(z_i|z_{-i})p(z_{-i})p(z_i^*|z_{-i})}
\]</span><br />
对于每个 Gibbs 采样步骤，<span
class="math inline">\(z_{-i}=z_{-i}^*\)</span>，这是由于每个维度 <span
class="math inline">\(i\)</span>
采样的时候，其余的参量保持不变。所以上式为1。于是 Gibbs
采样过程中，相当于找到了一个步骤，使得所有的接受率为 1。</p>
<h2 id="平稳分布">平稳分布</h2>
<p>定义随机矩阵：<br />
<span class="math display">\[
Q=\begin{pmatrix}Q_{11}&amp;Q_{12}&amp;\cdots&amp;Q_{1K}\\\vdots&amp;\vdots&amp;\vdots&amp;\vdots\\Q_{k1}&amp;Q_{k2}&amp;\cdots&amp;Q_{KK}\end{pmatrix}
\]</span><br />
这个矩阵每一行或者每一列的和都是1。随机矩阵的特征值都小于等于1。假设只有一个特征值为
<span
class="math inline">\(\lambda_i=1\)</span>。于是在马尔可夫过程中：<br />
<span class="math display">\[
q^{t+1}(x=j)=\sum\limits_{i=1}^Kq^t(x=i)Q_{ij}\\
\Rightarrow q^{t+1}=q^t\cdot Q=q^1Q^t
\]</span><br />
<span class="math inline">\(q^1Q^t\)</span> 表示 <span
class="math inline">\(q\)</span> 的1时刻 乘以 <span
class="math inline">\(Q\)</span> 的 <span
class="math inline">\(t\)</span> 次幂。</p>
<p><span class="math inline">\(Q\)</span> 可以分解为特征矩阵 <span
class="math inline">\(A\Lambda A^{-1}\)</span> 于是有：<br />
<span class="math display">\[
q^{t+1}=q^1A\Lambda^t A^{-1}
\]</span><br />
当 <span class="math inline">\(\Lambda\)</span>
中只有一个特征值的绝对值等于1，其余小于1，即<span
class="math inline">\(\Lambda^t=diag(0,0,\cdots,1,\cdots,0)\)</span>，如果
<span class="math inline">\(t\)</span> 足够大，那么就会得到 <span
class="math inline">\(q^{t+1}=q^{t}\)</span> ，即 <span
class="math inline">\(q\)</span>
趋于平稳分布了，从开始到平稳这段时间称为燃烧期。<br />
马尔可夫链可能具有平稳分布的性质，所以我们可以构建马尔可夫链使其平稳分布收敛于需要的概率分布（设计转移矩阵）。</p>
<p>在采样过程中，需要经历一定的时间（燃烧期/混合时间）才能达到平稳分布。但是
MCMC 方法有一些问题：<br />
（1）无法判断是否已经收敛。<br />
（2）燃烧期过长（维度太高，并且维度之间有关，可能无法采样到某些维度），例如在
GMM
中，可能无法越过低谷采样到其他峰。于是在一些模型中，需要对隐变量之间的关系作出约束，如
RBM 假设隐变量之间无关。<br />
（3）样本之间一定是有相关性的，如果每个时刻都取一个点，那么每个样本一定和前一个相关，这可以通过间隔一段时间采样。</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://soundmemories.github.io/2020/10/04/ML/26.VI/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avstar.png">
      <meta itemprop="name" content="SoundMemories">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="SoundMemories">
      <meta itemprop="description" content="今日事，今日毕">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | SoundMemories">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2020/10/04/ML/26.VI/" class="post-title-link" itemprop="url">VI</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2020-10-04 00:00:00" itemprop="dateCreated datePublished" datetime="2020-10-04T00:00:00+08:00">2020-10-04</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/ML/" itemprop="url" rel="index"><span itemprop="name">ML</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>1.7k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>6 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="变分推断">变分推断</h1>
<p>我们已经知道概率模型可以分为，频率派的优化问题和贝叶斯派的积分问题。从贝叶斯角度来看推断，对于
<span class="math inline">\(\hat{x}\)</span>
这样的新样本，需要得到：<br />
<span class="math display">\[
p(\hat{x}|X)=\int_\theta p(\hat{x},\theta|X)d\theta=\int_\theta
p(\theta|X)p(\hat{x}|\theta,X)d\theta
\]</span><br />
如果新样本和数据集独立，那么推断就是概率分布依参数后验<span
class="math inline">\(p(\theta|X)\)</span>分布的期望。</p>
<p>我们看到，推断问题的中心是参数后验<span
class="math inline">\(p(\theta|X)\)</span>分布的求解，推断分为：</p>
<p>1、精确推断-解析解<br />
2、近似推断：参数空间无法精确求解，如含有隐变量、参数空间复杂<br />
（1）确定性近似，如：变分推断<br />
（2）随机近似，如：MCMC，MH，Gibbs</p>
<h2 id="基于平均场假设的变分推断">基于平均场假设的变分推断</h2>
<p>我们记 <span class="math inline">\(Z\)</span>
为隐变量和参数的集合，<span class="math inline">\(Z_i\)</span> 为第
<span class="math inline">\(i\)</span> 维的参数，于是，回顾一下 EM
中的推导：<br />
<span class="math display">\[
\log p(X)=\log p(X,Z)-\log
p(Z|X)=\log\frac{p(X,Z)}{q(Z)}-\log\frac{p(Z|X)}{q(Z)}
\]</span><br />
左右两边分别积分：<br />
<span class="math display">\[
Left:\int_Zq(Z)\log p(X)dZ=\log p(X)
\]</span><br />
<span class="math display">\[
Right:\int_Z[\log \frac{p(X,Z)}{q(Z)}-\log
\frac{p(Z|X)}{q(Z)}]q(Z)dZ=ELBO+KL(q,p)
\]</span><br />
<span class="math inline">\(Right\)</span>
可以写为<strong>变分</strong>和<strong>KL散度</strong>的和：<br />
<span class="math display">\[
Right:L(q)+KL(q,p)
\]</span><br />
由于这个式子是常数，于是寻找 <span class="math inline">\(q\simeq
p\)</span> 的任务就相当于对 <span class="math inline">\(L(q)\)</span>
取最大值。<br />
<span class="math display">\[
\hat{q}(Z)=\mathop{argmax}_{q(Z)}L(q)
\]</span><br />
假设 <span class="math inline">\(q(Z)\)</span> 可以划分为 <span
class="math inline">\(M\)</span> 个组（平均场近似）：<br />
<span class="math display">\[
q(Z)=\prod\limits_{i=1}^Mq_i(Z_i)
\]</span><br />
因此，<span class="math inline">\(L(q)\)</span> 展开得：<br />
<span class="math display">\[
L(q)=\int_Zq(Z)\log p(X,Z)dZ-\int_Zq(Z)\log{q(Z)}
\]</span><br />
在 <span class="math inline">\(L(q)\)</span> 中，看其中某一个 <span
class="math inline">\(p(Z_j)\)</span> ，则 <span
class="math inline">\(L(q)\)</span> 第一项：<br />
<span class="math display">\[
\begin{aligned}\int_Zq(Z)\log
p(X,Z)dZ&amp;=\int_Z\prod\limits_{i=1}^Mq_i(Z_i)\log p(X,Z)dZ\\
&amp;=\int_{Z_j}q_j(Z_j)\int_{Z-Z_{j}}\prod\limits_{i\ne j}q_i(Z_i)\log
p(X,Z)dZ\\
&amp;=\int_{Z_j}q_j(Z_j)\mathbb{E}_{\prod\limits_{i\ne j}q_i(Z_i)}[\log
p(X,Z)]dZ_j
\end{aligned}
\]</span></p>
<p><span class="math inline">\(L(q)\)</span> 第二项：<br />
<span class="math display">\[
\int_Zq(Z)\log
q(Z)dZ=\int_Z\prod\limits_{i=1}^Mq_i(Z_i)\sum\limits_{i=1}^M\log
q_i(Z_i)dZ
\]</span><br />
<span class="math inline">\(L(q)\)</span>
第二项展开求和项，其第一项为：<br />
<span class="math display">\[
\int_Z\prod\limits_{i=1}^Mq_i(Z_i)\log q_1(Z_1)dZ=\int_{Z_1}q_1(Z_1)\log
q_1(Z_1)dZ_1+Const
\]</span><br />
<span class="math inline">\(L(q)\)</span>
第二项根据这个规律，得到：<br />
<span class="math display">\[
\int_Zq(Z)\log q(Z)dZ=\sum\limits_{i=1}^M\int_{Z_i}q_i(Z_i)\log
q_i(Z_i)dZ_i=\int_{Z_j}q_j(Z_j)\log q_j(Z_j)dZ_j+Const
\]</span><br />
<span class="math inline">\(L(q)\)</span> 两项相减，假设令 <span
class="math inline">\(\mathbb{E}_{\prod\limits_{i\ne j}q_i(Z_i)}[\log
p(X,Z)]=\log \hat{p}(X,Z_j)\)</span> 可以得到：<br />
<span class="math display">\[
\int_{Z_j}q_j(Z_j)\log\frac{\hat{p}(X,Z_j)}{q_j(Z_j)}dZ_j=-KL(q_j(Z_j)||\hat{p}(X,Z_j))\le
0
\]</span><br />
于是最大的 <span class="math inline">\(q_j(Z_j)=\hat{p}(X,Z_j)\)</span>
才能得到最大值。对每一个 <span
class="math inline">\(q_j\)</span>，求这个值时都是固定其余的 <span
class="math inline">\(q_i, \small i\ne
j\)</span>，于是可以使用坐标上升的方法进行迭代求解，上面的推导针对单个样本，但是对数据集也是适用的。</p>
<p>基于平均场假设的变分推断存在一些问题：<br />
（1）假设太强，<span class="math inline">\(Z\)</span>
非常复杂的情况下，假设不适用。<br />
（2）期望中的积分，可能无法计算。</p>
<h2 id="sgvi">SGVI</h2>
<p>从 <span class="math inline">\(Z\)</span> 到 <span
class="math inline">\(X\)</span>
的过程叫做生成过程或译码（decoder），反过来的过程叫推断过程或编码（encoder），基于平均场的变分推断可以导出坐标上升的算法，但是这个假设在一些情况下假设太强，同时积分也不一定能算。我们知道，优化方法除了坐标上升，还有梯度上升的方式，我们希望通过梯度上升来得到变分推断的另一种算法。</p>
<p>我们的目标函数：<br />
<span class="math display">\[
\hat{q}(Z)=\mathop{argmax}_{q(Z)}L(q)
\]</span><br />
假定 <span class="math inline">\(q(Z)=q_\phi(Z)\)</span>，<span
class="math inline">\(q(Z)\)</span> 是关于 <span
class="math inline">\(\phi\)</span> 这个参数的概率分布。于是：</p>
<p><span class="math display">\[
\mathop{argmax}_{q(Z)}L(q)=\mathop{argmax}_{\phi}L(\phi)=\mathop{argmax}_{\phi}\mathbb{E}_{q_\phi}[\log
p_\theta(x^i,z)-\log q_\phi(z)]
\]</span><br />
这里 <span class="math inline">\(x^i\)</span> 表示第 <span
class="math inline">\(i\)</span> 个样本。<br />
根据梯度上升法，每一次更新为 <span
class="math inline">\(\phi^{i+1}=\phi^{i}+\lambda \nabla_\phi
L(\phi)\)</span> ，<span class="math inline">\(\lambda\)</span>
为迭代步长，我们只求梯度：<br />
<span class="math display">\[
\begin{aligned}\nabla_\phi
L(\phi)&amp;=\nabla_\phi\mathbb{E}_{q_\phi}[\log p_\theta(x^i,z)-\log
q_\phi(z)]\\
&amp;=\nabla_\phi\int q_\phi(z)[\log p_\theta(x^i,z)-\log q_\phi(z)]dz\\
&amp;=\int\nabla_\phi q_\phi(z)[\log p_\theta(x^i,z)-\log
q_\phi(z)]dz+\int q_\phi(z)\nabla_\phi [\log p_\theta(x^i,z)-\log
q_\phi(z)]dz\\
&amp;=\int\nabla_\phi q_\phi(z)[\log p_\theta(x^i,z)-\log
q_\phi(z)]dz-\int q_\phi(z)\nabla_\phi \log q_\phi(z)dz\\
&amp;=\int\nabla_\phi q_\phi(z)[\log p_\theta(x^i,z)-\log
q_\phi(z)]dz-\int \nabla_\phi q_\phi(z)dz\\
&amp;=\int\nabla_\phi q_\phi(z)[\log p_\theta(x^i,z)-\log q_\phi(z)]dz\\
&amp;=\int q_\phi(\nabla_\phi\log q_\phi)(\log p_\theta(x^i,z)-\log
q_\phi(z))dz\\
&amp;=\mathbb{E}_{q_\phi}[(\nabla_\phi\log q_\phi)(\log
p_\theta(x^i,z)-\log q_\phi(z))]
\end{aligned}
\]</span><br />
这个期望可以通过<strong>蒙特卡洛采样</strong>来近似，从而得到梯度，然后利用梯度上升的方法来得到参数：<br />
<span class="math display">\[
z^l\sim q_\phi(z),\qquad l=1,2,...L
\]</span><br />
<span class="math display">\[
\mathbb{E}_{q_\phi}[(\nabla_\phi\log q_\phi)(\log p_\theta(x^i,z)-\log
q_\phi(z))]\sim \frac{1}{L}\sum\limits_{l=1}^L(\nabla_\phi\log
q_\phi)(\log p_\theta(x^i,z)-\log q_\phi(z))
\]</span><br />
但是由于求和符号中存在一个对数项，当 <span
class="math inline">\(q_\phi\)</span>
在接近于零得部分变化时，每一点点变化都会使对数值变化很大，导致直接采样的方差很大，这就需要采样的样本非常多，<strong>蒙特卡洛采样</strong>法并不一定work。</p>
<p>为了解决方差太大的问题，我们采用重参数化（Reparameterization）的技巧，在求梯度时：<br />
<span class="math display">\[
\nabla_\phi L(\phi)=\nabla_\phi\mathbb{E}_{q_\phi}[\log
p_\theta(x^i,z)-\log q_\phi(z)]
\]</span><br />
考虑 <span class="math inline">\(\mathbb{E}_{q_\phi}\)</span> 中 <span
class="math inline">\(q_\phi\)</span> 是一个确定得分布，和 <span
class="math inline">\(\phi\)</span> 无关，此时求关于 <span
class="math inline">\(\phi\)</span> 的梯度就很好计算了：<br />
<span class="math display">\[
\nabla_\phi\mathbb{E}_{q_\phi}[\log p_\theta(x^i,z)-\log
q_\phi(z)]=\mathbb{E}_{q_\phi}\nabla_\phi[\log p_\theta(x^i,z)-\log
q_\phi(z)]
\]</span></p>
<p>我们取：<span
class="math inline">\(z=g_\phi(\varepsilon,x^i),\varepsilon\sim
p(\varepsilon)\)</span>，于是对后验：<span class="math inline">\(z\sim
q_\phi(z|x^i)\)</span>，有 <span
class="math inline">\(|q_\phi(z|x^i)dz|=|p(\varepsilon)d\varepsilon|\)</span>。代入上面的梯度中：<br />
<span class="math display">\[
\begin{aligned}
\nabla_\phi L(\phi)&amp;=\nabla_\phi\mathbb{E}_{q_\phi}[\log
p_\theta(x^i,z)-\log q_\phi(z)]\\
&amp;=\mathbb{E}_{q_\phi}\nabla_\phi[\log p_\theta(x^i,z)-\log
q_\phi(z)]\\
&amp;=\mathbb{E}_{p(\varepsilon)}[\nabla_\phi[\log p_\theta(x^i,z)-\log
q_\phi(z)]]\\
&amp;=\mathbb{E}_{p(\varepsilon)}[\nabla_z[\log p_\theta(x^i,z)-\log
q_\phi(z)]\nabla_\phi z]\\
&amp;=\mathbb{E}_{p(\varepsilon)}[\nabla_z[\log p_\theta(x^i,z)-\log
q_\phi(z)]\nabla_\phi g_\phi(\varepsilon,x^i)]
\end{aligned}
\]</span><br />
对这个式子进行蒙特卡洛采样，然后计算期望，得到梯度。</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://soundmemories.github.io/2020/10/03/ML/25.GMM/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avstar.png">
      <meta itemprop="name" content="SoundMemories">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="SoundMemories">
      <meta itemprop="description" content="今日事，今日毕">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | SoundMemories">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2020/10/03/ML/25.GMM/" class="post-title-link" itemprop="url">GMM</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2020-10-03 00:00:00" itemprop="dateCreated datePublished" datetime="2020-10-03T00:00:00+08:00">2020-10-03</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/ML/" itemprop="url" rel="index"><span itemprop="name">ML</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>894</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>3 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="高斯混合模型">高斯混合模型</h1>
<p>为了解决高斯模型的单峰性的问题，我们引入多个高斯模型的加权平均来拟合多峰数据：<br />
<span class="math display">\[
p(x)=\sum\limits_{k=1}^K\alpha_k\mathcal{N}(\mu_k,\Sigma_k)
\]</span><br />
引入隐变量 <span
class="math inline">\(z\)</span>，这个变量表示对应的样本 <span
class="math inline">\(x\)</span>
属于哪一个高斯分布，这个变量是一个离散的随机变量：<br />
<span class="math display">\[
p(z=i)=p_i,\sum\limits_{i=1}^kp(z=i)=1
\]</span><br />
作为一个生成式模型，高斯混合模型通过隐变量 <span
class="math inline">\(z\)</span> 的分布来生成样本。</p>
<pre class="mermaid">
graph LR
z((z))--&gt;x((x))
</pre>
<p>其中，节点 <span class="math inline">\(z\)</span>
就是上面的概率，<span class="math inline">\(x\)</span>
就是生成的高斯分布。于是对 <span
class="math inline">\(p(x)\)</span>：<br />
<span class="math display">\[
p(x)=\sum\limits_zp(x,z)=\sum\limits_{k=1}^Kp(x,z=k)=\sum\limits_{k=1}^Kp(z=k)p(x|z=k)
\]</span><br />
因此：<br />
<span class="math display">\[
p(x)=\sum\limits_{k=1}^Kp_k\mathcal{N}(x|\mu_k,\Sigma_k)
\]</span></p>
<h2 id="极大似然估计">极大似然估计</h2>
<p>样本为 <span
class="math inline">\(X=(x_1,x_2,\cdots,x_N)\)</span>，$ (X,Z)$
为完全参数，参数为 <span
class="math inline">\(\theta=\{p_1,p_2,\cdots,p_K,\mu_1,\mu_2,\cdots,\mu_K\Sigma_1,\Sigma_2,\cdots,\Sigma_K\}\)</span>。我们通过极大似然估计得到
<span class="math inline">\(\theta\)</span> 的值：<br />
<span class="math display">\[
\begin{aligned}\theta_{MLE}&amp;=\mathop{argmax}\limits_{\theta}\log
p(X)=\mathop{argmax}_{\theta}\sum\limits_{i=1}^N\log p(x_i)\\
&amp;=\mathop{argmax}_\theta\sum\limits_{i=1}^N\log
\sum\limits_{k=1}^Kp_k\mathcal{N}(x_i|\mu_k,\Sigma_k)
\end{aligned}
\]</span><br />
这个表达式直接通过求导，由于连加号的存在，无法得到解析解。因此需要使用
EM 算法。</p>
<h2 id="em-求解-gmm">EM 求解 GMM</h2>
<p>EM 算法的基本表达式为：<span
class="math inline">\(\theta^{t+1}=\mathop{argmax}\limits_{\theta}\mathbb{E}_{z|x,\theta_t}[p(x,z|\theta)]\)</span>。套用
GMM 的表达式，对数据集来说：<br />
<span class="math display">\[
\begin{aligned}Q(\theta,\theta^t)&amp;=\sum\limits_z[\log\prod\limits_{i=1}^Np(x_i,z_i|\theta)]\prod
\limits_{i=1}^Np(z_i|x_i,\theta^t)\\
&amp;=\sum\limits_z[\sum\limits_{i=1}^N\log p(x_i,z_i|\theta)]\prod
\limits_{i=1}^Np(z_i|x_i,\theta^t)
\end{aligned}
\]</span><br />
对于中间的那个求和号，展开，第一项为：<br />
<span class="math display">\[
\begin{aligned}
\sum\limits_z\log
p(x_1,z_1|\theta)\prod\limits_{i=1}^Np(z_i|x_i,\theta^t)&amp;=\sum\limits_z\log
p(x_1,z_1|\theta)p(z_1|x_1,\theta^t)\prod\limits_{i=2}^Np(z_i|x_i,\theta^t)\\
&amp;=\sum\limits_{z_1}\log p(x_1,z_1|\theta)
p(z_1|x_1,\theta^t)\sum\limits_{z_2,\cdots,z_K}\prod\limits_{i=2}^Np(z_i|x_i,\theta^t)\\
&amp;=\sum\limits_{z_1}\log
p(x_1,z_1|\theta)p(z_1|x_1,\theta^t)\end{aligned}
\]</span><br />
类似地，<span class="math inline">\(Q\)</span> 可以写为：<br />
<span class="math display">\[
Q(\theta,\theta^t)=\sum\limits_{i=1}^N\sum\limits_{z_i}\log
p(x_i,z_i|\theta)p(z_i|x_i,\theta^t)
\]</span><br />
对于 <span class="math inline">\(p(x,z|\theta)\)</span>：<br />
<span class="math display">\[
p(x,z|\theta)=p(z|\theta)p(x|z,\theta)=p_z\mathcal{N}(x|\mu_z,\Sigma_z)
\]</span><br />
对 <span class="math inline">\(p(z|x,\theta^t)\)</span>：<br />
<span class="math display">\[
p(z|x,\theta^t)=\frac{p(x,z|\theta^t)}{p(x|\theta^t)}=\frac{p_z^t\mathcal{N}(x|\mu_z^t,\Sigma_z^t)}{\sum\limits_kp_k^t\mathcal{N}(x|\mu_k^t,\Sigma_k^t)}
\]</span><br />
代入 <span class="math inline">\(Q\)</span>：<br />
<span class="math display">\[
Q=\sum\limits_{i=1}^N\sum\limits_{z_i}\log
p_{z_i}\mathcal{N(x_i|\mu_{z_i},\Sigma_{z_i})}\frac{p_{z_i}^t\mathcal{N}(x_i|\mu_{z_i}^t,\Sigma_{z_i}^t)}{\sum\limits_kp_k^t\mathcal{N}(x_i|\mu_k^t,\Sigma_k^t)}
\]</span><br />
下面需要对 <span class="math inline">\(Q\)</span> 值求最大值：<br />
<span class="math display">\[
Q=\sum\limits_{k=1}^K\sum\limits_{i=1}^N[\log p_k+\log
\mathcal{N}(x_i|\mu_k,\Sigma_k)]p(z_i=k|x_i,\theta^t)
\]</span></p>
<ol type="1">
<li><p><span class="math inline">\(p_k^{t+1}\)</span>：<br />
<span class="math display">\[
p_k^{t+1}=\mathop{argmax}_{p_k}\sum\limits_{k=1}^K\sum\limits_{i=1}^N[\log
p_k+\log \mathcal{N}(x_i|\mu_k,\Sigma_k)]p(z_i=k|x_i,\theta^t)\ s.t.\
\sum\limits_{k=1}^Kp_k=1
\]</span><br />
即：<br />
<span class="math display">\[
p_k^{t+1}=\mathop{argmax}_{p_k}\sum\limits_{k=1}^K\sum\limits_{i=1}^N\log
p_kp(z_i=k|x_i,\theta^t)\ s.t.\ \sum\limits_{k=1}^Kp_k=1
\]</span><br />
引入 Lagrange 乘子：<span
class="math inline">\(L(p_k,\lambda)=\sum\limits_{k=1}^K\sum\limits_{i=1}^N\log
p_kp(z_i=k|x_i,\theta^t)-\lambda(1-\sum\limits_{k=1}^Kp_k)\)</span>。所以：<br />
<span class="math display">\[
\frac{\partial}{\partial
p_k}L=\sum\limits_{i=1}^N\frac{1}{p_k}p(z_i=k|x_i,\theta^t)+\lambda=0\\
\Rightarrow
\sum\limits_k\sum\limits_{i=1}^N\frac{1}{p_k}p(z_i=k|x_i,\theta^t)+\lambda\sum\limits_kp_k=0\\
\Rightarrow\lambda=-N
\]</span></p>
<p>于是有：<br />
<span class="math display">\[
p_k^{t+1}=\frac{1}{N}\sum\limits_{i=1}^Np(z_i=k|x_i,\theta^t)
\]</span></p></li>
<li><p><span
class="math inline">\(\mu_k,\Sigma_k\)</span>，这两个参数是无约束的，直接求导即可。</p></li>
</ol>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://soundmemories.github.io/2020/10/02/ML/24.EM/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avstar.png">
      <meta itemprop="name" content="SoundMemories">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="SoundMemories">
      <meta itemprop="description" content="今日事，今日毕">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | SoundMemories">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2020/10/02/ML/24.EM/" class="post-title-link" itemprop="url">EM</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2020-10-02 00:00:00" itemprop="dateCreated datePublished" datetime="2020-10-02T00:00:00+08:00">2020-10-02</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/ML/" itemprop="url" rel="index"><span itemprop="name">ML</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>1.3k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>5 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="期望最大">期望最大</h1>
<p>期望最大算法的目的是解决具有隐变量的混合模型的参数估计（极大似然估计）。MLE
对 <span class="math inline">\(p(x|\theta)\)</span>
参数的估计记为：<span
class="math inline">\(\theta_{MLE}=\mathop{argmax}\limits_\theta\log
p(x|\theta)\)</span>。EM
算法对这个问题的解决方法是采用迭代的方法：<br />
<span class="math display">\[
\theta^{t+1}=\mathop{argmax}\limits_{\theta}\int_z\log
[p(x,z|\theta)]p(z|x,\theta^t)dz=\mathbb{E}_{z|x,\theta^t}[\log
p(x,z|\theta)]
\]</span><br />
这个公式包含了迭代的两步：<br />
（1）E step：计算 <span class="math inline">\(\log
p(x,z|\theta)\)</span> 在概率分布 <span
class="math inline">\(p(z|x,\theta^t)\)</span> 下的期望<br />
（2）M step：计算使这个期望最大化的参数得到下一个 EM 步骤的输入</p>
<blockquote>
<p>收敛性证明，求证：<span class="math inline">\(\log
p(x|\theta^t)\le\log p(x|\theta^{t+1})\)</span></p>
<p>证明：<span class="math inline">\(\log p(x|\theta)=\log
p(z,x|\theta)-\log p(z|x,\theta)\)</span>，对左右两边求积分：<br />
<span class="math display">\[
  Left:\int_zp(z|x,\theta^t)\log p(x|\theta)dz=\log p(x|\theta)
  \]</span></p>
<p><span class="math display">\[
  Right:\int_zp(z|x,\theta^t)\log
p(x,z|\theta)dz-\int_zp(z|x,\theta^t)\log
p(z|x,\theta)dz=Q(\theta,\theta^t)-H(\theta,\theta^t)
  \]</span></p>
<p>所以：<br />
<span class="math display">\[
  \log p(x|\theta)=Q(\theta,\theta^t)-H(\theta,\theta^t)
  \]</span><br />
由于 <span
class="math inline">\(Q(\theta,\theta^t)=\int_zp(z|x,\theta^t)\log
p(x,z|\theta)dz\)</span>，而 <span
class="math inline">\(\theta^{t+1}=\mathop{argmax}\limits_{\theta}\int_z\log
[p(x,z|\theta)]p(z|x,\theta^t)dz\)</span>，所以 <span
class="math inline">\(Q(\theta^{t+1},\theta^t)\ge
Q(\theta^t,\theta^t)\)</span>。要证 <span class="math inline">\(\log
p(x|\theta^t)\le\log p(x|\theta^{t+1})\)</span>，需证：<span
class="math inline">\(H(\theta^t,\theta^t)\ge
H(\theta^{t+1},\theta^t)\)</span>：<br />
<span class="math display">\[
  \begin{aligned}H(\theta^{t+1},\theta^t)-H(\theta^{t},\theta^t)&amp;=\int_zp(z|x,\theta^{t})\log
p(z|x,\theta^{t+1})dz-\int_zp(z|x,\theta^t)\log p(z|x,\theta^{t})dz\\
  &amp;=\int_zp(z|x,\theta^t)\log\frac{p(z|x,\theta^{t+1})}{p(z|x,\theta^t)}=-KL(p(z|x,\theta^t),p(z|x,\theta^{t+1}))\le0
  \end{aligned}
  \]</span><br />
综合上面的结果：<br />
<span class="math display">\[
  \log p(x|\theta^t)\le\log p(x|\theta^{t+1})
  \]</span></p>
</blockquote>
<p>根据上面的证明，我们看到，似然函数在每一步都会增大。进一步的，我们看
EM 迭代过程中的式子是怎么来的：<br />
<span class="math display">\[
\log p(x|\theta)=\log p(z,x|\theta)-\log p(z|x,\theta)=\log
\frac{p(z,x|\theta)}{q(z)}-\log \frac{p(z|x,\theta)}{q(z)}
\]</span><br />
分别对两边求期望 <span
class="math inline">\(\mathbb{E}_{q(z)}\)</span>：<br />
<span class="math display">\[
\begin{aligned}
&amp;Left:\int_zq(z)\log p(x|\theta)dz=\log p(x|\theta)\\
&amp;Right:\int_zq(z)\log \frac{p(z,x|\theta)}{q(z)}dz-\int_zq(z)\log
\frac{p(z|x,\theta)}{q(z)}dz=ELBO+KL(q(z),p(z|x,\theta))
\end{aligned}
\]</span><br />
上式中，Evidence Lower Bound(ELBO)，是一个下界，所以 <span
class="math inline">\(\log p(x|\theta)\ge ELBO\)</span>，等于号取在 KL
散度为0是，即：<span
class="math inline">\(q(z)=p(z|x,\theta)\)</span>，EM 算法的目的是将
ELBO 最大化，根据上面的证明过程，在每一步 EM
后，求得了最大的ELBO，并根据这个使 ELBO 最大的参数代入下一步中：<br />
<span class="math display">\[
\hat{\theta}=\mathop{argmax}_{\theta}ELBO=\mathop{argmax}_\theta\int_zq(z)\log\frac{p(x,z|\theta)}{q(z)}dz
\]</span><br />
由于 <span class="math inline">\(q(z)=p(z|x,\theta^t)\)</span>
的时候，这一步的最大值才能取等号，所以：<br />
<span class="math display">\[
\begin{aligned}
\hat{\theta}=\mathop{argmax}_{\theta}ELBO&amp;=\mathop{argmax}_\theta\int_zq(z)\log\frac{p(x,z|\theta)}{q(z)}dz\\
&amp;=\mathop{argmax}_\theta\int_zp(z|x,\theta^t)\log\frac{p(x,z|\theta)}{p(z|x,\theta^t)}d
z\\
&amp;=\mathop{argmax}_\theta\int_z p(z|x,\theta^t)\log p(x,z|\theta)
\end{aligned}
\]</span><br />
这个式子就是上面 EM 迭代过程中的式子。</p>
<p>从 Jensen 不等式出发，也可以导出这个式子：<br />
<span class="math display">\[
\log
p(x|\theta)=\log\int_zp(x,z|\theta)dz=\log\int_z\frac{p(x,z|\theta)q(z)}{q(z)}dz\\
=\log \mathbb{E}_{q(z)}[\frac{p(x,z|\theta)}{q(z)}]\ge
\mathbb{E}_{q(z)}[\log\frac{p(x,z|\theta)}{q(z)}]
\]</span><br />
其中，右边的式子就是 ELBO，等号在 <span
class="math inline">\(p(x,z|\theta)=Cq(z)\)</span> 时成立。于是：<br />
<span class="math display">\[
\int_zq(z)dz=\frac{1}{C}\int_zp(x,z|\theta)dz=\frac{1}{C}p(x|\theta)=1\\
\Rightarrow q(z)=\frac{1}{p(x|\theta)}p(x,z|\theta)=p(z|x,\theta)
\]</span><br />
我们发现，这个过程就是上面的最大值取等号的条件。</p>
<h2 id="广义-em">广义 EM</h2>
<p>EM 模型解决了概率生成模型的参数估计的问题，通过引入隐变量 <span
class="math inline">\(z\)</span>，来学习 <span
class="math inline">\(\theta\)</span>，具体的模型对 <span
class="math inline">\(z\)</span> 有不同的假设。对学习任务 <span
class="math inline">\(p(x|\theta)\)</span>，就是学习任务 <span
class="math inline">\(\frac{p(x,z|\theta)}{p(z|x,\theta)}\)</span>。在这个式子中，我们假定了在
E 步骤中，<span
class="math inline">\(q(z)=p(z|x,\theta)\)</span>，但是这个<span
class="math inline">\(p(z|x,\theta)\)</span>
如果无法求解，那么必须使用采样（MCMC）或者变分推断等方法来近似推断这个后验。我们观察
KL 散度的表达式，为了最大化 ELBO，在固定的 <span
class="math inline">\(\theta\)</span> 时，我们需要最小化 KL
散度，于是：<br />
<span class="math display">\[
\hat{q}(z)=\mathop{argmin}_qKL(p,q)=\mathop{argmax}_qELBO
\]</span><br />
这就是广义 EM 的基本思路：</p>
<ol type="1">
<li><p>E step：<br />
<span class="math display">\[
\hat{q}^{t+1}(z)=\mathop{argmax}_q\int_zq^t(z)\log\frac{p(x,z|\theta)}{q^t(z)}dz,fixed\
\theta
\]</span></p></li>
<li><p>M step：<br />
<span class="math display">\[
\hat{\theta}=\mathop{argmax}_\theta
\int_zq^{t+1}(z)\log\frac{p(x,z|\theta)}{q^{t+1}(z)}dz,fixed\ \hat{q}
\]</span></p></li>
</ol>
<p>对于上面的积分：<br />
<span class="math display">\[
\begin{aligned}
ELBO&amp;=\int_zq(z)\log\frac{p(x,z|\theta)}{q(z)}dz\\
&amp;=\mathbb{E}_{q(z)}[\log p(x,z|\theta)-\log q(z)]\\
&amp;=\mathbb{E}_{q(z)}[\log p(x,z|\theta)]+Entropy(q(z))
\end{aligned}
\]</span><br />
因此，我们看到，广义 EM 相当于在原来的式子中加入熵这一项。</p>
<h2 id="em-的推广">EM 的推广</h2>
<p>EM
算法类似于坐标上升法，固定部分坐标，优化其他坐标，再一遍一遍的迭代。如果在
EM 框架中，无法求解 <span class="math inline">\(z\)</span>
后验概率，那么需要采用一些变种的 EM 来估算这个后验。</p>
<ol type="1">
<li>基于平均场的变分推断，VBEM/VEM</li>
<li>基于蒙特卡洛的EM，MCEM</li>
</ol>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://soundmemories.github.io/2020/10/01/ML/23.PGMIntro/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avstar.png">
      <meta itemprop="name" content="SoundMemories">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="SoundMemories">
      <meta itemprop="description" content="今日事，今日毕">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | SoundMemories">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2020/10/01/ML/23.PGMIntro/" class="post-title-link" itemprop="url">PGMIntro</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2020-10-01 00:00:00" itemprop="dateCreated datePublished" datetime="2020-10-01T00:00:00+08:00">2020-10-01</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/ML/" itemprop="url" rel="index"><span itemprop="name">ML</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>3k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>11 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="概率图模型">概率图模型</h1>
<p>概率图模型使用图的方式表示概率分布。为了在图中添加各种概率，首先总结一下随机变量分布的一些规则：<br />
<span class="math display">\[
\begin{align}
&amp;Sum\ Rule:p(x_1)=\int p(x_1,x_2)dx_2\\
&amp;Product\ Rule:p(x_1,x_2)=p(x_1|x_2)p(x_2)\\
&amp;Chain\
Rule:p(x_1,x_2,\cdots,x_p)=\prod\limits_{i=1}^pp(x_i|x_{i+1,x_{i+2}
\cdots}x_p)\\
&amp;Bayesian\ Rule:p(x_1|x_2)=\frac{p(x_2|x_1)p(x_1)}{p(x_2)}
\end{align}
\]</span><br />
可以看到，在链式法则中，如果数据维度特别高，那么的采样和计算非常困难，我们需要在一定程度上作出简化，在朴素贝叶斯中，作出了条件独立性假设。在
Markov
假设中，给定数据的维度是以时间顺序出现的，给定当前时间的维度，那么下一个维度与之前的维度独立。在
HMM 中，采用了齐次 Markov 假设。在 Markov
假设之上，更一般的，加入条件独立性假设，对维度划分集合 <span
class="math inline">\(A,B,C\)</span>，使得 <span
class="math inline">\(X_A\perp X_B|X_C\)</span>。</p>
<p>概率图模型采用图的特点表示上述的条件独立性假设，节点表示随机变量，边表示条件概率。概率图模型可以分为三大理论部分：</p>
<ol type="1">
<li>表示：
<ol type="1">
<li>有向图（离散）：贝叶斯网络</li>
<li>高斯图（连续）：高斯贝叶斯和高斯马尔可夫网路</li>
<li>无向图（离散）：马尔可夫网络</li>
</ol></li>
<li>推断
<ol type="1">
<li>精确推断</li>
<li>近似推断
<ol type="1">
<li>确定性近似（如变分推断）</li>
<li>随机近似（如 MCMC）</li>
</ol></li>
</ol></li>
<li>学习
<ol type="1">
<li>参数学习
<ol type="1">
<li>完备数据</li>
<li>隐变量：E-M 算法</li>
</ol></li>
<li>结构学习</li>
</ol></li>
</ol>
<h2 id="有向图-贝叶斯网络">有向图-贝叶斯网络</h2>
<p>已知联合分布中，各个随机变量之间的依赖关系，那么可以通过拓扑排序（根据依赖关系）可以获得一个有向图。而如果已知一个图，也可以直接得到联合概率分布的因子分解：<br />
<span class="math display">\[
p(x_1,x_2,\cdots,x_p)=\prod\limits_{i=1}^pp(x_i|x_{parent(i)})
\]</span><br />
那么实际的图中条件独立性是如何体现的呢？在局部任何三个节点，可以有三种结构：</p>
<ol type="1">
<li><pre class="mermaid">
graph TB
A((A))--&gt;B((B));
B--&gt;C((C));
</pre>
<p><span class="math display">\[
\begin{aligned}
p(A,B,C)&amp;=p(A)p(B|A)p(C|B)=p(A)p(B|A)p(C|B,A)\\
&amp;\Longrightarrow p(C|B)=p(C|B,A)\\
&amp;\Leftrightarrow p(C|B)p(A|B)=p(C|A,B)p(A|B)=p(C,A|B)\\
&amp;\Longrightarrow C\perp A|B
\end{aligned}
\]</span></p></li>
<li><pre class="mermaid">
graph TB
B((B))--&gt;A((A));
B--&gt;C((C));
    
</pre>
<p><span class="math display">\[
\begin{aligned}
p(A,B,C)&amp;=p(A|B)p(B)p(C|B)=p(B)p(A|B)p(C|A,B)\\
&amp;\Longrightarrow p(C|B)=p(C|B,A)\\
&amp;\Leftrightarrow p(C|B)p(A|B)=p(C|A,B)p(A|B)=p(C,A|B)\\
&amp;\Longrightarrow C\perp A|B
\end{aligned}
\]</span></p></li>
<li><pre class="mermaid">
graph TB
A((A))--&gt;B((B));
C((C))--&gt;B
    
</pre>
<p><span class="math display">\[
\begin{aligned}
p(A,B,C)&amp;=p(A)p(C)p(B|C,A)=p(A)p(C|A)p(B|C,A)\\
&amp;\Longrightarrow p(C)=p(C|A)\\
&amp;\Leftrightarrow C\perp A\\
\end{aligned}
\]</span></p>
<p>对这种结构，<span class="math inline">\(A,C\)</span> 不与 <span
class="math inline">\(B\)</span> 条件独立。</p></li>
</ol>
<p>从整体的图来看，可以引入 D 划分的概念。对于类似上面图 1和图
2的关系，引入集合A，B，那么满足 <span class="math inline">\(A\perp
B|C\)</span> 的 <span class="math inline">\(C\)</span> 集合中的点与
<span class="math inline">\(A,B\)</span> 中的点的关系都满足图
1，2，满足图3 关系的点都不在 <span class="math inline">\(C\)</span>
中。D 划分应用在贝叶斯定理中：<br />
<span class="math display">\[
p(x_i|x_{-i})=\frac{p(x)}{\int
p(x)dx_{i}}=\frac{\prod\limits_{j=1}^pp(x_j|x_{parents(j)})}{\int\prod\limits_{j=1}^pp(x_j|x_{parents(j)})dx_i}
\]</span><br />
可以发现，上下部分可以分为两部分，一部分是和 <span
class="math inline">\(x_i\)</span> 相关的，另一部分是和 <span
class="math inline">\(x_i\)</span>
无关的，而这个无关的部分可以相互约掉。于是计算只涉及和 <span
class="math inline">\(x_i\)</span> 相关的部分。</p>
<p>与 <span class="math inline">\(x_i\)</span>
相关的部分可以写成：<br />
<span class="math display">\[
p(x_i|x_{parents(i)})p(x_{child(i)}|x_i)
\]</span><br />
这些相关的部分又叫做 Markov 毯。</p>
<p>实际应用的模型中，对这些条件独立性作出了假设，从单一到混合，从有限到无限（时间，空间）可以分为：</p>
<ol type="1">
<li>朴素贝叶斯，单一的条件独立性假设 <span
class="math inline">\(p(x|y)=\prod\limits_{i=1}^pp(x_i|y)\)</span>，在 D
划分后，所有条件依赖的集合就是单个元素。</li>
<li>高斯混合模型：混合的条件独立。引入多类别的隐变量 <span
class="math inline">\(z_1, z_2,\cdots,z_k\)</span>， <span
class="math inline">\(p(x|z)=\mathcal{N}(\mu,\Sigma)\)</span>，条件依赖集合为多个元素。</li>
<li>与时间相关的条件依赖
<ol type="1">
<li>Markov 链</li>
<li>高斯过程（无限维高斯分布）</li>
</ol></li>
<li>连续：高斯贝叶斯网络</li>
<li>组合上面的分类
<ul>
<li>GMM 与时序结合：动态模型
<ul>
<li>HMM（离散）</li>
<li>线性动态系统 LDS（Kalman 滤波）</li>
<li>粒子滤波（非高斯，非线性）</li>
</ul></li>
</ul></li>
</ol>
<h2
id="无向图-马尔可夫网络马尔可夫随机场">无向图-马尔可夫网络（马尔可夫随机场）</h2>
<p>无向图没有了类似有向图的局部不同结构，在马尔可夫网络中，也存在 D
划分的概念。直接将条件独立的集合 <span class="math inline">\(x_A\perp
x_B|x_C\)</span> 划分为三个集合。这个也叫全局
Markov。对局部的节点，<span class="math inline">\(x\perp
(X-Neighbour(\mathcal{x}))|Neighbour(x)\)</span>。这也叫局部
Markov。对于成对的节点：<span class="math inline">\(x_i\perp
x_j|x_{-i-j}\)</span>，其中 <span class="math inline">\(i,j\)</span>
不能相邻。这也叫成对
Markov。事实上上面三个点局部全局成对是相互等价的。</p>
<p>有了这个条件独立性的划分，还需要因子分解来实际计算。引入团的概念：</p>
<blockquote>
<p>团，最大团：图中节点的集合，集合中的节点之间相互都是连接的叫做团，如果不能再添加节点，那么叫最大团。</p>
</blockquote>
<p>利用这个定义进行的 <span class="math inline">\(x\)</span>
所有维度的联合概率分布的因子分解为，假设有 <span
class="math inline">\(K\)</span> 个团，<span
class="math inline">\(Z\)</span> 就是对所有可能取值求和：<br />
<span class="math display">\[
\begin{align}p(x)=\frac{1}{Z}\prod\limits_{i=1}^{K}\phi(x_{ci})\\
Z=\sum\limits_{x\in\mathcal{X}}\prod\limits_{i=1}^{K}\phi(x_{ci})
\end{align}
\]</span><br />
其中 <span class="math inline">\(\phi(x_{ci})\)</span>
叫做势函数，它必须是一个正值，可以记为：<br />
<span class="math display">\[
\phi(x_{ci})=\exp(-E(x_{ci}))
\]</span><br />
这个分布叫做 Gibbs 分布（玻尔兹曼分布）。于是也可以记为：<span
class="math inline">\(p(x)=\frac{1}{Z}\exp(-\sum\limits_{i=1}^KE(x_{ci}))\)</span>。这个分解和条件独立性等价（Hammesley-Clifford
定理），这个分布的形式也和指数族分布形式上相同，于是满足最大熵原理。</p>
<h2 id="两种图的转换-道德图">两种图的转换-道德图</h2>
<p>我们常常想将有向图转为无向图，从而应用更一般的表达式。</p>
<ol type="1">
<li><p>链式：</p>
<pre class="mermaid">
graph TB
A((A))--&gt;B((B));
B--&gt;C((C));
    
</pre>
<p>直接去掉箭头，<span
class="math inline">\(p(a,b,c)=p(a)p(b|a)p(c|b)=\phi(a,b)\phi(b,c)\)</span>：</p>
<pre class="mermaid">
graph TB
A((A))---B((B));
B---C((C));
    
</pre></li>
<li><p>V 形：</p>
<pre class="mermaid">
graph TB
B((B))--&gt;A((A));
B--&gt;C((C));
    
</pre>
<p>由于 <span
class="math inline">\(p(a,b,c)=p(b)p(a|b)p(c|b)=\phi(a,b)\phi(b,c)\)</span>，直接去掉箭头：</p>
<pre class="mermaid">
graph TB
B((B))---A((A));
B---C((C));
    
</pre></li>
<li><p>倒 V 形：</p>
<pre class="mermaid">
graph TB
A((A))--&gt;B((B));
C((C))--&gt;B
    
</pre>
<p>由于 <span
class="math inline">\(p(a,b,c)=p(a)p(c)p(b|a,c)=\phi(a,b,c)\)</span>，于是在
<span class="math inline">\(a,c\)</span> 之间添加线：</p>
<pre class="mermaid">
graph TD
a((a))---b((b));
b---c((c));
a---c;
    
</pre>
<p>观察着三种情况可以概括为：</p>
<ol type="1">
<li>将每个节点的父节点两两相连</li>
<li>将有向边替换为无向边</li>
</ol></li>
</ol>
<h2 id="更精细的分解-因子图">更精细的分解-因子图</h2>
<p>对于一个有向图，可以通过引入环的方式，可以将其转换为无向图（Tree-like
graph），这个图就叫做道德图。但是我们上面的 BP
算法只对无环图有效，通过因子图可以变为无环图。</p>
<p>考虑一个无向图：</p>
<pre class="mermaid">
graph TD
a((a))---b((b));
b---c((c));
a---c;
</pre>
<p>可以将其转为：</p>
<pre class="mermaid">
graph TD
a((a))---f;
f---b((b));
f---c((c))
</pre>
<p>其中 <span
class="math inline">\(f=f(a,b,c)\)</span>。因子图不是唯一的，这是由于因式分解本身就对应一个特殊的因子图，将因式分解：<span
class="math inline">\(p(x)=\prod\limits_{s}f_s(x_s)\)</span>
可以进一步分解得到因子图。</p>
<h2 id="推断">推断</h2>
<p>推断的主要目的是求各种概率分布，包括边缘概率，条件概率，以及使用 MAP
来求得参数。通常推断可以分为：</p>
<ol type="1">
<li>精确推断
<ol type="1">
<li>Variable Elimination(VE)</li>
<li>Belief Propagation(BP, Sum-Product Algo)，从 VE 发展而来</li>
<li>Junction Tree，上面两种在树结构上应用，Junction Tree
在图结构上应用</li>
</ol></li>
<li>近似推断
<ol type="1">
<li>Loop Belief Propagation（针对有环图）</li>
<li>Mente Carlo Interference：例如 Importance Sampling，MCMC</li>
<li>Variational Inference</li>
</ol></li>
</ol>
<h3 id="推断-变量消除ve">推断-变量消除（VE）</h3>
<p>变量消除的方法是在求解概率分布的时候，将相关的条件概率先行求和或积分，从而一步步地消除变量，例如在马尔可夫链中：</p>
<pre class="mermaid">
graph LR
a((a))--&gt;b((b));
b--&gt;c((c));
c--&gt;d((d))
</pre>
<p><span class="math display">\[
p(d)=\sum\limits_{a,b,c}p(a,b,c,d)=\sum\limits_cp(d|c)\sum\limits_bp(c|b)\sum\limits_ap(b|a)p(a)
\]</span></p>
<p>变量消除的缺点很明显：</p>
<ol type="1">
<li>计算步骤无法存储</li>
<li>消除的最优次序是一个 NP-hard 问题</li>
</ol>
<h3 id="推断-信念传播bp">推断-信念传播（BP）</h3>
<p>为了克服 VE
的第一个缺陷-计算步骤无法存储。我们进一步地对上面的马尔可夫链进行观察：</p>
<pre class="mermaid">
graph LR
a((a))--&gt;b((b));
b--&gt;c((c));
c--&gt;d((d));
d--&gt;e((e));
</pre>
<p>要求 <span class="math inline">\(p(e)\)</span>，当然使用 VE，从 <span
class="math inline">\(a\)</span> 一直消除到 <span
class="math inline">\(d\)</span>，记 <span
class="math inline">\(\sum\limits_ap(a)p(b|a)=m_{a\to
b(b)}\)</span>，表示这是消除 <span class="math inline">\(a\)</span>
后的关于 <span class="math inline">\(b\)</span> 的概率，类似地，记 <span
class="math inline">\(\sum\limits_bp(c|b)m_{a\to b}(b)=m_{b\to
c}(c)\)</span>。于是 <span
class="math inline">\(p(e)=\sum\limits_dp(e|d)m_{b\to
c}(c)\)</span>。进一步观察，对 <span
class="math inline">\(p(c)\)</span>：<br />
<span class="math display">\[
p(c)=[\sum\limits_bp(c|b)\sum\limits_ap(b|a)p(a)]\cdot[\sum\limits_dp(d|c)\sum\limits_ep(e)p(e|d)]
\]</span><br />
我们发现了和上面计算 <span class="math inline">\(p(e)\)</span>
类似的结构，这个式子可以分成两个部分，一部分是从 <span
class="math inline">\(a\)</span> 传播过来的概率，第二部分是从 $ e$
传播过来的概率。</p>
<p>一般地，对于图（只对树形状的图）：</p>
<pre class="mermaid">
graph TD
a((a))---b((b));
b---c((c));
b---d((d));
</pre>
<p>这四个团（对于无向图是团，对于有向图就是概率为除了根的节点为1），有四个节点，三个边：<br />
<span class="math display">\[
p(a,b,c,d)=\frac{1}{Z}\phi_a(a)\phi_b(b)\phi_c(c)\phi_d(d)\cdot\phi_{ab}(a,b)\phi_{bc}(c,b)\phi_{bd}(d,b)
\]</span><br />
套用上面关于有向图的观察，如果求解边缘概率 <span
class="math inline">\(p(a)\)</span>，定义 <span
class="math inline">\(m_{c\to
b}(b)=\sum\limits_c\phi_c(c)\phi_{bc}(bc)\)</span>，<span
class="math inline">\(m_{d\to
b}(b)=\sum\limits_d\phi_d(d)\phi_{bd}(bd)\)</span>，<span
class="math inline">\(m_{b\to
a}(a)=\sum\limits_b\phi_{ba}(ba)\phi_b(b)m_{c\to b}(b)_{d\to
b}m(b)\)</span>，这样概率就一步步地传播到了 <span
class="math inline">\(a\)</span>：<br />
<span class="math display">\[
p(a)=\phi_a(a)m_{b\to a}(a)
\]</span><br />
写成一般的形式，对于相邻节点 <span
class="math inline">\(i,j\)</span>：<br />
<span class="math display">\[
m_{j\to i}(i)=\sum\limits_j\phi_j(j)\phi_{ij}(ij)\prod\limits_{k\in
Neighbour(j)-i}m_{k\to j}(j)
\]</span><br />
这个表达式，就可以保存计算过程了，只要对每条边的传播分别计算，对于一个无向树形图可以递归并行实现：</p>
<ol type="1">
<li>任取一个节点 <span class="math inline">\(a\)</span> 作为根节点</li>
<li>对这个根节点的邻居中的每一个节点，收集信息（计算入信息）</li>
<li>对根节点的邻居，分发信息（计算出信息）</li>
</ol>
<h3 id="推断-max-product-算法">推断-Max-Product 算法</h3>
<p>在推断任务中，MAP 也是常常需要的，MAP 的目的是寻找最佳参数：<br />
<span class="math display">\[
(\hat{a},\hat{b},\hat{c},\hat{d})=\mathop{argmax}_{a,b,c,d}p(a,b,c,d|E)
\]</span><br />
类似
BP，我们采用信息传递的方式来求得最优参数，不同的是，我们在所有信息传递中，传递的是最大化参数的概率，而不是将所有可能求和：<br />
<span class="math display">\[
m_{j\to i}=\max\limits_{j}\phi_j\phi_{ij}\prod\limits_{k\in
Neighbour(j)-i}m_{k\to j}
\]</span><br />
于是对于上面的图：<br />
<span class="math display">\[
\max_a p(a,b,c,d)=\max_a\phi_a\phi_{ab}m_{c\to b}m_{d\to b}
\]</span><br />
这个算法是 Sum-Product 算法的改进，也是在 HMM 中应用给的 Viterbi
算法的推广。</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://soundmemories.github.io/2020/09/30/ML/22.Exponentialfamily/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avstar.png">
      <meta itemprop="name" content="SoundMemories">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="SoundMemories">
      <meta itemprop="description" content="今日事，今日毕">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | SoundMemories">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2020/09/30/ML/22.Exponentialfamily/" class="post-title-link" itemprop="url">Exponentialfamily</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2020-09-30 00:00:00" itemprop="dateCreated datePublished" datetime="2020-09-30T00:00:00+08:00">2020-09-30</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/ML/" itemprop="url" rel="index"><span itemprop="name">ML</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>1.2k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>4 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="指数族分布">指数族分布</h1>
<p>指数族是一类分布，包括高斯分布、伯努利分布、二项分布、泊松分布、Beta
分布、Dirichlet 分布、Gamma
分布等一系列分布。指数族分布可以写为统一的形式：<br />
<span class="math display">\[
p(x|\eta)=h(x)\exp(\eta^T\phi(x)-A(\eta))=\frac{1}{\exp(A(\eta))}h(x)\exp(\eta^T\phi(x))
\]</span><br />
其中，<span class="math inline">\(\eta\)</span> 是参数向量，<span
class="math inline">\(A(\eta)\)</span>
是对数配分函数（归一化因子）。</p>
<p>在这个式子中，$ (x)$
叫做充分统计量，包含样本集合所有的信息，例如高斯分布中的均值和方差。充分统计量在在线学习中有应用，对于一个数据集，只需要记录样本的充分统计量即可。</p>
<p>对于一个模型分布假设（似然），那么我们在求解中，常常需要寻找一个共轭先验，使得先验与后验的形式相同，例如选取似然是二项分布，可取先验是
Beta 分布，那么后验也是 Beta
分布。指数族分布常常具有共轭的性质，于是我们在模型选择以及推断具有很大的便利。</p>
<p>共轭先验的性质便于计算，同时，指数族分布满足最大熵的思想（无信息先验），也就是说对于经验分布利用最大熵原理导出的分布就是指数族分布。</p>
<p>观察到指数族分布的表达式类似线性模型，事实上，指数族分布很自然地导出广义线性模型：<br />
<span class="math display">\[
y=f(w^Tx)\\
y|x\sim Exp Family
\]</span><br />
在更复杂的概率图模型中，例如在无向图模型中如受限玻尔兹曼机中，指数族分布也扮演着重要作用。</p>
<p>在推断的算法中，例如变分推断中，指数族分布也会大大简化计算。</p>
<h2 id="一维高斯分布">一维高斯分布</h2>
<p>一维高斯分布可以写成：<br />
<span class="math display">\[
p(x|\theta)=\frac{1}{\sqrt{2\pi}\sigma}\exp(-\frac{(x-\mu)^2}{2\sigma^2})
\]</span><br />
将这个式子改写：<br />
<span class="math display">\[
\frac{1}{\sqrt{2\pi\sigma^2}}\exp(-\frac{1}{2\sigma^2}(x^2-2\mu
x+\mu^2))\\
=\exp(\log(2\pi\sigma^2)^{-1/2})\exp(-\frac{1}{2\sigma^2}\begin{pmatrix}-2\mu&amp;1\end{pmatrix}\begin{pmatrix}x\\x^2\end{pmatrix}-\frac{\mu^2}{2\sigma^2})
\]</span><br />
所以：<br />
<span class="math display">\[
\eta=\begin{pmatrix}\frac{\mu}{\sigma^2}\\-\frac{1}{2\sigma^2}\end{pmatrix}=\begin{pmatrix}\eta_1\\\eta_2\end{pmatrix}
\]</span><br />
于是 <span class="math inline">\(A(\eta)\)</span>：<br />
<span class="math display">\[
A(\eta)=-\frac{\eta_1^2}{4\eta_2}+\frac{1}{2}\log(-\frac{\pi}{\eta_2})
\]</span></p>
<h2
id="充分统计量和对数配分函数的关系">充分统计量和对数配分函数的关系</h2>
<p>对概率密度函数求积分：<br />
<span class="math display">\[
\begin{align}
\exp(A(\eta))&amp;=\int h(x)\exp(\eta^T\phi(x))dx\nonumber
\end{align}
\]</span><br />
两边对参数求导：<br />
<span class="math display">\[
\exp(A(\eta))A&#39;(\eta)=\int h(x)\exp(\eta^T\phi(x))\phi(x)dx\\
\Longrightarrow A&#39;(\eta)=\mathbb{E}_{p(x|\eta)}[\phi(x)]
\]</span><br />
类似的：<br />
<span class="math display">\[
A&#39;&#39;(\eta)=Var_{p(x|\eta)}[\phi(x)]
\]</span><br />
由于方差为正，于是 <span class="math inline">\(A(\eta)\)</span>
一定是凸函数。</p>
<h2 id="充分统计量和极大似然估计">充分统计量和极大似然估计</h2>
<p>对于独立全同采样得到的数据集 <span
class="math inline">\(\mathcal{D}=\{x_1,x_2,\cdots,x_N\}\)</span>。<br />
$$<br />
<span
class="math display">\[\begin{align}\eta_{MLE}&amp;=\mathop{argmax}_\eta\sum\limits_{i=1}^N\log
p(x_i|\eta)\nonumber\\
&amp;=\mathop{argmax}_\eta\sum\limits_{i=1}^N(\eta^T\phi(x_i)-A(\eta))\nonumber\\
&amp;\Longrightarrow
A&#39;(\eta_{MLE})=\frac{1}{N}\sum\limits_{i=1}^N\phi(x_i)

\end{align}\]</span><br />
$$<br />
由此可以看到，为了估算参数，只需要知道充分统计量就可以了。</p>
<h2 id="最大熵">最大熵</h2>
<p>信息熵记为：<br />
<span class="math display">\[
Entropy=\int-p(x)\log(p(x))dx
\]</span></p>
<blockquote>
<p>一般地，对于完全随机的变量（等可能），信息熵最大。</p>
<p>我们的假设为最大熵原则，假设数据是离散分布的，<span
class="math inline">\(k\)</span> 个特征的概率分别为 <span
class="math inline">\(p_k\)</span>，最大熵原理可以表述为：<br />
<span class="math display">\[
  \max\{H(p)\}=\min\{\sum\limits_{k=1}^Kp_k\log p_k\}\ s.t.\
\sum\limits_{k=1}^Kp_k=1
  \]</span><br />
利用 Lagrange 乘子法：<br />
<span class="math display">\[
  L(p,\lambda)=\sum\limits_{k=1}^Kp_k\log
p_k+\lambda(1-\sum\limits_{k=1}^Kp_k)
  \]</span><br />
于是可得：<br />
<span class="math display">\[
  p_1=p_2=\cdots=p_K=\frac{1}{K}
  \]</span><br />
因此等可能的情况熵最大。</p>
</blockquote>
<p>一个数据集 <span
class="math inline">\(\mathcal{D}\)</span>，在这个数据集上的经验分布为
<span
class="math inline">\(\hat{p}(x)=\frac{Count(x)}{N}\)</span>，实际不可能满足所有的经验概率相同，于是在上面的最大熵原理中还需要加入这个经验分布的约束。</p>
<p>对任意一个函数，经验分布的经验期望可以求得为：<br />
<span class="math display">\[
\mathbb{E}_\hat{p}[f(x)]=\Delta
\]</span><br />
于是：<br />
<span class="math display">\[
\max\{H(p)\}=\min\{\sum\limits_{k=1}^Np_k\log p_k\}\ s.t.\
\sum\limits_{k=1}^Np_k=1,\mathbb{E}_p[f(x)]=\Delta
\]</span><br />
Lagrange 函数为：<br />
<span class="math display">\[
L(p,\lambda_0,\lambda)=\sum\limits_{k=1}^Np_k\log
p_k+\lambda_0(1-\sum\limits_{k=1}^Np_k)+\lambda^T(\Delta-\mathbb{E}_p[f(x)])
\]</span><br />
求导得到：<br />
<span class="math display">\[
\frac{\partial}{\partial p(x)}L=\sum\limits_{k=1}^N(\log
p(x)+1)-\sum\limits_{k=1}^N\lambda_0-\sum\limits_{k=1}^N\lambda^Tf(x)\\
\Longrightarrow\sum\limits_{k=1}^N\log p(x)+1-\lambda_0-\lambda^Tf(x)=0
\]</span><br />
由于数据集是任意的，对数据集求和也意味着求和项里面的每一项都是0：<br />
<span class="math display">\[
p(x)=\exp(\lambda^Tf(x)+\lambda_0-1)
\]</span><br />
这就是指数族分布。</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://soundmemories.github.io/2020/09/29/ML/21.SVM/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avstar.png">
      <meta itemprop="name" content="SoundMemories">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="SoundMemories">
      <meta itemprop="description" content="今日事，今日毕">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | SoundMemories">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2020/09/29/ML/21.SVM/" class="post-title-link" itemprop="url">SVM</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2020-09-29 00:00:00" itemprop="dateCreated datePublished" datetime="2020-09-29T00:00:00+08:00">2020-09-29</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/ML/" itemprop="url" rel="index"><span itemprop="name">ML</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>3.1k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>11 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="支撑向量机">支撑向量机</h1>
<p>支撑向量机（SVM）算法在分类问题中有着重要地位，其主要思想是最大化两类之间的间隔。按照数据集的特点：</p>
<ol type="1">
<li>线性可分问题，如之前的感知机算法处理的问题</li>
<li>线性可分，只有一点点错误点，如感知机算法发展出来的 Pocket
算法处理的问题</li>
<li>非线性问题，完全不可分，如在感知机问题发展出来的多层感知机和深度学习</li>
</ol>
<p>这三种情况对于 SVM 分别有下面三种处理手段：</p>
<ol type="1">
<li>hard-margin SVM</li>
<li>soft-margin SVM</li>
<li>kernel Method</li>
</ol>
<p>SVM 的求解中，大量用到了 Lagrange
乘子法，首先对这种方法进行介绍。</p>
<h2 id="约束优化问题">约束优化问题</h2>
<p>一般地，约束优化问题（原问题）可以写成：<br />
$$<br />
<span class="math display">\[\begin{align}

&amp;\min_{x\in\mathbb{R^p}}f(x)\\
&amp;s.t.\ m_i(x)\le0,i=1,2,\cdots,M\\
&amp;\ \ \ \ \ \ \ \ n_j(x)=0,j=1,2,\cdots,N

\end{align}\]</span><br />
<span class="math display">\[
定义 Lagrange 函数：
\]</span><br />
L(x,,)=f(x)+<em>{i=1}<sup>M<em>im_i(x)+</em>{i=1}</sup>N<em>in_i(x)<br />
<span class="math display">\[
那么原问题可以等价于无约束形式：
\]</span><br />
</em>{x^p}</em>{,}L(x,,) s.t. _i<br />
$$<br />
这是由于，当满足原问题的不等式约束的时候，<span
class="math inline">\(\lambda_i=0\)</span>
才能取得最大值，直接等价于原问题，如果不满足原问题的不等式约束，那么最大值就为
<span
class="math inline">\(+\infty\)</span>，由于需要取最小值，于是不会取到这个情况。</p>
<p>这个问题的对偶形式：<br />
<span class="math display">\[
\max_{\lambda,\eta}\min_{x\in\mathbb{R}^p}L(x,\lambda,\eta)\ s.t.\
\lambda_i\ge0
\]</span><br />
对偶问题是关于 $ , $ 的最大化问题。</p>
<p>由于：<br />
<span class="math display">\[
\max_{\lambda_i,\eta_j}\min_{x}L(x,\lambda_i,\eta_j)\le\min_{x}\max_{\lambda_i,\eta_j}L(x,\lambda_i,\eta_j)
\]</span></p>
<blockquote>
<p>证明：显然有 <span class="math inline">\(\min\limits_{x}L\le
L\le\max\limits_{\lambda,\eta}L\)</span>，于是显然有 <span
class="math inline">\(\max\limits_{\lambda,\eta}\min\limits_{x}L\le
L\)</span>，且 <span
class="math inline">\(\min\limits_{x}\max\limits_{\lambda,\eta}L\ge
L\)</span>。</p>
</blockquote>
<p>对偶问题的解小于原问题，有两种情况：</p>
<ol type="1">
<li>强对偶：可以取等于号</li>
<li>弱对偶：不可以取等于号</li>
</ol>
<p>其实这一点也可以通过一张图来说明：</p>
<p><img src="/images/SVM/SVM.png" width="90%" height="90%"></p>
<p>对于一个凸优化问题，有如下定理：</p>
<blockquote>
<p>如果凸优化问题满足某些条件如 Slater
条件，那么它和其对偶问题满足强对偶关系。记问题的定义域为：<span
class="math inline">\(\mathcal{D}=domf(x)\cap dom m_i(x)\cap
domn_j(x)\)</span>。于是 Slater 条件为：<br />
<span class="math display">\[
  \exists\hat{x}\in Relint\mathcal{D}\ s.t.\ \forall
i=1,2,\cdots,M,m_i(x)\lt0
  \]</span><br />
其中 Relint 表示相对内部（不包含边界的内部）。</p>
</blockquote>
<ol type="1">
<li>对于大多数凸优化问题，Slater 条件成立。</li>
<li>松弛 Slater 条件，如果 M 个不等式约束中，有 K
个函数为仿射函数，那么只要其余的函数满足 Slater 条件即可。</li>
</ol>
<p>上面介绍了原问题和对偶问题的对偶关系，但是实际还需要对参数进行求解，求解方法使用
KKT 条件进行：</p>
<blockquote>
<p>KKT 条件和强对偶关系是等价关系。KKT 条件对最优解的条件为：</p>
<ol type="1">
<li><p>可行域：<br />
<span class="math display">\[
\begin{align}
m_i(x^*)\le0\\
n_j(x^*)=0\\
\lambda^*\ge0
\end{align}
\]</span></p></li>
<li><p>互补松弛 <span class="math inline">\(\lambda^*m_i(x^*)=0,\forall
m_i\)</span>，对偶问题的最佳值为 <span
class="math inline">\(d^*\)</span>，原问题为 <span
class="math inline">\(p^*\)</span><br />
<span class="math display">\[
\begin{align}
d^*&amp;=\max_{\lambda,\eta}g(\lambda,\eta)=g(\lambda^*,\eta^*)\nonumber\\
&amp;=\min_{x}L(x,\lambda^*,\eta^*)\nonumber\\
&amp;\le L(x^*,\lambda^*,\eta^*)\nonumber\\
&amp;=f(x^*)+\sum\limits_{i=1}^M\lambda^*m_i(x^*)\nonumber\\
&amp;\le f(x^*)=p^*
\end{align}
\]</span><br />
为了满足相等，两个不等式必须成立，于是，对于第一个不等于号，需要有梯度为0条件，对于第二个不等于号需要满足互补松弛条件。</p></li>
<li><p>梯度为0：<span class="math inline">\(\frac{\partial
L(x,\lambda^*,\eta^*)}{\partial x}|_{x=x^*}=0\)</span></p></li>
</ol>
</blockquote>
<h2 id="hard-margin-svm">Hard-margin SVM</h2>
<p>支撑向量机也是一种硬分类模型，在之前的感知机模型中，我们在线性模型的基础上叠加了符号函数，在几何直观上，可以看到，如果两类分的很开的话，那么其实会存在无穷多条线可以将两类分开。在
SVM
中，我们引入最大化间隔这个概念，间隔指的是数据和直线的距离的最小值，因此最大化这个值反映了我们的模型倾向。</p>
<p>分割的超平面可以写为：<br />
<span class="math display">\[
0=w^Tx+b
\]</span><br />
那么最大化间隔（约束为分类任务的要求）：<br />
<span class="math display">\[
\mathop{argmax}_{w,b}[\min_i\frac{|w^Tx_i+b|}{||w||}]\ s.t.\
y_i(w^Tx_i+b)&gt;0\\
\Longrightarrow\mathop{argmax}_{w,b}[\min_i\frac{y_i(w^Tx_i+b)}{||w||}]\
s.t.\ y_i(w^Tx_i+b)&gt;0
\]</span><br />
对于这个约束 <span
class="math inline">\(y_i(w^Tx_i+b)&gt;0\)</span>，不妨固定 <span
class="math inline">\(\min
y_i(w^Tx_i+b)=1&gt;0\)</span>，这是由于分开两类的超平面的系数经过比例放缩不会改变这个平面，这也相当于给超平面的系数作出了约束。化简后的式子可以表示为：<br />
<span class="math display">\[
\mathop{argmin}_{w,b}\frac{1}{2}w^Tw\ s.t.\ \min_iy_i(w^Tx_i+b)=1\\
\Rightarrow\mathop{argmin}_{w,b}\frac{1}{2}w^Tw\ s.t.\
y_i(w^Tx_i+b)\ge1,i=1,2,\cdots,N
\]</span><br />
这就是一个包含 <span class="math inline">\(N\)</span>
个约束的凸优化问题，有很多求解这种问题的软件。</p>
<p>但是，如果样本数量或维度非常高，直接求解困难甚至不可解，于是需要对这个问题进一步处理。引入
Lagrange 函数：<br />
<span class="math display">\[
L(w,b,\lambda)=\frac{1}{2}w^Tw+\sum\limits_{i=1}^N\lambda_i(1-y_i(w^Tx_i+b))
\]</span><br />
我们有原问题就等价于：<br />
<span class="math display">\[
\mathop{argmin}_{w,b}\max_{\lambda}L(w,b,\lambda_i)\ s.t.\ \lambda_i\ge0
\]</span><br />
我们交换最小和最大值的符号得到对偶问题：<br />
<span class="math display">\[
\max_{\lambda_i}\min_{w,b}L(w,b,\lambda_i)\ s.t.\ \lambda_i\ge0
\]</span><br />
由于不等式约束是仿射函数，对偶问题和原问题等价：</p>
<ul>
<li><p><span class="math inline">\(b\)</span>：<span
class="math inline">\(\frac{\partial}{\partial
b}L=0\Rightarrow\sum\limits_{i=1}^N\lambda_iy_i=0\)</span></p></li>
<li><p><span class="math inline">\(w\)</span>：首先将 <span
class="math inline">\(b\)</span> 代入：<br />
<span class="math display">\[
L(w,b,\lambda_i)=\frac{1}{2}w^Tw+\sum\limits_{i=1}^N\lambda_i(1-y_iw^Tx_i-y_ib)=\frac{1}{2}w^Tw+\sum\limits_{i=1}^N\lambda_i-\sum\limits_{i=1}^N\lambda_iy_iw^Tx_i
\]</span><br />
所以：<br />
<span class="math display">\[
\frac{\partial}{\partial w}L=0\Rightarrow
w=\sum\limits_{i=1}^N\lambda_iy_ix_i
\]</span></p></li>
<li><p>将上面两个参数代入：<br />
<span class="math display">\[
L(w,b,\lambda_i)=-\frac{1}{2}\sum\limits_{i=1}^N\sum\limits_{j=1}^N\lambda_i\lambda_jy_iy_jx_i^Tx_j+\sum\limits_{i=1}^N\lambda_i
\]</span></p></li>
</ul>
<p>因此，对偶问题就是：<br />
<span class="math display">\[
\max_{\lambda}-\frac{1}{2}\sum\limits_{i=1}^N\sum\limits_{j=1}^N\lambda_i\lambda_jy_iy_jx_i^Tx_j+\sum\limits_{i=1}^N\lambda_i,\
s.t.\ \lambda_i\ge0
\]</span><br />
从 KKT 条件得到超平面的参数：</p>
<blockquote>
<p>原问题和对偶问题满足强对偶关系的充要条件为其满足 KKT 条件：<br />
<span class="math display">\[
  \begin{align}
  &amp;\frac{\partial L}{\partial w}=0,\frac{\partial L}{\partial b}=0
  \\&amp;\lambda_k(1-y_k(w^Tx_k+b))=0(slackness\ complementary)\\
  &amp;\lambda_i\ge0\\
  &amp;1-y_i(w^Tx_i+b)\le0
  \end{align}
  \]</span></p>
</blockquote>
<p>根据这个条件就得到了对应的最佳参数：<br />
<span class="math display">\[
\hat{w}=\sum\limits_{i=1}^N\lambda_iy_ix_i\\
\hat{b}=y_k-w^Tx_k=y_k-\sum\limits_{i=1}^N\lambda_iy_ix_i^Tx_k,\exists
k,1-y_k(w^Tx_k+b)=0
\]</span><br />
于是这个超平面的参数 <span class="math inline">\(w\)</span>
就是数据点的线性组合，最终的参数值就是部分满足 <span
class="math inline">\(y_i(w^Tx_i+b)=1\)</span>向量的线性组合（互补松弛条件给出），这些向量也叫支撑向量。</p>
<h2 id="soft-margin-svm">Soft-margin SVM</h2>
<p>Hard-margin 的 SVM
只对可分数据可解，如果不可分的情况，我们的基本想法是在损失函数中加入错误分类的可能性。错误分类的个数可以写成：<br />
<span class="math display">\[
error=\sum\limits_{i=1}^N\mathbb{I}\{y_i(w^Tx_i+b)\lt1\}
\]</span><br />
这个函数不连续，可以将其改写为：<br />
<span class="math display">\[
error=\sum\limits_{i=1}^N\max\{0,1-y_i(w^Tx_i+b)\}
\]</span><br />
求和符号中的式子又叫做 Hinge Function。</p>
<p>将这个错误加入 Hard-margin SVM 中，于是：<br />
<span class="math display">\[
\mathop{argmin}_{w,b}\frac{1}{2}w^Tw+C\sum\limits_{i=1}^N\max\{0,1-y_i(w^Tx_i+b)\}\
s.t.\ y_i(w^Tx_i+b)\ge1-\xi_i,i=1,2,\cdots,N
\]</span><br />
这个式子中，常数 <span class="math inline">\(C\)</span>
可以看作允许的错误水平，同时上式为了进一步消除 <span
class="math inline">\(\max\)</span>
符号，对数据集中的每一个观测，我们可以认为其大部分满足约束，但是其中部分违反约束，因此这部分约束变成
<span class="math inline">\(y_i(w^Tx+b)\ge1-\xi_i\)</span>，其中 <span
class="math inline">\(\xi_i=1-y_i(w^Tx_i+b)\)</span>，进一步的化简：<br />
<span class="math display">\[
\mathop{argmin}_{w,b}\frac{1}{2}w^Tw+C\sum\limits_{i=1}^N\xi_i\ s.t.\
y_i(w^Tx_i+b)\ge1-\xi_i,\xi_i\ge0,i=1,2,\cdots,N
\]</span></p>
<h2 id="kernel-method">Kernel Method</h2>
<p>核方法可以应用在很多问题上，在分类问题中，对于严格不可分问题，我们引入一个特征转换函数将原来的不可分的数据集变为可分的数据集，然后再来应用已有的模型。往往将低维空间的数据集变为高维空间的数据集后，数据会变得可分（数据变得更为稀疏）：</p>
<blockquote>
<p>Cover TH：高维空间比低维空间更易线性可分。</p>
</blockquote>
<p>应用在 SVM 中时，观察上面的 SVM 对偶问题：<br />
<span class="math display">\[
\max_{\lambda}-\frac{1}{2}\sum\limits_{i=1}^N\sum\limits_{j=1}^N\lambda_i\lambda_jy_iy_jx_i^Tx_j+\sum\limits_{i=1}^N\lambda_i,\
s.t.\ \lambda_i\ge0
\]</span><br />
在求解的时候需要求得内积，于是不可分数据在通过特征变换后，需要求得变换后的内积。我们常常很难求得变换函数的内积。于是直接引入内积的变换函数：<br />
<span class="math display">\[
\forall x,x&#39;\in\mathcal{X},\exists\phi\in\mathcal{H}:x\rightarrow z\
s.t.\ k(x,x&#39;)=\phi(x)^T\phi(x)
\]</span><br />
称 <span class="math inline">\(k(x,x&#39;)\)</span>
为一个正定核函数，其中<span class="math inline">\(\mathcal{H}\)</span>
是 Hilbert
空间（完备的线性内积空间），如果去掉内积这个条件我们简单地称为核函数。</p>
<blockquote>
<p><span
class="math inline">\(k(x,x&#39;)=\exp(-\frac{(x-x&#39;)^2}{2\sigma^2})\)</span>
是一个核函数。</p>
<p>证明：<br />
<span class="math display">\[
  \begin{align}
  \exp(-\frac{(x-x&#39;)^2}{2\sigma^2})&amp;=\exp(-\frac{x^2}{2\sigma^2})\exp(\frac{xx&#39;}{\sigma^2})\exp(-\frac{x&#39;^2}{2\sigma^2})\nonumber\\
  &amp;=\exp(-\frac{x^2}{2\sigma^2})\sum\limits_{n=0}^{+\infty}\frac{x^nx&#39;^n}{\sigma^{2n}n!}\exp(-\frac{x&#39;^2}{2\sigma^2})\nonumber\\
  &amp;=\exp(-\frac{x^2}{2\sigma^2})\varphi(x)\varphi(x&#39;)\exp(-\frac{x&#39;^2}{2\sigma^2})\nonumber\\
  &amp;=\phi(x)\phi(x&#39;)
  \end{align}
  \]</span></p>
</blockquote>
<p>正定核函数有下面的等价定义：</p>
<blockquote>
<p>如果核函数满足：</p>
<ol type="1">
<li>对称性</li>
<li>正定性</li>
</ol>
<p>那么这个核函数时正定核函数。</p>
<p>证明：</p>
<ol type="1">
<li>对称性 <span class="math inline">\(\Leftrightarrow\)</span> <span
class="math inline">\(k(x,z)=k(z,x)\)</span>，显然满足内积的定义</li>
<li>正定性 <span class="math inline">\(\Leftrightarrow\)</span> <span
class="math inline">\(\forall
N,x_1,x_2,\cdots,x_N\in\mathcal{X}\)</span>，对应的 Gram Matrix <span
class="math inline">\(K=[k(x_i,x_j)]\)</span> 是半正定的。</li>
</ol>
<p>要证：<span
class="math inline">\(k(x,z)=\phi(x)^T\phi(z)\Leftrightarrow K\)</span>
半正定+对称性。</p>
<ol type="1">
<li><p><span
class="math inline">\(\Rightarrow\)</span>：首先，对称性是显然的，对于正定性：<br />
<span class="math display">\[
K=\begin{pmatrix}k(x_1,x_2)&amp;\cdots&amp;k(x_1,x_N)\\\vdots&amp;\vdots&amp;\vdots\\k(x_N,x_1)&amp;\cdots&amp;k(x_N,x_N)\end{pmatrix}
\]</span><br />
任意取 <span
class="math inline">\(\alpha\in\mathbb{R}^N\)</span>，即需要证明 <span
class="math inline">\(\alpha^TK\alpha\ge0\)</span>：<br />
<span class="math display">\[
\alpha^TK\alpha=\sum\limits_{i,j}\alpha_i\alpha_jK_{ij}=\sum\limits_{i,j}\alpha_i\phi^T(x_i)\phi(x_j)\alpha_j=\sum\limits_{i}\alpha_i\phi^T(x_i)\sum\limits_{j}\alpha_j\phi(x_j)
\]</span><br />
这个式子就是内积的形式，Hilbert
空间满足线性性，于是正定性的证。</p></li>
<li><p><span class="math inline">\(\Leftarrow\)</span>：对于 <span
class="math inline">\(K\)</span> 进行分解，对于对称矩阵 <span
class="math inline">\(K=V\Lambda V^T\)</span>，那么令 <span
class="math inline">\(\phi(x_i)=\sqrt{\lambda_i}V_i\)</span>，其中 <span
class="math inline">\(V_i\)</span>是特征向量，于是就构造了 <span
class="math inline">\(k(x,z)=\sqrt{\lambda_i\lambda_j}V_i^TV_j\)</span></p></li>
</ol>
</blockquote>
<h2 id="小结">小结</h2>
<p>分类问题在很长一段时间都依赖 SVM，对于严格可分的数据集，Hard-margin
SVM
选定一个超平面，保证所有数据到这个超平面的距离最大，对这个平面施加约束，固定
<span
class="math inline">\(y_i(w^Tx_i+b)=1\)</span>，得到了一个凸优化问题并且所有的约束条件都是仿射函数，于是满足
Slater
条件，将这个问题变换成为对偶的问题，可以得到等价的解，并求出约束参数：<br />
<span class="math display">\[
\max_{\lambda}-\frac{1}{2}\sum\limits_{i=1}^N\sum\limits_{j=1}^N\lambda_i\lambda_jy_iy_jx_i^Tx_j+\sum\limits_{i=1}^N\lambda_i,\
s.t.\ \lambda_i\ge0
\]</span><br />
对需要的超平面参数的求解采用强对偶问题的 KKT 条件进行。<br />
<span class="math display">\[
\begin{align}
&amp;\frac{\partial L}{\partial w}=0,\frac{\partial L}{\partial b}=0
\\&amp;\lambda_k(1-y_k(w^Tx_k+b))=0(slackness\ complementary)\\
&amp;\lambda_i\ge0\\
&amp;1-y_i(w^Tx_i+b)\le0
\end{align}
\]</span><br />
解就是：<br />
<span class="math display">\[
\hat{w}=\sum\limits_{i=1}^N\lambda_iy_ix_i\\
\hat{b}=y_k-w^Tx_k=y_k-\sum\limits_{i=1}^N\lambda_iy_ix_i^Tx_k,\exists
k,1-y_k(w^Tx_k+b)=0
\]</span><br />
当允许一点错误的时候，可以在 Hard-margin SVM 中加入错误项。用 Hinge
Function 表示错误项的大小，得到：<br />
<span class="math display">\[
\mathop{argmin}_{w,b}\frac{1}{2}w^Tw+C\sum\limits_{i=1}^N\xi_i\ s.t.\
y_i(w^Tx_i+b)\ge1-\xi_i,\xi_i\ge0,i=1,2,\cdots,N
\]</span><br />
对于完全不可分的问题，我们采用特征转换的方式，在 SVM
中，我们引入正定核函数来直接对内积进行变换，只要这个变换满足对称性和正定性，那么就可以用做核函数。</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://soundmemories.github.io/2020/09/28/ML/20.DimentionReduction/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avstar.png">
      <meta itemprop="name" content="SoundMemories">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="SoundMemories">
      <meta itemprop="description" content="今日事，今日毕">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | SoundMemories">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2020/09/28/ML/20.DimentionReduction/" class="post-title-link" itemprop="url">DimentionReduction</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2020-09-28 00:00:00" itemprop="dateCreated datePublished" datetime="2020-09-28T00:00:00+08:00">2020-09-28</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/ML/" itemprop="url" rel="index"><span itemprop="name">ML</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>1.5k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>5 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="降维">降维</h1>
<p>我们知道，解决过拟合的问题除了正则化和添加数据之外，降维就是最好的方法。降维的思路来源于维度灾难的问题，我们知道
<span class="math inline">\(n\)</span> 维球的体积为：<br />
<span class="math display">\[
CR^n
\]</span><br />
那么在球体积与边长为 <span class="math inline">\(2R\)</span>
的超立方体比值为：<br />
<span class="math display">\[
\lim\limits_{n\rightarrow0}\frac{CR^n}{2^nR^n}=0
\]</span></p>
<p>这就是所谓的维度灾难，在高维数据中，主要样本都分布在立方体的边缘，所以数据集更加稀疏。</p>
<p>降维的算法分为：</p>
<ol type="1">
<li>直接降维，特征选择</li>
<li>线性降维，PCA，MDS等</li>
<li>分线性，流形包括 Isomap，LLE 等</li>
</ol>
<p>为了方便，我们首先将协方差矩阵（数据集）写成中心化的形式：<br />
<span class="math display">\[
\begin{align}S&amp;=\frac{1}{N}\sum\limits_{i=1}^N(x_i-\overline{x})(x_i-\overline{x})^T\nonumber\\
&amp;=\frac{1}{N}(x_1-\overline{x},x_2-\overline{x},\cdots,x_N-\overline{x})(x_1-\overline{x},x_2-\overline{x},\cdots,x_N-\overline{x})^T\nonumber\\
&amp;=\frac{1}{N}(X^T-\frac{1}{N}X^T\mathbb{I}_{N1}\mathbb{I}_{N1}^T)(X^T-\frac{1}{N}X^T\mathbb{I}_{N1}\mathbb{I}_{N1}^T)^T\nonumber\\
&amp;=\frac{1}{N}X^T(E_N-\frac{1}{N}\mathbb{I}_{N1}\mathbb{I}_{1N})(E_N-\frac{1}{N}\mathbb{I}_{N1}\mathbb{I}_{1N})^TX\nonumber\\
&amp;=\frac{1}{N}X^TH_NH_N^TX\nonumber\\
&amp;=\frac{1}{N}X^TH_NH_NX=\frac{1}{N}X^THX
\end{align}
\]</span><br />
这个式子利用了中心矩阵 $ H$的对称性，这也是一个投影矩阵。</p>
<h2 id="线性降维-主成分分析-pca">线性降维-主成分分析 PCA</h2>
<h3 id="损失函数">损失函数</h3>
<p>主成分分析中，我们的基本想法是将所有数据投影到一个字空间中，从而达到降维的目标，为了寻找这个子空间，我们基本想法是：</p>
<ol type="1">
<li>所有数据在子空间中更为分散</li>
<li>损失的信息最小，即：在补空间的分量少</li>
</ol>
<p>原来的数据很有可能各个维度之间是相关的，于是我们希望找到一组 <span
class="math inline">\(p\)</span> 个新的线性无关的单位基 <span
class="math inline">\(u_i\)</span>，降维就是取其中的 <span
class="math inline">\(q\)</span> 个基。于是对于一个样本 <span
class="math inline">\(x_i\)</span>，经过这个坐标变换后：<br />
<span class="math display">\[
\hat{x_i}=\sum\limits_{i=1}^p(u_i^Tx_i)u_i=\sum\limits_{i=1}^q(u_i^Tx_i)u_i+\sum\limits_{i=q+1}^p(u_i^Tx_i)u_i
\]</span><br />
对于数据集来说，我们首先将其中心化然后再去上面的式子的第一项，并使用其系数的平方平均作为损失函数并最大化：<br />
<span class="math display">\[
\begin{align}J&amp;=\frac{1}{N}\sum\limits_{i=1}^N\sum\limits_{j=1}^q((x_i-\overline{x})^Tu_j)^2\nonumber\\
&amp;=\sum\limits_{j=1}^qu_j^TSu_j\ ,\ s.t.\ u_j^Tu_j=1
\end{align}
\]</span><br />
由于每个基都是线性无关的，于是每一个 <span
class="math inline">\(u_j\)</span>
的求解可以分别进行，使用拉格朗日乘子法：<br />
<span class="math display">\[
\mathop{argmax}_{u_j}L(u_j,\lambda)=\mathop{argmax}_{u_j}u_j^TSu_j+\lambda(1-u_j^Tu_j)
\]</span><br />
于是：<br />
<span class="math display">\[
Su_j=\lambda u_j
\]</span><br />
可见，我们需要的基就是协方差矩阵的本征矢。损失函数最大取在本征值前 <span
class="math inline">\(q\)</span> 个最大值。</p>
<p>下面看其损失的信息最少这个条件，同样适用系数的平方平均作为损失函数，并最小化：<br />
<span class="math display">\[
\begin{align}J&amp;=\frac{1}{N}\sum\limits_{i=1}^N\sum\limits_{j=q+1}^p((x_i-\overline{x})^Tu_j)^2\nonumber\\
&amp;=\sum\limits_{j=q+1}^pu_j^TSu_j\ ,\ s.t.\ u_j^Tu_j=1
\end{align}
\]</span><br />
同样的：<br />
<span class="math display">\[
\mathop{argmin}_{u_j}L(u_j,\lambda)=\mathop{argmin}_{u_j}u_j^TSu_j+\lambda(1-u_j^Tu_j)
\]</span><br />
损失函数最小取在本征值剩下的个最小的几个值。数据集的协方差矩阵可以写成
<span class="math inline">\(S=U\Lambda
U^T\)</span>，直接对这个表达式当然可以得到本征矢。</p>
<h3 id="svd-与-pcoa">SVD 与 PCoA</h3>
<p>下面使用实际训练时常常使用的 SVD 直接求得这个 <span
class="math inline">\(q\)</span> 个本征矢。</p>
<p>对中心化后的数据集进行奇异值分解：<br />
<span class="math display">\[
HX=U\Sigma V^T,U^TU=E_N,V^TV=E_p,\Sigma:N\times p
\]</span></p>
<p>于是：<br />
<span class="math display">\[
S=\frac{1}{N}X^THX=\frac{1}{N}X^TH^THX=\frac{1}{N}V\Sigma^T\Sigma V^T
\]</span><br />
因此，我们直接对中心化后的数据集进行 SVD，就可以得到特征值和特征向量
<span class="math inline">\(V\)</span>，在新坐标系中的坐标就是：<br />
<span class="math display">\[
HX\cdot V
\]</span><br />
由上面的推导，我们也可以得到另一种方法 PCoA
主坐标分析，定义并进行特征值分解：<br />
<span class="math display">\[
T=HXX^TH=U\Sigma\Sigma^TU^T
\]</span><br />
由于：<br />
<span class="math display">\[
TU\Sigma=U\Sigma(\Sigma^T\Sigma)
\]</span><br />
于是可以直接得到坐标。这两种方法都可以得到主成分，但是由于方差矩阵是
<span class="math inline">\(p\times p\)</span> 的，而 <span
class="math inline">\(T\)</span> 是 <span class="math inline">\(N\times
N\)</span> 的，所以对样本量较少的时候可以采用 PCoA的方法。</p>
<h3 id="p-pca">p-PCA</h3>
<p>下面从概率的角度对 PCA 进行分析，概率方法也叫
p-PCA。我们使用线性模型，类似之前 LDA，我们选定一个方向，对原数据 <span
class="math inline">\(x\in\mathbb{R}^p\)</span> ，降维后的数据为 <span
class="math inline">\(z\in\mathbb{R}^q,q&lt;p\)</span>。降维通过一个矩阵变换（投影）进行：<br />
<span class="math display">\[
\begin{align}
z&amp;\sim\mathcal{N}(\mathbb{O}_{q1},\mathbb{I}_{qq})\\
x&amp;=Wz+\mu+\varepsilon\\
\varepsilon&amp;\sim\mathcal{N}(0,\sigma^2\mathbb{I}_{pp})
\end{align}
\]</span><br />
对于这个模型，我么可以使用期望-最大（EM）的算法进行学习，在进行推断的时候需要求得
<span
class="math inline">\(p(z|x)\)</span>，推断的求解过程和线性高斯模型类似。<br />
<span class="math display">\[
\begin{align}
&amp;p(z|x)=\frac{p(x|z)p(z)}{p(x)}\\
&amp;\mathbb{E}[x]=\mathbb{E}[Wz+\mu+\varepsilon]=\mu\\
&amp;Var[x]=WW^T+\sigma^2\mathbb{I}_{pp}\\
\Longrightarrow
p(z|x)=\mathcal{N}(W^T(WW^T+&amp;\sigma^2\mathbb{I})^{-1}(x-\mu),\mathbb{I}-W^T(WW^T+\sigma^2\mathbb{I})^{-1}W)
\end{align}
\]</span></p>
<h2 id="小结">小结</h2>
<p>降维是解决维度灾难和过拟合的重要方法，除了直接的特征选择外，我们还可以采用算法的途径对特征进行筛选，线性的降维方法以
PCA 为代表，在 PCA
中，我们只要直接对数据矩阵进行中心化然后求奇异值分解或者对数据的协方差矩阵进行分解就可以得到其主要维度。非线性学习的方法如流形学习将投影面从平面改为超曲面。</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




  <nav class="pagination">
    <a class="extend prev" rel="prev" title="上一页" aria-label="上一页" href="/page/4/"><i class="fa fa-angle-left"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/4/">4</a><span class="page-number current">5</span><a class="page-number" href="/page/6/">6</a><span class="space">&hellip;</span><a class="page-number" href="/page/13/">13</a><a class="extend next" rel="next" title="下一页" aria-label="下一页" href="/page/6/"><i class="fa fa-angle-right"></i></a>
  </nav>

</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="copyright">
    &copy; 2019 – 
    <span itemprop="copyrightYear">2024</span>
    <span class="with-love">
      <i class="fa fa-heart"></i>
    </span>
    <span class="author" itemprop="copyrightHolder">SoundMemories</span>
  </div>
  <div class="powered-by">由 <span class="exturl" data-url="aHR0cHM6Ly9oZXhvLmlv">Hexo</span> & <span class="exturl" data-url="aHR0cHM6Ly90aGVtZS1uZXh0LmpzLm9yZy9tdXNlLw==">NexT.Muse</span> 强力驱动
  </div>

    </div>
  </footer>

  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>
  <div class="sidebar-dimmer"></div>
  <div class="back-to-top" role="button" aria-label="返回顶部">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>
  <a role="button" class="book-mark-link book-mark-link-fixed"></a>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/next-theme-pjax/0.6.0/pjax.min.js" integrity="sha256-vxLn1tSKWD4dqbMRyv940UYw4sXgMtYcK6reefzZrao=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.7.0/jquery.min.js" integrity="sha256-2Pmvv0kuTBOenSvLm6bvfBSSHrUJ+3A7x6P5Ebd07/g=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.js" integrity="sha256-yt2kYMy0w8AbtF89WXb2P1rfjcP/HTHLT7097U8Y5b8=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/medium-zoom/1.0.8/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/muse.js"></script><script src="/js/next-boot.js"></script><script src="/js/bookmark.js"></script><script src="/js/pjax.js"></script>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-generator-searchdb/1.4.1/search.js" integrity="sha256-1kfA5uHPf65M5cphT2dvymhkuyHPQp5A53EGZOnOLmc=" crossorigin="anonymous"></script>
<script src="/js/third-party/search/local-search.js"></script>

  <script class="next-config" data-name="pdf" type="application/json">{"object_url":{"url":"https://cdnjs.cloudflare.com/ajax/libs/pdfobject/2.2.12/pdfobject.min.js","integrity":"sha256-g2xji1rlE3KsGVClvuxTbcR0Kn2+wtQADSff2Tbb4zA="},"url":"/lib/pdf/web/viewer.html"}</script>
  <script src="/js/third-party/tags/pdf.js"></script>

  <script class="next-config" data-name="mermaid" type="application/json">{"enable":true,"theme":{"light":"neutral","dark":"neutral"},"js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mermaid/10.2.3/mermaid.min.js","integrity":"sha256-JFptYy4KzJ5OQP+Q9fubNf3cxpPPmZKqUOovyEONKrQ="}}</script>
  <script src="/js/third-party/tags/mermaid.js"></script>


  <script src="/js/third-party/fancybox.js"></script>



  




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"none","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>


  <script src="https://cdnjs.cloudflare.com/ajax/libs/quicklink/2.3.0/quicklink.umd.js" integrity="sha256-yvJQOINiH9fWemHn0vCA5lsHWJaHs6/ZmO+1Ft04SvM=" crossorigin="anonymous"></script>
  <script class="next-config" data-name="quicklink" type="application/json">{"enable":true,"home":true,"archive":true,"delay":true,"timeout":3000,"priority":true,"url":"https://soundmemories.github.io/page/5/"}</script>
  <script src="/js/third-party/quicklink.js"></script>

</body>
</html>
