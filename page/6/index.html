<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222" media="(prefers-color-scheme: light)">
<meta name="theme-color" content="#222" media="(prefers-color-scheme: dark)"><meta name="generator" content="Hexo 6.3.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="https://fonts.loli.net/css?family=Monda:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">

<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css" integrity="sha256-HtsXJanqjKTc8vVQjO4YMhiqFoXkfBsjBWcX91T1jr8=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.css" integrity="sha256-Vzbj7sDDS/woiFS3uNKo8eIuni59rjyNGtXfstRzStA=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"soundmemories.github.io","root":"/","images":"/images","scheme":"Muse","darkmode":true,"version":"8.17.1","exturl":true,"sidebar":{"position":"left","display":"always","padding":18,"offset":12},"copycode":{"enable":true,"style":null},"bookmark":{"enable":true,"color":"#222","save":"auto"},"mediumzoom":true,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"},"path":"/search.xml","localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false}}</script><script src="/js/config.js"></script>

    <meta name="description" content="今日事，今日毕">
<meta property="og:type" content="website">
<meta property="og:title" content="SoundMemories">
<meta property="og:url" content="https://soundmemories.github.io/page/6/index.html">
<meta property="og:site_name" content="SoundMemories">
<meta property="og:description" content="今日事，今日毕">
<meta property="og:locale" content="zh_CN">
<meta property="article:author" content="SoundMemories">
<meta name="twitter:card" content="summary">


<link rel="canonical" href="https://soundmemories.github.io/page/6/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":true,"isPost":false,"lang":"zh-CN","comments":"","permalink":"","path":"page/6/index.html","title":""}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>SoundMemories</title>
  








  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <h1 class="site-title">SoundMemories</h1>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="搜索" role="button">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签<span class="badge">10</span></a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类<span class="badge">10</span></a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档<span class="badge">127</span></a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup"><div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" maxlength="80"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close" role="button">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="search-result-container no-result">
  <div class="search-result-icon">
    <i class="fa fa-spinner fa-pulse fa-5x"></i>
  </div>
</div>

    </div>
  </div>

</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-overview-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="SoundMemories"
      src="/images/avstar.png">
  <p class="site-author-name" itemprop="name">SoundMemories</p>
  <div class="site-description" itemprop="description">今日事，今日毕</div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">127</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">10</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">10</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author animated">
      <span class="links-of-author-item">
        <span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL3NvdW5kbWVtb3JpZXM=" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;soundmemories"><i class="fab fa-github fa-fw"></i>GitHub</span>
      </span>
      <span class="links-of-author-item">
        <span class="exturl" data-url="bWFpbHRvOnNvdW5kbWVtb3JpZXNAMTYzLmNvbQ==" title="E-Mail → mailto:soundmemories@163.com"><i class="fa fa-envelope fa-fw"></i>E-Mail</span>
      </span>
  </div>
  <div class="cc-license animated" itemprop="license">
    <span class="exturl cc-opacity" data-url="aHR0cHM6Ly9jcmVhdGl2ZWNvbW1vbnMub3JnL2xpY2Vuc2VzL2J5LW5jLXNhLzQuMC8="><img src="https://cdnjs.cloudflare.com/ajax/libs/creativecommons-vocabulary/2020.11.3/assets/license_badges/small/by_nc_sa.svg" alt="Creative Commons"></span>
  </div>

        </div>
      </div>
    </div>

    
        <div class="pjax">
        </div>
  </aside>


    </div>

    <div class="main-inner index posts-expand">

    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://soundmemories.github.io/2020/10/02/ML/24.EM/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avstar.png">
      <meta itemprop="name" content="SoundMemories">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="SoundMemories">
      <meta itemprop="description" content="今日事，今日毕">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | SoundMemories">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2020/10/02/ML/24.EM/" class="post-title-link" itemprop="url">EM</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2020-10-02 00:00:00" itemprop="dateCreated datePublished" datetime="2020-10-02T00:00:00+08:00">2020-10-02</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/ML/" itemprop="url" rel="index"><span itemprop="name">ML</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>1.3k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>5 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="期望最大">期望最大</h1>
<p>期望最大算法的目的是解决具有隐变量的混合模型的参数估计（极大似然估计）。MLE
对 <span class="math inline">\(p(x|\theta)\)</span>
参数的估计记为：<span
class="math inline">\(\theta_{MLE}=\mathop{argmax}\limits_\theta\log
p(x|\theta)\)</span>。EM
算法对这个问题的解决方法是采用迭代的方法：<br />
<span class="math display">\[
\theta^{t+1}=\mathop{argmax}\limits_{\theta}\int_z\log
[p(x,z|\theta)]p(z|x,\theta^t)dz=\mathbb{E}_{z|x,\theta^t}[\log
p(x,z|\theta)]
\]</span><br />
这个公式包含了迭代的两步：<br />
（1）E step：计算 <span class="math inline">\(\log
p(x,z|\theta)\)</span> 在概率分布 <span
class="math inline">\(p(z|x,\theta^t)\)</span> 下的期望<br />
（2）M step：计算使这个期望最大化的参数得到下一个 EM 步骤的输入</p>
<blockquote>
<p>收敛性证明，求证：<span class="math inline">\(\log
p(x|\theta^t)\le\log p(x|\theta^{t+1})\)</span></p>
<p>证明：<span class="math inline">\(\log p(x|\theta)=\log
p(z,x|\theta)-\log p(z|x,\theta)\)</span>，对左右两边求积分：<br />
<span class="math display">\[
  Left:\int_zp(z|x,\theta^t)\log p(x|\theta)dz=\log p(x|\theta)
  \]</span></p>
<p><span class="math display">\[
  Right:\int_zp(z|x,\theta^t)\log
p(x,z|\theta)dz-\int_zp(z|x,\theta^t)\log
p(z|x,\theta)dz=Q(\theta,\theta^t)-H(\theta,\theta^t)
  \]</span></p>
<p>所以：<br />
<span class="math display">\[
  \log p(x|\theta)=Q(\theta,\theta^t)-H(\theta,\theta^t)
  \]</span><br />
由于 <span
class="math inline">\(Q(\theta,\theta^t)=\int_zp(z|x,\theta^t)\log
p(x,z|\theta)dz\)</span>，而 <span
class="math inline">\(\theta^{t+1}=\mathop{argmax}\limits_{\theta}\int_z\log
[p(x,z|\theta)]p(z|x,\theta^t)dz\)</span>，所以 <span
class="math inline">\(Q(\theta^{t+1},\theta^t)\ge
Q(\theta^t,\theta^t)\)</span>。要证 <span class="math inline">\(\log
p(x|\theta^t)\le\log p(x|\theta^{t+1})\)</span>，需证：<span
class="math inline">\(H(\theta^t,\theta^t)\ge
H(\theta^{t+1},\theta^t)\)</span>：<br />
<span class="math display">\[
  \begin{aligned}H(\theta^{t+1},\theta^t)-H(\theta^{t},\theta^t)&amp;=\int_zp(z|x,\theta^{t})\log
p(z|x,\theta^{t+1})dz-\int_zp(z|x,\theta^t)\log p(z|x,\theta^{t})dz\\
  &amp;=\int_zp(z|x,\theta^t)\log\frac{p(z|x,\theta^{t+1})}{p(z|x,\theta^t)}=-KL(p(z|x,\theta^t),p(z|x,\theta^{t+1}))\le0
  \end{aligned}
  \]</span><br />
综合上面的结果：<br />
<span class="math display">\[
  \log p(x|\theta^t)\le\log p(x|\theta^{t+1})
  \]</span></p>
</blockquote>
<p>根据上面的证明，我们看到，似然函数在每一步都会增大。进一步的，我们看
EM 迭代过程中的式子是怎么来的：<br />
<span class="math display">\[
\log p(x|\theta)=\log p(z,x|\theta)-\log p(z|x,\theta)=\log
\frac{p(z,x|\theta)}{q(z)}-\log \frac{p(z|x,\theta)}{q(z)}
\]</span><br />
分别对两边求期望 <span
class="math inline">\(\mathbb{E}_{q(z)}\)</span>：<br />
<span class="math display">\[
\begin{aligned}
&amp;Left:\int_zq(z)\log p(x|\theta)dz=\log p(x|\theta)\\
&amp;Right:\int_zq(z)\log \frac{p(z,x|\theta)}{q(z)}dz-\int_zq(z)\log
\frac{p(z|x,\theta)}{q(z)}dz=ELBO+KL(q(z),p(z|x,\theta))
\end{aligned}
\]</span><br />
上式中，Evidence Lower Bound(ELBO)，是一个下界，所以 <span
class="math inline">\(\log p(x|\theta)\ge ELBO\)</span>，等于号取在 KL
散度为0是，即：<span
class="math inline">\(q(z)=p(z|x,\theta)\)</span>，EM 算法的目的是将
ELBO 最大化，根据上面的证明过程，在每一步 EM
后，求得了最大的ELBO，并根据这个使 ELBO 最大的参数代入下一步中：<br />
<span class="math display">\[
\hat{\theta}=\mathop{argmax}_{\theta}ELBO=\mathop{argmax}_\theta\int_zq(z)\log\frac{p(x,z|\theta)}{q(z)}dz
\]</span><br />
由于 <span class="math inline">\(q(z)=p(z|x,\theta^t)\)</span>
的时候，这一步的最大值才能取等号，所以：<br />
<span class="math display">\[
\begin{aligned}
\hat{\theta}=\mathop{argmax}_{\theta}ELBO&amp;=\mathop{argmax}_\theta\int_zq(z)\log\frac{p(x,z|\theta)}{q(z)}dz\\
&amp;=\mathop{argmax}_\theta\int_zp(z|x,\theta^t)\log\frac{p(x,z|\theta)}{p(z|x,\theta^t)}d
z\\
&amp;=\mathop{argmax}_\theta\int_z p(z|x,\theta^t)\log p(x,z|\theta)
\end{aligned}
\]</span><br />
这个式子就是上面 EM 迭代过程中的式子。</p>
<p>从 Jensen 不等式出发，也可以导出这个式子：<br />
<span class="math display">\[
\log
p(x|\theta)=\log\int_zp(x,z|\theta)dz=\log\int_z\frac{p(x,z|\theta)q(z)}{q(z)}dz\\
=\log \mathbb{E}_{q(z)}[\frac{p(x,z|\theta)}{q(z)}]\ge
\mathbb{E}_{q(z)}[\log\frac{p(x,z|\theta)}{q(z)}]
\]</span><br />
其中，右边的式子就是 ELBO，等号在 <span
class="math inline">\(p(x,z|\theta)=Cq(z)\)</span> 时成立。于是：<br />
<span class="math display">\[
\int_zq(z)dz=\frac{1}{C}\int_zp(x,z|\theta)dz=\frac{1}{C}p(x|\theta)=1\\
\Rightarrow q(z)=\frac{1}{p(x|\theta)}p(x,z|\theta)=p(z|x,\theta)
\]</span><br />
我们发现，这个过程就是上面的最大值取等号的条件。</p>
<h2 id="广义-em">广义 EM</h2>
<p>EM 模型解决了概率生成模型的参数估计的问题，通过引入隐变量 <span
class="math inline">\(z\)</span>，来学习 <span
class="math inline">\(\theta\)</span>，具体的模型对 <span
class="math inline">\(z\)</span> 有不同的假设。对学习任务 <span
class="math inline">\(p(x|\theta)\)</span>，就是学习任务 <span
class="math inline">\(\frac{p(x,z|\theta)}{p(z|x,\theta)}\)</span>。在这个式子中，我们假定了在
E 步骤中，<span
class="math inline">\(q(z)=p(z|x,\theta)\)</span>，但是这个<span
class="math inline">\(p(z|x,\theta)\)</span>
如果无法求解，那么必须使用采样（MCMC）或者变分推断等方法来近似推断这个后验。我们观察
KL 散度的表达式，为了最大化 ELBO，在固定的 <span
class="math inline">\(\theta\)</span> 时，我们需要最小化 KL
散度，于是：<br />
<span class="math display">\[
\hat{q}(z)=\mathop{argmin}_qKL(p,q)=\mathop{argmax}_qELBO
\]</span><br />
这就是广义 EM 的基本思路：</p>
<ol type="1">
<li><p>E step：<br />
<span class="math display">\[
\hat{q}^{t+1}(z)=\mathop{argmax}_q\int_zq^t(z)\log\frac{p(x,z|\theta)}{q^t(z)}dz,fixed\
\theta
\]</span></p></li>
<li><p>M step：<br />
<span class="math display">\[
\hat{\theta}=\mathop{argmax}_\theta
\int_zq^{t+1}(z)\log\frac{p(x,z|\theta)}{q^{t+1}(z)}dz,fixed\ \hat{q}
\]</span></p></li>
</ol>
<p>对于上面的积分：<br />
<span class="math display">\[
\begin{aligned}
ELBO&amp;=\int_zq(z)\log\frac{p(x,z|\theta)}{q(z)}dz\\
&amp;=\mathbb{E}_{q(z)}[\log p(x,z|\theta)-\log q(z)]\\
&amp;=\mathbb{E}_{q(z)}[\log p(x,z|\theta)]+Entropy(q(z))
\end{aligned}
\]</span><br />
因此，我们看到，广义 EM 相当于在原来的式子中加入熵这一项。</p>
<h2 id="em-的推广">EM 的推广</h2>
<p>EM
算法类似于坐标上升法，固定部分坐标，优化其他坐标，再一遍一遍的迭代。如果在
EM 框架中，无法求解 <span class="math inline">\(z\)</span>
后验概率，那么需要采用一些变种的 EM 来估算这个后验。</p>
<ol type="1">
<li>基于平均场的变分推断，VBEM/VEM</li>
<li>基于蒙特卡洛的EM，MCEM</li>
</ol>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://soundmemories.github.io/2020/10/01/ML/23.PGMIntro/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avstar.png">
      <meta itemprop="name" content="SoundMemories">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="SoundMemories">
      <meta itemprop="description" content="今日事，今日毕">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | SoundMemories">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2020/10/01/ML/23.PGMIntro/" class="post-title-link" itemprop="url">PGMIntro</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2020-10-01 00:00:00" itemprop="dateCreated datePublished" datetime="2020-10-01T00:00:00+08:00">2020-10-01</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/ML/" itemprop="url" rel="index"><span itemprop="name">ML</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>3k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>11 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="概率图模型">概率图模型</h1>
<p>概率图模型使用图的方式表示概率分布。为了在图中添加各种概率，首先总结一下随机变量分布的一些规则：<br />
<span class="math display">\[
\begin{align}
&amp;Sum\ Rule:p(x_1)=\int p(x_1,x_2)dx_2\\
&amp;Product\ Rule:p(x_1,x_2)=p(x_1|x_2)p(x_2)\\
&amp;Chain\
Rule:p(x_1,x_2,\cdots,x_p)=\prod\limits_{i=1}^pp(x_i|x_{i+1,x_{i+2}
\cdots}x_p)\\
&amp;Bayesian\ Rule:p(x_1|x_2)=\frac{p(x_2|x_1)p(x_1)}{p(x_2)}
\end{align}
\]</span><br />
可以看到，在链式法则中，如果数据维度特别高，那么的采样和计算非常困难，我们需要在一定程度上作出简化，在朴素贝叶斯中，作出了条件独立性假设。在
Markov
假设中，给定数据的维度是以时间顺序出现的，给定当前时间的维度，那么下一个维度与之前的维度独立。在
HMM 中，采用了齐次 Markov 假设。在 Markov
假设之上，更一般的，加入条件独立性假设，对维度划分集合 <span
class="math inline">\(A,B,C\)</span>，使得 <span
class="math inline">\(X_A\perp X_B|X_C\)</span>。</p>
<p>概率图模型采用图的特点表示上述的条件独立性假设，节点表示随机变量，边表示条件概率。概率图模型可以分为三大理论部分：</p>
<ol type="1">
<li>表示：
<ol type="1">
<li>有向图（离散）：贝叶斯网络</li>
<li>高斯图（连续）：高斯贝叶斯和高斯马尔可夫网路</li>
<li>无向图（离散）：马尔可夫网络</li>
</ol></li>
<li>推断
<ol type="1">
<li>精确推断</li>
<li>近似推断
<ol type="1">
<li>确定性近似（如变分推断）</li>
<li>随机近似（如 MCMC）</li>
</ol></li>
</ol></li>
<li>学习
<ol type="1">
<li>参数学习
<ol type="1">
<li>完备数据</li>
<li>隐变量：E-M 算法</li>
</ol></li>
<li>结构学习</li>
</ol></li>
</ol>
<h2 id="有向图-贝叶斯网络">有向图-贝叶斯网络</h2>
<p>已知联合分布中，各个随机变量之间的依赖关系，那么可以通过拓扑排序（根据依赖关系）可以获得一个有向图。而如果已知一个图，也可以直接得到联合概率分布的因子分解：<br />
<span class="math display">\[
p(x_1,x_2,\cdots,x_p)=\prod\limits_{i=1}^pp(x_i|x_{parent(i)})
\]</span><br />
那么实际的图中条件独立性是如何体现的呢？在局部任何三个节点，可以有三种结构：</p>
<ol type="1">
<li><pre class="mermaid">
graph TB
A((A))--&gt;B((B));
B--&gt;C((C));
</pre>
<p><span class="math display">\[
\begin{aligned}
p(A,B,C)&amp;=p(A)p(B|A)p(C|B)=p(A)p(B|A)p(C|B,A)\\
&amp;\Longrightarrow p(C|B)=p(C|B,A)\\
&amp;\Leftrightarrow p(C|B)p(A|B)=p(C|A,B)p(A|B)=p(C,A|B)\\
&amp;\Longrightarrow C\perp A|B
\end{aligned}
\]</span></p></li>
<li><pre class="mermaid">
graph TB
B((B))--&gt;A((A));
B--&gt;C((C));
    
</pre>
<p><span class="math display">\[
\begin{aligned}
p(A,B,C)&amp;=p(A|B)p(B)p(C|B)=p(B)p(A|B)p(C|A,B)\\
&amp;\Longrightarrow p(C|B)=p(C|B,A)\\
&amp;\Leftrightarrow p(C|B)p(A|B)=p(C|A,B)p(A|B)=p(C,A|B)\\
&amp;\Longrightarrow C\perp A|B
\end{aligned}
\]</span></p></li>
<li><pre class="mermaid">
graph TB
A((A))--&gt;B((B));
C((C))--&gt;B
    
</pre>
<p><span class="math display">\[
\begin{aligned}
p(A,B,C)&amp;=p(A)p(C)p(B|C,A)=p(A)p(C|A)p(B|C,A)\\
&amp;\Longrightarrow p(C)=p(C|A)\\
&amp;\Leftrightarrow C\perp A\\
\end{aligned}
\]</span></p>
<p>对这种结构，<span class="math inline">\(A,C\)</span> 不与 <span
class="math inline">\(B\)</span> 条件独立。</p></li>
</ol>
<p>从整体的图来看，可以引入 D 划分的概念。对于类似上面图 1和图
2的关系，引入集合A，B，那么满足 <span class="math inline">\(A\perp
B|C\)</span> 的 <span class="math inline">\(C\)</span> 集合中的点与
<span class="math inline">\(A,B\)</span> 中的点的关系都满足图
1，2，满足图3 关系的点都不在 <span class="math inline">\(C\)</span>
中。D 划分应用在贝叶斯定理中：<br />
<span class="math display">\[
p(x_i|x_{-i})=\frac{p(x)}{\int
p(x)dx_{i}}=\frac{\prod\limits_{j=1}^pp(x_j|x_{parents(j)})}{\int\prod\limits_{j=1}^pp(x_j|x_{parents(j)})dx_i}
\]</span><br />
可以发现，上下部分可以分为两部分，一部分是和 <span
class="math inline">\(x_i\)</span> 相关的，另一部分是和 <span
class="math inline">\(x_i\)</span>
无关的，而这个无关的部分可以相互约掉。于是计算只涉及和 <span
class="math inline">\(x_i\)</span> 相关的部分。</p>
<p>与 <span class="math inline">\(x_i\)</span>
相关的部分可以写成：<br />
<span class="math display">\[
p(x_i|x_{parents(i)})p(x_{child(i)}|x_i)
\]</span><br />
这些相关的部分又叫做 Markov 毯。</p>
<p>实际应用的模型中，对这些条件独立性作出了假设，从单一到混合，从有限到无限（时间，空间）可以分为：</p>
<ol type="1">
<li>朴素贝叶斯，单一的条件独立性假设 <span
class="math inline">\(p(x|y)=\prod\limits_{i=1}^pp(x_i|y)\)</span>，在 D
划分后，所有条件依赖的集合就是单个元素。</li>
<li>高斯混合模型：混合的条件独立。引入多类别的隐变量 <span
class="math inline">\(z_1, z_2,\cdots,z_k\)</span>， <span
class="math inline">\(p(x|z)=\mathcal{N}(\mu,\Sigma)\)</span>，条件依赖集合为多个元素。</li>
<li>与时间相关的条件依赖
<ol type="1">
<li>Markov 链</li>
<li>高斯过程（无限维高斯分布）</li>
</ol></li>
<li>连续：高斯贝叶斯网络</li>
<li>组合上面的分类
<ul>
<li>GMM 与时序结合：动态模型
<ul>
<li>HMM（离散）</li>
<li>线性动态系统 LDS（Kalman 滤波）</li>
<li>粒子滤波（非高斯，非线性）</li>
</ul></li>
</ul></li>
</ol>
<h2
id="无向图-马尔可夫网络马尔可夫随机场">无向图-马尔可夫网络（马尔可夫随机场）</h2>
<p>无向图没有了类似有向图的局部不同结构，在马尔可夫网络中，也存在 D
划分的概念。直接将条件独立的集合 <span class="math inline">\(x_A\perp
x_B|x_C\)</span> 划分为三个集合。这个也叫全局
Markov。对局部的节点，<span class="math inline">\(x\perp
(X-Neighbour(\mathcal{x}))|Neighbour(x)\)</span>。这也叫局部
Markov。对于成对的节点：<span class="math inline">\(x_i\perp
x_j|x_{-i-j}\)</span>，其中 <span class="math inline">\(i,j\)</span>
不能相邻。这也叫成对
Markov。事实上上面三个点局部全局成对是相互等价的。</p>
<p>有了这个条件独立性的划分，还需要因子分解来实际计算。引入团的概念：</p>
<blockquote>
<p>团，最大团：图中节点的集合，集合中的节点之间相互都是连接的叫做团，如果不能再添加节点，那么叫最大团。</p>
</blockquote>
<p>利用这个定义进行的 <span class="math inline">\(x\)</span>
所有维度的联合概率分布的因子分解为，假设有 <span
class="math inline">\(K\)</span> 个团，<span
class="math inline">\(Z\)</span> 就是对所有可能取值求和：<br />
<span class="math display">\[
\begin{align}p(x)=\frac{1}{Z}\prod\limits_{i=1}^{K}\phi(x_{ci})\\
Z=\sum\limits_{x\in\mathcal{X}}\prod\limits_{i=1}^{K}\phi(x_{ci})
\end{align}
\]</span><br />
其中 <span class="math inline">\(\phi(x_{ci})\)</span>
叫做势函数，它必须是一个正值，可以记为：<br />
<span class="math display">\[
\phi(x_{ci})=\exp(-E(x_{ci}))
\]</span><br />
这个分布叫做 Gibbs 分布（玻尔兹曼分布）。于是也可以记为：<span
class="math inline">\(p(x)=\frac{1}{Z}\exp(-\sum\limits_{i=1}^KE(x_{ci}))\)</span>。这个分解和条件独立性等价（Hammesley-Clifford
定理），这个分布的形式也和指数族分布形式上相同，于是满足最大熵原理。</p>
<h2 id="两种图的转换-道德图">两种图的转换-道德图</h2>
<p>我们常常想将有向图转为无向图，从而应用更一般的表达式。</p>
<ol type="1">
<li><p>链式：</p>
<pre class="mermaid">
graph TB
A((A))--&gt;B((B));
B--&gt;C((C));
    
</pre>
<p>直接去掉箭头，<span
class="math inline">\(p(a,b,c)=p(a)p(b|a)p(c|b)=\phi(a,b)\phi(b,c)\)</span>：</p>
<pre class="mermaid">
graph TB
A((A))---B((B));
B---C((C));
    
</pre></li>
<li><p>V 形：</p>
<pre class="mermaid">
graph TB
B((B))--&gt;A((A));
B--&gt;C((C));
    
</pre>
<p>由于 <span
class="math inline">\(p(a,b,c)=p(b)p(a|b)p(c|b)=\phi(a,b)\phi(b,c)\)</span>，直接去掉箭头：</p>
<pre class="mermaid">
graph TB
B((B))---A((A));
B---C((C));
    
</pre></li>
<li><p>倒 V 形：</p>
<pre class="mermaid">
graph TB
A((A))--&gt;B((B));
C((C))--&gt;B
    
</pre>
<p>由于 <span
class="math inline">\(p(a,b,c)=p(a)p(c)p(b|a,c)=\phi(a,b,c)\)</span>，于是在
<span class="math inline">\(a,c\)</span> 之间添加线：</p>
<pre class="mermaid">
graph TD
a((a))---b((b));
b---c((c));
a---c;
    
</pre>
<p>观察着三种情况可以概括为：</p>
<ol type="1">
<li>将每个节点的父节点两两相连</li>
<li>将有向边替换为无向边</li>
</ol></li>
</ol>
<h2 id="更精细的分解-因子图">更精细的分解-因子图</h2>
<p>对于一个有向图，可以通过引入环的方式，可以将其转换为无向图（Tree-like
graph），这个图就叫做道德图。但是我们上面的 BP
算法只对无环图有效，通过因子图可以变为无环图。</p>
<p>考虑一个无向图：</p>
<pre class="mermaid">
graph TD
a((a))---b((b));
b---c((c));
a---c;
</pre>
<p>可以将其转为：</p>
<pre class="mermaid">
graph TD
a((a))---f;
f---b((b));
f---c((c))
</pre>
<p>其中 <span
class="math inline">\(f=f(a,b,c)\)</span>。因子图不是唯一的，这是由于因式分解本身就对应一个特殊的因子图，将因式分解：<span
class="math inline">\(p(x)=\prod\limits_{s}f_s(x_s)\)</span>
可以进一步分解得到因子图。</p>
<h2 id="推断">推断</h2>
<p>推断的主要目的是求各种概率分布，包括边缘概率，条件概率，以及使用 MAP
来求得参数。通常推断可以分为：</p>
<ol type="1">
<li>精确推断
<ol type="1">
<li>Variable Elimination(VE)</li>
<li>Belief Propagation(BP, Sum-Product Algo)，从 VE 发展而来</li>
<li>Junction Tree，上面两种在树结构上应用，Junction Tree
在图结构上应用</li>
</ol></li>
<li>近似推断
<ol type="1">
<li>Loop Belief Propagation（针对有环图）</li>
<li>Mente Carlo Interference：例如 Importance Sampling，MCMC</li>
<li>Variational Inference</li>
</ol></li>
</ol>
<h3 id="推断-变量消除ve">推断-变量消除（VE）</h3>
<p>变量消除的方法是在求解概率分布的时候，将相关的条件概率先行求和或积分，从而一步步地消除变量，例如在马尔可夫链中：</p>
<pre class="mermaid">
graph LR
a((a))--&gt;b((b));
b--&gt;c((c));
c--&gt;d((d))
</pre>
<p><span class="math display">\[
p(d)=\sum\limits_{a,b,c}p(a,b,c,d)=\sum\limits_cp(d|c)\sum\limits_bp(c|b)\sum\limits_ap(b|a)p(a)
\]</span></p>
<p>变量消除的缺点很明显：</p>
<ol type="1">
<li>计算步骤无法存储</li>
<li>消除的最优次序是一个 NP-hard 问题</li>
</ol>
<h3 id="推断-信念传播bp">推断-信念传播（BP）</h3>
<p>为了克服 VE
的第一个缺陷-计算步骤无法存储。我们进一步地对上面的马尔可夫链进行观察：</p>
<pre class="mermaid">
graph LR
a((a))--&gt;b((b));
b--&gt;c((c));
c--&gt;d((d));
d--&gt;e((e));
</pre>
<p>要求 <span class="math inline">\(p(e)\)</span>，当然使用 VE，从 <span
class="math inline">\(a\)</span> 一直消除到 <span
class="math inline">\(d\)</span>，记 <span
class="math inline">\(\sum\limits_ap(a)p(b|a)=m_{a\to
b(b)}\)</span>，表示这是消除 <span class="math inline">\(a\)</span>
后的关于 <span class="math inline">\(b\)</span> 的概率，类似地，记 <span
class="math inline">\(\sum\limits_bp(c|b)m_{a\to b}(b)=m_{b\to
c}(c)\)</span>。于是 <span
class="math inline">\(p(e)=\sum\limits_dp(e|d)m_{b\to
c}(c)\)</span>。进一步观察，对 <span
class="math inline">\(p(c)\)</span>：<br />
<span class="math display">\[
p(c)=[\sum\limits_bp(c|b)\sum\limits_ap(b|a)p(a)]\cdot[\sum\limits_dp(d|c)\sum\limits_ep(e)p(e|d)]
\]</span><br />
我们发现了和上面计算 <span class="math inline">\(p(e)\)</span>
类似的结构，这个式子可以分成两个部分，一部分是从 <span
class="math inline">\(a\)</span> 传播过来的概率，第二部分是从 $ e$
传播过来的概率。</p>
<p>一般地，对于图（只对树形状的图）：</p>
<pre class="mermaid">
graph TD
a((a))---b((b));
b---c((c));
b---d((d));
</pre>
<p>这四个团（对于无向图是团，对于有向图就是概率为除了根的节点为1），有四个节点，三个边：<br />
<span class="math display">\[
p(a,b,c,d)=\frac{1}{Z}\phi_a(a)\phi_b(b)\phi_c(c)\phi_d(d)\cdot\phi_{ab}(a,b)\phi_{bc}(c,b)\phi_{bd}(d,b)
\]</span><br />
套用上面关于有向图的观察，如果求解边缘概率 <span
class="math inline">\(p(a)\)</span>，定义 <span
class="math inline">\(m_{c\to
b}(b)=\sum\limits_c\phi_c(c)\phi_{bc}(bc)\)</span>，<span
class="math inline">\(m_{d\to
b}(b)=\sum\limits_d\phi_d(d)\phi_{bd}(bd)\)</span>，<span
class="math inline">\(m_{b\to
a}(a)=\sum\limits_b\phi_{ba}(ba)\phi_b(b)m_{c\to b}(b)_{d\to
b}m(b)\)</span>，这样概率就一步步地传播到了 <span
class="math inline">\(a\)</span>：<br />
<span class="math display">\[
p(a)=\phi_a(a)m_{b\to a}(a)
\]</span><br />
写成一般的形式，对于相邻节点 <span
class="math inline">\(i,j\)</span>：<br />
<span class="math display">\[
m_{j\to i}(i)=\sum\limits_j\phi_j(j)\phi_{ij}(ij)\prod\limits_{k\in
Neighbour(j)-i}m_{k\to j}(j)
\]</span><br />
这个表达式，就可以保存计算过程了，只要对每条边的传播分别计算，对于一个无向树形图可以递归并行实现：</p>
<ol type="1">
<li>任取一个节点 <span class="math inline">\(a\)</span> 作为根节点</li>
<li>对这个根节点的邻居中的每一个节点，收集信息（计算入信息）</li>
<li>对根节点的邻居，分发信息（计算出信息）</li>
</ol>
<h3 id="推断-max-product-算法">推断-Max-Product 算法</h3>
<p>在推断任务中，MAP 也是常常需要的，MAP 的目的是寻找最佳参数：<br />
<span class="math display">\[
(\hat{a},\hat{b},\hat{c},\hat{d})=\mathop{argmax}_{a,b,c,d}p(a,b,c,d|E)
\]</span><br />
类似
BP，我们采用信息传递的方式来求得最优参数，不同的是，我们在所有信息传递中，传递的是最大化参数的概率，而不是将所有可能求和：<br />
<span class="math display">\[
m_{j\to i}=\max\limits_{j}\phi_j\phi_{ij}\prod\limits_{k\in
Neighbour(j)-i}m_{k\to j}
\]</span><br />
于是对于上面的图：<br />
<span class="math display">\[
\max_a p(a,b,c,d)=\max_a\phi_a\phi_{ab}m_{c\to b}m_{d\to b}
\]</span><br />
这个算法是 Sum-Product 算法的改进，也是在 HMM 中应用给的 Viterbi
算法的推广。</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://soundmemories.github.io/2020/09/30/ML/22.Exponentialfamily/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avstar.png">
      <meta itemprop="name" content="SoundMemories">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="SoundMemories">
      <meta itemprop="description" content="今日事，今日毕">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | SoundMemories">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2020/09/30/ML/22.Exponentialfamily/" class="post-title-link" itemprop="url">Exponentialfamily</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2020-09-30 00:00:00" itemprop="dateCreated datePublished" datetime="2020-09-30T00:00:00+08:00">2020-09-30</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/ML/" itemprop="url" rel="index"><span itemprop="name">ML</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>1.2k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>4 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="指数族分布">指数族分布</h1>
<p>指数族是一类分布，包括高斯分布、伯努利分布、二项分布、泊松分布、Beta
分布、Dirichlet 分布、Gamma
分布等一系列分布。指数族分布可以写为统一的形式：<br />
<span class="math display">\[
p(x|\eta)=h(x)\exp(\eta^T\phi(x)-A(\eta))=\frac{1}{\exp(A(\eta))}h(x)\exp(\eta^T\phi(x))
\]</span><br />
其中，<span class="math inline">\(\eta\)</span> 是参数向量，<span
class="math inline">\(A(\eta)\)</span>
是对数配分函数（归一化因子）。</p>
<p>在这个式子中，$ (x)$
叫做充分统计量，包含样本集合所有的信息，例如高斯分布中的均值和方差。充分统计量在在线学习中有应用，对于一个数据集，只需要记录样本的充分统计量即可。</p>
<p>对于一个模型分布假设（似然），那么我们在求解中，常常需要寻找一个共轭先验，使得先验与后验的形式相同，例如选取似然是二项分布，可取先验是
Beta 分布，那么后验也是 Beta
分布。指数族分布常常具有共轭的性质，于是我们在模型选择以及推断具有很大的便利。</p>
<p>共轭先验的性质便于计算，同时，指数族分布满足最大熵的思想（无信息先验），也就是说对于经验分布利用最大熵原理导出的分布就是指数族分布。</p>
<p>观察到指数族分布的表达式类似线性模型，事实上，指数族分布很自然地导出广义线性模型：<br />
<span class="math display">\[
y=f(w^Tx)\\
y|x\sim Exp Family
\]</span><br />
在更复杂的概率图模型中，例如在无向图模型中如受限玻尔兹曼机中，指数族分布也扮演着重要作用。</p>
<p>在推断的算法中，例如变分推断中，指数族分布也会大大简化计算。</p>
<h2 id="一维高斯分布">一维高斯分布</h2>
<p>一维高斯分布可以写成：<br />
<span class="math display">\[
p(x|\theta)=\frac{1}{\sqrt{2\pi}\sigma}\exp(-\frac{(x-\mu)^2}{2\sigma^2})
\]</span><br />
将这个式子改写：<br />
<span class="math display">\[
\frac{1}{\sqrt{2\pi\sigma^2}}\exp(-\frac{1}{2\sigma^2}(x^2-2\mu
x+\mu^2))\\
=\exp(\log(2\pi\sigma^2)^{-1/2})\exp(-\frac{1}{2\sigma^2}\begin{pmatrix}-2\mu&amp;1\end{pmatrix}\begin{pmatrix}x\\x^2\end{pmatrix}-\frac{\mu^2}{2\sigma^2})
\]</span><br />
所以：<br />
<span class="math display">\[
\eta=\begin{pmatrix}\frac{\mu}{\sigma^2}\\-\frac{1}{2\sigma^2}\end{pmatrix}=\begin{pmatrix}\eta_1\\\eta_2\end{pmatrix}
\]</span><br />
于是 <span class="math inline">\(A(\eta)\)</span>：<br />
<span class="math display">\[
A(\eta)=-\frac{\eta_1^2}{4\eta_2}+\frac{1}{2}\log(-\frac{\pi}{\eta_2})
\]</span></p>
<h2
id="充分统计量和对数配分函数的关系">充分统计量和对数配分函数的关系</h2>
<p>对概率密度函数求积分：<br />
<span class="math display">\[
\begin{align}
\exp(A(\eta))&amp;=\int h(x)\exp(\eta^T\phi(x))dx\nonumber
\end{align}
\]</span><br />
两边对参数求导：<br />
<span class="math display">\[
\exp(A(\eta))A&#39;(\eta)=\int h(x)\exp(\eta^T\phi(x))\phi(x)dx\\
\Longrightarrow A&#39;(\eta)=\mathbb{E}_{p(x|\eta)}[\phi(x)]
\]</span><br />
类似的：<br />
<span class="math display">\[
A&#39;&#39;(\eta)=Var_{p(x|\eta)}[\phi(x)]
\]</span><br />
由于方差为正，于是 <span class="math inline">\(A(\eta)\)</span>
一定是凸函数。</p>
<h2 id="充分统计量和极大似然估计">充分统计量和极大似然估计</h2>
<p>对于独立全同采样得到的数据集 <span
class="math inline">\(\mathcal{D}=\{x_1,x_2,\cdots,x_N\}\)</span>。<br />
$$<br />
<span
class="math display">\[\begin{align}\eta_{MLE}&amp;=\mathop{argmax}_\eta\sum\limits_{i=1}^N\log
p(x_i|\eta)\nonumber\\
&amp;=\mathop{argmax}_\eta\sum\limits_{i=1}^N(\eta^T\phi(x_i)-A(\eta))\nonumber\\
&amp;\Longrightarrow
A&#39;(\eta_{MLE})=\frac{1}{N}\sum\limits_{i=1}^N\phi(x_i)

\end{align}\]</span><br />
$$<br />
由此可以看到，为了估算参数，只需要知道充分统计量就可以了。</p>
<h2 id="最大熵">最大熵</h2>
<p>信息熵记为：<br />
<span class="math display">\[
Entropy=\int-p(x)\log(p(x))dx
\]</span></p>
<blockquote>
<p>一般地，对于完全随机的变量（等可能），信息熵最大。</p>
<p>我们的假设为最大熵原则，假设数据是离散分布的，<span
class="math inline">\(k\)</span> 个特征的概率分别为 <span
class="math inline">\(p_k\)</span>，最大熵原理可以表述为：<br />
<span class="math display">\[
  \max\{H(p)\}=\min\{\sum\limits_{k=1}^Kp_k\log p_k\}\ s.t.\
\sum\limits_{k=1}^Kp_k=1
  \]</span><br />
利用 Lagrange 乘子法：<br />
<span class="math display">\[
  L(p,\lambda)=\sum\limits_{k=1}^Kp_k\log
p_k+\lambda(1-\sum\limits_{k=1}^Kp_k)
  \]</span><br />
于是可得：<br />
<span class="math display">\[
  p_1=p_2=\cdots=p_K=\frac{1}{K}
  \]</span><br />
因此等可能的情况熵最大。</p>
</blockquote>
<p>一个数据集 <span
class="math inline">\(\mathcal{D}\)</span>，在这个数据集上的经验分布为
<span
class="math inline">\(\hat{p}(x)=\frac{Count(x)}{N}\)</span>，实际不可能满足所有的经验概率相同，于是在上面的最大熵原理中还需要加入这个经验分布的约束。</p>
<p>对任意一个函数，经验分布的经验期望可以求得为：<br />
<span class="math display">\[
\mathbb{E}_\hat{p}[f(x)]=\Delta
\]</span><br />
于是：<br />
<span class="math display">\[
\max\{H(p)\}=\min\{\sum\limits_{k=1}^Np_k\log p_k\}\ s.t.\
\sum\limits_{k=1}^Np_k=1,\mathbb{E}_p[f(x)]=\Delta
\]</span><br />
Lagrange 函数为：<br />
<span class="math display">\[
L(p,\lambda_0,\lambda)=\sum\limits_{k=1}^Np_k\log
p_k+\lambda_0(1-\sum\limits_{k=1}^Np_k)+\lambda^T(\Delta-\mathbb{E}_p[f(x)])
\]</span><br />
求导得到：<br />
<span class="math display">\[
\frac{\partial}{\partial p(x)}L=\sum\limits_{k=1}^N(\log
p(x)+1)-\sum\limits_{k=1}^N\lambda_0-\sum\limits_{k=1}^N\lambda^Tf(x)\\
\Longrightarrow\sum\limits_{k=1}^N\log p(x)+1-\lambda_0-\lambda^Tf(x)=0
\]</span><br />
由于数据集是任意的，对数据集求和也意味着求和项里面的每一项都是0：<br />
<span class="math display">\[
p(x)=\exp(\lambda^Tf(x)+\lambda_0-1)
\]</span><br />
这就是指数族分布。</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://soundmemories.github.io/2020/09/29/ML/21.SVM/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avstar.png">
      <meta itemprop="name" content="SoundMemories">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="SoundMemories">
      <meta itemprop="description" content="今日事，今日毕">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | SoundMemories">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2020/09/29/ML/21.SVM/" class="post-title-link" itemprop="url">SVM</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2020-09-29 00:00:00" itemprop="dateCreated datePublished" datetime="2020-09-29T00:00:00+08:00">2020-09-29</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/ML/" itemprop="url" rel="index"><span itemprop="name">ML</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>3.1k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>11 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="支撑向量机">支撑向量机</h1>
<p>支撑向量机（SVM）算法在分类问题中有着重要地位，其主要思想是最大化两类之间的间隔。按照数据集的特点：</p>
<ol type="1">
<li>线性可分问题，如之前的感知机算法处理的问题</li>
<li>线性可分，只有一点点错误点，如感知机算法发展出来的 Pocket
算法处理的问题</li>
<li>非线性问题，完全不可分，如在感知机问题发展出来的多层感知机和深度学习</li>
</ol>
<p>这三种情况对于 SVM 分别有下面三种处理手段：</p>
<ol type="1">
<li>hard-margin SVM</li>
<li>soft-margin SVM</li>
<li>kernel Method</li>
</ol>
<p>SVM 的求解中，大量用到了 Lagrange
乘子法，首先对这种方法进行介绍。</p>
<h2 id="约束优化问题">约束优化问题</h2>
<p>一般地，约束优化问题（原问题）可以写成：<br />
$$<br />
<span class="math display">\[\begin{align}

&amp;\min_{x\in\mathbb{R^p}}f(x)\\
&amp;s.t.\ m_i(x)\le0,i=1,2,\cdots,M\\
&amp;\ \ \ \ \ \ \ \ n_j(x)=0,j=1,2,\cdots,N

\end{align}\]</span><br />
<span class="math display">\[
定义 Lagrange 函数：
\]</span><br />
L(x,,)=f(x)+<em>{i=1}<sup>M<em>im_i(x)+</em>{i=1}</sup>N<em>in_i(x)<br />
<span class="math display">\[
那么原问题可以等价于无约束形式：
\]</span><br />
</em>{x^p}</em>{,}L(x,,) s.t. _i<br />
$$<br />
这是由于，当满足原问题的不等式约束的时候，<span
class="math inline">\(\lambda_i=0\)</span>
才能取得最大值，直接等价于原问题，如果不满足原问题的不等式约束，那么最大值就为
<span
class="math inline">\(+\infty\)</span>，由于需要取最小值，于是不会取到这个情况。</p>
<p>这个问题的对偶形式：<br />
<span class="math display">\[
\max_{\lambda,\eta}\min_{x\in\mathbb{R}^p}L(x,\lambda,\eta)\ s.t.\
\lambda_i\ge0
\]</span><br />
对偶问题是关于 $ , $ 的最大化问题。</p>
<p>由于：<br />
<span class="math display">\[
\max_{\lambda_i,\eta_j}\min_{x}L(x,\lambda_i,\eta_j)\le\min_{x}\max_{\lambda_i,\eta_j}L(x,\lambda_i,\eta_j)
\]</span></p>
<blockquote>
<p>证明：显然有 <span class="math inline">\(\min\limits_{x}L\le
L\le\max\limits_{\lambda,\eta}L\)</span>，于是显然有 <span
class="math inline">\(\max\limits_{\lambda,\eta}\min\limits_{x}L\le
L\)</span>，且 <span
class="math inline">\(\min\limits_{x}\max\limits_{\lambda,\eta}L\ge
L\)</span>。</p>
</blockquote>
<p>对偶问题的解小于原问题，有两种情况：</p>
<ol type="1">
<li>强对偶：可以取等于号</li>
<li>弱对偶：不可以取等于号</li>
</ol>
<p>其实这一点也可以通过一张图来说明：</p>
<p><img src="/images/SVM/SVM.png" width="90%" height="90%"></p>
<p>对于一个凸优化问题，有如下定理：</p>
<blockquote>
<p>如果凸优化问题满足某些条件如 Slater
条件，那么它和其对偶问题满足强对偶关系。记问题的定义域为：<span
class="math inline">\(\mathcal{D}=domf(x)\cap dom m_i(x)\cap
domn_j(x)\)</span>。于是 Slater 条件为：<br />
<span class="math display">\[
  \exists\hat{x}\in Relint\mathcal{D}\ s.t.\ \forall
i=1,2,\cdots,M,m_i(x)\lt0
  \]</span><br />
其中 Relint 表示相对内部（不包含边界的内部）。</p>
</blockquote>
<ol type="1">
<li>对于大多数凸优化问题，Slater 条件成立。</li>
<li>松弛 Slater 条件，如果 M 个不等式约束中，有 K
个函数为仿射函数，那么只要其余的函数满足 Slater 条件即可。</li>
</ol>
<p>上面介绍了原问题和对偶问题的对偶关系，但是实际还需要对参数进行求解，求解方法使用
KKT 条件进行：</p>
<blockquote>
<p>KKT 条件和强对偶关系是等价关系。KKT 条件对最优解的条件为：</p>
<ol type="1">
<li><p>可行域：<br />
<span class="math display">\[
\begin{align}
m_i(x^*)\le0\\
n_j(x^*)=0\\
\lambda^*\ge0
\end{align}
\]</span></p></li>
<li><p>互补松弛 <span class="math inline">\(\lambda^*m_i(x^*)=0,\forall
m_i\)</span>，对偶问题的最佳值为 <span
class="math inline">\(d^*\)</span>，原问题为 <span
class="math inline">\(p^*\)</span><br />
<span class="math display">\[
\begin{align}
d^*&amp;=\max_{\lambda,\eta}g(\lambda,\eta)=g(\lambda^*,\eta^*)\nonumber\\
&amp;=\min_{x}L(x,\lambda^*,\eta^*)\nonumber\\
&amp;\le L(x^*,\lambda^*,\eta^*)\nonumber\\
&amp;=f(x^*)+\sum\limits_{i=1}^M\lambda^*m_i(x^*)\nonumber\\
&amp;\le f(x^*)=p^*
\end{align}
\]</span><br />
为了满足相等，两个不等式必须成立，于是，对于第一个不等于号，需要有梯度为0条件，对于第二个不等于号需要满足互补松弛条件。</p></li>
<li><p>梯度为0：<span class="math inline">\(\frac{\partial
L(x,\lambda^*,\eta^*)}{\partial x}|_{x=x^*}=0\)</span></p></li>
</ol>
</blockquote>
<h2 id="hard-margin-svm">Hard-margin SVM</h2>
<p>支撑向量机也是一种硬分类模型，在之前的感知机模型中，我们在线性模型的基础上叠加了符号函数，在几何直观上，可以看到，如果两类分的很开的话，那么其实会存在无穷多条线可以将两类分开。在
SVM
中，我们引入最大化间隔这个概念，间隔指的是数据和直线的距离的最小值，因此最大化这个值反映了我们的模型倾向。</p>
<p>分割的超平面可以写为：<br />
<span class="math display">\[
0=w^Tx+b
\]</span><br />
那么最大化间隔（约束为分类任务的要求）：<br />
<span class="math display">\[
\mathop{argmax}_{w,b}[\min_i\frac{|w^Tx_i+b|}{||w||}]\ s.t.\
y_i(w^Tx_i+b)&gt;0\\
\Longrightarrow\mathop{argmax}_{w,b}[\min_i\frac{y_i(w^Tx_i+b)}{||w||}]\
s.t.\ y_i(w^Tx_i+b)&gt;0
\]</span><br />
对于这个约束 <span
class="math inline">\(y_i(w^Tx_i+b)&gt;0\)</span>，不妨固定 <span
class="math inline">\(\min
y_i(w^Tx_i+b)=1&gt;0\)</span>，这是由于分开两类的超平面的系数经过比例放缩不会改变这个平面，这也相当于给超平面的系数作出了约束。化简后的式子可以表示为：<br />
<span class="math display">\[
\mathop{argmin}_{w,b}\frac{1}{2}w^Tw\ s.t.\ \min_iy_i(w^Tx_i+b)=1\\
\Rightarrow\mathop{argmin}_{w,b}\frac{1}{2}w^Tw\ s.t.\
y_i(w^Tx_i+b)\ge1,i=1,2,\cdots,N
\]</span><br />
这就是一个包含 <span class="math inline">\(N\)</span>
个约束的凸优化问题，有很多求解这种问题的软件。</p>
<p>但是，如果样本数量或维度非常高，直接求解困难甚至不可解，于是需要对这个问题进一步处理。引入
Lagrange 函数：<br />
<span class="math display">\[
L(w,b,\lambda)=\frac{1}{2}w^Tw+\sum\limits_{i=1}^N\lambda_i(1-y_i(w^Tx_i+b))
\]</span><br />
我们有原问题就等价于：<br />
<span class="math display">\[
\mathop{argmin}_{w,b}\max_{\lambda}L(w,b,\lambda_i)\ s.t.\ \lambda_i\ge0
\]</span><br />
我们交换最小和最大值的符号得到对偶问题：<br />
<span class="math display">\[
\max_{\lambda_i}\min_{w,b}L(w,b,\lambda_i)\ s.t.\ \lambda_i\ge0
\]</span><br />
由于不等式约束是仿射函数，对偶问题和原问题等价：</p>
<ul>
<li><p><span class="math inline">\(b\)</span>：<span
class="math inline">\(\frac{\partial}{\partial
b}L=0\Rightarrow\sum\limits_{i=1}^N\lambda_iy_i=0\)</span></p></li>
<li><p><span class="math inline">\(w\)</span>：首先将 <span
class="math inline">\(b\)</span> 代入：<br />
<span class="math display">\[
L(w,b,\lambda_i)=\frac{1}{2}w^Tw+\sum\limits_{i=1}^N\lambda_i(1-y_iw^Tx_i-y_ib)=\frac{1}{2}w^Tw+\sum\limits_{i=1}^N\lambda_i-\sum\limits_{i=1}^N\lambda_iy_iw^Tx_i
\]</span><br />
所以：<br />
<span class="math display">\[
\frac{\partial}{\partial w}L=0\Rightarrow
w=\sum\limits_{i=1}^N\lambda_iy_ix_i
\]</span></p></li>
<li><p>将上面两个参数代入：<br />
<span class="math display">\[
L(w,b,\lambda_i)=-\frac{1}{2}\sum\limits_{i=1}^N\sum\limits_{j=1}^N\lambda_i\lambda_jy_iy_jx_i^Tx_j+\sum\limits_{i=1}^N\lambda_i
\]</span></p></li>
</ul>
<p>因此，对偶问题就是：<br />
<span class="math display">\[
\max_{\lambda}-\frac{1}{2}\sum\limits_{i=1}^N\sum\limits_{j=1}^N\lambda_i\lambda_jy_iy_jx_i^Tx_j+\sum\limits_{i=1}^N\lambda_i,\
s.t.\ \lambda_i\ge0
\]</span><br />
从 KKT 条件得到超平面的参数：</p>
<blockquote>
<p>原问题和对偶问题满足强对偶关系的充要条件为其满足 KKT 条件：<br />
<span class="math display">\[
  \begin{align}
  &amp;\frac{\partial L}{\partial w}=0,\frac{\partial L}{\partial b}=0
  \\&amp;\lambda_k(1-y_k(w^Tx_k+b))=0(slackness\ complementary)\\
  &amp;\lambda_i\ge0\\
  &amp;1-y_i(w^Tx_i+b)\le0
  \end{align}
  \]</span></p>
</blockquote>
<p>根据这个条件就得到了对应的最佳参数：<br />
<span class="math display">\[
\hat{w}=\sum\limits_{i=1}^N\lambda_iy_ix_i\\
\hat{b}=y_k-w^Tx_k=y_k-\sum\limits_{i=1}^N\lambda_iy_ix_i^Tx_k,\exists
k,1-y_k(w^Tx_k+b)=0
\]</span><br />
于是这个超平面的参数 <span class="math inline">\(w\)</span>
就是数据点的线性组合，最终的参数值就是部分满足 <span
class="math inline">\(y_i(w^Tx_i+b)=1\)</span>向量的线性组合（互补松弛条件给出），这些向量也叫支撑向量。</p>
<h2 id="soft-margin-svm">Soft-margin SVM</h2>
<p>Hard-margin 的 SVM
只对可分数据可解，如果不可分的情况，我们的基本想法是在损失函数中加入错误分类的可能性。错误分类的个数可以写成：<br />
<span class="math display">\[
error=\sum\limits_{i=1}^N\mathbb{I}\{y_i(w^Tx_i+b)\lt1\}
\]</span><br />
这个函数不连续，可以将其改写为：<br />
<span class="math display">\[
error=\sum\limits_{i=1}^N\max\{0,1-y_i(w^Tx_i+b)\}
\]</span><br />
求和符号中的式子又叫做 Hinge Function。</p>
<p>将这个错误加入 Hard-margin SVM 中，于是：<br />
<span class="math display">\[
\mathop{argmin}_{w,b}\frac{1}{2}w^Tw+C\sum\limits_{i=1}^N\max\{0,1-y_i(w^Tx_i+b)\}\
s.t.\ y_i(w^Tx_i+b)\ge1-\xi_i,i=1,2,\cdots,N
\]</span><br />
这个式子中，常数 <span class="math inline">\(C\)</span>
可以看作允许的错误水平，同时上式为了进一步消除 <span
class="math inline">\(\max\)</span>
符号，对数据集中的每一个观测，我们可以认为其大部分满足约束，但是其中部分违反约束，因此这部分约束变成
<span class="math inline">\(y_i(w^Tx+b)\ge1-\xi_i\)</span>，其中 <span
class="math inline">\(\xi_i=1-y_i(w^Tx_i+b)\)</span>，进一步的化简：<br />
<span class="math display">\[
\mathop{argmin}_{w,b}\frac{1}{2}w^Tw+C\sum\limits_{i=1}^N\xi_i\ s.t.\
y_i(w^Tx_i+b)\ge1-\xi_i,\xi_i\ge0,i=1,2,\cdots,N
\]</span></p>
<h2 id="kernel-method">Kernel Method</h2>
<p>核方法可以应用在很多问题上，在分类问题中，对于严格不可分问题，我们引入一个特征转换函数将原来的不可分的数据集变为可分的数据集，然后再来应用已有的模型。往往将低维空间的数据集变为高维空间的数据集后，数据会变得可分（数据变得更为稀疏）：</p>
<blockquote>
<p>Cover TH：高维空间比低维空间更易线性可分。</p>
</blockquote>
<p>应用在 SVM 中时，观察上面的 SVM 对偶问题：<br />
<span class="math display">\[
\max_{\lambda}-\frac{1}{2}\sum\limits_{i=1}^N\sum\limits_{j=1}^N\lambda_i\lambda_jy_iy_jx_i^Tx_j+\sum\limits_{i=1}^N\lambda_i,\
s.t.\ \lambda_i\ge0
\]</span><br />
在求解的时候需要求得内积，于是不可分数据在通过特征变换后，需要求得变换后的内积。我们常常很难求得变换函数的内积。于是直接引入内积的变换函数：<br />
<span class="math display">\[
\forall x,x&#39;\in\mathcal{X},\exists\phi\in\mathcal{H}:x\rightarrow z\
s.t.\ k(x,x&#39;)=\phi(x)^T\phi(x)
\]</span><br />
称 <span class="math inline">\(k(x,x&#39;)\)</span>
为一个正定核函数，其中<span class="math inline">\(\mathcal{H}\)</span>
是 Hilbert
空间（完备的线性内积空间），如果去掉内积这个条件我们简单地称为核函数。</p>
<blockquote>
<p><span
class="math inline">\(k(x,x&#39;)=\exp(-\frac{(x-x&#39;)^2}{2\sigma^2})\)</span>
是一个核函数。</p>
<p>证明：<br />
<span class="math display">\[
  \begin{align}
  \exp(-\frac{(x-x&#39;)^2}{2\sigma^2})&amp;=\exp(-\frac{x^2}{2\sigma^2})\exp(\frac{xx&#39;}{\sigma^2})\exp(-\frac{x&#39;^2}{2\sigma^2})\nonumber\\
  &amp;=\exp(-\frac{x^2}{2\sigma^2})\sum\limits_{n=0}^{+\infty}\frac{x^nx&#39;^n}{\sigma^{2n}n!}\exp(-\frac{x&#39;^2}{2\sigma^2})\nonumber\\
  &amp;=\exp(-\frac{x^2}{2\sigma^2})\varphi(x)\varphi(x&#39;)\exp(-\frac{x&#39;^2}{2\sigma^2})\nonumber\\
  &amp;=\phi(x)\phi(x&#39;)
  \end{align}
  \]</span></p>
</blockquote>
<p>正定核函数有下面的等价定义：</p>
<blockquote>
<p>如果核函数满足：</p>
<ol type="1">
<li>对称性</li>
<li>正定性</li>
</ol>
<p>那么这个核函数时正定核函数。</p>
<p>证明：</p>
<ol type="1">
<li>对称性 <span class="math inline">\(\Leftrightarrow\)</span> <span
class="math inline">\(k(x,z)=k(z,x)\)</span>，显然满足内积的定义</li>
<li>正定性 <span class="math inline">\(\Leftrightarrow\)</span> <span
class="math inline">\(\forall
N,x_1,x_2,\cdots,x_N\in\mathcal{X}\)</span>，对应的 Gram Matrix <span
class="math inline">\(K=[k(x_i,x_j)]\)</span> 是半正定的。</li>
</ol>
<p>要证：<span
class="math inline">\(k(x,z)=\phi(x)^T\phi(z)\Leftrightarrow K\)</span>
半正定+对称性。</p>
<ol type="1">
<li><p><span
class="math inline">\(\Rightarrow\)</span>：首先，对称性是显然的，对于正定性：<br />
<span class="math display">\[
K=\begin{pmatrix}k(x_1,x_2)&amp;\cdots&amp;k(x_1,x_N)\\\vdots&amp;\vdots&amp;\vdots\\k(x_N,x_1)&amp;\cdots&amp;k(x_N,x_N)\end{pmatrix}
\]</span><br />
任意取 <span
class="math inline">\(\alpha\in\mathbb{R}^N\)</span>，即需要证明 <span
class="math inline">\(\alpha^TK\alpha\ge0\)</span>：<br />
<span class="math display">\[
\alpha^TK\alpha=\sum\limits_{i,j}\alpha_i\alpha_jK_{ij}=\sum\limits_{i,j}\alpha_i\phi^T(x_i)\phi(x_j)\alpha_j=\sum\limits_{i}\alpha_i\phi^T(x_i)\sum\limits_{j}\alpha_j\phi(x_j)
\]</span><br />
这个式子就是内积的形式，Hilbert
空间满足线性性，于是正定性的证。</p></li>
<li><p><span class="math inline">\(\Leftarrow\)</span>：对于 <span
class="math inline">\(K\)</span> 进行分解，对于对称矩阵 <span
class="math inline">\(K=V\Lambda V^T\)</span>，那么令 <span
class="math inline">\(\phi(x_i)=\sqrt{\lambda_i}V_i\)</span>，其中 <span
class="math inline">\(V_i\)</span>是特征向量，于是就构造了 <span
class="math inline">\(k(x,z)=\sqrt{\lambda_i\lambda_j}V_i^TV_j\)</span></p></li>
</ol>
</blockquote>
<h2 id="小结">小结</h2>
<p>分类问题在很长一段时间都依赖 SVM，对于严格可分的数据集，Hard-margin
SVM
选定一个超平面，保证所有数据到这个超平面的距离最大，对这个平面施加约束，固定
<span
class="math inline">\(y_i(w^Tx_i+b)=1\)</span>，得到了一个凸优化问题并且所有的约束条件都是仿射函数，于是满足
Slater
条件，将这个问题变换成为对偶的问题，可以得到等价的解，并求出约束参数：<br />
<span class="math display">\[
\max_{\lambda}-\frac{1}{2}\sum\limits_{i=1}^N\sum\limits_{j=1}^N\lambda_i\lambda_jy_iy_jx_i^Tx_j+\sum\limits_{i=1}^N\lambda_i,\
s.t.\ \lambda_i\ge0
\]</span><br />
对需要的超平面参数的求解采用强对偶问题的 KKT 条件进行。<br />
<span class="math display">\[
\begin{align}
&amp;\frac{\partial L}{\partial w}=0,\frac{\partial L}{\partial b}=0
\\&amp;\lambda_k(1-y_k(w^Tx_k+b))=0(slackness\ complementary)\\
&amp;\lambda_i\ge0\\
&amp;1-y_i(w^Tx_i+b)\le0
\end{align}
\]</span><br />
解就是：<br />
<span class="math display">\[
\hat{w}=\sum\limits_{i=1}^N\lambda_iy_ix_i\\
\hat{b}=y_k-w^Tx_k=y_k-\sum\limits_{i=1}^N\lambda_iy_ix_i^Tx_k,\exists
k,1-y_k(w^Tx_k+b)=0
\]</span><br />
当允许一点错误的时候，可以在 Hard-margin SVM 中加入错误项。用 Hinge
Function 表示错误项的大小，得到：<br />
<span class="math display">\[
\mathop{argmin}_{w,b}\frac{1}{2}w^Tw+C\sum\limits_{i=1}^N\xi_i\ s.t.\
y_i(w^Tx_i+b)\ge1-\xi_i,\xi_i\ge0,i=1,2,\cdots,N
\]</span><br />
对于完全不可分的问题，我们采用特征转换的方式，在 SVM
中，我们引入正定核函数来直接对内积进行变换，只要这个变换满足对称性和正定性，那么就可以用做核函数。</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://soundmemories.github.io/2020/09/28/ML/20.DimentionReduction/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avstar.png">
      <meta itemprop="name" content="SoundMemories">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="SoundMemories">
      <meta itemprop="description" content="今日事，今日毕">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | SoundMemories">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2020/09/28/ML/20.DimentionReduction/" class="post-title-link" itemprop="url">DimentionReduction</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2020-09-28 00:00:00" itemprop="dateCreated datePublished" datetime="2020-09-28T00:00:00+08:00">2020-09-28</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/ML/" itemprop="url" rel="index"><span itemprop="name">ML</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>1.5k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>5 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="降维">降维</h1>
<p>我们知道，解决过拟合的问题除了正则化和添加数据之外，降维就是最好的方法。降维的思路来源于维度灾难的问题，我们知道
<span class="math inline">\(n\)</span> 维球的体积为：<br />
<span class="math display">\[
CR^n
\]</span><br />
那么在球体积与边长为 <span class="math inline">\(2R\)</span>
的超立方体比值为：<br />
<span class="math display">\[
\lim\limits_{n\rightarrow0}\frac{CR^n}{2^nR^n}=0
\]</span></p>
<p>这就是所谓的维度灾难，在高维数据中，主要样本都分布在立方体的边缘，所以数据集更加稀疏。</p>
<p>降维的算法分为：</p>
<ol type="1">
<li>直接降维，特征选择</li>
<li>线性降维，PCA，MDS等</li>
<li>分线性，流形包括 Isomap，LLE 等</li>
</ol>
<p>为了方便，我们首先将协方差矩阵（数据集）写成中心化的形式：<br />
<span class="math display">\[
\begin{align}S&amp;=\frac{1}{N}\sum\limits_{i=1}^N(x_i-\overline{x})(x_i-\overline{x})^T\nonumber\\
&amp;=\frac{1}{N}(x_1-\overline{x},x_2-\overline{x},\cdots,x_N-\overline{x})(x_1-\overline{x},x_2-\overline{x},\cdots,x_N-\overline{x})^T\nonumber\\
&amp;=\frac{1}{N}(X^T-\frac{1}{N}X^T\mathbb{I}_{N1}\mathbb{I}_{N1}^T)(X^T-\frac{1}{N}X^T\mathbb{I}_{N1}\mathbb{I}_{N1}^T)^T\nonumber\\
&amp;=\frac{1}{N}X^T(E_N-\frac{1}{N}\mathbb{I}_{N1}\mathbb{I}_{1N})(E_N-\frac{1}{N}\mathbb{I}_{N1}\mathbb{I}_{1N})^TX\nonumber\\
&amp;=\frac{1}{N}X^TH_NH_N^TX\nonumber\\
&amp;=\frac{1}{N}X^TH_NH_NX=\frac{1}{N}X^THX
\end{align}
\]</span><br />
这个式子利用了中心矩阵 $ H$的对称性，这也是一个投影矩阵。</p>
<h2 id="线性降维-主成分分析-pca">线性降维-主成分分析 PCA</h2>
<h3 id="损失函数">损失函数</h3>
<p>主成分分析中，我们的基本想法是将所有数据投影到一个字空间中，从而达到降维的目标，为了寻找这个子空间，我们基本想法是：</p>
<ol type="1">
<li>所有数据在子空间中更为分散</li>
<li>损失的信息最小，即：在补空间的分量少</li>
</ol>
<p>原来的数据很有可能各个维度之间是相关的，于是我们希望找到一组 <span
class="math inline">\(p\)</span> 个新的线性无关的单位基 <span
class="math inline">\(u_i\)</span>，降维就是取其中的 <span
class="math inline">\(q\)</span> 个基。于是对于一个样本 <span
class="math inline">\(x_i\)</span>，经过这个坐标变换后：<br />
<span class="math display">\[
\hat{x_i}=\sum\limits_{i=1}^p(u_i^Tx_i)u_i=\sum\limits_{i=1}^q(u_i^Tx_i)u_i+\sum\limits_{i=q+1}^p(u_i^Tx_i)u_i
\]</span><br />
对于数据集来说，我们首先将其中心化然后再去上面的式子的第一项，并使用其系数的平方平均作为损失函数并最大化：<br />
<span class="math display">\[
\begin{align}J&amp;=\frac{1}{N}\sum\limits_{i=1}^N\sum\limits_{j=1}^q((x_i-\overline{x})^Tu_j)^2\nonumber\\
&amp;=\sum\limits_{j=1}^qu_j^TSu_j\ ,\ s.t.\ u_j^Tu_j=1
\end{align}
\]</span><br />
由于每个基都是线性无关的，于是每一个 <span
class="math inline">\(u_j\)</span>
的求解可以分别进行，使用拉格朗日乘子法：<br />
<span class="math display">\[
\mathop{argmax}_{u_j}L(u_j,\lambda)=\mathop{argmax}_{u_j}u_j^TSu_j+\lambda(1-u_j^Tu_j)
\]</span><br />
于是：<br />
<span class="math display">\[
Su_j=\lambda u_j
\]</span><br />
可见，我们需要的基就是协方差矩阵的本征矢。损失函数最大取在本征值前 <span
class="math inline">\(q\)</span> 个最大值。</p>
<p>下面看其损失的信息最少这个条件，同样适用系数的平方平均作为损失函数，并最小化：<br />
<span class="math display">\[
\begin{align}J&amp;=\frac{1}{N}\sum\limits_{i=1}^N\sum\limits_{j=q+1}^p((x_i-\overline{x})^Tu_j)^2\nonumber\\
&amp;=\sum\limits_{j=q+1}^pu_j^TSu_j\ ,\ s.t.\ u_j^Tu_j=1
\end{align}
\]</span><br />
同样的：<br />
<span class="math display">\[
\mathop{argmin}_{u_j}L(u_j,\lambda)=\mathop{argmin}_{u_j}u_j^TSu_j+\lambda(1-u_j^Tu_j)
\]</span><br />
损失函数最小取在本征值剩下的个最小的几个值。数据集的协方差矩阵可以写成
<span class="math inline">\(S=U\Lambda
U^T\)</span>，直接对这个表达式当然可以得到本征矢。</p>
<h3 id="svd-与-pcoa">SVD 与 PCoA</h3>
<p>下面使用实际训练时常常使用的 SVD 直接求得这个 <span
class="math inline">\(q\)</span> 个本征矢。</p>
<p>对中心化后的数据集进行奇异值分解：<br />
<span class="math display">\[
HX=U\Sigma V^T,U^TU=E_N,V^TV=E_p,\Sigma:N\times p
\]</span></p>
<p>于是：<br />
<span class="math display">\[
S=\frac{1}{N}X^THX=\frac{1}{N}X^TH^THX=\frac{1}{N}V\Sigma^T\Sigma V^T
\]</span><br />
因此，我们直接对中心化后的数据集进行 SVD，就可以得到特征值和特征向量
<span class="math inline">\(V\)</span>，在新坐标系中的坐标就是：<br />
<span class="math display">\[
HX\cdot V
\]</span><br />
由上面的推导，我们也可以得到另一种方法 PCoA
主坐标分析，定义并进行特征值分解：<br />
<span class="math display">\[
T=HXX^TH=U\Sigma\Sigma^TU^T
\]</span><br />
由于：<br />
<span class="math display">\[
TU\Sigma=U\Sigma(\Sigma^T\Sigma)
\]</span><br />
于是可以直接得到坐标。这两种方法都可以得到主成分，但是由于方差矩阵是
<span class="math inline">\(p\times p\)</span> 的，而 <span
class="math inline">\(T\)</span> 是 <span class="math inline">\(N\times
N\)</span> 的，所以对样本量较少的时候可以采用 PCoA的方法。</p>
<h3 id="p-pca">p-PCA</h3>
<p>下面从概率的角度对 PCA 进行分析，概率方法也叫
p-PCA。我们使用线性模型，类似之前 LDA，我们选定一个方向，对原数据 <span
class="math inline">\(x\in\mathbb{R}^p\)</span> ，降维后的数据为 <span
class="math inline">\(z\in\mathbb{R}^q,q&lt;p\)</span>。降维通过一个矩阵变换（投影）进行：<br />
<span class="math display">\[
\begin{align}
z&amp;\sim\mathcal{N}(\mathbb{O}_{q1},\mathbb{I}_{qq})\\
x&amp;=Wz+\mu+\varepsilon\\
\varepsilon&amp;\sim\mathcal{N}(0,\sigma^2\mathbb{I}_{pp})
\end{align}
\]</span><br />
对于这个模型，我么可以使用期望-最大（EM）的算法进行学习，在进行推断的时候需要求得
<span
class="math inline">\(p(z|x)\)</span>，推断的求解过程和线性高斯模型类似。<br />
<span class="math display">\[
\begin{align}
&amp;p(z|x)=\frac{p(x|z)p(z)}{p(x)}\\
&amp;\mathbb{E}[x]=\mathbb{E}[Wz+\mu+\varepsilon]=\mu\\
&amp;Var[x]=WW^T+\sigma^2\mathbb{I}_{pp}\\
\Longrightarrow
p(z|x)=\mathcal{N}(W^T(WW^T+&amp;\sigma^2\mathbb{I})^{-1}(x-\mu),\mathbb{I}-W^T(WW^T+\sigma^2\mathbb{I})^{-1}W)
\end{align}
\]</span></p>
<h2 id="小结">小结</h2>
<p>降维是解决维度灾难和过拟合的重要方法，除了直接的特征选择外，我们还可以采用算法的途径对特征进行筛选，线性的降维方法以
PCA 为代表，在 PCA
中，我们只要直接对数据矩阵进行中心化然后求奇异值分解或者对数据的协方差矩阵进行分解就可以得到其主要维度。非线性学习的方法如流形学习将投影面从平面改为超曲面。</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://soundmemories.github.io/2020/09/27/ML/19.LinearClassification/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avstar.png">
      <meta itemprop="name" content="SoundMemories">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="SoundMemories">
      <meta itemprop="description" content="今日事，今日毕">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | SoundMemories">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2020/09/27/ML/19.LinearClassification/" class="post-title-link" itemprop="url">LinearClassification</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2020-09-27 00:00:00" itemprop="dateCreated datePublished" datetime="2020-09-27T00:00:00+08:00">2020-09-27</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/ML/" itemprop="url" rel="index"><span itemprop="name">ML</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>3.7k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>13 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="线性分类">线性分类</h1>
<p>对于分类任务，线性回归模型就无能为力了，但是我们可以在线性模型的函数进行后再加入一层激活函数，这个函数是非线性的，激活函数的反函数叫做链接函数。我们有两种线性分类的方式：</p>
<ol type="1">
<li>硬分类，我们直接需要输出观测对应的分类。这类模型的代表为：
<ol type="1">
<li>线性判别分析（Fisher 判别）</li>
<li>感知机</li>
</ol></li>
<li>软分类，产生不同类别的概率，这类算法根据概率方法的不同分为两种
<ol type="1">
<li>生成式（根据贝叶斯定理先计算参数后验，再进行推断）：高斯判别分析（GDA）和朴素贝叶斯等为代表
<ol type="1">
<li>GDA</li>
<li>Naive Bayes</li>
</ol></li>
<li>判别式（直接对条件概率进行建模）：Logistic 回归</li>
</ol></li>
</ol>
<h2 id="两分类-硬分类-感知机算法">两分类-硬分类-感知机算法</h2>
<p>我们选取激活函数为：<br />
<span class="math display">\[
sign(a)=\left\{\begin{matrix}+1,a\ge0\\-1,a\lt0\end{matrix}\right.
\]</span><br />
这样就可以将线性回归的结果映射到两分类的结果上了。</p>
<p>定义损失函数为错误分类的数目，比较直观的方式是使用指示函数，但是指示函数不可导，因此可以定义：<br />
<span class="math display">\[
L(w)=\sum\limits_{x_i\in\mathcal{D}_{wrong}}-y_iw^Tx_i
\]</span><br />
其中，<span
class="math inline">\(\mathcal{D}_{wrong}\)</span>是错误分类集合，实际在每一次训练的时候，我们采用梯度下降的算法。损失函数对
<span class="math inline">\(w\)</span> 的偏导为：<br />
<span class="math display">\[
\frac{\partial}{\partial
w}L(w)=\sum\limits_{x_i\in\mathcal{D}_{wrong}}-y_ix_i
\]</span><br />
但是如果样本非常多的情况下，计算复杂度较高，但是，实际上我们并不需要绝对的损失函数下降的方向，我们只需要损失函数的期望值下降，但是计算期望需要知道真实的概率分布，我们实际只能根据训练数据抽样来估算这个概率分布（经验风险）：<br />
<span class="math display">\[
\mathbb{E}_{\mathcal
D}[\mathbb{E}_\hat{p}[\nabla_wL(w)]]=\mathbb{E}_{\mathcal
D}[\frac{1}{N}\sum\limits_{i=1}^N\nabla_wL(w)]
\]</span><br />
我们知道， <span class="math inline">\(N\)</span>
越大，样本近似真实分布越准确，但是对于一个标准差为 <span
class="math inline">\(\sigma\)</span> 的数据，可以确定的标准差仅和 <span
class="math inline">\(\sqrt{N}\)</span> 成反比，而计算速度却和 <span
class="math inline">\(N\)</span>
成正比。因此可以每次使用较少样本，则在数学期望的意义上损失降低的同时，有可以提高计算速度，如果每次只使用一个错误样本，我们有下面的更新策略（根据泰勒公式，在负方向）：<br />
<span class="math display">\[
w^{t+1}\leftarrow w^{t}+\lambda y_ix_i
\]</span><br />
是可以收敛的，同时使用单个观测更新也可以在一定程度上增加不确定度，从而减轻陷入局部最小的可能。在更大规模的数据上，常用的是小批量随机梯度下降法。</p>
<h2 id="两分类-硬分类-线性判别分析-lda">两分类-硬分类-线性判别分析
LDA</h2>
<p>在 LDA
中，我们的基本想法是选定一个方向，将试验样本顺着这个方向投影，投影后的数据需要满足两个条件，从而可以更好地分类：</p>
<ol type="1">
<li>相同类内部的试验样本距离接近。</li>
<li>不同类别之间的距离较大。</li>
</ol>
<p>首先是投影，我们假定原来的数据是向量 <span
class="math inline">\(x\)</span>，那么顺着 $ w$
方向的投影就是标量：<br />
<span class="math display">\[
z=w^T\cdot x(=|w|\cdot|x|\cos\theta)
\]</span><br />
对第一点，相同类内部的样本更为接近，我们假设属于两类的试验样本数量分别是
<span class="math inline">\(N_1\)</span>和 <span
class="math inline">\(N_2\)</span>，那么我们采用方差矩阵来表征每一个类内的总体分布，这里我们使用了协方差的定义，用
<span class="math inline">\(S\)</span> 表示原数据的协方差：<br />
<span class="math display">\[
\begin{align}
C_1:Var_z[C_1]&amp;=\frac{1}{N_1}\sum\limits_{i=1}^{N_1}(z_i-\overline{z_{c1}})(z_i-\overline{z_{c1}})^T\nonumber\\
&amp;=\frac{1}{N_1}\sum\limits_{i=1}^{N_1}(w^Tx_i-\frac{1}{N_1}\sum\limits_{j=1}^{N_1}w^Tx_j)(w^Tx_i-\frac{1}{N_1}\sum\limits_{j=1}^{N_1}w^Tx_j)^T\nonumber\\
&amp;=w^T\frac{1}{N_1}\sum\limits_{i=1}^{N_1}(x_i-\overline{x_{c1}})(x_i-\overline{x_{c1}})^Tw\nonumber\\
&amp;=w^TS_1w\\
C_2:Var_z[C_2]&amp;=\frac{1}{N_2}\sum\limits_{i=1}^{N_2}(z_i-\overline{z_{c2}})(z_i-\overline{z_{c2}})^T\nonumber\\
&amp;=w^TS_2w
\end{align}
\]</span><br />
所以类内距离可以记为：<br />
<span class="math display">\[
\begin{align}
Var_z[C_1]+Var_z[C_2]=w^T(S_1+S_2)w
\end{align}
\]</span><br />
对于第二点，我们可以用两类的均值表示这个距离：<br />
<span class="math display">\[
\begin{align}
(\overline{z_{c1}}-\overline{z_{c2}})^2&amp;=(\frac{1}{N_1}\sum\limits_{i=1}^{N_1}w^Tx_i-\frac{1}{N_2}\sum\limits_{i=1}^{N_2}w^Tx_i)^2\nonumber\\
&amp;=(w^T(\overline{x_{c1}}-\overline{x_{c2}}))^2\nonumber\\
&amp;=w^T(\overline{x_{c1}}-\overline{x_{c2}})(\overline{x_{c1}}-\overline{x_{c2}})^Tw
\end{align}
\]</span><br />
综合这两点，由于协方差是一个矩阵，于是我们用将这两个值相除来得到我们的损失函数，并最大化这个值：<br />
<span class="math display">\[
\begin{align}
\hat{w}=\mathop{argmax}\limits_wJ(w)&amp;=\mathop{argmax}\limits_w\frac{(\overline{z_{c1}}-\overline{z_{c2}})^2}{Var_z[C_1]+Var_z[C_2]}\nonumber\\
&amp;=\mathop{argmax}\limits_w\frac{w^T(\overline{x_{c1}}-\overline{x_{c2}})(\overline{x_{c1}}-\overline{x_{c2}})^Tw}{w^T(S_1+S_2)w}\nonumber\\
&amp;=\mathop{argmax}\limits_w\frac{w^TS_bw}{w^TS_ww}
\end{align}
\]</span><br />
这样，我们就把损失函数和原数据集以及参数结合起来了。下面对这个损失函数求偏导，注意我们其实对
<span class="math inline">\(w\)</span>
的绝对值没有任何要求，只对方向有要求，因此只要一个方程就可以求解了：<br />
<span class="math display">\[
\begin{align}
&amp;\frac{\partial}{\partial
w}J(w)=2S_bw(w^TS_ww)^{-1}-2w^TS_bw(w^TS_ww)^{-2}S_ww=0\nonumber\\
&amp;\Longrightarrow S_bw(w^TS_ww)=(w^TS_bw)S_ww\nonumber\\
&amp;\Longrightarrow w\propto
S_w^{-1}S_bw=S_w^{-1}(\overline{x_{c1}}-\overline{x_{c2}})(\overline{x_{c1}}-\overline{x_{c2}})^Tw\propto
S_w^{-1}(\overline{x_{c1}}-\overline{x_{c2}})
\end{align}
\]</span><br />
于是 $ S_w^{-1}(-)$ 就是我们需要寻找的方向。最后可以归一化求得单位的
<span class="math inline">\(w\)</span> 值。</p>
<h2
id="两分类-软分类-概率判别模型-logistic-回归">两分类-软分类-概率判别模型-Logistic
回归</h2>
<p>有时候我们只要得到一个类别的概率，那么我们需要一种能输出 <span
class="math inline">\([0,1]\)</span>
区间的值的函数。考虑两分类模型，我们利用判别模型，希望对 <span
class="math inline">\(p(C|x)\)</span> 建模，利用贝叶斯定理：<br />
<span class="math display">\[
p(C_1|x)=\frac{p(x|C_1)p(C_1)}{p(x|C_1)p(C_1)+p(x|C_2)p(C_2)}
\]</span><br />
取 <span
class="math inline">\(a=\ln\frac{p(x|C_1)p(C_1)}{p(x|C_2)p(C_2)}\)</span>，于是：<br />
<span class="math display">\[
p(C_1|x)=\frac{1}{1+\exp(-a)}
\]</span><br />
上面的式子叫 Logistic Sigmoid
函数，其参数表示了两类联合概率比值的对数。在判别式中，不关心这个参数的具体值，模型假设直接对
<span class="math inline">\(a\)</span> 进行。</p>
<p>Logistic 回归的模型假设是：<br />
<span class="math display">\[
a=w^Tx
\]</span><br />
于是，通过寻找 $  w$
的最佳值可以得到在这个模型假设下的最佳模型。概率判别模型常用最大似然估计的方式来确定参数。</p>
<p>对于一次观测，获得分类 <span class="math inline">\(y\)</span>
的概率为（假定<span class="math inline">\(C_1=1,C_2=0\)</span>）：<br />
<span class="math display">\[
p(y|x)=p_1^yp_0^{1-y}
\]</span></p>
<p>那么对于 <span class="math inline">\(N\)</span> 次独立全同的观测
MLE为：<br />
<span class="math display">\[
\hat{w}=\mathop{argmax}_wJ(w)=\mathop{argmax}_w\sum\limits_{i=1}^N(y_i\log
p_1+(1-y_i)\log p_0)
\]</span><br />
注意到，这个表达式是交叉熵表达式的相反数乘 <span
class="math inline">\(N\)</span>，MLE
中的对数也保证了可以和指数函数相匹配，从而在大的区间汇总获取稳定的梯度。</p>
<p>对这个函数求导数，注意到：<br />
<span class="math display">\[
p_1&#39;=(\frac{1}{1+\exp(-a)})&#39;=p_1(1-p_1)
\]</span><br />
则：<br />
<span class="math display">\[
J&#39;(w)=\sum\limits_{i=1}^Ny_i(1-p_1)x_i-p_1x_i+y_ip_1x_i=\sum\limits_{i=1}^N(y_i-p_1)x_i
\]</span><br />
由于概率值的非线性，放在求和符号中时，这个式子无法直接求解。于是在实际训练的时候，和感知机类似，也可以使用不同大小的批量随机梯度上升（对于最小化就是梯度下降）来获得这个函数的极大值。</p>
<h2
id="两分类-软分类-概率生成模型-高斯判别分析-gda">两分类-软分类-概率生成模型-高斯判别分析
GDA</h2>
<p>生成模型中，我们对联合概率分布进行建模，然后采用 MAP
来获得参数的最佳值。两分类的情况，我们采用的假设：</p>
<ol type="1">
<li><span class="math inline">\(y\sim Bernoulli(\phi)\)</span></li>
<li><span
class="math inline">\(x|y=1\sim\mathcal{N}(\mu_1,\Sigma)\)</span></li>
<li><span
class="math inline">\(x|y=0\sim\mathcal{N}(\mu_0,\Sigma)\)</span></li>
</ol>
<p>那么独立全同的数据集最大后验概率可以表示为：<br />
<span class="math display">\[
\begin{align}
\mathop{argmax}_{\phi,\mu_0,\mu_1,\Sigma}\log
p(X|Y)p(Y)=\mathop{argmax}_{\phi,\mu_0,\mu_1,\Sigma}\sum\limits_{i=1}^N
(\log p(x_i|y_i)+\log p(y_i))\nonumber\\
=\mathop{argmax}_{\phi,\mu_0,\mu_1,\Sigma}\sum\limits_{i=1}^N((1-y_i)\log\mathcal{N}(\mu_0,\Sigma)+y_i\log
\mathcal{N}(\mu_1,\Sigma)+y_i\log\phi+(1-y_i)\log(1-\phi))
\end{align}
\]</span></p>
<ul>
<li><p>首先对 <span class="math inline">\(\phi\)</span>
进行求解，将式子对 <span class="math inline">\(\phi\)</span>
求偏导：<br />
<span class="math display">\[
\begin{align}\sum\limits_{i=1}^N\frac{y_i}{\phi}+\frac{y_i-1}{1-\phi}=0\nonumber\\
\Longrightarrow\phi=\frac{\sum\limits_{i=1}^Ny_i}{N}=\frac{N_1}{N}
\end{align}
\]</span></p></li>
<li><p>然后求解 <span class="math inline">\(\mu_1\)</span>：<br />
<span class="math display">\[
\begin{align}\hat{\mu_1}&amp;=\mathop{argmax}_{\mu_1}\sum\limits_{i=1}^Ny_i\log\mathcal{N}(\mu_1,\Sigma)\nonumber\\
&amp;=\mathop{argmin}_{\mu_1}\sum\limits_{i=1}^Ny_i(x_i-\mu_1)^T\Sigma^{-1}(x_i-\mu_1)
\end{align}
\]</span><br />
由于：<br />
<span class="math display">\[
\sum\limits_{i=1}^Ny_i(x_i-\mu_1)^T\Sigma^{-1}(x_i-\mu_1)=\sum\limits_{i=1}^Ny_ix_i^T\Sigma^{-1}x_i-2y_i\mu_1^T\Sigma^{-1}x_i+y_i\mu_1^T\Sigma^{-1}\mu_1
\]</span><br />
求微分左边乘以 <span class="math inline">\(\Sigma\)</span>
可以得到：<br />
<span class="math display">\[
\begin{align}
\sum\limits_{i=1}^N-2y_i\Sigma^{-1}x_i+2y_i\Sigma^{-1}\mu_1=0\nonumber\\
\Longrightarrow\mu_1=\frac{\sum\limits_{i=1}^Ny_ix_i}{\sum\limits_{i=1}^Ny_i}=\frac{\sum\limits_{i=1}^Ny_ix_i}{N_1}
\end{align}
\]</span></p></li>
<li><p>求解 <span
class="math inline">\(\mu_0\)</span>，由于正反例是对称的，所以：<br />
<span class="math display">\[
\mu_0=\frac{\sum\limits_{i=1}^N(1-y_i)x_i}{N_0}
\]</span></p></li>
<li><p>最为困难的是求解 <span
class="math inline">\(\Sigma\)</span>，我们的模型假设对正反例采用相同的协方差矩阵，当然从上面的求解中我们可以看到，即使采用不同的矩阵也不会影响之前的三个参数。首先我们有：<br />
<span class="math display">\[
\begin{align}
\sum\limits_{i=1}^N\log\mathcal{N}(\mu,\Sigma)&amp;=\sum\limits_{i=1}^N\log(\frac{1}{(2\pi)^{p/2}|\Sigma|^{1/2}})+(-\frac{1}{2}(x_i-\mu)^T\Sigma^{-1}(x_i-\mu))\nonumber\\
&amp;=Const-\frac{1}{2}N\log|\Sigma|-\frac{1}{2}Trace((x_i-\mu)^T\Sigma^{-1}(x_i-\mu))\nonumber\\
&amp;=Const-\frac{1}{2}N\log|\Sigma|-\frac{1}{2}Trace((x_i-\mu)(x_i-\mu)^T\Sigma^{-1})\nonumber\\
&amp;=Const-\frac{1}{2}N\log|\Sigma|-\frac{1}{2}NTrace(S\Sigma^{-1})
\end{align}
\]</span><br />
在这个表达式中，我们在标量上加入迹从而可以交换矩阵的顺序，对于包含绝对值和迹的表达式的导数，我们有：<br />
<span class="math display">\[
\begin{align}
\frac{\partial}{\partial A}(|A|)&amp;=|A|A^{-1}\\
\frac{\partial}{\partial A}Trace(AB)&amp;=B^T
\end{align}
\]</span><br />
因此：<br />
<span class="math display">\[
\begin{align}[\sum\limits_{i=1}^N((1-y_i)\log\mathcal{N}(\mu_0,\Sigma)+y_i\log
\mathcal{N}(\mu_1,\Sigma)]&#39;
\nonumber\\=Const-\frac{1}{2}N\log|\Sigma|-\frac{1}{2}N_1Trace(S_1\Sigma^{-1})-\frac{1}{2}N_2Trace(S_2\Sigma^{-1})
\end{align}
\]</span><br />
其中，<span class="math inline">\(S_1,S_2\)</span>
分别为两个类数据内部的协方差矩阵，于是：<br />
<span class="math display">\[
\begin{align}N\Sigma^{-1}-N_1S_1^T\Sigma^{-2}-N_2S_2^T\Sigma^{-2}=0\nonumber
\\\Longrightarrow\Sigma=\frac{N_1S_1+N_2S_2}{N}
\end{align}
\]</span><br />
这里应用了类协方差矩阵的对称性。</p></li>
</ul>
<p>于是我们就利用最大后验的方法求得了我们模型假设里面的所有参数，根据模型，可以得到联合分布，也就可以得到用于推断的条件分布了。</p>
<h2
id="两分类-软分类-概率生成模型-朴素贝叶斯">两分类-软分类-概率生成模型-朴素贝叶斯</h2>
<p>上面的高斯判别分析的是对数据集的分布作出了高斯分布的假设，同时引入伯努利分布作为类先验，从而利用最大后验求得这些假设中的参数。</p>
<p>朴素贝叶斯队数据的属性之间的关系作出了假设，一般地，我们有需要得到
<span class="math inline">\(p(x|y)\)</span> 这个概率值，由于 <span
class="math inline">\(x\)</span> 有 <span
class="math inline">\(p\)</span>
个维度，因此需要对这么多的维度的联合概率进行采样，但是我们知道这么高维度的空间中采样需要的样本数量非常大才能获得较为准确的概率近似。</p>
<p>在一般的有向概率图模型中，对各个属性维度之间的条件独立关系作出了不同的假设，其中最为简单的一个假设就是在朴素贝叶斯模型描述中的条件独立性假设。<br />
<span class="math display">\[
p(x|y)=\prod\limits_{i=1}^pp(x_i|y)
\]</span><br />
即：<br />
<span class="math display">\[
x_i\perp x_j|y,\forall\  i\ne j
\]</span><br />
于是利用贝叶斯定理，对于单次观测：<br />
<span class="math display">\[
p(y|x)=\frac{p(x|y)p(y)}{p(x)}=\frac{\prod\limits_{i=1}^pp(x_i|y)p(y)}{p(x)}
\]</span><br />
对于单个维度的条件概率以及类先验作出进一步的假设：</p>
<ol type="1">
<li><span class="math inline">\(x_i\)</span> 为连续变量：<span
class="math inline">\(p(x_i|y)=\mathcal{N}(\mu_i,\sigma_i^2)\)</span></li>
<li><span class="math inline">\(x_i\)</span>
为离散变量：类别分布（Categorical）：<span
class="math inline">\(p(x_i=i|y)=\theta_i,\sum\limits_{i=1}^K\theta_i=1\)</span></li>
<li><span class="math inline">\(p(y)=\phi^y(1-\phi)^{1-y}\)</span></li>
</ol>
<p>对这些参数的估计，常用 MLE
的方法直接在数据集上估计，由于不需要知道各个维度之间的关系，因此，所需数据量大大减少了。估算完这些参数，再代入贝叶斯定理中得到类别的后验分布。</p>
<h2 id="小结">小结</h2>
<p>分类任务分为两类，对于需要直接输出类别的任务，感知机算法中我们在线性模型的基础上加入符号函数作为激活函数，那么就能得到这个类别，但是符号函数不光滑，于是我们采用错误驱动的方式，引入
<span
class="math inline">\(\sum\limits_{x_i\in\mathcal{D}_{wrong}}-y_iw^Tx_i\)</span>
作为损失函数，然后最小化这个误差，采用批量随机梯度下降的方法来获取最佳的参数值。而在线性判别分析中，我们将线性模型看作是数据点在某一个方向的投影，采用类内小，类间大的思路来定义损失函数，其中类内小定义为两类数据的方差之和，类间大定义为两类数据中心点的间距，对损失函数求导得到参数的方向，这个方向就是
<span class="math inline">\(S_w^{-1}(\overline x_{c1}-\overline
x_{c2})\)</span>，其中 <span class="math inline">\(S_w\)</span>
为原数据集两类的方差之和。</p>
<p>另一种任务是输出分类的概率，对于概率模型，我们有两种方案，第一种是判别模型，也就是直接对类别的条件概率建模，将线性模型套入
Logistic 函数中，我们就得到了 Logistic
回归模型，这里的概率解释是两类的联合概率比值的对数是线性的，我们定义的损失函数是交叉熵（等价于
MLE），对这个函数求导得到 <span
class="math inline">\(\frac{1}{N}\sum\limits_{i=1}^N(y_i-p_1)x_i\)</span>，同样利用批量随机梯度（上升）的方法进行优化。第二种是生成模型，生成模型引入了类别的先验，在高斯判别分析中，我们对数据集的数据分布作出了假设，其中类先验是二项分布，而每一类的似然是高斯分布，对这个联合分布的对数似然进行最大化就得到了参数，
<span
class="math inline">\(\frac{\sum\limits_{i=1}^Ny_ix_i}{N_1},\frac{\sum\limits_{i=1}^N(1-y_i)x_i}{N_0},\frac{N_1S_1+N_2S_2}{N},\frac{N_1}{N}\)</span>。在朴素贝叶斯中，我们进一步对属性的各个维度之间的依赖关系作出假设，条件独立性假设大大减少了数据量的需求。</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://soundmemories.github.io/2020/09/25/ML/18.LinearRegression/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avstar.png">
      <meta itemprop="name" content="SoundMemories">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="SoundMemories">
      <meta itemprop="description" content="今日事，今日毕">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | SoundMemories">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2020/09/25/ML/18.LinearRegression/" class="post-title-link" itemprop="url">LinearRegression</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2020-09-25 00:00:00" itemprop="dateCreated datePublished" datetime="2020-09-25T00:00:00+08:00">2020-09-25</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/ML/" itemprop="url" rel="index"><span itemprop="name">ML</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>1.5k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>6 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="线性回归">线性回归</h1>
<p>假设数据集为：<br />
<span class="math display">\[
\mathcal{D}=\{(x_1, y_1),(x_2, y_2),\cdots,(x_N, y_N)\}
\]</span><br />
后面我们记：<br />
<span class="math display">\[
X=(x_1,x_2,\cdots,x_N)^T,Y=(y_1,y_2,\cdots,y_N)^T
\]</span><br />
线性回归假设：<br />
<span class="math display">\[
f(w)=w^Tx
\]</span></p>
<h2 id="最小二乘法">最小二乘法</h2>
<p>对这个问题，采用二范数定义的平方误差来定义损失函数：<br />
<span class="math display">\[
L(w)=\sum\limits_{i=1}^N||w^Tx_i-y_i||^2_2
\]</span><br />
展开得到：<br />
<span class="math display">\[
\begin{aligned}
L(w)&amp;=(w^Tx_1-y_1,\cdots,w^Tx_N-y_N)\cdot(w^Tx_1-y_1,\cdots,w^Tx_N-y_N)^T\\
&amp;=(w^TX^T-Y^T)\cdot(Xw-Y)=w^TX^TXw-Y^TXw-w^TX^TY+Y^TY\\
&amp;=w^TX^TXw-2w^TX^TY+Y^TY
\end{aligned}
\]</span><br />
最小化这个值的 <span class="math inline">\(\hat{w}\)</span> ：<br />
<span class="math display">\[
\begin{aligned}
\hat{w}=\mathop{argmin}\limits_wL(w)&amp;\longrightarrow\frac{\partial}{\partial
w}L(w)=0\\
&amp;\longrightarrow2X^TX\hat{w}-2X^TY=0\\
&amp;\longrightarrow \hat{w}=(X^TX)^{-1}X^TY=X^+Y
\end{aligned}
\]</span><br />
这个式子中 <span class="math inline">\((X^TX)^{-1}X^T\)</span>
又被称为伪逆。对于行满秩或者列满秩的 <span
class="math inline">\(X\)</span>，可以直接求解，但是对于非满秩的样本集合，需要使用奇异值分解（SVD）的方法，对
<span class="math inline">\(X\)</span> 求奇异值分解，得到<br />
<span class="math display">\[
X=U\Sigma V^T
\]</span><br />
于是：<br />
<span class="math display">\[
X^+=V\Sigma^{-1}U^T
\]</span><br />
在几何上，最小二乘法相当于模型（这里就是直线）和试验值的距离的平方求和，假设我们的试验样本张成一个
<span class="math inline">\(p\)</span> 维空间（满秩的情况）：<span
class="math inline">\(X=Span(x_1,\cdots,x_N)\)</span>，而模型可以写成
<span class="math inline">\(f(w)=X\beta\)</span>，也就是 <span
class="math inline">\(x_1,\cdots,x_N\)</span>
的某种组合，而最小二乘法就是说希望 <span
class="math inline">\(Y\)</span>
和这个模型距离越小越好，于是它们的差应该与这个张成的空间垂直：<br />
<span class="math display">\[
X^T\cdot(Y-X\beta)=0\longrightarrow\beta=(X^TX)^{-1}X^TY
\]</span></p>
<h2 id="噪声为高斯分布的-mle">噪声为高斯分布的 MLE</h2>
<p>对于一维的情况，记 <span
class="math inline">\(y=w^Tx+\epsilon,\epsilon\sim\mathcal{N}(0,\sigma^2)\)</span>，那么
<span
class="math inline">\(y\sim\mathcal{N}(w^Tx,\sigma^2)\)</span>。代入极大似然估计中：<br />
<span class="math display">\[
\begin{aligned}
L(w)=\log p(Y|X,w)&amp;=\log\prod\limits_{i=1}^Np(y_i|x_i,w)\\
&amp;=\sum\limits_{i=1}^N\log(\frac{1}{\sqrt{2\pi\sigma}}e^{-\frac{(y_i-w^Tx_i)^2}{2\sigma^2}})\\
\mathop{argmax}\limits_wL(w)&amp;=\mathop{argmin}\limits_w\sum\limits_{i=1^N}(y_i-w^Tx_i)^2
\end{aligned}
\]</span><br />
这个表达式和最小二乘估计得到的结果一样。</p>
<h2 id="权重先验也为高斯分布的-map">权重先验也为高斯分布的 MAP</h2>
<p>取先验分布 <span
class="math inline">\(w\sim\mathcal{N}(0,\sigma_0^2)\)</span>。于是： <br />
<span class="math display">\[
\begin{aligned}
\hat{w}=\mathop{argmax}\limits_wp(w|Y)&amp;=\mathop{argmax}\limits_wp(Y|w)p(w)\\
&amp;=\mathop{argmax}\limits_w\log p(Y|w)p(w)\\
&amp;=\mathop{argmax}\limits_w(\log p(Y|w)+\log p(w))\\
&amp;=\mathop{argmin}\limits_w[(y-w^Tx)^2+\frac{\sigma^2}{\sigma_0^2}w^Tw]
\end{aligned}
\]</span><br />
这里省略了 <span class="math inline">\(X\)</span>，<span
class="math inline">\(p(Y)\)</span>和 <span
class="math inline">\(w\)</span> 没有关系，同时也利用了上面高斯分布的
MLE的结果。</p>
<p>我们将会看到，超参数 <span
class="math inline">\(\sigma_0\)</span>的存在和下面会介绍的 Ridge
正则项可以对应，同样的如果将先验分布取为 Laplace 分布，那么就会得到和 L1
正则类似的结果。</p>
<h2 id="正则化">正则化</h2>
<p>在实际应用时，如果样本容量不远远大于样本的特征维度，很可能造成过拟合，对这种情况，我们有下面三个解决方式：</p>
<ol type="1">
<li>加数据</li>
<li>特征选择（降低特征维度）如 PCA 算法。</li>
<li>正则化</li>
</ol>
<p>正则化一般是在损失函数（如上面介绍的最小二乘损失）上加入正则化项（表示模型的复杂度对模型的惩罚），下面我们介绍一般情况下的两种正则化框架。<br />
<span class="math display">\[
\begin{aligned}
L1&amp;:\mathop{argmin}\limits_wL(w)+\lambda||w||_1,\lambda\gt0\\
L2&amp;:\mathop{argmin}\limits_wL(w)+\lambda||w||^2_2,\lambda \gt 0
\end{aligned}
\]</span><br />
下面对最小二乘误差分别分析这两者的区别。</p>
<h3 id="l1-lasso">L1 Lasso</h3>
<p>L1正则化可以引起稀疏解。</p>
<p>从最小化损失的角度看，由于 L1
项求导在0附近的左右导数都不是0，因此更容易取到0解。</p>
<p>从另一个方面看，L1 正则化相当于：<br />
<span class="math display">\[
\mathop{argmin}\limits_wL(w)\\
s.t. ||w||_1\lt C
\]</span><br />
我们已经看到平方误差损失函数在 <span class="math inline">\(w\)</span>
空间是一个椭球，因此上式求解就是椭球和 <span
class="math inline">\(||w||_1=C\)</span>的切点，因此更容易相切在坐标轴上。</p>
<h3 id="l2-ridge">L2 Ridge</h3>
<p><span class="math display">\[
\begin{aligned}
\hat{w}=\mathop{argmin}\limits_wL(w)+\lambda
w^Tw&amp;\longrightarrow\frac{\partial}{\partial w}L(w)+2\lambda w=0\\
&amp;\longrightarrow2X^TX\hat{w}-2X^TY+2\lambda \hat w=0\\
&amp;\longrightarrow \hat{w}=(X^TX+\lambda \mathbb{I})^{-1}X^TY
\end{aligned}
\]</span></p>
<p>可以看到，这个正则化参数和前面的 MAP
结果不谋而合。利用2范数进行正则化不仅可以是模型选择 <span
class="math inline">\(w\)</span> 较小的参数，同时也避免
$ X^TX$不可逆的问题。</p>
<h2 id="小结">小结</h2>
<p>线性回归模型是最简单的模型，但是麻雀虽小，五脏俱全，在这里，我们利用最小二乘误差得到了闭式解。同时也发现，在噪声为高斯分布的时候，MLE
的解等价于最小二乘误差，而增加了正则项后，最小二乘误差加上 L2
正则项等价于高斯噪声先验下的 MAP解，加上 L1 正则项后，等价于 Laplace
噪声先验。</p>
<p>传统的机器学习方法或多或少都有线性回归模型的影子：</p>
<ol type="1">
<li>线性模型往往不能很好地拟合数据，因此有三种方案克服这一劣势：
<ol type="1">
<li>对特征的维数进行变换，例如多项式回归模型就是在线性特征的基础上加入高次项。</li>
<li>在线性方程后面加入一个非线性变换，即引入一个非线性的激活函数，典型的有线性分类模型如感知机。</li>
<li>对于一致的线性系数，我们进行多次变换，这样同一个特征不仅仅被单个系数影响，例如多层感知机（深度前馈网络）。</li>
</ol></li>
<li>线性回归在整个样本空间都是线性的，我们修改这个限制，在不同区域引入不同的线性或非线性，例如线性样条回归和决策树模型。</li>
<li>线性回归中使用了所有的样本，但是对数据预先进行加工学习的效果可能更好（所谓的维数灾难，高维度数据更难学习），例如
PCA 算法和流形学习。</li>
</ol>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://soundmemories.github.io/2020/09/21/ML/17.MathBasics/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avstar.png">
      <meta itemprop="name" content="SoundMemories">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="SoundMemories">
      <meta itemprop="description" content="今日事，今日毕">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | SoundMemories">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2020/09/21/ML/17.MathBasics/" class="post-title-link" itemprop="url">MathBasics</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2020-09-21 00:00:00" itemprop="dateCreated datePublished" datetime="2020-09-21T00:00:00+08:00">2020-09-21</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/ML/" itemprop="url" rel="index"><span itemprop="name">ML</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>1.9k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>7 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2 id="高斯分布">高斯分布</h2>
<h3 id="一维情况-mle">一维情况 MLE</h3>
<p>高斯分布在机器学习中占有举足轻重的作用。在 MLE 方法中：</p>
<p><span class="math display">\[
\theta=(\mu,\Sigma)=(\mu,\sigma^{2}),\theta_{MLE}=\mathop{argmax}\limits
_{\theta}\log p(X|\theta)\mathop{=}\limits _{iid}\mathop{argmax}\limits
_{\theta}\sum\limits _{i=1}^{N}\log p(x_{i}|\theta)
\]</span><br />
一般地，高斯分布的概率密度函数PDF写为：</p>
<p><span class="math display">\[
p(x|\mu,\Sigma)=\frac{1}{(2\pi)^{p/2}|\Sigma|^{1/2}}e^{-\frac{1}{2}(x-\mu)^{T}\Sigma^{-1}(x-\mu)}
\]</span><br />
带入 MLE 中我们考虑一维的情况</p>
<p><span class="math display">\[
\log p(X|\theta)=\sum\limits _{i=1}^{N}\log p(x_{i}|\theta)=\sum\limits
_{i=1}^{N}\log\frac{1}{\sqrt{2\pi}\sigma}\exp(-(x_{i}-\mu)^{2}/2\sigma^{2})
\]</span><br />
首先对 <span class="math inline">\(\mu\)</span> 的极值可以得到 ：<br />
<span class="math display">\[
\mu_{MLE}=\mathop{argmax}\limits _{\mu}\log
p(X|\theta)=\mathop{argmax}\limits _{\mu}\sum\limits
_{i=1}^{N}(x_{i}-\mu)^{2}
\]</span><br />
于是：<br />
<span class="math display">\[
\frac{\partial}{\partial\mu}\sum\limits
_{i=1}^{N}(x_{i}-\mu)^{2}=0\longrightarrow\mu_{MLE}=\frac{1}{N}\sum\limits
_{i=1}^{N}x_{i}
\]</span><br />
其次对 <span class="math inline">\(\theta\)</span> 中的另一个参数 <span
class="math inline">\(\sigma\)</span> ，有：<br />
<span class="math display">\[
\begin{aligned}
\sigma_{MLE}=\mathop{argmax}\limits _{\sigma}\log
p(X|\theta)&amp;=\mathop{argmax}\limits _{\sigma}\sum\limits
_{i=1}^{N}[-\log\sigma-\frac{1}{2\sigma^{2}}(x_{i}-\mu)^{2}]\\&amp;=\mathop{argmin}\limits
_{\sigma}\sum\limits
_{i=1}^{N}[\log\sigma+\frac{1}{2\sigma^{2}}(x_{i}-\mu)^{2}]
\end{aligned}
\]</span><br />
于是：<br />
<span class="math display">\[
\frac{\partial}{\partial\sigma}\sum\limits
_{i=1}^{N}[\log\sigma+\frac{1}{2\sigma^{2}}(x_{i}-\mu)^{2}]=0\longrightarrow\sigma_{MLE}^{2}=\frac{1}{N}\sum\limits
_{i=1}^{N}(x_{i}-\mu)^{2}
\]</span><br />
值得注意的是，上面的推导中，首先对 <span
class="math inline">\(\mu\)</span> 求 MLE， 然后利用这个结果求 <span
class="math inline">\(\sigma_{MLE}\)</span>
，因此可以预期的是对数据集求期望时 <span
class="math inline">\(\mathbb{E}_{\mathcal{D}}[\mu_{MLE}]\)</span>
是无偏差的：<br />
<span class="math display">\[
\mathbb{E}_{\mathcal{D}}[\mu_{MLE}]=\mathbb{E}_{\mathcal{D}}[\frac{1}{N}\sum\limits
_{i=1}^{N}x_{i}]=\frac{1}{N}\sum\limits
_{i=1}^{N}\mathbb{E}_{\mathcal{D}}[x_{i}]=\mu
\]</span><br />
但是当对 <span class="math inline">\(\sigma_{MLE}\)</span> 求
期望的时候由于使用了单个数据集的 <span
class="math inline">\(\mu_{MLE}\)</span>，因此对所有数据集求期望的时候我们会发现
<span class="math inline">\(\sigma_{MLE}\)</span> 是 有偏的：</p>
<p><span class="math display">\[
\begin{aligned}
\mathbb{E}_{\mathcal{D}}[\sigma_{MLE}^{2}]&amp;=\mathbb{E}_{\mathcal{D}}[\frac{1}{N}\sum\limits
_{i=1}^{N}(x_{i}-\mu_{MLE})^{2}]=\mathbb{E}_{\mathcal{D}}[\frac{1}{N}\sum\limits
_{i=1}^{N}(x_{i}^{2}-2x_{i}\mu_{MLE}+\mu_{MLE}^{2})
\\&amp;=\mathbb{E}_{\mathcal{D}}[\frac{1}{N}\sum\limits
_{i=1}^{N}x_{i}^{2}-\mu_{MLE}^{2}]=\mathbb{E}_{\mathcal{D}}[\frac{1}{N}\sum\limits
_{i=1}^{N}x_{i}^{2}-\mu^{2}+\mu^{2}-\mu_{MLE}^{2}]\\
&amp;= \mathbb{E}_{\mathcal{D}}[\frac{1}{N}\sum\limits
_{i=1}^{N}x_{i}^{2}-\mu^{2}]-\mathbb{E}_{\mathcal{D}}[\mu_{MLE}^{2}-\mu^{2}]=\sigma^{2}-(\mathbb{E}_{\mathcal{D}}[\mu_{MLE}^{2}]-\mu^{2})\\&amp;=\sigma^{2}-(\mathbb{E}_{\mathcal{D}}[\mu_{MLE}^{2}]-\mathbb{E}_{\mathcal{D}}^{2}[\mu_{MLE}])=\sigma^{2}-Var[\mu_{MLE}]\\&amp;=\sigma^{2}-Var[\frac{1}{N}\sum\limits
_{i=1}^{N}x_{i}]=\sigma^{2}-\frac{1}{N^{2}}\sum\limits
_{i=1}^{N}Var[x_{i}]=\frac{N-1}{N}\sigma^{2}
\end{aligned}
\]</span><br />
所以：<br />
<span class="math display">\[
\hat{\sigma}^{2}=\frac{1}{N-1}\sum\limits _{i=1}^{N}(x_{i}-\mu)^{2}
\]</span></p>
<h3 id="多维情况">多维情况</h3>
<p>多维高斯分布表达式为：<br />
<span class="math display">\[
p(x|\mu,\Sigma)=\frac{1}{(2\pi)^{p/2}|\Sigma|^{1/2}}e^{-\frac{1}{2}(x-\mu)^{T}\Sigma^{-1}(x-\mu)}
\]</span><br />
其中 <span
class="math inline">\(x,\mu\in\mathbb{R}^{p},\Sigma\in\mathbb{R}^{p\times
p}\)</span> ，<span class="math inline">\(\Sigma\)</span>
为协方差矩阵，一般而言也是半正定矩阵。这里我们只考虑正定矩阵。首先我们处理指数上的数字，指数上的数字可以记为
<span class="math inline">\(x\)</span> 和 <span
class="math inline">\(\mu\)</span>
之间的马氏距离。对于对称的协方差矩阵可进行特征值分解，<span
class="math inline">\(\Sigma=U\Lambda
U^{T}=(u_{1},u_{2},\cdots,u_{p})diag(\lambda_{i})(u_{1},u_{2},\cdots,u_{p})^{T}=\sum\limits
_{i=1}^{p}u_{i}\lambda_{i}u_{i}^{T}\)</span> ，于是：</p>
<p><span class="math display">\[
\Sigma^{-1}=\sum\limits _{i=1}^{p}u_{i}\frac{1}{\lambda_{i}}u_{i}^{T}
\]</span></p>
<p><span class="math display">\[
\Delta=(x-\mu)^{T}\Sigma^{-1}(x-\mu)=\sum\limits
_{i=1}^{p}(x-\mu)^{T}u_{i}\frac{1}{\lambda_{i}}u_{i}^{T}(x-\mu)=\sum\limits
_{i=1}^{p}\frac{y_{i}^{2}}{\lambda_{i}}
\]</span></p>
<p>我们注意到 <span class="math inline">\(y_{i}\)</span> 是 <span
class="math inline">\(x-\mu\)</span> 在特征向量 <span
class="math inline">\(u_{i}\)</span> 上的投影长度，因此上式子就是 <span
class="math inline">\(\Delta\)</span> 取不同值时的同心椭圆。</p>
<p>下面我们看多维高斯模型在实际应用时的两个问题</p>
<ol type="1">
<li><p>参数 <span class="math inline">\(\Sigma,\mu\)</span> 的自由度为
<span class="math inline">\(O(p^{2})\)</span>
对于维度很高的数据其自由度太高。解决方案：高自由度的来源是 <span
class="math inline">\(\Sigma\)</span> 有 <span
class="math inline">\(\frac{p(p+1)}{2}\)</span>
个自由参数，可以假设其是对角矩阵，甚至在各向同性假设中假设其对角线上的元素都相同。前一种的算法有
Factor Analysis，后一种有概率 PCA(p-PCA) 。</p></li>
<li><p>第二个问题是单个高斯分布是单峰的，对有多个峰的数据分布不能得到好的结果。解决方案：高斯混合GMM
模型。</p></li>
</ol>
<p>下面对多维高斯分布的常用定理进行介绍。</p>
<p>我们记 <span class="math inline">\(x=(x_1,
x_2,\cdots,x_p)^T=(x_{a,m\times 1},
x_{b,n\times1})^T,\mu=(\mu_{a,m\times1},
\mu_{b,n\times1}),\Sigma=\begin{pmatrix}\Sigma_{aa}&amp;\Sigma_{ab}\\\Sigma_{ba}&amp;\Sigma_{bb}\end{pmatrix}\)</span>，已知
<span class="math inline">\(x\sim\mathcal{N}(\mu,\Sigma)\)</span>。</p>
<p>首先是一个高斯分布的定理：</p>
<blockquote>
<p>定理：已知 <span class="math inline">\(x\sim\mathcal{N}(\mu,\Sigma),
y\sim Ax+b\)</span>，那么 <span
class="math inline">\(y\sim\mathcal{N}(A\mu+b, A\Sigma
A^T)\)</span>。</p>
<p>证明：<span
class="math inline">\(\mathbb{E}[y]=\mathbb{E}[Ax+b]=A\mathbb{E}[x]+b=A\mu+b\)</span>，<span
class="math inline">\(Var[y]=Var[Ax+b]=Var[Ax]=A\cdot Var[x]\cdot
A^T\)</span>。</p>
</blockquote>
<p>下面利用这个定理得到 <span
class="math inline">\(p(x_a),p(x_b),p(x_a|x_b),p(x_b|x_a)\)</span>
这四个量。</p>
<ol type="1">
<li><p><span
class="math inline">\(x_a=\begin{pmatrix}\mathbb{I}_{m\times
m}&amp;\mathbb{O}_{m\times
n}\end{pmatrix}\begin{pmatrix}x_a\\x_b\end{pmatrix}\)</span>，代入定理中得到：<br />
<span class="math display">\[
\mathbb{E}[x_a]=\begin{pmatrix}\mathbb{I}&amp;\mathbb{O}\end{pmatrix}\begin{pmatrix}\mu_a\\\mu_b\end{pmatrix}=\mu_a\\
Var[x_a]=\begin{pmatrix}\mathbb{I}&amp;\mathbb{O}\end{pmatrix}\begin{pmatrix}\Sigma_{aa}&amp;\Sigma_{ab}\\\Sigma_{ba}&amp;\Sigma_{bb}\end{pmatrix}\begin{pmatrix}\mathbb{I}\\\mathbb{O}\end{pmatrix}=\Sigma_{aa}
\]</span><br />
所以 <span
class="math inline">\(x_a\sim\mathcal{N}(\mu_a,\Sigma_{aa})\)</span>。</p></li>
<li><p>同样的，<span
class="math inline">\(x_b\sim\mathcal{N}(\mu_b,\Sigma_{bb})\)</span>。</p></li>
<li><p>对于两个条件概率，我们引入三个量：<br />
<span class="math display">\[
x_{b\cdot a}=x_b-\Sigma_{ba}\Sigma_{aa}^{-1}x_a\\
\mu_{b\cdot a}=\mu_b-\Sigma_{ba}\Sigma_{aa}^{-1}\mu_a\\
\Sigma_{bb\cdot a}=\Sigma_{bb}-\Sigma_{ba}\Sigma_{aa}^{-1}\Sigma_{ab}
\]</span><br />
特别的，最后一个式子叫做 <span
class="math inline">\(\Sigma_{bb}\)</span> 的 Schur
Complementary。可以看到：<br />
<span class="math display">\[
x_{b\cdot
a}=\begin{pmatrix}-\Sigma_{ba}\Sigma_{aa}^{-1}&amp;\mathbb{I}_{n\times
n}\end{pmatrix}\begin{pmatrix}x_a\\x_b\end{pmatrix}
\]</span><br />
所以：<br />
<span class="math display">\[
\mathbb{E}[x_{b\cdot
a}]=\begin{pmatrix}-\Sigma_{ba}\Sigma_{aa}^{-1}&amp;\mathbb{I}_{n\times
n}\end{pmatrix}\begin{pmatrix}\mu_a\\\mu_b\end{pmatrix}=\mu_{b\cdot a}\\
Var[x_{b\cdot
a}]=\begin{pmatrix}-\Sigma_{ba}\Sigma_{aa}^{-1}&amp;\mathbb{I}_{n\times
n}\end{pmatrix}\begin{pmatrix}\Sigma_{aa}&amp;\Sigma_{ab}\\\Sigma_{ba}&amp;\Sigma_{bb}\end{pmatrix}\begin{pmatrix}-\Sigma_{aa}^{-1}\Sigma_{ba}^T\\\mathbb{I}_{n\times
n}\end{pmatrix}=\Sigma_{bb\cdot a}
\]</span><br />
利用这三个量可以得到 <span class="math inline">\(x_b=x_{b\cdot
a}+\Sigma_{ba}\Sigma_{aa}^{-1}x_a\)</span>。因此：<br />
<span class="math display">\[
\mathbb{E}[x_b|x_a]=\mu_{b\cdot a}+\Sigma_{ba}\Sigma_{aa}^{-1}x_a
\]</span></p>
<p><span class="math display">\[
Var[x_b|x_a]=\Sigma_{bb\cdot a}
\]</span></p>
<p>这里同样用到了定理。</p></li>
<li><p>同样：<br />
<span class="math display">\[
x_{a\cdot b}=x_a-\Sigma_{ab}\Sigma_{bb}^{-1}x_b\\
\mu_{a\cdot b}=\mu_a-\Sigma_{ab}\Sigma_{bb}^{-1}\mu_b\\
\Sigma_{aa\cdot b}=\Sigma_{aa}-\Sigma_{ab}\Sigma_{bb}^{-1}\Sigma_{ba}
\]</span><br />
所以：<br />
<span class="math display">\[
\mathbb{E}[x_a|x_b]=\mu_{a\cdot b}+\Sigma_{ab}\Sigma_{bb}^{-1}x_b
\]</span></p>
<p><span class="math display">\[
Var[x_a|x_b]=\Sigma_{aa\cdot b}
\]</span></p></li>
</ol>
<p>下面利用上边四个量，求解线性模型：</p>
<blockquote>
<p>已知：<span
class="math inline">\(p(x)=\mathcal{N}(\mu,\Lambda^{-1}),p(y|x)=\mathcal{N}(Ax+b,L^{-1})\)</span>，求解：<span
class="math inline">\(p(y),p(x|y)\)</span>。</p>
<p>解：令 <span
class="math inline">\(y=Ax+b+\epsilon,\epsilon\sim\mathcal{N}(0,L^{-1})\)</span>，所以
<span
class="math inline">\(\mathbb{E}[y]=\mathbb{E}[Ax+b+\epsilon]=A\mu+b\)</span>，<span
class="math inline">\(Var[y]=A
\Lambda^{-1}A^T+L^{-1}\)</span>，因此：<br />
<span class="math display">\[
  p(y)=\mathcal{N}(A\mu+b,L^{-1}+A\Lambda^{-1}A^T)
  \]</span><br />
引入 <span
class="math inline">\(z=\begin{pmatrix}x\\y\end{pmatrix}\)</span>，我们可以得到
<span
class="math inline">\(Cov[x,y]=\mathbb{E}[(x-\mathbb{E}[x])(y-\mathbb{E}[y])^T]\)</span>。对于这个协方差可以直接计算：<br />
<span class="math display">\[
  \begin{aligned}Cov(x,y)&amp;=\mathbb{E}[(x-\mu)(Ax-A\mu+\epsilon)^T]=\mathbb{E}[(x-\mu)(x-\mu)^TA^T]=Var[x]A^T=\Lambda^{-1}A^T\end{aligned}
\]</span><br />
注意到协方差矩阵的对称性，所以 <span
class="math inline">\(p(z)=\mathcal{N}\begin{pmatrix}\mu\\A\mu+b\end{pmatrix},\begin{pmatrix}\Lambda^{-1}&amp;\Lambda^{-1}A^T\\A\Lambda^{-1}&amp;L^{-1}+A\Lambda^{-1}A^T\end{pmatrix})\)</span>。根据之前的公式，我们可以得到：<br />
<span class="math display">\[
  \mathbb{E}[x|y]=\mu+\Lambda^{-1}A^T(L^{-1}+A\Lambda^{-1}A^T)^{-1}(y-A\mu-b)
  \]</span></p>
<p><span class="math display">\[
  Var[x|y]=\Lambda^{-1}-\Lambda^{-1}A^T(L^{-1}+A\Lambda^{-1}A^T)^{-1}A\Lambda^{-1}
  \]</span></p>
</blockquote>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://soundmemories.github.io/2020/09/20/ML/16.Introduction/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avstar.png">
      <meta itemprop="name" content="SoundMemories">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="SoundMemories">
      <meta itemprop="description" content="今日事，今日毕">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | SoundMemories">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2020/09/20/ML/16.Introduction/" class="post-title-link" itemprop="url">Introduction</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2020-09-20 00:00:00" itemprop="dateCreated datePublished" datetime="2020-09-20T00:00:00+08:00">2020-09-20</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/ML/" itemprop="url" rel="index"><span itemprop="name">ML</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>657</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>2 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>对概率的诠释有两大学派，一种是频率派另一种是贝叶斯派。后面我们对观测集采用下面记号：<br />
<span class="math display">\[
X_{N\times
p}=(x_{1},x_{2},\cdots,x_{N})^{T},x_{i}=(x_{i1},x_{i2},\cdots,x_{ip})^{T}
\]</span><br />
这个记号表示有 <span class="math inline">\(N\)</span>
个样本，每个样本都是 <span class="math inline">\(p\)</span>
维向量。其中每个观测都是由 <span
class="math inline">\(p(x|\theta)\)</span> 生成的。</p>
<h1 id="频率派的观点">频率派的观点</h1>
<p>频率派认为 <span class="math inline">\(p(x|\theta)\)</span> 中的
<span class="math inline">\(\theta\)</span> 是一个常量。对于 <span
class="math inline">\(N\)</span> 个观测来说观测集的概率为 <span
class="math inline">\(p(X|\theta)\mathop{=}\limits _{iid}\prod\limits
_{i=1}^{N}p(x_{i}|\theta))\)</span> 。为了求 <span
class="math inline">\(\theta\)</span>
的大小，我们采用最大对数似然MLE的方法：</p>
<p><span class="math display">\[
\theta_{MLE}=\mathop{argmax}\limits _{\theta}\log
p(X|\theta)\mathop{=}\limits _{iid}\mathop{argmax}\limits
_{\theta}\sum\limits _{i=1}^{N}\log p(x_{i}|\theta)
\]</span></p>
<h1 id="贝叶斯派的观点">贝叶斯派的观点</h1>
<p>贝叶斯派认为 <span class="math inline">\(p(x|\theta)\)</span> 中的
<span class="math inline">\(\theta\)</span> 不是一个常量，而是和 <span
class="math inline">\(x\)</span> 一样为一个变量。这个 <span
class="math inline">\(\theta\)</span> 满足一个预设的先验的分布 <span
class="math inline">\(\theta\sim p(\theta)\)</span>
。于是根据贝叶斯定理依赖观测集参数的后验可以写成：</p>
<p><span class="math display">\[
p(\theta|X)=\frac{p(X|\theta)\cdot
p(\theta)}{p(X)}=\frac{p(X|\theta)\cdot p(\theta)}{\int\limits
_{\theta}p(X|\theta)\cdot p(\theta)d\theta}
\]</span><br />
为了求 <span class="math inline">\(\theta\)</span>
的值，我们要最大化这个参数后验MAP：</p>
<p><span class="math display">\[
\theta_{MAP}=\mathop{argmax}\limits
_{\theta}p(\theta|X)=\mathop{argmax}\limits _{\theta}p(X|\theta)\cdot
p(\theta)
\]</span><br />
其中第二个等号是由于分母和 <span class="math inline">\(\theta\)</span>
没有关系。求解这个 <span class="math inline">\(\theta\)</span>
值后计算<span class="math inline">\(\frac{p(X|\theta)\cdot
p(\theta)}{\int\limits _{\theta}p(X|\theta)\cdot
p(\theta)d\theta}\)</span> ，就得到了参数的后验概率。其中 <span
class="math inline">\(p(X|\theta)\)</span>
叫似然，是我们的模型分布。得到了参数的后验分布后，我们可以将这个分布用于预测贝叶斯预测：<br />
<span class="math display">\[
p(x_{new}|X)=\int\limits _{\theta}p(x_{new}|\theta)\cdot
p(\theta|X)d\theta
\]</span><br />
其中积分中的被乘数是模型，乘数是后验分布。</p>
<h1 id="小结">小结</h1>
<p>频率派和贝叶斯派分别给出了一系列的机器学习算法。频率派的观点导出了一系列的统计机器学习算法，而贝叶斯派导出了概率图理论。在应用频率派的
MLE
方法时最优化理论占有重要地位。而贝叶斯派的算法无论是后验概率的建模还是应用这个后验进行推断时积分占有重要地位。因此采样积分方法如
MCMC 有很多应用。</p>
<p>如果对此有疑问的，可以参考知乎这篇文章：<span class="exturl" data-url="aHR0cHM6Ly96aHVhbmxhbi56aGlodS5jb20vcC8zMjQ4MDgxMA==">聊一聊机器学习的MLE和MAP：最大似然估计和最大后验估计<i class="fa fa-external-link-alt"></i></span>。</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://soundmemories.github.io/2020/06/20/ML/13.ML-%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avstar.png">
      <meta itemprop="name" content="SoundMemories">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="SoundMemories">
      <meta itemprop="description" content="今日事，今日毕">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | SoundMemories">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2020/06/20/ML/13.ML-%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/" class="post-title-link" itemprop="url">ML-模型评估</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2020-06-20 00:00:00" itemprop="dateCreated datePublished" datetime="2020-06-20T00:00:00+08:00">2020-06-20</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/ML/" itemprop="url" rel="index"><span itemprop="name">ML</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>4.9k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>18 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="常用词">常用词</h1>
<p><strong>错误率</strong> ：分类错误的样本数占总样本数的比例。<br />
<strong>精度</strong>：1-错误率。</p>
<p><strong>训练误差/经验误差</strong>
：机器学习器在训练集上的误差。<br />
<strong>泛化误差</strong> ：机器学习器在新样本上的误差。</p>
<p><strong>过拟合</strong>：我们希望学习器在新样本下也能有好的表现，因此应该从训练样本中尽可能学出适用于所有潜在潜在样本的“普遍规律”，然而有时候学习器把训练样本学得“太好了”，很可能把训练样本自身的一些特点当作了所有潜在样本都具有的一般性质，这样就会导致泛化能力下降，这就称为过拟合。<br />
<strong>欠拟合</strong>：和过拟合相对，即对训练样本的一般性质尚未学好。</p>
<p><strong>生成模型与判别模型</strong>：<br />
<strong>生成模型</strong>：学习得到联合概率分布<span
class="math inline">\(P(x,y)\)</span>，即特征x和标记y共同出现的概率，然后求条件概率分布。能够学习到数据生成的机制。通过求输入与输出的<strong>联合概率分布</strong>，再求解类别归类的概率。比如，朴素贝叶斯生成的模型。生成模型，对数据要求较高（比如朴素贝叶斯要求数据是离散的），速度会快些。<br />
<strong>判别模型</strong>：学习得到条件概率分布<span
class="math inline">\(P(y|x)\)</span>，即在特征x出现的情况下标记y出现的概率。直接获得输出对应最大分类的概率。比如，KNN。判别模型，要求较低，有很好的容忍度，但速度会慢些。</p>
<h1 id="数据划分">数据划分</h1>
<p><strong>训练集</strong>：训练和拟合模型。提高训练集是好的，但比例不能变。<br />
<strong>验证集</strong>：当通过训练集训练出多个模型后，使用验证集纠偏或比较预测。<br />
<strong>测试集</strong>：模型的泛化能力考量。<br />
<strong>测试集</strong>应该尽可能与<strong>训练集</strong>互斥，即测试样本尽量不在训练集中出现，未在训练过程中使用过。</p>
<p>一般情况下：<strong>训练集:测试集:验证集=6：2：2</strong>。<br />
有时候会忽略验证集，而通过 训练集-测试集
不断的尝试来达到目的，此时训练集、测试集比例一般为8：2。</p>
<h2 id="留出法hold-out">留出法（hold-out）</h2>
<p>直接将数据集划分为两个互斥集合，一个作为训练，一个作为测试。</p>
<p>注意点：<br />
1. <strong>保持数据分布一致性</strong>
：如保留类别比例一致的分层采样。<br />
2. <strong>多次重复划分</strong>
：单次使用留出法得到的估计结果往往不够稳定可靠，一般要采用若干次随机划分、重复进行实验评估后取平均值作为留出法的结果。<br />
3.
<strong>训练集和测试集比例问题</strong>：在划分时，如果训练集包含绝大多数样本，则训练出来的模型更接近于未划分的数据集训练出的模型，由于测试集比较小，评估的结果可能不够准确；若测试集包含的样本多些，则训练集和未划分的数据集差别更大，这样训练出的模型偏离初衷，而且降低评估结果的保真性。常见的作法是2/3<sub>4/5的样本用于训练，1/3</sub>1/5的样本用于测试。</p>
<h2 id="k折交叉验证k-cross-validation">K折交叉验证（k-cross
validation）</h2>
<p><img src="/images/模型评估/K折交叉验证.png" width="80%"></p>
<p>每个子集<span
class="math inline">\(D_i\)</span>都尽可能的保持数据分布的一致性，即从<span
class="math inline">\(D\)</span>中通过分层采样得到。<br />
最终返回的是这k个测试结果的均值，所以交叉验证评估结果的稳定性和保真性很大程度取决于k的取值。</p>
<p><strong>常用的k值为5，10。</strong><br />
<strong>为减少因样本划分不同而引入的差别，<span
class="math inline">\(k\)</span>折交叉验证通常要随机使用不同的划分重复<span
class="math inline">\(p\)</span>次，最终的评估结果是这<span
class="math inline">\(p\)</span>次<span
class="math inline">\(k\)</span>折交叉验证结果的均值，例如常见的有10次10折交叉验证。</strong></p>
<p><strong>当数据集<span class="math inline">\(D\)</span>中包含<span
class="math inline">\(m\)</span>个样本，若令<span
class="math inline">\(k=m\)</span>，则得到了交叉验证法的一个特例，留一法（Leave-One-Out）。</strong><br />
即<span
class="math inline">\(m\)</span>个样本只有唯一的方式划分为m个子集——每个子集包含一个样本；留一法使用的训练集与初始数据集相比只少了一个样本，这就使得在绝大多数情况下，留一法被实际评估的模型与期望评估的用<span
class="math inline">\(D\)</span>训练出的模型很相似，因此留一法的评估结果往往被认为比较准确。<br />
然而，留一法也有其缺陷：在数据集比较大的时候，计算开销非常大。</p>
<h2 id="自助法boostrapping">自助法（boostrapping）</h2>
<p>不管是留出法还是交叉验证法，都保留了一部分样本用于测试，因此实际评估的模型所使用的训练集比原始数据集<span
class="math inline">\(D\)</span>小，这必然会引入一些<strong>因训练样本规模不同而导致的估计偏差</strong>。留一法受训练样本规模变化的影响较小，但计算复杂度太高。<strong>有没有办法可以减少训练样本规模不同造成的影响，同时还能比较高效地进行实验估计呢？</strong>————<strong>自助法（boostrapping）</strong>。</p>
<p><strong>自助法（boostrapping）直接以自助采样法为基础。给定包含<span
class="math inline">\(m\)</span>个样本的数据集<span
class="math inline">\(D\)</span>，对它进行采样产生数据集<span
class="math inline">\(D^\prime\)</span>：每次随机从<span
class="math inline">\(D\)</span>中挑选一个样本，将其拷贝放入<span
class="math inline">\(D^\prime\)</span>，然后再将该样本放回初始数据集<span
class="math inline">\(D\)</span>中，使得该样本在下次采样时仍可能被采到；这个过程重复执行<span
class="math inline">\(m\)</span>次后，我们就得到了包含<span
class="math inline">\(m\)</span>个样本的数据集<span
class="math inline">\(D^\prime\)</span>（和原数据集<span
class="math inline">\(D\)</span>同规模），这就是自助采样的结果，显然<span
class="math inline">\(D\)</span>中有一部分样本会在<span
class="math inline">\(D^\prime\)</span>中多次出现，而另一部分样本不出现，可以做一个简单估计，样本<span
class="math inline">\(m\)</span>次采样中始种不被采到的概率是<span
class="math inline">\((1-\dfrac{1}{m})^{m}\)</span>，取极限得到：<span
class="math inline">\(\lim\limits_{m \to \infty}(1-\dfrac{1}{m})^{m} \to
\dfrac{1}{e} \approx 0.368\)</span>，即初始数据集<span
class="math inline">\(D\)</span>中约有<span
class="math inline">\(36.8\%\)</span> 的样本未出现在采样数据集<span
class="math inline">\(D^\prime\)</span>中，可以使用这些样本作为测试集（约为总量1/3），<span
class="math inline">\(D^\prime\)</span>作为训练集，这样的测试结果亦称为包外估计（out-of-bagestimate）</strong></p>
<p><strong>优点：自助法在数据集较小、难以有效划分训练/测试集时很有效。</strong><br />
<strong>缺点：自助法产生的数据集改变了初始数据集的分布，这会引入估计偏差，因此在数据量足够时，使用留出法/交叉验证更好</strong></p>
<h1 id="模型评估">模型评估</h1>
<p>主要为：分类模型、回归模型、聚类模型、关联模型的评估。</p>
<h2 id="分类模型">分类模型</h2>
<h3 id="混淆矩阵">混淆矩阵</h3>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<table>
<tr>
<td rowspan="2">
真实情况
</td>
<td colspan="2">
预测结果　　　　　
</td>
</tr>
<tr>
<td>
正例　　
</td>
<td>
反例　　
</td>
</tr>
<tr>
<td>
正例
</td>
<td>
TP（真正例）
</td>
<td>
FN（假反例）
</td>
</tr>
<tr>
<td>
反例
</td>
<td>
FP（假正例）
</td>
<td>
TN（真反例）
</td>
</tr>
</table>
<p><strong>准确率（Accuracy）</strong>：<span
class="math inline">\(\dfrac{TP+TN}{TP+FP+FN+TN}\)</span>，注意只用准确率不行，比如样本极度不平衡标签0：1的比例为99：1，而模型只要是输入就判别为0，准确率还是能达到99%，所以只用准确率判别是不好的。</p>
<p><strong>精确率（Precision）</strong>：<span
class="math inline">\(P=\dfrac{TP}{TP+FP}\)</span>，预测结果正类中，正确的程度。</p>
<p><strong>召回率/真正率（Recall/TPR）</strong>：<span
class="math inline">\(R=\dfrac{TP}{TP+FN}\)</span>，真实情况正类中，预测出的占比。</p>
<p><strong>假正率（False Positive Rate/FPR）：</strong><span
class="math inline">\(\dfrac{FP}{FP+TN}\)</span>，真实负类中，被预测为正类的比例。</p>
<p><strong>错误拒绝率（False Rejection Rate/FRR）：</strong><span
class="math inline">\(\dfrac{FN}{TP+FN}\)</span>，真实正类中，被预测为负类的比例。</p>
<p><strong><span class="math inline">\(F1\)</span>-score</strong>：<span
class="math inline">\(\dfrac{2\ast P\ast R}{P + R}=\dfrac{2\ast
TP}{\text{样例总数}+TP-TN}\)</span>，F1充分考量了精确率和召回率，比单独使用准确率更好。</p>
<p><strong><span
class="math inline">\(F_\beta\)</span>-score</strong>：<span
class="math inline">\(\dfrac{(1+\beta^2)\ast P\ast R}{\beta^2\ast
P+R}\)</span>，F1的一般形式，<span class="math inline">\(\beta
&gt;1\)</span>时召回率有更大的影响；<span
class="math inline">\(\beta&lt;1\)</span>时精确率有更大影响。</p>
<h3 id="多元混淆矩阵">多元混淆矩阵</h3>
<p>在多次训练/测试后会有多个二分类混淆矩阵，或者在多个数据集上训练/测试后希望估计算法全局性能，或者执行多分类任务，每两两类别的组合都能对应一个混淆矩阵，总之我们希望在n个二分类混淆矩阵上综合考察精确率和召回率。</p>
<p>一种直接的做法是先在各混淆矩阵上分别计算出精确率和召回率，记为<span
class="math inline">\((P_1,R_1),(P_2,R_2),...,(P_n,R_n)\)</span>，再计算平均值，这样就得到<strong>宏精确率</strong>（macro-P）和<strong>宏召回率</strong>（macro-R），以及相应的<strong>宏F1</strong>（macro-F1）。<br />
<strong>macro-P</strong>：<span
class="math inline">\(\dfrac{1}{n}\sum\limits_{i=1}^NP_i\)</span></p>
<p><strong>macro-R</strong>：<span
class="math inline">\(\dfrac{1}{n}\sum\limits_{i=1}^NR_i\)</span></p>
<p><strong>macro-F1</strong>：<span class="math inline">\(\dfrac{2\times
\text{macro-P}\times\text{macro-R}}{\text{macro-P}+\text{macro-R}}\)</span></p>
<p>另一种是，先将混淆矩阵的对应元素进行平均，得到<span
class="math inline">\(TP\)</span>、<span
class="math inline">\(FP\)</span>、<span
class="math inline">\(TN\)</span>、<span
class="math inline">\(FN\)</span>的平均值，分别记为<span
class="math inline">\(\overline{TP}\)</span>、<span
class="math inline">\(\overline{FP}\)</span>、<span
class="math inline">\(\overline{TN}\)</span>、<span
class="math inline">\(\overline{FN}\)</span>，再基于这些平均值计算出<strong>微精确率</strong>（micro-P）和<strong>微召回率</strong>（micro-R），以及相应的<strong>微F1</strong>（micro-F1）。<br />
<strong>micro-P</strong>：<span
class="math inline">\(\dfrac{\overline{TP}}{\overline{TP}+\overline{FP}}\)</span></p>
<p><strong>micro-R</strong>：<span
class="math inline">\(\dfrac{\overline{TP}}{\overline{TP}+\overline{FN}}\)</span></p>
<p><strong>micro-F1</strong>：<span class="math inline">\(\dfrac{2\times
\text{macro-P}\times\text{macro-R}}{\text{macro-P}+\text{macro-R}}\)</span></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> recall_score</span><br><span class="line"></span><br><span class="line">recall_score(</span><br><span class="line">    y_true,</span><br><span class="line">    y_pred,</span><br><span class="line">    labels=<span class="literal">None</span>,</span><br><span class="line">    pos_label=<span class="number">1</span>,</span><br><span class="line">    average=<span class="string">&#x27;binary&#x27;</span>,<span class="comment">#默认是二分类;&#x27;micro&#x27;就是1的计算方式;&#x27;macro&#x27;就是2的不加权计算方式;&#x27;weighted&#x27;就是2的加权计算方式</span></span><br><span class="line">    sample_weight=<span class="literal">None</span>,</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<h3 id="pr曲线">PR曲线</h3>
<p>根据学习器的预测结果，对样例进行排序，排在最前面的是学习器认为最可能是正例的样本，排在最后面的是学习器认为最不可能是正例的样本。按此顺序逐个把样本作为正例进行预测，则每次可以计算出当前的召回率（查全率）、精确率（查准率）。以精确率为纵轴，召回率为横轴，作图就得到了精确率-召回率曲线，简称P-R曲线。<br />
<img src="/images/模型评估/PR.png" width="40%"></p>
<p>P-R曲线能直观的显示出学习器在样本总体上的精确率和、召回率。在比较时，若一个学习器的P-R曲线被另一个学习器的曲线完全包住，则可断言后者性能优于前者。</p>
<h3 id="roc曲线与auc">ROC曲线与AUC</h3>
<p><strong>ROC</strong>（Receiver Operating characteristic
Curve）根据学习器的结果对样例进行排序，按此顺序逐个把样本作为正例进行预测，每次可以计算出当前的<strong>召回率TPR</strong>和<strong>错误接收率FPR</strong>，作为纵轴和横轴。我们希望TPR越大越好，FPR越小越好。ROC曲线能很容易的查出任意阈值时，对性能的识别能力。一般我们取拐点处的阈值最佳，这兼顾了TPR和FPR。</p>
<p><strong>TPR</strong>：<span
class="math inline">\(\dfrac{TP}{TP+FN}\)</span><br />
<strong>FPR</strong>：<span
class="math inline">\(\dfrac{FP}{TN+FP}\)</span></p>
<p><img src="/images/模型评估/ROC和AUC.png" width="80%"></p>
<p>若一个学习器的ROC曲线被另一个学习器的曲线完全包住，则可断言后者性能优于前者。若两个学习器的ROC曲线发生交叉，则难以断定孰优孰劣，此时看AUC面积。<br />
<strong>AUC</strong>（Area Under
Curve）就是ROC曲线下方的面积，它反映了曲线向左上方靠近程度。越大越好。</p>
<p><strong>ROC和PR区别</strong>：<br />
<img src="/images/模型评估/ROC和PR.png" width="60%"></p>
<p><span class="exturl" data-url="aHR0cHM6Ly93d3cubWF0aC51Y2RhdmlzLmVkdS9+c2FpdG8vZGF0YS9yb2MvZmF3Y2V0dC1yb2MucGRm">An
introduction to ROC analysis<i class="fa fa-external-link-alt"></i></span><br />
根据作者原文来看，负例增加了10倍，ROC曲线没有改变，而PR曲线则变了很多。作者认为这是ROC曲线的优点，即具有鲁棒性，在类别分布发生明显改变的情况下依然能客观地识别出较好的分类器。<br />
不过在类别不平衡的背景下，负例的数目众多致使FPR的增长不明显，导致ROC曲线呈现一个过分的效果估计。</p>
<p><strong>ROC和PR应用场景</strong>：<br />
ROC曲线由于兼容正例与负例，所以适合评估分类器的整体性能，相比而言PR曲线完全聚焦于正例。</p>
<p>如果有<strong>多份数据且存在不同的类别分布</strong>，比如信用卡欺诈问题中，每个月正例和负例的比例可能都不相同，这时候如果只想单纯地比较分类器的性能且剔除类别分布改变的影响，则ROC曲线比较适合，因为类别分布的改变可能使得PR曲线发生变化时好时坏，这种时候难以进行模型比较。反之，如果想测试不同类别分布下，对分类器的性能的影响，则PR曲线比较合适。</p>
<p>如果想要评估在<strong>相同的类别分布</strong>下正例的预测情况，则宜选PR曲线。</p>
<p>类别不平衡问题中，ROC曲线通常会给一个乐观的效果估计，所以大部分时候还是PR曲线更好。</p>
<h3 id="增益图与ks图">增益图与KS图</h3>
<p><img src="/images/模型评估/增益与KS图.png" width="60%"></p>
<p>左图是<strong>增益图</strong>，虚线是1。比如挖取游戏作弊玩家，100个玩家，10个作弊的：计算得分，取出得分前10的用户，发现有9个作弊的，那么正样本比例是9/10=0.9，测试集取样比例为10/100=0.1，正样本比例/平均比例为0.9/0.1=9。增益图最好情况是在取样比例为0.5为转折点，前部分全是最高值，后部分全是最低值。增益图宏观上反映了分类器的分类效果。</p>
<p>右图是<strong>KS图</strong>，我们关心的是两条曲线的最大差值，横坐标是阈值，纵坐标是差值<span
class="math inline">\(TPR-FPR\)</span>，这个差值反映了对正类样本的区分度。一般选择差值最大的阈值作为分类阈值。</p>
<h2 id="回归模型">回归模型</h2>
<p><strong>MSE（Mean Square Error）均方误差:</strong> <span
class="math inline">\(\dfrac{1}{n}\sum\limits_{i=1}^n (f_i -
y_i)^2\)</span>，真实值和预测值差值的平方和。</p>
<p><strong>RMSE（Root MSE）均方根误差:</strong> <span
class="math inline">\(\sqrt{MSE}\)</span>，对MSE开根号，如果MSE都是比较小的，可以放大它们之间尺度。</p>
<p><strong>MAE（Mean Absolute Error）平均绝对误差:</strong> <span
class="math inline">\(\dfrac{1}{n}\sum\limits_{i=1}^n |f_i -
y_i|\)</span>，真实值和预测值差值的绝对值和。用此为指标，求导非常麻烦，常用MSE。</p>
<p><strong>MAPE（Mean Absolute Percentage
Error）平均绝对百分比误差：</strong> <span
class="math inline">\(\dfrac{100\%}{n}\sum\limits_{i=1}^n |\dfrac{f_i -
y_i}{y_i}|\)</span>，范围<span
class="math inline">\([0,+\infty)\)</span>，MAPE 为0%表示完美模型，MAPE
大于 100 %则表示劣质模型。<span class="exturl" data-url="aHR0cHM6Ly96aHVhbmxhbi56aGlodS5jb20vcC8yNTk2NjI4NjQ=">mape和smape<i class="fa fa-external-link-alt"></i></span>。</p>
<p><strong>r2_score（决定系数）:</strong> <span
class="math inline">\(R^2=\dfrac{\sum\limits_{i=1}^n ( y_i -
f_i)^2}{\sum\limits_{i=1}^n ( y_i - \bar y)^2}\)</span>，<span
class="math inline">\(f_i\)</span>是预测值，<span
class="math inline">\(y_i\)</span>是实际值，<span
class="math inline">\(\bar
y\)</span>是实际值的平均值。分子代表预测值对实际值的离散程度，分母代表真实值的离散程度。R平方为回归平方和与总离差平方和的比值，表示总离差平方和中可以由回归平方和解释的比例，这一比例越大越好，模型越精确，回归效果越显著。R平方介于0~1之间，越接近1，回归拟合效果越好，一般认为超过0.8的模型拟合优度比较高。</p>
<h2 id="聚类模型">聚类模型</h2>
<p><strong>RMS（Root Mean Square）：</strong> <span
class="math inline">\(\dfrac{1}{n}\sqrt{\sum\limits_{i=1}^n (x_i-\bar
x)^2}\)</span>，聚类的值减去每个类的平均值，然后平方和开根号，除以n。越小越好，越大表示每个类和它的中心比较远，效果差。</p>
<p><strong>轮廓系数：</strong> <span
class="math inline">\(s(i)=\dfrac{b(i)-a(i)}{max\{a(i),b(i)\}}\)</span>，<span
class="math inline">\(a(i)\)</span>为样本i与簇内其他样本的平均距离，也叫内聚度；<span
class="math inline">\(b(i)\)</span>为样本i与其他某簇样本的平均距离，多个簇<span
class="math inline">\(b(i)\)</span>取最小的，也叫分离度。它结合了内聚度和分离度来评价，可以在相同数据的基础上评价不同的算法，或者算法的不同运行方式对聚类结果产生的影响。这个值越趋近于1，越好；越接近于-1，越差。</p>
<h2 id="关联模型">关联模型</h2>
<p>支持度、置信度、提升度即可。</p>
<h1 id="代码">代码</h1>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> MinMaxScaler,StandardScaler</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> LabelEncoder,OneHotEncoder</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> Normalizer</span><br><span class="line"><span class="keyword">from</span> sklearn.discriminant_analysis <span class="keyword">import</span> LinearDiscriminantAnalysis</span><br><span class="line"><span class="keyword">from</span> sklearn.decomposition <span class="keyword">import</span> PCA</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> pydotplus</span><br><span class="line">os.environ[<span class="string">&quot;PATH&quot;</span>]+=os.pathsep+<span class="string">&quot;D:/Program/Graphviz/bin/&quot;</span></span><br><span class="line"><span class="comment">#sl:satisfaction_level---False:MinMaxScaler;True:StandardScaler</span></span><br><span class="line"><span class="comment">#le:last_evaluation---False:MinMaxScaler;True:StandardScaler</span></span><br><span class="line"><span class="comment">#npr:number_project---False:MinMaxScaler;True:StandardScaler</span></span><br><span class="line"><span class="comment">#amh:average_monthly_hours--False:MinMaxScaler;True:StandardScaler</span></span><br><span class="line"><span class="comment">#tsc:time_spend_company--False:MinMaxScaler;True:StandardScaler</span></span><br><span class="line"><span class="comment">#wa:Work_accident--False:MinMaxScaler;True:StandardScaler</span></span><br><span class="line"><span class="comment">#pl5:promotion_last_5years--False:MinMaxScaler;True:StandardScaler</span></span><br><span class="line"><span class="comment">#dp:department--False:LabelEncoding;True:OneHotEncoding</span></span><br><span class="line"><span class="comment">#slr:salary--False:LabelEncoding;True:OneHotEncoding</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">hr_preprocessing</span>(<span class="params">sl=<span class="literal">False</span>,le=<span class="literal">False</span>,npr=<span class="literal">False</span>,amh=<span class="literal">False</span>,tsc=<span class="literal">False</span>,wa=<span class="literal">False</span>,pl5=<span class="literal">False</span>,dp=<span class="literal">False</span>,slr=<span class="literal">False</span>,lower_d=<span class="literal">False</span>,ld_n=<span class="number">1</span></span>):</span><br><span class="line">    df=pd.read_csv(<span class="string">&quot;./data/HR.csv&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">#1、清洗数据</span></span><br><span class="line">    df=df.dropna(subset=[<span class="string">&quot;satisfaction_level&quot;</span>,<span class="string">&quot;last_evaluation&quot;</span>])</span><br><span class="line">    df=df[df[<span class="string">&quot;satisfaction_level&quot;</span>]&lt;=<span class="number">1</span>][df[<span class="string">&quot;salary&quot;</span>]!=<span class="string">&quot;nme&quot;</span>]</span><br><span class="line">    <span class="comment">#2、得到标注</span></span><br><span class="line">    label = df[<span class="string">&quot;left&quot;</span>]</span><br><span class="line">    df = df.drop(<span class="string">&quot;left&quot;</span>, axis=<span class="number">1</span>)</span><br><span class="line">    <span class="comment">#3、特征选择</span></span><br><span class="line">    <span class="comment">#4、特征处理</span></span><br><span class="line">    scaler_lst=[sl,le,npr,amh,tsc,wa,pl5]</span><br><span class="line">    column_lst=[<span class="string">&quot;satisfaction_level&quot;</span>,<span class="string">&quot;last_evaluation&quot;</span>,<span class="string">&quot;number_project&quot;</span>,\</span><br><span class="line">                <span class="string">&quot;average_monthly_hours&quot;</span>,<span class="string">&quot;time_spend_company&quot;</span>,<span class="string">&quot;Work_accident&quot;</span>,\</span><br><span class="line">                <span class="string">&quot;promotion_last_5years&quot;</span>]</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(scaler_lst)):</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> scaler_lst[i]:</span><br><span class="line">            df[column_lst[i]]=\</span><br><span class="line">                MinMaxScaler().fit_transform(df[column_lst[i]].values.reshape(-<span class="number">1</span>,<span class="number">1</span>)).reshape(<span class="number">1</span>,-<span class="number">1</span>)[<span class="number">0</span>]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            df[column_lst[i]]=\</span><br><span class="line">                StandardScaler().fit_transform(df[column_lst[i]].values.reshape(-<span class="number">1</span>,<span class="number">1</span>)).reshape(<span class="number">1</span>,-<span class="number">1</span>)[<span class="number">0</span>]</span><br><span class="line">    scaler_lst=[slr,dp]</span><br><span class="line">    column_lst=[<span class="string">&quot;salary&quot;</span>,<span class="string">&quot;department&quot;</span>]</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(scaler_lst)):</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> scaler_lst[i]:</span><br><span class="line">            <span class="keyword">if</span> column_lst[i]==<span class="string">&quot;salary&quot;</span>:</span><br><span class="line">                df[column_lst[i]]=[map_salary(s) <span class="keyword">for</span> s <span class="keyword">in</span> df[<span class="string">&quot;salary&quot;</span>].values]</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                df[column_lst[i]]=LabelEncoder().fit_transform(df[column_lst[i]])</span><br><span class="line">            df[column_lst[i]]=MinMaxScaler().fit_transform(df[column_lst[i]].values.reshape(-<span class="number">1</span>,<span class="number">1</span>)).reshape(<span class="number">1</span>,-<span class="number">1</span>)[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            df=pd.get_dummies(df,columns=[column_lst[i]])</span><br><span class="line">    <span class="keyword">if</span> lower_d:</span><br><span class="line">        <span class="keyword">return</span> PCA(n_components=ld_n).fit_transform(df.values),label</span><br><span class="line">    <span class="keyword">return</span> df,label</span><br><span class="line">d=<span class="built_in">dict</span>([(<span class="string">&quot;low&quot;</span>,<span class="number">0</span>),(<span class="string">&quot;medium&quot;</span>,<span class="number">1</span>),(<span class="string">&quot;high&quot;</span>,<span class="number">2</span>)])</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">map_salary</span>(<span class="params">s</span>):</span><br><span class="line">    <span class="keyword">return</span> d.get(s,<span class="number">0</span>)</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">hr_modeling_nn</span>(<span class="params">features,label</span>):</span><br><span class="line">    <span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line">    f_v = features.values</span><br><span class="line">    f_names = features.columns.values</span><br><span class="line">    l_v = label.values</span><br><span class="line">    X_tt, X_validation, Y_tt, Y_validation = train_test_split(f_v, l_v, test_size=<span class="number">0.2</span>)</span><br><span class="line">    X_train, X_test, Y_train, Y_test = train_test_split(X_tt, Y_tt, test_size=<span class="number">0.25</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">from</span> keras.models <span class="keyword">import</span> Sequential</span><br><span class="line">    <span class="keyword">from</span> keras.layers.core <span class="keyword">import</span> Dense, Activation</span><br><span class="line">    <span class="keyword">from</span> keras.optimizers <span class="keyword">import</span> SGD</span><br><span class="line">    mdl = Sequential()</span><br><span class="line">    mdl.add(Dense(<span class="number">50</span>, input_dim=<span class="built_in">len</span>(f_v[<span class="number">0</span>])))</span><br><span class="line">    mdl.add(Activation(<span class="string">&quot;sigmoid&quot;</span>))</span><br><span class="line">    mdl.add(Dense(<span class="number">2</span>))</span><br><span class="line">    mdl.add(Activation(<span class="string">&quot;softmax&quot;</span>))</span><br><span class="line">    sgd = SGD(lr=<span class="number">0.1</span>)</span><br><span class="line">    mdl.<span class="built_in">compile</span>(loss=<span class="string">&quot;mean_squared_error&quot;</span>, optimizer=<span class="string">&quot;adam&quot;</span>)</span><br><span class="line">    mdl.fit(X_train, np.array([[<span class="number">0</span>, <span class="number">1</span>] <span class="keyword">if</span> i == <span class="number">1</span> <span class="keyword">else</span> [<span class="number">1</span>, <span class="number">0</span>] <span class="keyword">for</span> i <span class="keyword">in</span> Y_train]), nb_epoch=<span class="number">1000</span>, batch_size=<span class="number">8999</span>)</span><br><span class="line">    xy_lst = [(X_train, Y_train), (X_validation, Y_validation), (X_test, Y_test)]</span><br><span class="line">    <span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 评价指标</span></span><br><span class="line">    <span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> roc_curve, auc, roc_auc_score</span><br><span class="line">    f = plt.figure()</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(xy_lst)):</span><br><span class="line">        X_part = xy_lst[i][<span class="number">0</span>]</span><br><span class="line">        Y_part = xy_lst[i][<span class="number">1</span>]</span><br><span class="line">        Y_pred = mdl.predict(X_part)</span><br><span class="line">        <span class="built_in">print</span>(Y_pred)</span><br><span class="line">        Y_pred = np.array(Y_pred[:, <span class="number">1</span>]).reshape((<span class="number">1</span>, -<span class="number">1</span>))[<span class="number">0</span>]</span><br><span class="line">        <span class="comment"># print(i)</span></span><br><span class="line">        <span class="comment"># print(&quot;NN&quot;, &quot;-ACC:&quot;, accuracy_score(Y_part, Y_pred))</span></span><br><span class="line">        <span class="comment"># print(&quot;NN&quot;, &quot;-REC:&quot;, recall_score(Y_part, Y_pred))</span></span><br><span class="line">        <span class="comment"># print(&quot;NN&quot;, &quot;-F1:&quot;, f1_score(Y_part, Y_pred))</span></span><br><span class="line">        f.add_subplot(<span class="number">1</span>, <span class="number">3</span>, i + <span class="number">1</span>)</span><br><span class="line">        <span class="comment"># 使用roc_curve绘制ROC曲线，返回三部分</span></span><br><span class="line">        fpr, tpr, threshold = roc_curve(Y_part, Y_pred)</span><br><span class="line">        plt.plot(fpr, tpr)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;NN&quot;</span>, <span class="string">&quot;AUC&quot;</span>, auc(fpr, tpr))</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;NN&quot;</span>, <span class="string">&quot;AUC_Score&quot;</span>, roc_auc_score(Y_part, Y_pred))</span><br><span class="line">    plt.show()</span><br><span class="line">    <span class="keyword">return</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">hr_modeling</span>(<span class="params">features,label</span>):</span><br><span class="line">    <span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line">    f_v=features.values</span><br><span class="line">    f_names=features.columns.values</span><br><span class="line">    l_v=label.values</span><br><span class="line">    X_tt,X_validation,Y_tt,Y_validation=train_test_split(f_v,l_v,test_size=<span class="number">0.2</span>)</span><br><span class="line">    X_train,X_test,Y_train,Y_test=train_test_split(X_tt,Y_tt,test_size=<span class="number">0.25</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 评价指标</span></span><br><span class="line">    <span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score, recall_score, f1_score</span><br><span class="line">    <span class="keyword">from</span> sklearn.neighbors <span class="keyword">import</span> NearestNeighbors,KNeighborsClassifier</span><br><span class="line">    <span class="keyword">from</span> sklearn.naive_bayes <span class="keyword">import</span> GaussianNB,BernoulliNB</span><br><span class="line">    <span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> DecisionTreeClassifier,export_graphviz</span><br><span class="line">    <span class="keyword">from</span> sklearn.externals.six <span class="keyword">import</span> StringIO</span><br><span class="line">    <span class="keyword">from</span> sklearn.svm <span class="keyword">import</span> SVC</span><br><span class="line">    <span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> RandomForestClassifier</span><br><span class="line">    <span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> AdaBoostClassifier</span><br><span class="line">    <span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</span><br><span class="line">    <span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> GradientBoostingClassifier</span><br><span class="line">    models=[]</span><br><span class="line">    models.append((<span class="string">&quot;KNN&quot;</span>,KNeighborsClassifier(n_neighbors=<span class="number">3</span>)))</span><br><span class="line">    models.append((<span class="string">&quot;GaussianNB&quot;</span>,GaussianNB()))</span><br><span class="line">    models.append((<span class="string">&quot;BernoulliNB&quot;</span>,BernoulliNB()))</span><br><span class="line">    models.append((<span class="string">&quot;DecisionTreeGini&quot;</span>,DecisionTreeClassifier()))</span><br><span class="line">    models.append((<span class="string">&quot;DecisionTreeEntropy&quot;</span>,DecisionTreeClassifier(criterion=<span class="string">&quot;entropy&quot;</span>)))</span><br><span class="line">    models.append((<span class="string">&quot;SVM Classifier&quot;</span>,SVC(C=<span class="number">1000</span>)))</span><br><span class="line">    models.append((<span class="string">&quot;OriginalRandomForest&quot;</span>,RandomForestClassifier()))</span><br><span class="line">    models.append((<span class="string">&quot;RandomForest&quot;</span>,RandomForestClassifier(n_estimators=<span class="number">11</span>,max_features=<span class="literal">None</span>)))</span><br><span class="line">    models.append((<span class="string">&quot;Adaboost&quot;</span>,AdaBoostClassifier(n_estimators=<span class="number">100</span>)))</span><br><span class="line">    models.append((<span class="string">&quot;LogisticRegression&quot;</span>,LogisticRegression(C=<span class="number">1000</span>,tol=<span class="number">1e-10</span>,solver=<span class="string">&quot;sag&quot;</span>,max_iter=<span class="number">10000</span>)))</span><br><span class="line">    models.append((<span class="string">&quot;GBDT&quot;</span>,GradientBoostingClassifier(max_depth=<span class="number">6</span>,n_estimators=<span class="number">100</span>)))</span><br><span class="line">    <span class="keyword">for</span> clf_name,clf <span class="keyword">in</span> models:</span><br><span class="line">        clf.fit(X_train,Y_train)</span><br><span class="line">        xy_lst=[(X_train,Y_train),(X_validation,Y_validation),(X_test,Y_test)]</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(xy_lst)):</span><br><span class="line">            X_part=xy_lst[i][<span class="number">0</span>]</span><br><span class="line">            Y_part=xy_lst[i][<span class="number">1</span>]</span><br><span class="line">            Y_pred=clf.predict(X_part)</span><br><span class="line">            <span class="built_in">print</span>(i)</span><br><span class="line">            <span class="built_in">print</span>(clf_name,<span class="string">&quot;-ACC:&quot;</span>,accuracy_score(Y_part,Y_pred))</span><br><span class="line">            <span class="built_in">print</span>(clf_name,<span class="string">&quot;-REC:&quot;</span>,recall_score(Y_part,Y_pred))</span><br><span class="line">            <span class="built_in">print</span>(clf_name,<span class="string">&quot;-F1:&quot;</span>,f1_score(Y_part,Y_pred))</span><br><span class="line">            <span class="comment"># dot_data=StringIO()</span></span><br><span class="line">            <span class="comment"># export_graphviz(clf,out_file=dot_data,</span></span><br><span class="line">            <span class="comment">#                          feature_names=f_names,</span></span><br><span class="line">            <span class="comment">#                          class_names=[&quot;NL&quot;,&quot;L&quot;],</span></span><br><span class="line">            <span class="comment">#                          filled=True,</span></span><br><span class="line">            <span class="comment">#                          rounded=True,</span></span><br><span class="line">            <span class="comment">#                          special_characters=True)</span></span><br><span class="line">            <span class="comment"># graph=pydotplus.graph_from_dot_data(dot_data.getvalue())</span></span><br><span class="line">            <span class="comment"># graph.write_pdf(&quot;dt_tree_2.pdf&quot;)</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">regr_test</span>(<span class="params">features,label</span>):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;X&quot;</span>,features)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Y&quot;</span>,label)</span><br><span class="line">    <span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LinearRegression,Ridge,Lasso</span><br><span class="line">    <span class="comment">#regr=LinearRegression()</span></span><br><span class="line">    regr=Ridge(alpha=<span class="number">1</span>)</span><br><span class="line">    regr.fit(features.values,label.values)</span><br><span class="line">    Y_pred=regr.predict(features.values)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Coef:&quot;</span>,regr.coef_)</span><br><span class="line">    <span class="comment"># 回归模型评价指标</span></span><br><span class="line">    <span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> mean_squared_error,mean_absolute_error,r2_score</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;MSE:&quot;</span>,mean_squared_error(label.values,Y_pred))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;MAE:&quot;</span>,mean_absolute_error(label.values,Y_pred))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;R2:&quot;</span>,r2_score(label.values,Y_pred))</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">main</span>():</span><br><span class="line">    features,label=hr_preprocessing()</span><br><span class="line">    regr_test(features[[<span class="string">&quot;number_project&quot;</span>,<span class="string">&quot;average_monthly_hours&quot;</span>]],features[<span class="string">&quot;last_evaluation&quot;</span>])</span><br><span class="line">    <span class="comment">#hr_modeling(features,label)</span></span><br><span class="line"><span class="keyword">if</span> __name__==<span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    main()</span><br><span class="line"></span><br></pre></td></tr></table></figure>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




  <nav class="pagination">
    <a class="extend prev" rel="prev" title="上一页" aria-label="上一页" href="/page/5/"><i class="fa fa-angle-left"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/5/">5</a><span class="page-number current">6</span><a class="page-number" href="/page/7/">7</a><span class="space">&hellip;</span><a class="page-number" href="/page/13/">13</a><a class="extend next" rel="next" title="下一页" aria-label="下一页" href="/page/7/"><i class="fa fa-angle-right"></i></a>
  </nav>

</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="copyright">
    &copy; 2019 – 
    <span itemprop="copyrightYear">2024</span>
    <span class="with-love">
      <i class="fa fa-heart"></i>
    </span>
    <span class="author" itemprop="copyrightHolder">SoundMemories</span>
  </div>
  <div class="powered-by">由 <span class="exturl" data-url="aHR0cHM6Ly9oZXhvLmlv">Hexo</span> & <span class="exturl" data-url="aHR0cHM6Ly90aGVtZS1uZXh0LmpzLm9yZy9tdXNlLw==">NexT.Muse</span> 强力驱动
  </div>

    </div>
  </footer>

  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>
  <div class="sidebar-dimmer"></div>
  <div class="back-to-top" role="button" aria-label="返回顶部">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>
  <a role="button" class="book-mark-link book-mark-link-fixed"></a>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/next-theme-pjax/0.6.0/pjax.min.js" integrity="sha256-vxLn1tSKWD4dqbMRyv940UYw4sXgMtYcK6reefzZrao=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.7.0/jquery.min.js" integrity="sha256-2Pmvv0kuTBOenSvLm6bvfBSSHrUJ+3A7x6P5Ebd07/g=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.js" integrity="sha256-yt2kYMy0w8AbtF89WXb2P1rfjcP/HTHLT7097U8Y5b8=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/medium-zoom/1.0.8/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/muse.js"></script><script src="/js/next-boot.js"></script><script src="/js/bookmark.js"></script><script src="/js/pjax.js"></script>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-generator-searchdb/1.4.1/search.js" integrity="sha256-1kfA5uHPf65M5cphT2dvymhkuyHPQp5A53EGZOnOLmc=" crossorigin="anonymous"></script>
<script src="/js/third-party/search/local-search.js"></script>

  <script class="next-config" data-name="pdf" type="application/json">{"object_url":{"url":"https://cdnjs.cloudflare.com/ajax/libs/pdfobject/2.2.12/pdfobject.min.js","integrity":"sha256-g2xji1rlE3KsGVClvuxTbcR0Kn2+wtQADSff2Tbb4zA="},"url":"/lib/pdf/web/viewer.html"}</script>
  <script src="/js/third-party/tags/pdf.js"></script>

  <script class="next-config" data-name="mermaid" type="application/json">{"enable":true,"theme":{"light":"neutral","dark":"neutral"},"js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mermaid/10.2.3/mermaid.min.js","integrity":"sha256-JFptYy4KzJ5OQP+Q9fubNf3cxpPPmZKqUOovyEONKrQ="}}</script>
  <script src="/js/third-party/tags/mermaid.js"></script>


  <script src="/js/third-party/fancybox.js"></script>



  




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"none","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>


  <script src="https://cdnjs.cloudflare.com/ajax/libs/quicklink/2.3.0/quicklink.umd.js" integrity="sha256-yvJQOINiH9fWemHn0vCA5lsHWJaHs6/ZmO+1Ft04SvM=" crossorigin="anonymous"></script>
  <script class="next-config" data-name="quicklink" type="application/json">{"enable":true,"home":true,"archive":true,"delay":true,"timeout":3000,"priority":true,"url":"https://soundmemories.github.io/page/6/"}</script>
  <script src="/js/third-party/quicklink.js"></script>

</body>
</html>
