<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.2.0">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="https://fonts.loli.net/css?family=Monda:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">

<link rel="stylesheet" href="//cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.1/css/all.min.css">
  <link rel="stylesheet" href="//cdn.jsdelivr.net/npm/animate.css@3.1.1/animate.min.css">
  <link rel="stylesheet" href="//cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.css">

<script class="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"soundmemories.github.io","root":"/","images":"/images","scheme":"Muse","version":"8.0.2","exturl":true,"sidebar":{"position":"left","display":"always","padding":18,"offset":12},"copycode":true,"bookmark":{"enable":true,"color":"#222","save":"auto"},"fancybox":true,"mediumzoom":true,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"},"path":"/search.xml","localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false}};
  </script>

  <meta name="description" content="概述1、什么是spark？基于内存的计算引擎，它的计算速度非常快。但是仅仅只涉及到数据的计算，并没有涉及到数据的存储。 2、为什么要学习spark？MapReduce框架局限性：（1）Map结果写磁盘，Reduce写HDFS，多个MR之间通过HDFS交换数据。（2）任务调度和启动开销大。（3）无法充分利用内存。（4）不适合迭代计算（如机器学习、图计算等等），交互式处理（数据挖掘）。（5）不适合流式">
<meta property="og:type" content="article">
<meta property="og:title" content="ML-Spark">
<meta property="og:url" content="https://soundmemories.github.io/2020/05/10/Machine%20Learning/07.ML-Spark/index.html">
<meta property="og:site_name" content="SoundMemories">
<meta property="og:description" content="概述1、什么是spark？基于内存的计算引擎，它的计算速度非常快。但是仅仅只涉及到数据的计算，并没有涉及到数据的存储。 2、为什么要学习spark？MapReduce框架局限性：（1）Map结果写磁盘，Reduce写HDFS，多个MR之间通过HDFS交换数据。（2）任务调度和启动开销大。（3）无法充分利用内存。（4）不适合迭代计算（如机器学习、图计算等等），交互式处理（数据挖掘）。（5）不适合流式">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://soundmemories.github.io/images/Spark/pyspark.png">
<meta property="og:image" content="https://soundmemories.github.io/images/Spark/spark-core.png">
<meta property="og:image" content="https://soundmemories.github.io/images/Spark/SparkComponents.png">
<meta property="og:image" content="https://soundmemories.github.io/images/Spark/spark1.png">
<meta property="og:image" content="https://soundmemories.github.io/images/Spark/spark2.png">
<meta property="og:image" content="https://soundmemories.github.io/images/Spark/spark3.png">
<meta property="article:published_time" content="2020-05-09T16:00:00.000Z">
<meta property="article:modified_time" content="2021-06-10T18:33:28.769Z">
<meta property="article:author" content="SoundMemories">
<meta property="article:tag" content="Machine Learning">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://soundmemories.github.io/images/Spark/pyspark.png">


<link rel="canonical" href="https://soundmemories.github.io/2020/05/10/Machine%20Learning/07.ML-Spark/">


<script data-pjax class="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>ML-Spark | SoundMemories</title>
  






  <noscript>
  <style>
  body { margin-top: 2rem; }

  .use-motion .menu-item,
  .use-motion .sidebar,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header {
    visibility: visible;
  }

  .use-motion .header,
  .use-motion .site-brand-container .toggle,
  .use-motion .footer { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle,
  .use-motion .custom-logo-image {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line {
    transform: scaleX(1);
  }

  .search-pop-overlay, .sidebar-nav { display: none; }
  .sidebar-panel { display: block; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <h1 class="site-title">SoundMemories</h1>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu">
        <li class="menu-item menu-item-主页">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>主页</a>

  </li>
        <li class="menu-item menu-item-分类">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a>

  </li>
        <li class="menu-item menu-item-标签">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a>

  </li>
        <li class="menu-item menu-item-归档">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
        <li class="menu-item menu-item-关于">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" maxlength="80"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="search-result-container no-result">
  <div class="search-result-icon">
    <i class="fa fa-spinner fa-pulse fa-5x"></i>
  </div>
</div>

    </div>
  </div>

</div>
        
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>

  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>
	   
		  
      <div class="sidebar-panel-container">
        <!--noindex-->
        <section class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%A6%82%E8%BF%B0"><span class="nav-number">1.</span> <span class="nav-text">概述</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%AE%89%E8%A3%85%E4%BD%BF%E7%94%A8"><span class="nav-number">2.</span> <span class="nav-text">安装使用</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Spark-Core"><span class="nav-number">3.</span> <span class="nav-text">Spark-Core</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#RDD"><span class="nav-number">4.</span> <span class="nav-text">RDD</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#RDD%E6%A0%B8%E5%BF%83%E6%A6%82%E5%BF%B5"><span class="nav-number">4.1.</span> <span class="nav-text">RDD核心概念</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#SparkContext"><span class="nav-number">4.2.</span> <span class="nav-text">SparkContext</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#RDD%E5%88%9B%E5%BB%BA"><span class="nav-number">4.3.</span> <span class="nav-text">RDD创建</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#RDD%E8%BD%AC%E6%8D%A2%E6%93%8D%E4%BD%9C"><span class="nav-number">4.4.</span> <span class="nav-text">RDD转换操作</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%80%9A%E7%94%A8%E8%BD%AC%E6%8D%A2%E6%93%8D%E4%BD%9C"><span class="nav-number">4.4.1.</span> <span class="nav-text">通用转换操作</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Pair-RDD%E8%BD%AC%E6%8D%A2%E6%93%8D%E4%BD%9C"><span class="nav-number">4.4.2.</span> <span class="nav-text">Pair RDD转换操作</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#RDD%E8%A1%8C%E5%8A%A8%E6%93%8D%E4%BD%9C"><span class="nav-number">4.5.</span> <span class="nav-text">RDD行动操作</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%80%9A%E7%94%A8%E8%A1%8C%E5%8A%A8%E6%93%8D%E4%BD%9C"><span class="nav-number">4.5.1.</span> <span class="nav-text">通用行动操作</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Pair-RDD%E8%A1%8C%E5%8A%A8%E6%93%8D%E4%BD%9C"><span class="nav-number">4.5.2.</span> <span class="nav-text">Pair RDD行动操作</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#RDD%E5%85%B6%E4%BB%96%E6%96%B9%E6%B3%95%E5%92%8C%E5%B1%9E%E6%80%A7"><span class="nav-number">4.6.</span> <span class="nav-text">RDD其他方法和属性</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#RDD%E6%8C%81%E4%B9%85%E5%8C%96"><span class="nav-number">4.7.</span> <span class="nav-text">RDD持久化</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#RDD%E5%88%86%E5%8C%BA"><span class="nav-number">4.8.</span> <span class="nav-text">RDD分区</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#RDD%E6%B7%B7%E6%B4%97"><span class="nav-number">4.9.</span> <span class="nav-text">RDD混洗</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Spark-SQL"><span class="nav-number">5.</span> <span class="nav-text">Spark SQL</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#SparkSession"><span class="nav-number">5.1.</span> <span class="nav-text">SparkSession</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#DataFrame"><span class="nav-number">5.2.</span> <span class="nav-text">DataFrame</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%88%9B%E5%BB%BA"><span class="nav-number">5.2.1.</span> <span class="nav-text">创建</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BF%9D%E5%AD%98"><span class="nav-number">5.2.2.</span> <span class="nav-text">保存</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%B1%9E%E6%80%A7"><span class="nav-number">5.2.3.</span> <span class="nav-text">属性</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%BD%AC%E6%8D%A2%E6%93%8D%E4%BD%9C"><span class="nav-number">5.2.4.</span> <span class="nav-text">转换操作</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%A1%8C%E5%8A%A8%E6%93%8D%E4%BD%9C"><span class="nav-number">5.2.5.</span> <span class="nav-text">行动操作</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Row%E5%92%8CColumn"><span class="nav-number">5.2.6.</span> <span class="nav-text">Row和Column</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#GroupedData"><span class="nav-number">5.2.7.</span> <span class="nav-text">GroupedData</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#functions"><span class="nav-number">5.2.8.</span> <span class="nav-text">functions</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8F%8D%E5%B0%84%E6%8E%A8%E6%96%AD"><span class="nav-number">5.2.9.</span> <span class="nav-text">反射推断</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%BB%BC%E5%90%88%E7%A4%BA%E4%BE%8B"><span class="nav-number">5.2.10.</span> <span class="nav-text">综合示例</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Spark-SQL-1"><span class="nav-number">5.3.</span> <span class="nav-text">Spark SQL</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Spark-Streaming"><span class="nav-number">6.</span> <span class="nav-text">Spark-Streaming</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%BB%84%E4%BB%B6"><span class="nav-number">6.1.</span> <span class="nav-text">组件</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%BC%96%E7%A0%81%E5%AE%9E%E8%B7%B5"><span class="nav-number">6.2.</span> <span class="nav-text">编码实践</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%8A%B6%E6%80%81%E6%93%8D%E4%BD%9C"><span class="nav-number">6.3.</span> <span class="nav-text">状态操作</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Windows"><span class="nav-number">6.4.</span> <span class="nav-text">Windows</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E7%B4%AF%E5%8A%A0%E5%99%A8"><span class="nav-number">7.</span> <span class="nav-text">累加器</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%B9%BF%E6%92%AD%E5%8F%98%E9%87%8F"><span class="nav-number">8.</span> <span class="nav-text">广播变量</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E7%AE%80%E7%AD%94%E9%A2%98"><span class="nav-number">9.</span> <span class="nav-text">简答题</span></a></li></ol></div>
        </section>
        <!--/noindex-->

        <section class="site-overview-wrap sidebar-panel">
          <div class="site-author site-overview-item animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="SoundMemories"
      src="https://i.loli.net/2020/11/04/6JhNuwtBe4adylS.png">
  <p class="site-author-name" itemprop="name">SoundMemories</p>
  <div class="site-description" itemprop="description">今日事，今日毕</div>
</div>
<div class="site-state-wrap site-overview-item animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives">
          <span class="site-state-item-count">85</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
        <span class="site-state-item-count">7</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
        <span class="site-state-item-count">7</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author site-overview-item animated">
      <span class="links-of-author-item">
        <span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL3NvdW5kbWVtb3JpZXM=" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;soundmemories"><i class="fab fa-github fa-fw"></i>GitHub</span>
      </span>
      <span class="links-of-author-item">
        <span class="exturl" data-url="bWFpbHRvOnNvdW5kbWVtb3JpZXNAMTYzLmNvbQ==" title="E-Mail → mailto:soundmemories@163.com"><i class="fa fa-envelope fa-fw"></i>E-Mail</span>
      </span>
  </div>
  <div class="cc-license site-overview-item animated" itemprop="license">
    <span class="exturl cc-opacity" data-url="aHR0cHM6Ly9jcmVhdGl2ZWNvbW1vbnMub3JnL2xpY2Vuc2VzL2J5LW5jLXNhLzQuMC8="><img src="/images/cc-by-nc-sa.svg" alt="Creative Commons"></span>
  </div>



        </section>
      </div>
    </div>
  </aside>
  <div class="sidebar-dimmer"></div>


    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>
  <a role="button" class="book-mark-link book-mark-link-fixed"></a>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


    <div class="main-inner post posts-expand">
      

      

  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://soundmemories.github.io/2020/05/10/Machine%20Learning/07.ML-Spark/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="https://i.loli.net/2020/11/04/6JhNuwtBe4adylS.png">
      <meta itemprop="name" content="SoundMemories">
      <meta itemprop="description" content="今日事，今日毕">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="SoundMemories">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          ML-Spark
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2020-05-10 00:00:00" itemprop="dateCreated datePublished" datetime="2020-05-10T00:00:00+08:00">2020-05-10</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Machine-Learning/" itemprop="url" rel="index"><span itemprop="name">Machine Learning</span></a>
        </span>
    </span>

  
      </div>
      <div class="post-meta">
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>122k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>1:51</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
        <h1 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h1><p><strong>1、什么是spark？</strong><br>基于内存的计算引擎，它的计算速度非常快。但是仅仅只涉及到数据的计算，并没有涉及到数据的存储。</p>
<p><strong>2、为什么要学习spark？</strong><br>MapReduce框架局限性：<br>（1）Map结果写磁盘，Reduce写HDFS，多个MR之间通过HDFS交换数据。<br>（2）任务调度和启动开销大。<br>（3）无法充分利用内存。<br>（4）不适合迭代计算（如机器学习、图计算等等），交互式处理（数据挖掘）。<br>（5）不适合流式处理（点击日志分析）。<br>（6）MapReduce编程不够灵活，仅支持Map和Reduce两种操作。</p>
<p><strong>3、生态圈</strong><br>Hadoop提供的一套服务：<br>离线计算/交互式计算：MapReduce、Hive、Pig。<br>实时计算/流式计算：Storm、flink。<br>交互式计算：Impala、presto。</p>
<p>需要一种灵活的框架可同时进行批处理、流式计算、交互式计算—-&gt;Spark：<br>（1）内存计算引擎，提供cache机制来支持需要反复迭代计算或者多次数据共享，减少数据读取的IO开销。<br>（2）DAG引擎，较少多次计算之间中间结果写到HDFS的开销。<br>（3）使用多线程模型来减少task启动开销，shuffle过程中避免不必要的sort操作以及减少磁盘IO。<br>Spark缺点：吃内存，不太稳定。</p>
<p>Spark提供的一套完整的服务：<br>离线计算/交互式计算：Spark core（类似mapreduce，基于rdd）、Spark SQL（类似hive，基于dataframe）。<br>实时计算/流式计算：Spark Streaming（类似storm、flink）。<br>机器学习：Spark MLlib（基于dataframe）。</p>
<p><strong>4、Spark特点</strong><br>（1）速度快：比mapreduce在内存中快100倍，在磁盘中快10倍。Spark中的job中间结果可以不落地，可以存放在内存中。mapreduce中map和reduce任务都是以进程的方式运行着，而spark中的job是以线程方式运行在进程中。<br>（2）易用性：可以通过java/scala/python/R开发spark应用程序。<br>（3）通用性：可以使用spark sql/spark streaming/mlib/Graphx。<br>（4）兼容性：spark程序可以运行在standalone/yarn/mesos。</p>
<h1 id="安装使用"><a href="#安装使用" class="headerlink" title="安装使用"></a>安装使用</h1><p><strong>安装和配置（Win）</strong><br>首先要保证安装了Java(JDK)。</p>
<p>其次，从<span class="exturl" data-url="aHR0cDovL3NwYXJrLmFwYWNoZS5vcmcvZG93bmxvYWRzLmh0bWw=">官方下载地址<i class="fa fa-external-link-alt"></i></span>下载<code>Pre-built Apache Hadoop xx and later</code>的版本<br>解压即可，在系统变量中加入<code>SPARK_HOME</code>路径为文件地址，在PATH中添加<code>%SPARK_HOME%\bin</code>和<code>%SPARK_HOME%\sbin</code><br>注意Spark的不同版本要求了不同的Scala和Hadoop版本，一定要在后续安装Scala和Hadoop时注意。</p>
<p>对于Scala，从<span class="exturl" data-url="aHR0cHM6Ly93d3cuc2NhbGEtbGFuZy5vcmcvZG93bmxvYWQvYWxsLmh0bWw=">官方下载地址<i class="fa fa-external-link-alt"></i></span>下载安装包，默认安装即可。</p>
<p>对于Hadoop，从<span class="exturl" data-url="aHR0cHM6Ly9hcmNoaXZlLmFwYWNoZS5vcmcvZGlzdC9oYWRvb3AvY29tbW9uLw==">官方下载地址<i class="fa fa-external-link-alt"></i></span>需要的版本解压即可，在系统变量中加入<code>HADOOP_HOME</code>路径为文件地址，在PATH中添加<code>%HADOOP_HOME%\bin</code>。<br>下载<span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL3N0ZXZlbG91Z2hyYW4vd2ludXRpbHM=">winutils<i class="fa fa-external-link-alt"></i></span>，找到对应版本的Hadoop，替换原版Hadoop的bin文件夹即可。winutils用于改变文件或文件夹读写权限的工具。</p>
<p>测试以上程序是否安装成功，启动cmd：<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">java -version</span><br><span class="line">&#x27;&#x27;&#x27;</span><br><span class="line">java version &quot;1.8.0_211&quot;</span><br><span class="line">...</span><br><span class="line">&#x27;&#x27;&#x27;</span><br><span class="line">hadoop</span><br><span class="line">&#x27;&#x27;&#x27;</span><br><span class="line">Usage: hadoop [--config confdir] [--loglevel loglevel] COMMAND</span><br><span class="line">...</span><br><span class="line">&#x27;&#x27;&#x27;</span><br></pre></td></tr></table></figure><br>以上证明安装成功！</p>
<p>如果使用pyspark，需要<code>pip install py4j</code>和<code>pip install findspark</code>。<br>在程序中使用：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> findspark</span><br><span class="line">findspark.intit()</span><br></pre></td></tr></table></figure><br>执行一次以上两句，以后就可以<code>import pysaprk</code>了。</p>
<p>如果你想把python脚本提交到Spark上执行，使用Spark自带的<code>bin/spark-submit</code>脚本来运行：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin&#x2F;spark-submit my_script.py</span><br></pre></td></tr></table></figure><br>spark-submit 会帮助我们引入python程序的Spark依赖。<br>注意，这种方法和调用pyspark一样需要你自己初始化SparkContext，一般这么写：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark <span class="keyword">import</span> SparkConf, SparkContext</span><br><span class="line"><span class="comment"># 首先创建一个SparkConf对象来配置应用，然后基于该SparkConf来创建一个SparkContext对象</span></span><br><span class="line"><span class="comment"># .setMaster()给出了集群的URL，告诉spark如何连接到集群上。这里&#x27;local&#x27;表示让spark运行在单机单线程上</span></span><br><span class="line"><span class="comment"># .setAppName()给出了应用的名字。当连接到一个集群上时，这个值可以帮助你在集群管理器的用户界面上找到你的应用</span></span><br><span class="line">conf = SparkConf().setMaster(<span class="string">&#x27;local&#x27;</span>).setAppName(<span class="string">&#x27;My App&#x27;</span>)</span><br><span class="line">sc = SparkContext(conf = conf)</span><br></pre></td></tr></table></figure><br>只有在终端使用<code>pyspark</code>和<code>spark-shell</code>命令启动spark，不需要自己初始化SparkContext，它在启动时自动初始化SparkContext为sc。</p>
<p><strong>local模式启动</strong>：<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 在<span class="variable">$SPARK_HOME</span>/sbin目录下执行pyspark</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> pyspark</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 直接启动spark用下面命令，默认scala语言。</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 建议使用上面的pyspark</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> spark-shell</span></span><br></pre></td></tr></table></figure><br><img src="/images/Spark/pyspark.png"></p>
<h1 id="Spark-Core"><a href="#Spark-Core" class="headerlink" title="Spark-Core"></a>Spark-Core</h1><p>Spark支持多种运行模式。单机部署下，既可以用本地（Local）模式运行，也可以使用伪分布式模式来运行；当以分布式集群部署的时候，可以根据实际情况选择Spark自带的独立（Standalone）运行模式、YARN运行模式或者Mesos模式。虽然模式多，但是Spark的运行架构基本由三部分组成，包括<strong>SparkContext（驱动程序）</strong>、<strong>ClusterManager（集群资源管理器）</strong>和<strong>Executor（任务执行进程）</strong>。　<br><img src="/images/Spark/spark-core.png"></p>
<ul>
<li>SparkContext提交作业，向ClusterManager申请资源；后面会用SparkContext进行初始化。</li>
<li>ClusterManager（资源管理和调度）会根据当前集群的资源使用情况，进行有条件的FIFO策略：<ul>
<li>先分配的应用程序尽可能多地获取资源；</li>
<li>后分配的应用程序则在剩余资源中筛选；</li>
<li>没有合适资源的应用程序只能等待其他应用程序释放资源；</li>
</ul>
</li>
<li>ClusterManager有两种分配方法：<ul>
<li>默认情况下会将应用程序分布在尽可能多的Worker(资源节点)上，这种分配算法有利于充分利用集群资源，适合内存使用多的场景，以便更好地做到数据处理的本地性；</li>
<li>另一种则是分布在尽可能少的Worker上，这种适合CPU密集型且内存使用较少的场景；</li>
</ul>
</li>
<li>ClusterManager分配到worker上就会创建Excutor：<ul>
<li>Excutor创建后与SparkContext保持通讯，SparkContext分配任务集给Excutor，Excutor按照一定的调度策略执行任务集。</li>
</ul>
</li>
</ul>
<p>Spark包含1个<strong>driver(笔记本电脑/集群网关机器上)</strong>和若干个<strong>executor(在各个节点上)</strong>，通过SparkContext(简称sc)连接<strong>Spark集群、创建RDD、累加器（accumlator）、广播变量（broadcast variables）</strong>，简单可以认为SparkContext（驱动程序）是Spark程序的根本。<br><img src="/images/Spark/SparkComponents.png" width="40%"></p>
<p><strong>Driver</strong>会把计算任务分成一系列小的<strong>task</strong>，然后送到<strong>executor</strong>执行。executor之间可以通信，在每个executor完成自己的task以后，所有的信息会被传回。</p>
<p><strong>Spark集群架构(Standalone模式)</strong><br><img src="/images/Spark/spark1.png" width="80%"></p>
<ul>
<li>Application：用户自己写的Spark应用程序，批处理作业的集合。Application的main方法为应用程序的入口，用户通过Spark的API，定义了RDD和对RDD的操作。</li>
<li>Master和Worker：整个集群分为 Master 节点和 Worker 节点，相当于 Hadoop 的 Master 和 Slave 节点。<ul>
<li>Master：Standalone模式中主控节点，负责接收Client提交的作业，管理Worker，并命令Worker启动Driver和Executor。</li>
<li>Worker：Standalone模式中slave节点上的守护进程，负责管理本节点的资源，定期向Master汇报心跳，接收Master的命令，启动Driver和Executor。</li>
</ul>
</li>
<li>Client：客户端进程，负责提交作业到Master。</li>
<li>Driver： 一个Spark作业运行时包括一个Driver进程，也是作业的主进程，负责作业的解析、生成Stage并调度Task到Executor上。包括DAGScheduler，TaskScheduler。</li>
<li>Executor：即真正执行作业的地方，一个集群一般包含多个Executor，每个Executor接收Driver的命令Launch Task，一个Executor可以执行一到多个Task。<a id="more"></a></li>
<li>Spark作业相关概念：<ul>
<li>Stage：一个Spark作业一般包含一到多个Stage。</li>
<li>Task：一个Stage包含一到多个Task，通过多个Task实现并行运行的功能。</li>
<li>DAGScheduler： 实现将Spark作业分解成一到多个Stage，每个Stage根据RDD的Partition个数决定Task的个数，然后生成相应的Task set放到TaskScheduler中。</li>
<li>TaskScheduler：实现Task分配到Executor上执行。<div class="note info"><p>Spark的Task是以线程为单位，Hadoop的Task是以进程为单位。</p>
</div>
</li>
</ul>
</li>
</ul>
<p>在Spark中，所有RDD的转换都是是惰性求值的。RDD的转换操作transformation会生成新的RDD，新的RDD的数据依赖于原来的RDD的数据，每个RDD又包含多个分区。那么一段程序实际上就构造了一个由相互依赖的多个RDD组成的有向无环图（DAG）。并通过在RDD上执行action动作将这个有向无环图作为一个Job提交给Spark执行。</p>
<p>在DAG中又进行stage的划分，划分的依据是依赖算子是否是shuffle(如reduceByKey,Join等)的，每个stage又可以划分成若干task。接下来的事情就是driver发送task到executor，executor自己的线程池去执行这些task，完成之后将结果返回给driver。action算子是划分不同job的依据。</p>
<p>RDD和它依赖的父RDD（s）的关系有两种不同的类型，即窄依赖（narrow dependency）和宽依赖（wide dependency）。<br>窄依赖指的是每一个父RDD的Partition最多被子RDD的一个Partition使用。<br>宽依赖指的是多个子RDD的Partition会依赖同一个父RDD的Partition。<br><img src="/images/Spark/spark2.png" width="60%"></p>
<p>DAG(Directed Acyclic Graph)叫做有向无环图，原始的RDD通过一系列的转换就就形成了DAG，根据RDD之间的依赖关系的不同将DAG划分成不同的Stage，对于窄依赖，partition的转换处理在Stage中完成计算。对于宽依赖，由于有Shuffle的存在，只能在parent RDD处理完成后，才能开始接下来的计算，因此宽依赖是划分Stage的依据。<br><img src="/images/Spark/spark3.png" width="60%"></p>
<h1 id="RDD"><a href="#RDD" class="headerlink" title="RDD"></a>RDD</h1><p><strong>RDD(弹性分布式数据集Resilient Distributed Dataset)</strong>:Spark中数据的核心抽象,它代表一个不可变,可分区,里面的元素可并行计算的集合。</p>
<p><strong>Dataset</strong>：一个数据集，简单的理解为集合，用于存放数据的。RDD 是不可变的分布式对象集合。RDD 可以包含Python、Java、Scala 中任意类型的对象。<br><strong>Distributed</strong>：它的数据是分布式存储，并且可以做分布式的计算。每个 RDD 都被分为多个分区，这些分区运行在集群中的不同节点上。<br><strong>Resilient</strong>：弹性的。它表示的是数据可以保存在磁盘，也可以保存在内存中。数据分布式也是弹性的：<br>（1）RDD会在多个节点上存储，就和hdfs的分布式道理是一样的。hdfs文件被切分为多个block存储在各个节点上，而RDD是被切分为多个partition。不同的partition可能在不同的节点上<br>（2）spark读取hdfs的场景下，spark把hdfs的block读到内存就会抽象为spark的partition。<br>（3）spark计算结束，一般会把数据做持久化到hive，hbase，hdfs等等。我们就拿hdfs举例，将RDD持久化到hdfs上，RDD的每个partition就会存成一个文件，如果文件小于128M，就可以理解为一个partition对应hdfs的一个block。反之，如果大于128M，就会被且分为多个block，这样，一个partition就会对应多个block。</p>
<p>RDD是一个包含诸多元素、被划分到不同节点上进行并行处理的数据集合，可以将RDD持久化到内存中，这样就可以有效地在并行操作中复用（在机器学习这种需要反复迭代的任务中非常有效）。在节点发生错误时RDD也可以自动恢复。</p>
<h2 id="RDD核心概念"><a href="#RDD核心概念" class="headerlink" title="RDD核心概念"></a>RDD核心概念</h2><p>在Spark里，所有的处理和计算任务都会被组织成一系列<strong>RDD</strong>上的<strong>transformations(转换)</strong> 和 <strong>actions(动作)</strong>。在这些背后，spark自动将RDD中的数据分发到集群上，并将actions并行化执行。</p>
<p>RDD相关的函数/操作分为三类：<br>（1）<strong>RDD创建</strong>。<br>（2）<strong>transformation（转换）</strong>：转换已有的RDD，它会从一个RDD 生成一个新的RDD。<br>（3）<strong>action（行动）</strong>：对RDD求值。它会对RDD计算出一个结果，并将结果返回到driver程序中(或把结果存储到外部存储系统,如HDFS中)。<br><div class="note info"><p>如果你不知道一个函数是transformation还是action，你可以考察它的返回值：如果返回的是RDD，则是transformation。如果返回的是其它数据类型，则是action。</p>
</div></p>
<p><strong>1、每个Spark程序或者shell会话都按照如下流程工作：</strong><br>（1）从外部数据创建输入的RDD。<br>（2）使用transformation操作对RDD进行转换（惰性），以定义新的RDD。<br>（3）对需要被重复用的中间结果RDD执行<code>persist()</code>操作。<br>（4）使用action操作触发一次并行计算，Spark会对计算进行优化之后再执行。</p>
<p><strong>2、transformation和action区别：action操作会触发实际的计算</strong><br>（1）你可以在任意时候定义新的RDD，但是Spark只会惰性计算这些RDD：只有第一次在一个action中用到时，才会真正计算（所有返回RDD的操作都是惰性的，包括读取数据的<code>sc.textFile()</code>函数）。<br>（2）在计算RDD时，它所有依赖的中间RDD也会被求值，通过完整的转换链，Spark只计算求值过程中需要的那些数据。</p>
<p><strong>3、每次action会重新计算</strong><br>默认情况下，spark的RDD会在你每次对它进行action操作时重新计算。<br>如果希望在多个action操作中重用同一个RDD，则可以使用<code>RDD.persist()</code>让spark把这个RDD缓存起来。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">lines = sc.textFile(<span class="string">&quot;xxx.txt&quot;</span>)</span><br><span class="line">lines.persist()</span><br><span class="line">lines.count() <span class="comment"># 计算lines，此时将lines缓存</span></span><br><span class="line">lines.first() <span class="comment"># 使用缓存的lines</span></span><br></pre></td></tr></table></figure><br>在第一次对持久化的RDD计算之后，Spark会把RDD的内容保存到内存中（以分区的方式存储到集群的各个机器上）。然后在此后的行动操作中，可以重用这些数据。</p>
<p>之所以默认不缓存RDD的计算结果，是因为Spark可以直接遍历一遍数据然后计算出结果，没必要浪费存储空间。</p>
<p><strong>4、Spark中的转化操作和行动操作，一般需要用户传入一个可调用对象。</strong><br>在Python中有三种方式：lambda 表达式、全局定义的函数、局部定义的函数。<br><div class="note info"><p>注意：python可能会把函数所在的对象也序列化之后向外传递。<br>当传递的函数是某个对象的成员，或者包含了某个对象中一个字段的引用（如self.xxx时），Spark 会把整个对象发送到工作节点上。</p>
<p>如果python不知道如何序列化该对象，则程序运行失败。如果该序列化对象太大，则传输的数据较多影响效率。</p>
</div><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 经常出现序列化的问题</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">A</span>:</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self,x</span>):</span></span><br><span class="line">    self._x = x</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">is_match</span>(<span class="params">self,s</span>):</span></span><br><span class="line">    <span class="keyword">return</span> <span class="string">&#x27;test&#x27;</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">get_match1</span>(<span class="params">self,rdd</span>):</span></span><br><span class="line">    <span class="comment"># 不好，传递的self.is_match是对象的成员</span></span><br><span class="line">    <span class="keyword">return</span> rdd.<span class="built_in">filter</span>(self.is_match) </span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">get_match2</span>(<span class="params">self,rdd</span>):</span></span><br><span class="line">    <span class="comment"># 不好，传递的lambda函数包含了对象的成员 self._x</span></span><br><span class="line">    <span class="keyword">return</span> rdd.<span class="built_in">filter</span>(<span class="keyword">lambda</span> s: self._x <span class="keyword">in</span> s) </span><br><span class="line"></span><br><span class="line"><span class="comment"># 解决方法：将你需要的字段从对象中取出，放到一个局部变量中</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">A</span>:</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">is_match</span>(<span class="params">self,s</span>):</span></span><br><span class="line">    <span class="keyword">return</span> <span class="string">&#x27;test&#x27;</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">get_match1</span>(<span class="params">self,rdd</span>):</span></span><br><span class="line">    <span class="comment"># 局部变量存储成员对象</span></span><br><span class="line">    _is_match = self.is_match</span><br><span class="line">    <span class="keyword">return</span> rdd.<span class="built_in">filter</span>(_is_match) <span class="comment"># OK</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">get_match2</span>(<span class="params">self,rdd</span>):</span></span><br><span class="line">    <span class="comment"># 局部变量存储成员对象</span></span><br><span class="line">    _x = self._x</span><br><span class="line">    <span class="keyword">return</span> rdd.<span class="built_in">filter</span>(<span class="keyword">lambda</span> s: _x <span class="keyword">in</span> s) <span class="comment"># OK</span></span><br></pre></td></tr></table></figure><br>注意，在Python中，如果操作对应的RDD数据类型不正确，则导致运行报错。</p>
<h2 id="SparkContext"><a href="#SparkContext" class="headerlink" title="SparkContext"></a>SparkContext</h2><p>每个spark 应用都由一个驱动器程序(driver program)来发起集群上的各种并行操作：<br>（1）driver program 包含了应用的main函数，并且定义了集群上的分布式数据集，还对这些分布式数据集应用了相关操作。<br>（2）driver program 通过一个SparkContext对象来访问Spark。<br>（3）driver program 一般要管理多个执行器(executor) 节点。<br>（4）SparkContext：该对象代表了对计算集群的一个连接。</p>
<p>在shell中启动<code>pyspark</code>时，会自动创建了一个 SparkContext 对象，它叫做sc，通常用它来创建RDD。我们常用的是交互式方式建立连接。</p>
<p><strong>创建链接</strong><br>（1）<strong>SparkConf</strong>：创建SparkContext的时候需要一个SparkConf， 用来传递Spark应用的基本信息。<br>（2）<strong>SparkContext</strong>：Spark程序的入口。SparkContext代表了和Spark集群的链接, 在Spark集群中通过SparkContext来创建RDD。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pyspark</span><br><span class="line"><span class="keyword">from</span> pyspark <span class="keyword">import</span> SparkContext <span class="comment"># 驱动</span></span><br><span class="line"><span class="keyword">from</span> pyspark <span class="keyword">import</span> SparkConf <span class="comment"># 基本设定，比如executor设定可以用多少的计算资源，任务叫什么名字</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">首先创建一个SparkConf对象来配置应用，然后基于该SparkConf来创建一个SparkContext对象</span></span><br><span class="line"><span class="string">.setMaster(appName)给出了集群的URL（master在哪），告诉spark如何连接到集群上</span></span><br><span class="line"><span class="string">.setAppName(master)给出了任务的名字。当连接到一个集群上时，这个值可以帮助你在集群管理器的用户界面上找到你的应用</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="comment"># miniProject任务名字</span></span><br><span class="line"><span class="comment"># local[*]:使用K个Worker线程本地化运行Spark(这里K自动设置为机器的CPU核数)</span></span><br><span class="line"><span class="comment"># local[k]:使用K个Worker线程本地化运行Spark</span></span><br><span class="line">conf = SparkConf().setAppName(<span class="string">&quot;miniProject&quot;</span>).setMaster(<span class="string">&quot;local[*]&quot;</span>)</span><br><span class="line">sc = SparkContext.getOrCreate(conf=conf) <span class="comment"># 初始化</span></span><br></pre></td></tr></table></figure><br>之前讲了在cmd中用<code>pyspark</code>命令启动，它自动初始化sparkContext为sc，所以不再讲解，这里以Notebook交互编程为主。如果你有兴趣，可以在spark UI中看到当前的Spark作业，在浏览器访问本地的4040端口即可。</p>
<p>或者使用config配置好后启动：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建spark config对象</span></span><br><span class="line">conf = SparkConf()    </span><br><span class="line">config = (</span><br><span class="line">	(<span class="string">&quot;spark.app.name&quot;</span>, SPARK_APP_NAME),    <span class="comment"># 设置启动的spark的app名称，没有提供，将随机产生一个名称</span></span><br><span class="line">	(<span class="string">&quot;spark.executor.memory&quot;</span>, <span class="string">&quot;6g&quot;</span>),       <span class="comment"># 设置该app启动时占用的内存用量，默认1g</span></span><br><span class="line">	(<span class="string">&quot;spark.master&quot;</span>, SPARK_URL),           <span class="comment"># spark master的地址</span></span><br><span class="line">	(<span class="string">&quot;spark.executor.cores&quot;</span>, <span class="string">&quot;4&quot;</span>),       <span class="comment"># 设置spark executor使用的CPU核心数</span></span><br><span class="line">)</span><br><span class="line"><span class="comment"># 以下三项配置，可以控制执行器数量</span></span><br><span class="line"><span class="comment"># (&quot;spark.dynamicAllocation.enabled&quot;, True),</span></span><br><span class="line"><span class="comment"># (&quot;spark.dynamicAllocation.initialExecutors&quot;, 1), # 1个执行器</span></span><br><span class="line"><span class="comment"># (&quot;spark.shuffle.service.enabled&quot;, True)</span></span><br><span class="line"><span class="comment"># (&#x27;spark.sql.pivotMaxValues&#x27;, &#x27;99999&#x27;), # 当需要pivot DF，且值很多时，需要修改，默认是10000</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看更详细配置及说明：https://spark.apache.org/docs/latest/configuration.html</span></span><br><span class="line">conf.setAll(config)</span><br></pre></td></tr></table></figure><br>如果想调用远程服务器上的解释器和执行器，可以设置环境变量：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="comment"># 配置spark driver和pyspark运行时，所使用的python解释器路径</span></span><br><span class="line">PYSPARK_PYTHON = <span class="string">&quot;/home/hadoop/miniconda3/envs/datapy365spark23/bin/python&quot;</span></span><br><span class="line">JAVA_HOME=<span class="string">&#x27;/home/hadoop/app/jdk1.8.0_191&#x27;</span></span><br><span class="line"><span class="comment"># 当存在多个版本时，不指定很可能会导致出错</span></span><br><span class="line">os.environ[<span class="string">&quot;PYSPARK_PYTHON&quot;</span>] = PYSPARK_PYTHON <span class="comment"># 解释器</span></span><br><span class="line">os.environ[<span class="string">&quot;PYSPARK_DRIVER_PYTHON&quot;</span>] = PYSPARK_PYTHON <span class="comment"># 执行器</span></span><br><span class="line">os.environ[<span class="string">&#x27;JAVA_HOME&#x27;</span>]=JAVA_HOME</span><br><span class="line"><span class="comment"># spark配置信息</span></span><br></pre></td></tr></table></figure></p>
<h2 id="RDD创建"><a href="#RDD创建" class="headerlink" title="RDD创建"></a>RDD创建</h2><p>用户可以通过两种方式创建RDD：<br>（1）<strong>读取一个外部数据集方式创建RDD</strong>：<strong>你的每一行都会被当做一个item</strong>，不过需要注意的一点是，<strong>Spark一般默认你的路径是指向HDFS的，如果你要从本地读取文件的话，给一个<code>file:///</code>开头的全局路径</strong>。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 这里有个yob1880.txt，读取文件显示前5行</span></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;./data/names/yob1880.txt&#x27;</span>, <span class="string">&#x27;r+&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">    <span class="keyword">for</span> x <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">5</span>):</span><br><span class="line">        print(f.readline(),end=<span class="string">&#x27;&#x27;</span>)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">Mary,F,7065</span></span><br><span class="line"><span class="string">Anna,F,2604</span></span><br><span class="line"><span class="string">Emma,F,2003</span></span><br><span class="line"><span class="string">Elizabeth,F,1939</span></span><br><span class="line"><span class="string">Minnie,F,1746</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="comment"># 获取当前路径</span></span><br><span class="line">cwd = os.getcwd() </span><br><span class="line"><span class="comment"># rdd是惰性的，并没有去读取</span></span><br><span class="line">rdd = sc.textFile(<span class="string">&quot;file:///&quot;</span> + cwd + <span class="string">&quot;/data/names/yob1880.txt&quot;</span>) </span><br><span class="line">rdd</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">file:///E:/data/names/yob1880.txt MapPartitionsRDD[3] at textFile at NativeMethodAccessorImpl.java:0</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="comment"># 当第一次action操作时，才去读取</span></span><br><span class="line">rdd.first() </span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">&#x27;Mary,F,7065&#x27;</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><br>你甚至可以很粗暴地读入整个文件夹的所有文件。<br>但是要特别注意，这种读法，<strong>RDD中的每个item实际上是一个形如(文件名，文件所有内容)的元组。</strong><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 读取整个文件夹内容。</span></span><br><span class="line">rdd = sc.wholeTextFiles(<span class="string">&quot;file:///&quot;</span> + cwd + <span class="string">r&quot;\data\names&quot;</span>) </span><br><span class="line"><span class="comment"># 此时为元组:(文件路径，文件所有内容)</span></span><br><span class="line">rdd.first() </span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">(&#x27;file:/E:/data/names/yob1880.txt&#x27;,</span></span><br><span class="line"><span class="string"> &#x27;Mary,F,7065\r\nAnna,F,2604\r\nEmma,F,2003\r\nElizabeth,F,\</span></span><br><span class="line"><span class="string"> 1939\r\nMinnie,F,1746\r\nMargaret,F,1578\r\nIda,F,\</span></span><br><span class="line"><span class="string"> 1472\r\nAlice,F,1414\r\nBertha,F,1320\r\nSarah,F,\</span></span><br><span class="line"><span class="string"> 1288\r\nAnnie,F,1258\r\nClara,F,1226\r\nElla,F,\</span></span><br><span class="line"><span class="string"> 1156\r\nFlorence,F,1063\r\nCora,F,1045\r\nMartha,F,\</span></span><br><span class="line"><span class="string"> 1040\r\nLaura,F,1012\r\nNellie,F,995\r\nGrace,F,\</span></span><br><span class="line"><span class="string"> 982\r\nCarrie,F,949\r\nMaude,F,858\r\nMabel,F,808\r\nBessie,\</span></span><br><span class="line"><span class="string"> F,796\r\nJennie,F,793\r\nGertrude,F,787\r\nJulia,F,...)</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><br>读取的数据，同时执行多个任务时（多线程时），每个任务都会去复制一份数据，会造成内存资源浪费，如果你只是查询任务，那么可以使用广播共享数据源，告诉之后的任务共享同一个数据，减少内存开销：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">rdd = sc.textFile(<span class="string">&quot;test.txt&quot;</span>) </span><br><span class="line"><span class="comment"># 广播数据</span></span><br><span class="line">rdd_broadcast = sc.broadcast(rdd.collect())</span><br><span class="line"><span class="comment"># 使用时.value即可</span></span><br><span class="line">rdd_broadcast.value</span><br></pre></td></tr></table></figure><br>RDD还可以通过其他的方式初始化，包括：<strong>HDFS上的文件</strong>、<strong>Hive中的数据库与表</strong>、<strong>Spark SQL得到的结果</strong>。都可以使用<code>sc.textFile()</code>初始化RDD。</p>
<p>（2）<strong>在driver程序中分发driver程序中的对象集合（如list 或者set）</strong>：可以通过<code>sc.parallelize(c, numSlices=None)</code>去初始化一个RDD。当你执行这个操作以后，list中的元素将被自动分块（partitioned），并且把每一块送到集群上的不同机器上。<br><div class="note info"><p>使用sc.parallelize，你可以把Python list，NumPy array或者Pandas Series、Pandas DataFrame转成Spark RDD。</p>
<p>这种方式通常仅仅用于开发和测试。在生产环境中，它很少用。因为这种方式需要将整个数据集先放到driver程序所在的机器的内存中。</p>
</div><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 存放在当前环境内存当中的list</span></span><br><span class="line">my_list = [<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>] </span><br><span class="line"></span><br><span class="line"><span class="comment"># 并行化，得到一个对象</span></span><br><span class="line">rdd = sc.parallelize(my_list)</span><br><span class="line">rdd</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">ParallelCollectionRDD[0] at parallelize at PythonRDD.scala:195</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="comment"># rdd存了多少份</span></span><br><span class="line">rdd.getNumPartitions()</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">8</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="comment"># 看看分区状况</span></span><br><span class="line"><span class="comment"># collect是个危险函数，它会把分区数据取回本地机器，用list显示，会爆掉内存</span></span><br><span class="line"><span class="comment"># 此例子是一个8-core的CPU笔记本，Spark创建了8个executor，然后把数据分成8个块</span></span><br><span class="line">rdd.glom().collect()</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">[[], [1], [], [2], [3], [], [4], [5]]</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><br>注意，有时需必要设置numSlices，能保证每块最低数量。不然在某些情况下可能执行的结果和你想的不一样：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 有些操作，分区会影响其结果，比如想做一个累减</span></span><br><span class="line">rdd2 = sc.parallelize(my_list, <span class="number">2</span>) </span><br><span class="line">rdd2.glom().collect()</span><br><span class="line">rdd2.reduce(<span class="keyword">lambda</span> x,y:x-y)</span><br><span class="line"><span class="comment"># 结果的-5：先算1-2为-1，再算3-4-5为-6，最后算-1-(-6)为5</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">[[1, 2], [3, 4, 5]]</span></span><br><span class="line"><span class="string">5</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="comment"># 设置numSlices最低为1</span></span><br><span class="line">rdd3 = sc.parallelize([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>],<span class="number">1</span>)</span><br><span class="line">rdd3.glom().collect()</span><br><span class="line">rdd3.reduce(<span class="keyword">lambda</span> x,y:x-y)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">[[1, 2, 3, 4, 5]]</span></span><br><span class="line"><span class="string">-13</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure></p>
<p><strong>Pair RDD</strong><br>(key, value)键值对形式的RDD的元素（是一个二元元组而不是单个值）：<br>（1）键值对RDD通常被称作Pair RDD。<br>（2）键值对RDD常常用于聚合计算。<br>（3）spark为键值对RDD提供了并行操作各个键、跨节点重新进行数据分组的接口。</p>
<p><strong>Pair RDD的创建</strong><br>通过对常规RDD执行transformation来创建Pair RDD：<br>（1）从常规RDD中抽取某些字段，将该字段作为Pair RDD的键。<br>（2）对于存储很多键值对格式的数据，当读取该数据时，直接返回由其键值对数据组成的Pair RDD。<br>（3）当数据集已经在内存时，如果数据集由二元元组组成，那么直接调用<code>sc.parallelize()</code>方法就可以创建Pair RDD。</p>
<h2 id="RDD转换操作"><a href="#RDD转换操作" class="headerlink" title="RDD转换操作"></a>RDD转换操作</h2><p>transformation转换操作会从一个RDD生成一个新的RDD：<br>（1）<strong>所有的transformation操作都是惰性的，不会立即计算结果，计算发生在action操作中。</strong>它只记下应用于数据集的transformation操作。<br>（2）在transformation过程中并不会改变输入的RDD（RDD是不可变的），而是创建并返回一个新的RDD。</p>
<p>Spark会使用谱系图来记录各个RDD之间的依赖关系：<br>（1）在对RDD行动操作中，需要这个依赖关系来按需计算每个中间RDD。<br>（2）当持久化的RDD丢失部分数据时，也需要这个依赖关系来恢复丢失的数据。</p>
<h3 id="通用转换操作"><a href="#通用转换操作" class="headerlink" title="通用转换操作"></a>通用转换操作</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">map</span>(f, preservesPartitioning=<span class="literal">False</span>)：</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">将函数 f 作用于当前RDD的每个元素，返回值构成新的RDD。</span></span><br><span class="line"><span class="string">preservesPartitioning如果为True，则新的RDD会 保留旧RDD 的分区。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">mapPartitions(f, preservesPartitioning=<span class="literal">False</span>)：</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">将函数 f 作用于当前RDD的每个分区，将返回的迭代器的内容构成了新的RDD。</span></span><br><span class="line"><span class="string">这里 f 函数的参数是一个集合（表示一个分区的数据）。</span></span><br><span class="line"><span class="string">preservesPartitioning如果为True，则新的RDD会 保留旧RDD 的分区。</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">我的经验是，数据处理时 map 需要一条一条处理，会处理一条连一次资源。</span></span><br><span class="line"><span class="string">           使用mapPartitions一份份处理会减少链接资源次数，提高效率。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">flatMap(f, preservesPartitioning=<span class="literal">False</span>)：</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">将函数 f 作用于当前RDD的每个元素，将返回的迭代器的内容扁平化构成了新的RDD。</span></span><br><span class="line"><span class="string">preservesPartitioning如果为True，则新的RDD会 保留旧RDD 的分区。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="built_in">filter</span>(f)：</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">将函数 f 作用于当前RDD的每个元素，通过 f 的那些元素构成新的RDD。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">distinct(numPartitions=<span class="literal">None</span>)：</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">返回一个由当前RDD元素去重之后的结果组成新的RDD。</span></span><br><span class="line"><span class="string">numPartitions指定了新的RDD的分区数。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">sample(withReplacement, fraction, seed=<span class="literal">None</span>)：</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">对当前RDD进行采样，采样结果组成新的RDD。</span></span><br><span class="line"><span class="string">（1）withReplacement：如果为True，则可以重复采样；否则是无放回采样。</span></span><br><span class="line"><span class="string">（2）fractions：新的RDD的期望大小（占旧RDD的比例）。spark 并不保证结果刚好满足这个比例（只是一个期望值）。</span></span><br><span class="line"><span class="string">　　　　如果withReplacement=True：则表示每个元素期望被选择的次数，分数必大于等于0;</span></span><br><span class="line"><span class="string">　　　　如果withReplacement=False：则表示每个元素期望被选择的概率，分数一定是[0,1]。</span></span><br><span class="line"><span class="string">（3）seed：随机数生成器的种子。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">sortBy(keyfunc, ascending=<span class="literal">True</span>, numPartitions=<span class="literal">None</span>)：</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">对当前RDD进行排序，排序结果组成新的RDD。</span></span><br><span class="line"><span class="string">keyfunc：自定义的比较函数。</span></span><br><span class="line"><span class="string">ascending：如果为True，则升序排列。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">glom()：</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">返回一个RDD，它将旧RDD每个分区的元素聚合成一个列表，作为新RDD的元素。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">groupBy(f, numPartitions=<span class="literal">None</span>, partitionFunc=&lt;function portable_hash at <span class="number">0x7f51f1ac0668</span>&gt;)：</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">返回一个分组的RDD。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>
<p>针对两个RDD的转换操作：<br>尽管RDD不是集合，但是它也支持数学上的集合操作。注意：这些操作都要求被操作的RDD是相同数据类型的。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line">union(other)：</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">合并两个RDD中所有元素，生成一个新的RDD。</span></span><br><span class="line"><span class="string">该操作并不会检查两个输入RDD的重复元素，只是简单的将二者合并（并不会去重）。</span></span><br><span class="line"><span class="string">other：另一个RDD </span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">intersection(other)：</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">取两个RDD元素的交集，生成一个新的RDD。</span></span><br><span class="line"><span class="string">该操作会保证结果是去重的，因此它的性能很差。因为它需要通过网络混洗数据来发现重复的元素。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">subtract(other, numPartitions=<span class="literal">None</span>)：</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">存在于第一个RDD而不存在于第二个RDD中的所有元素组成的新的RDD。</span></span><br><span class="line"><span class="string">该操作也会保证结果是去重的，因此它的性能很差。因为它需要通过网络混洗数据来发现重复的元素。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">cartesian(other)：</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">两个RDD的笛卡尔积，生成一个新的RDD。</span></span><br><span class="line"><span class="string">新RDD中的元素是元组 (a,b)，其中 a 来自于第一个RDD，b 来自于第二个RDD。</span></span><br><span class="line"><span class="string">注意：求大规模的RDD的笛卡尔积开销巨大。</span></span><br><span class="line"><span class="string">该操作不会保证结果是去重的，它并不需要网络混洗数据。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">keyBy(f)：</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">创建一个RDD，它的元素是元组(f(x),x)。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">pipe(command, env=<span class="literal">None</span>, checkCode=<span class="literal">False</span>)：</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">返回一个RDD，它由外部进程的输出结果组成。</span></span><br><span class="line"><span class="string">command：外部进程命令</span></span><br><span class="line"><span class="string">env：环境变量</span></span><br><span class="line"><span class="string">checkCode：如果为True，则校验进程的返回值</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">randomSplit(weights, seed=<span class="literal">None</span>)：</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">返回一组新的RDD，它是旧RDD的随机拆分。</span></span><br><span class="line"><span class="string">weights：一个double的列表。它给出了每个结果DataFrame 的相对大小。如果列表的数值之和不等于 1.0，则它将被归一化为 1.0</span></span><br><span class="line"><span class="string">seed：随机数种子</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="built_in">zip</span>(other)：</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">返回一个Pair RDD，其中键来自于self，值来自于other。</span></span><br><span class="line"><span class="string">它假设两个RDD 拥有同样数量的分区，且每个分区拥有同样数量的元素</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">zipWithIndex()：</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">返回一个Pair RDD，其中键来自于self，值就是键的索引。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">zipWithUniqueId()：</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">返回一个Pair RDD，其中键来自于self，值是一个独一无二的id。</span></span><br><span class="line"><span class="string">它不会触发一个spark job，这是它与zipWithIndex的重要区别。</span></span><br></pre></td></tr></table></figure></p>
<p>示例：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark <span class="keyword">import</span> SparkContext <span class="comment"># 驱动</span></span><br><span class="line"><span class="keyword">from</span> pyspark <span class="keyword">import</span> SparkConf <span class="comment"># 基本设定，比如executor设定可以用多少的计算资源，任务叫什么名字</span></span><br><span class="line">conf = SparkConf().setAppName(<span class="string">&quot;miniProject&quot;</span>).setMaster(<span class="string">&quot;local[*]&quot;</span>)<span class="comment"># setAppName设定任务名字，setMaster指定master在哪</span></span><br><span class="line">sc = SparkContext.getOrCreate(conf) <span class="comment"># 初始化</span></span><br><span class="line"></span><br><span class="line">rdd = sc.parallelize(<span class="built_in">range</span>(<span class="number">100</span>), <span class="number">4</span>)</span><br><span class="line">rdd.count()</span><br><span class="line">rdd.sample(<span class="literal">False</span>, <span class="number">0.1</span>, <span class="number">81</span>).collect()</span><br><span class="line">rdd.sample(<span class="literal">True</span>, <span class="number">0.1</span>, <span class="number">81</span>).collect()</span><br><span class="line">rdd.takeSample(<span class="literal">False</span>, <span class="number">3</span>, <span class="number">81</span>)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">100</span></span><br><span class="line"><span class="string">[4, 26, 39, 41, 42, 52, 63, 76, 80, 86, 97]</span></span><br><span class="line"><span class="string">[8, 8, 17, 25, 27, 30, 33, 68, 70, 73, 95, 98, 98]</span></span><br><span class="line"><span class="string">[30, 80, 37]</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">numbersRDD = sc.parallelize(<span class="built_in">range</span>(<span class="number">1</span>,<span class="number">10</span>+<span class="number">1</span>))</span><br><span class="line">print(numbersRDD.collect())</span><br><span class="line"></span><br><span class="line">squaresRDD = numbersRDD.<span class="built_in">map</span>(<span class="keyword">lambda</span> x: x**<span class="number">2</span>)  <span class="comment"># 1进1出</span></span><br><span class="line">print(squaresRDD.collect())</span><br><span class="line"></span><br><span class="line">filteredRDD = numbersRDD.<span class="built_in">filter</span>(<span class="keyword">lambda</span> x: x % <span class="number">2</span> == <span class="number">0</span>)  <span class="comment"># Only the evens</span></span><br><span class="line">print(filteredRDD.collect())</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]</span></span><br><span class="line"><span class="string">[1, 4, 9, 16, 25, 36, 49, 64, 81, 100]</span></span><br><span class="line"><span class="string">[2, 4, 6, 8, 10]</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">sentencesRDD = sc.parallelize([<span class="string">&#x27;Hello world&#x27;</span>, <span class="string">&#x27;My name is Patrick&#x27;</span>])</span><br><span class="line">wordsRDD = sentencesRDD.flatMap(<span class="keyword">lambda</span> sentence: sentence.split(<span class="string">&quot; &quot;</span>))</span><br><span class="line">wordsRDD2 = sentencesRDD.<span class="built_in">map</span>(<span class="keyword">lambda</span> sentence: sentence.split(<span class="string">&quot; &quot;</span>))</span><br><span class="line">print(wordsRDD.collect())</span><br><span class="line">print(wordsRDD.count())</span><br><span class="line">print(wordsRDD2.collect())</span><br><span class="line">print(wordsRDD2.count())</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">[&#x27;Hello&#x27;, &#x27;world&#x27;, &#x27;My&#x27;, &#x27;name&#x27;, &#x27;is&#x27;, &#x27;Patrick&#x27;]</span></span><br><span class="line"><span class="string">6</span></span><br><span class="line"><span class="string">[[&#x27;Hello&#x27;, &#x27;world&#x27;], [&#x27;My&#x27;, &#x27;name&#x27;, &#x27;is&#x27;, &#x27;Patrick&#x27;]]</span></span><br><span class="line"><span class="string">2</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure></p>
<h3 id="Pair-RDD转换操作"><a href="#Pair-RDD转换操作" class="headerlink" title="Pair RDD转换操作"></a>Pair RDD转换操作</h3><p>Pair RDD 可以使用所有标准RDD上的可用的转换操作：由于Pair RDD 的元素是二元元组，因此传入的函数应当操作二元元组，而不是独立的元素。</p>
<p>基本转换操作：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">keys()：</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">返回一个新的RDD，包含了旧RDD每个元素的键。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">values()：</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">返回一个新的RDD，包含了旧RDD每个元素的值。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">mapValues(f)：</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">返回一个新的RDD，元素为 [K,f(V)]（保留原来的键不变，通过f改变值）。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">flatMapValues(f)：</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">返回一个新的RDD，元素为 [K,f(V)]（保留原来的键不变，通过f改变值）。当元素是集合时，会扁平化。</span></span><br><span class="line"><span class="string">x = sc.parallelize([(&quot;a&quot;, [&quot;x&quot;, &quot;y&quot;, &quot;z&quot;]), (&quot;b&quot;, [&quot;p&quot;, &quot;r&quot;])])</span></span><br><span class="line"><span class="string">x1 = x.flatMapValues(lambda t:t).collect() </span></span><br><span class="line"><span class="string"># x1: [(&#x27;a&#x27;, &#x27;x&#x27;), (&#x27;a&#x27;, &#x27;y&#x27;), (&#x27;a&#x27;, &#x27;z&#x27;), (&#x27;b&#x27;, &#x27;p&#x27;), (&#x27;b&#x27;, &#x27;r&#x27;)]</span></span><br><span class="line"><span class="string">x2 = x.mapValues(lambda t:t).collect()</span></span><br><span class="line"><span class="string"># x2: [(&quot;a&quot;, [&quot;x&quot;, &quot;y&quot;, &quot;z&quot;]), (&quot;b&quot;, [&quot;p&quot;, &quot;r&quot;])]</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">sortByKey(ascending=<span class="literal">True</span>, numPartitions=<span class="literal">None</span>, keyfunc=&lt;function &lt;<span class="keyword">lambda</span>&gt; at <span class="number">0x7f51f1ab5050</span>&gt;)：</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">对当前Pair RDD进行排序，排序结果组成新的RDD。</span></span><br><span class="line"><span class="string">keyfunc：自定义的比较函数</span></span><br><span class="line"><span class="string">ascending：如果为True，则升序排列</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">sampleByKey(withReplacement, fractions, seed=<span class="literal">None</span>)：</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">基于键的采样（即：分层采样）。</span></span><br><span class="line"><span class="string">withReplacement：如果为True，则是有放回的采样；否则是无放回的采样</span></span><br><span class="line"><span class="string">fractions：一个字典，指定了键上的每个取值的采样比例（不同取值之间的采样比例无关，不需要加起来为1）</span></span><br><span class="line"><span class="string">seed：随机数种子</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">subtractByKey(other, numPartitions=<span class="literal">None</span>)：</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">基于键的差集。返回一个新的RDD，其中每个(key,value)都位于self中，且不在other中。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><br>基于键的聚合操作：<br>在常规RDD上，fold()、aggregate()、reduce() 等都是行动操作。在Pair RDD 上，有类似的一组操作，用于针对相同键的元素进行聚合。这些操作返回RDD，因此是转化操作而不是行动操作。<br>返回的新RDD 的键为原来的键，值为针对键的元素聚合的结果。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line">reduceByKey(f,numPartitions=<span class="literal">None</span>,partitionFunc=&lt;function portable_hash at <span class="number">0x7f51f1ac0668</span>&gt;)：</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">合并具有相同键的元素。f 作用于同一个键的那些元素的值。</span></span><br><span class="line"><span class="string">它为每个键进行并行的规约操作，每个规约操作将键相同的值合并起来。</span></span><br><span class="line"><span class="string">因为数据集中可能有大量的键，因此该操作返回的是一个新的RDD：由键，以及对应的规约结果组成。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">foldByKey(zeroValue,f,numPartitions=<span class="literal">None</span>,partitionFunc=&lt;function portable_hash at <span class="number">0x7f51f1ac0668</span>&gt;)：</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">通过f聚合具有相同键的元素。其中zeroValue为零值。参见行动操作的fold()。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">aggregateByKey(zeroValue,seqFunc,combFunc,numPartitions=<span class="literal">None</span>,</span><br><span class="line">                partitionFunc=&lt;function portable_hash at <span class="number">0x7f51f1ac0668</span>&gt;)：</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">通过f聚合具有相同键的元素。其中zeroValue为零值。参见行动操作的aggregate()。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">combineByKey(createCombiner,mergeValue,mergeCombiners, numPartitions=<span class="literal">None</span>,</span><br><span class="line">              partitionFunc=&lt;function portable_hash at <span class="number">0x7f51f1ac0668</span>&gt;)：</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">把RDD[(K, V)]转换为RDD[(K, C)]形式，用于“组合类型”C。</span></span><br><span class="line"><span class="string">它是最为常用的基于键的聚合函数，大多数基于键的聚合函数都是用它实现的。</span></span><br><span class="line"><span class="string">和aggregate()一样，combineByKey()可以让用户返回与输入数据类型不同的返回值。</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">你需要提供三个函数：</span></span><br><span class="line"><span class="string">createCombiner(v)：v表示键对应的值。把v转换成单个元素的C形式。</span></span><br><span class="line"><span class="string">                   比如，按key改变对应的value为列表（第一个v的形式转换）。</span></span><br><span class="line"><span class="string">mergeValue(c,v)：c表示当前累加器，v表示键对应的值。返回一个C类型的值。</span></span><br><span class="line"><span class="string">                 一般为按key对应的value相加（每个分区内操作）。</span></span><br><span class="line"><span class="string">mergeCombiners(c1,c2)：c1表示某个分区某个键的累加器，c2表示同一个键另一个分区的累加器。返回一个C类型的值。</span></span><br><span class="line"><span class="string">                       一般为按key对应的value相加（分区间的合并，注意此时value都为列表）。</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">t = sc.parallelize([(&quot;a&quot;, 1), (&quot;b&quot;, 1), (&quot;a&quot;, 2)])</span></span><br><span class="line"><span class="string">t.combineByKey(lambda x:[x], lambda a,b:a+[b], lambda c,d:c+d).collect()</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">其工作流程是：遍历分区中的所有元素，考察该元素的键：</span></span><br><span class="line"><span class="string">（1）如果键从未在该分区中出现过，表明这是分区中的一个新的键。则使用createCombiner()函数来创建该键对应的累加器的初始值。</span></span><br><span class="line"><span class="string">     注意：这一过程发生在每个分区中，第一次出现各个键的时候发生。而不仅仅是整个RDD 中第一次出现一个键时发生。</span></span><br><span class="line"><span class="string">（2）如果键已经在该分区中出现过，则使用mergeValue() 函数将该键的累加器对应的当前值与这个新的值合并。</span></span><br><span class="line"><span class="string">（3）由于每个分区是独立处理的，因此同一个键可以有多个累加器。</span></span><br><span class="line"><span class="string">     如果有两个或者更多的分区都有同一个键的累加器，则使用mergeCombiners()函数将各个分区的结果合并。</span></span><br></pre></td></tr></table></figure><br>数据分组：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">groupByKey(numPartitions=<span class="literal">None</span>, partitionFunc=&lt;function portable_hash at <span class="number">0x7f51f1ac0668</span>&gt;)：</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">根据键来进行分组。</span></span><br><span class="line"><span class="string">返回一个新的RDD，类型为[K,Iterable[V]]，其中K为原来RDD的键的类型，V 为原来RDD的值的类型。</span></span><br><span class="line"><span class="string">如果你分组的目的是为了聚合，那么直接使用reduceByKey、aggregateByKey性能更好。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">cogroup(other,numPartitions=<span class="literal">None</span>)：</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">它基于self 和 other 两个TDD 中的所有的键来进行分组，它提供了为多个RDD 进行数据分组的方法。</span></span><br><span class="line"><span class="string">返回一个新的RDD，类型为[K,(Iterable[V],Iterable[W])]:</span></span><br><span class="line"><span class="string">   其中 K 为两个输入RDD的键的类型，V 为原来self的值的类型，W 为other的值的类型。</span></span><br><span class="line"><span class="string">如果某个键只存在于一个输入RDD中，另一个输入RDD中不存在，则对应的迭代器为空。</span></span><br><span class="line"><span class="string">它是groupWith的别名，但是groupWith支持更多的RDD来分组。</span></span><br></pre></td></tr></table></figure><br>数据连接：<br>数据连接操作的输出RDD会包含来自两个输入RDD的每一组相对应的记录。输出RDD 的类型为[K,(V,W)] ，其中K为两个输入RDD的键的类型，V为原来self的值的类型，W为other的值的类型。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">join(other,numPartitions=<span class="literal">None</span>)：</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">返回一个新的RDD，它是两个输入RDD的内连接。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">leftOuterJoin(other,numPartitions=<span class="literal">None</span>)：</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">返回一个新的RDD，它是两个输入RDD的左外连接。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">rightOuterJoin(other,numPartitions=<span class="literal">None</span>)：</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">返回一个新的RDD，它是两个输入RDD的右外连接。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">fullOuterJoin(other, numPartitions=<span class="literal">None</span>)：</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">执行right outer join</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><br>示例：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark <span class="keyword">import</span> SparkContext <span class="comment"># 驱动</span></span><br><span class="line"><span class="keyword">from</span> pyspark <span class="keyword">import</span> SparkConf <span class="comment"># 基本设定，比如executor设定可以用多少的计算资源，任务叫什么名字</span></span><br><span class="line">conf = SparkConf().setAppName(<span class="string">&quot;miniProject&quot;</span>).setMaster(<span class="string">&quot;local[*]&quot;</span>)<span class="comment"># setAppName设定任务名字，setMaster指定master在哪</span></span><br><span class="line">sc = SparkContext.getOrCreate(conf) <span class="comment"># 初始化</span></span><br><span class="line"></span><br><span class="line">numbersRDD = sc.parallelize([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>])</span><br><span class="line">moreNumbersRDD = sc.parallelize([<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>])</span><br><span class="line"></span><br><span class="line">numbersRDD.union(moreNumbersRDD).collect() <span class="comment"># 两个rdd组合成一个</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">[1, 2, 3, 2, 3, 4]</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">numbersRDD.intersection(moreNumbersRDD).collect() <span class="comment">#取交集</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">[2, 3]</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">numbersRDD.subtract(moreNumbersRDD).collect() <span class="comment">#取numbersRDD独有的</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">[1]</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">numbersRDD.cartesian(moreNumbersRDD).collect() <span class="comment">#笛卡尔组合</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">[(1, 2), (1, 3), (1, 4), (2, 2), (2, 3), (2, 4), (3, 2), (3, 3), (3, 4)]</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">rdd = sc.parallelize([<span class="string">&quot;Hello hello&quot;</span>, <span class="string">&quot;Hello New York&quot;</span>, <span class="string">&quot;York says hello&quot;</span>])</span><br><span class="line">resultRDD = (</span><br><span class="line">    rdd</span><br><span class="line">    .flatMap(<span class="keyword">lambda</span> sentence: sentence.split(<span class="string">&quot; &quot;</span>))  <span class="comment"># split into words</span></span><br><span class="line">    .<span class="built_in">map</span>(<span class="keyword">lambda</span> word: word.lower())                 <span class="comment"># lowercase</span></span><br><span class="line">    .<span class="built_in">map</span>(<span class="keyword">lambda</span> word: (word, <span class="number">1</span>))                    <span class="comment"># count each appearance</span></span><br><span class="line">    .reduceByKey(<span class="keyword">lambda</span> x, y: x + y) </span><br><span class="line">)</span><br><span class="line">resultRDD.collect()</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">[(&#x27;hello&#x27;, 4), (&#x27;york&#x27;, 2), (&#x27;new&#x27;, 1), (&#x27;says&#x27;, 1)]</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="comment"># 将结果以k-v字典的形式返回</span></span><br><span class="line">result = resultRDD.collectAsMap()</span><br><span class="line">result</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">&#123;&#x27;hello&#x27;: 4, &#x27;york&#x27;: 2, &#x27;new&#x27;: 1, &#x27;says&#x27;: 1&#125;</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="comment"># 如果你想要出现频次最高的2个词，可以这么做:</span></span><br><span class="line"><span class="comment"># 按照出现次数降序统计前2。x=(word,count)</span></span><br><span class="line">resultRDD.sortBy(keyfunc=<span class="keyword">lambda</span> x: x[<span class="number">1</span>], ascending=<span class="literal">False</span>).take(<span class="number">2</span>)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">[(&#x27;hello&#x27;, 4), (&#x27;york&#x27;, 2)]</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="comment"># Home of different people</span></span><br><span class="line">homesRDD = sc.parallelize([</span><br><span class="line">        (<span class="string">&#x27;Brussels&#x27;</span>, <span class="string">&#x27;John&#x27;</span>),</span><br><span class="line">        (<span class="string">&#x27;Brussels&#x27;</span>, <span class="string">&#x27;Jack&#x27;</span>),</span><br><span class="line">        (<span class="string">&#x27;Leuven&#x27;</span>, <span class="string">&#x27;Jane&#x27;</span>),</span><br><span class="line">        (<span class="string">&#x27;Antwerp&#x27;</span>, <span class="string">&#x27;Jill&#x27;</span>),</span><br><span class="line">    ])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Quality of life index for various cities</span></span><br><span class="line">lifeQualityRDD = sc.parallelize([</span><br><span class="line">        (<span class="string">&#x27;Brussels&#x27;</span>, <span class="number">10</span>),</span><br><span class="line">        (<span class="string">&#x27;Antwerp&#x27;</span>, <span class="number">7</span>),</span><br><span class="line">        (<span class="string">&#x27;RestOfFlanders&#x27;</span>, <span class="number">5</span>),</span><br><span class="line">    ])</span><br><span class="line"><span class="comment">#类似sql的join on操作</span></span><br><span class="line">homesRDD.join(lifeQualityRDD).collect()</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">[(&#x27;Brussels&#x27;, (&#x27;John&#x27;, 10)),</span></span><br><span class="line"><span class="string"> (&#x27;Brussels&#x27;, (&#x27;Jack&#x27;, 10)),</span></span><br><span class="line"><span class="string"> (&#x27;Antwerp&#x27;, (&#x27;Jill&#x27;, 7))]</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="comment"># left join</span></span><br><span class="line">homesRDD.leftOuterJoin(lifeQualityRDD).collect()</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">[(&#x27;Brussels&#x27;, (&#x27;John&#x27;, 10)),</span></span><br><span class="line"><span class="string"> (&#x27;Brussels&#x27;, (&#x27;Jack&#x27;, 10)),</span></span><br><span class="line"><span class="string"> (&#x27;Antwerp&#x27;, (&#x27;Jill&#x27;, 7)),</span></span><br><span class="line"><span class="string"> (&#x27;Leuven&#x27;, (&#x27;Jane&#x27;, None))]</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="comment"># right join</span></span><br><span class="line">homesRDD.rightOuterJoin(lifeQualityRDD).collect()</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">[(&#x27;Brussels&#x27;, (&#x27;John&#x27;, 10)),</span></span><br><span class="line"><span class="string"> (&#x27;Brussels&#x27;, (&#x27;Jack&#x27;, 10)),</span></span><br><span class="line"><span class="string"> (&#x27;Antwerp&#x27;, (&#x27;Jill&#x27;, 7)),</span></span><br><span class="line"><span class="string"> (&#x27;RestOfFlanders&#x27;, (None, 5))]</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="comment"># outer join 惰性</span></span><br><span class="line">homesRDD.cogroup(lifeQualityRDD).collect()</span><br><span class="line">[(x, <span class="built_in">tuple</span>(<span class="built_in">map</span>(<span class="built_in">list</span>, y))) <span class="keyword">for</span> x, y <span class="keyword">in</span> homesRDD.cogroup(lifeQualityRDD).collect()]</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">[(&#x27;Brussels&#x27;,</span></span><br><span class="line"><span class="string">  (&lt;pyspark.resultiterable.ResultIterable at 0x229adbf04a8&gt;,</span></span><br><span class="line"><span class="string">   &lt;pyspark.resultiterable.ResultIterable at 0x229adbf05c0&gt;)),</span></span><br><span class="line"><span class="string"> (&#x27;Antwerp&#x27;,</span></span><br><span class="line"><span class="string">  (&lt;pyspark.resultiterable.ResultIterable at 0x229adbf04e0&gt;,</span></span><br><span class="line"><span class="string">   &lt;pyspark.resultiterable.ResultIterable at 0x229adbf06a0&gt;)),</span></span><br><span class="line"><span class="string"> (&#x27;RestOfFlanders&#x27;,</span></span><br><span class="line"><span class="string">  (&lt;pyspark.resultiterable.ResultIterable at 0x229adbf0668&gt;,</span></span><br><span class="line"><span class="string">   &lt;pyspark.resultiterable.ResultIterable at 0x229adbf0710&gt;)),</span></span><br><span class="line"><span class="string"> (&#x27;Leuven&#x27;,</span></span><br><span class="line"><span class="string">  (&lt;pyspark.resultiterable.ResultIterable at 0x229adbf05f8&gt;,</span></span><br><span class="line"><span class="string">   &lt;pyspark.resultiterable.ResultIterable at 0x229adbf07f0&gt;))]</span></span><br><span class="line"><span class="string">[(&#x27;Brussels&#x27;, ([&#x27;John&#x27;, &#x27;Jack&#x27;], [10])),</span></span><br><span class="line"><span class="string"> (&#x27;Antwerp&#x27;, ([&#x27;Jill&#x27;], [7])),</span></span><br><span class="line"><span class="string"> (&#x27;RestOfFlanders&#x27;, ([], [5])),</span></span><br><span class="line"><span class="string"> (&#x27;Leuven&#x27;, ([&#x27;Jane&#x27;], []))]</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="comment"># Oops!  Those &lt;ResultIterable&gt;s are Spark&#x27;s way of returning a list</span></span><br><span class="line"><span class="comment"># that we can walk over, without materializing the list.</span></span><br><span class="line"><span class="comment"># Let&#x27;s materialize the lists to make the above more readable:</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># x=(home, (allPeople, allQualities))</span></span><br><span class="line">homesRDD.cogroup(lifeQualityRDD).<span class="built_in">map</span>(<span class="keyword">lambda</span> x:(x[<span class="number">0</span>], (<span class="built_in">list</span>(x[<span class="number">1</span>][<span class="number">0</span>]), <span class="built_in">list</span>(x[<span class="number">1</span>][<span class="number">1</span>])))).collect()</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">[(&#x27;Brussels&#x27;, ([&#x27;John&#x27;, &#x27;Jack&#x27;], [10])),</span></span><br><span class="line"><span class="string"> (&#x27;Antwerp&#x27;, ([&#x27;Jill&#x27;], [7])),</span></span><br><span class="line"><span class="string"> (&#x27;RestOfFlanders&#x27;, ([], [5])),</span></span><br><span class="line"><span class="string"> (&#x27;Leuven&#x27;, ([&#x27;Jane&#x27;], []))]</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure></p>
<h2 id="RDD行动操作"><a href="#RDD行动操作" class="headerlink" title="RDD行动操作"></a>RDD行动操作</h2><p>（1）行动操作（action）会对RDD计算出一个结果，并将结果返回到driver程序中（或者把结果存储到外部存储系统，如HDFS 中）。行动操作会强制执行依赖的中间RDD的计算。<br>（2）每当调用一个新的行动操作时，整个RDD都会从头开始计算。要避免这种低效的行为，用户可以将中间RDD持久化。<br>（3）在调用<code>sc.textFile()</code>时，并没有执行读取，而是在使用时候读取。如果未对读取的结果RDD缓存，则该读取操作可能被多次执行。<br>（4）Spark采取惰性求值的原因：通过惰性求值，可以把一些操作合并起来从而简化计算过程。</p>
<h3 id="通用行动操作"><a href="#通用行动操作" class="headerlink" title="通用行动操作"></a>通用行动操作</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">reduce(f)：</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">通过 f 来聚合当前RDD。</span></span><br><span class="line"><span class="string">f 操作两个相同元素类型的RDD数据，并且返回一个同样类型的新元素。</span></span><br><span class="line"><span class="string">该行动操作的结果得到一个值（类型与RDD中的元素类型相同）。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">fold(zeroValue,op)：</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">通过 op 操作聚合当前RDD。该操作首先对每个分区中的元素进行聚合（聚合的第一个数由zeroValue 提供）。</span></span><br><span class="line"><span class="string">    然后将分区聚合结果与zeroValue再进行聚合。</span></span><br><span class="line"><span class="string">f 操作两个相同元素类型的RDD数据，并且返回一个同样类型的新元素</span></span><br><span class="line"><span class="string">该行动操作的结果得到一个值（类型与RDD中的元素类型相同）。</span></span><br><span class="line"><span class="string">zeroValue参与了分区元素聚合过程，也参与了分区聚合结果的再聚合过程。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">aggregate(zeroValue,seqOp,combOp)：</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">该操作也是聚合当前RDD。聚合的步骤为：</span></span><br><span class="line"><span class="string">（1）首先以分区为单位，对当前RDD执行seqOp来进行聚合。聚合的结果不一定与当前RDD元素相同类型。</span></span><br><span class="line"><span class="string">（2）然后以zeroValue为初始值，将分区聚合结果按照combOp来聚合（聚合的第一个数由zeroValue提供），得到最终的聚合结果。</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">zeroValue：combOp聚合函数的初始值。类型与最终结果类型相同。</span></span><br><span class="line"><span class="string">seqOp：分区内的聚合函数，返回类型与zeroValue相同。</span></span><br><span class="line"><span class="string">combOp：分区之间的聚合函数。 </span></span><br><span class="line"><span class="string">zeroValue参与了分区元素聚合过程，也参与了分区聚合结果的再聚合过程。</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">示例，取均值：</span></span><br><span class="line"><span class="string">sum_count = nums.aggregate((0,0),</span></span><br><span class="line"><span class="string">         (lambda acc,value:(acc[0]+value,acc[1]+1),</span></span><br><span class="line"><span class="string">         (lambda acc1,acc2:(acc1[0]+acc2[0],acc1[1]+acc2[1]))</span></span><br><span class="line"><span class="string">)</span></span><br><span class="line"><span class="string">avg = sum_count[0]/float(sum_count[1])</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>
<p>获取RDD中的全部或者部分元素：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">collect()：</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">它将整个RDD的内容以列表的形式返回到driver 程序中。</span></span><br><span class="line"><span class="string">通常在测试中使用，且当RDD 内容不大时使用，要求所有结果都能存入单台机器的内存中</span></span><br><span class="line"><span class="string">它返回元素的顺序可能与你预期的不一致。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">take(n)：</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">以列表的形式返回RDD中的n个元素到driver程序中。</span></span><br><span class="line"><span class="string">它会尽可能的使用尽量少的分区。</span></span><br><span class="line"><span class="string">它返回元素的顺序可能与你预期的不一致。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">takeOrdered(n,key=<span class="literal">None</span>)：</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">以列表的形式按照指定的顺序（默认降序）返回RDD中的n个元素到driver程序中。</span></span><br><span class="line"><span class="string">你也可以提供key参数来指定比较函数。</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">sc.parallelize([10, 1, 2, 9, 3, 4, 5, 6, 7]).takeOrdered(6) #升序取前6个</span></span><br><span class="line"><span class="string">sc.parallelize([10, 1, 2, 9, 3, 4, 5, 6, 7]).takeOrdered(6, key=lambda x: -x)#相当于降序取前6个</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">takeSample(withReplacement,num,seed=<span class="literal">None</span>)：</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">以列表的形式返回对RDD随机采样的结果。</span></span><br><span class="line"><span class="string">withReplacement：如果为True，则可以重复采样；否则是无放回采样</span></span><br><span class="line"><span class="string">num：预期采样结果的数量。如果是重复采样，则最终采样结果就是num。如果是无放回采样，则最终采样结果不能超过RDD的大小</span></span><br><span class="line"><span class="string">seed：随机数生成器的种子</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">top(n,key=<span class="literal">None</span>)：</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">获取RDD 的前n个元素。</span></span><br><span class="line"><span class="string">默认情况下，它使用数据降序的 top n。你也可以提供key参数来指定比较函数。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">first()：</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">获取RDD 中的第一个元素。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><br>计数：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">count()：</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">返回RDD的元素总数量（不考虑去重）。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">countByValue()：</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">以字典的形式返回RDD中，各元素出现的次数。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">histogram(buckets)：</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">计算分桶。</span></span><br><span class="line"><span class="string">buckets：指定如何分桶。</span></span><br><span class="line"><span class="string">    如果是一个序列，则它指定了桶的区间。如[1,10,20,50] 代表了区间[1,10) [10,20) [20,50]（最后一个桶是闭区间）。</span></span><br><span class="line"><span class="string">        该序列必须是排序好的，且不能包含重复数字，且至少包含两个数字。</span></span><br><span class="line"><span class="string">    如果是一个数字，则指定了桶的数量。它会自动将数据划分到min~max 之间的、均匀分布的桶中。它必须大于等于1。</span></span><br><span class="line"><span class="string">返回值：一个元组 (桶区间序列，桶内元素个数序列)</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">foreach(f)：</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">对当前RDD的每个元素执行函数f。</span></span><br><span class="line"><span class="string">它与map(f)不同。map是转换操作，而foreach是行动操作。</span></span><br><span class="line"><span class="string">通常foreach用于将RDD的数据以json格式发送到网络服务器上，或者写入到数据库中。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">foreachPartition(f)：</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">对当前RDD的每个分区应用函数f。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><br>统计方法：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">max</span>(key=<span class="literal">None</span>)：</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">返回RDD的最大值。</span></span><br><span class="line"><span class="string">key：对RDD中的值进行映射，比较的是key(x)之后的结果（但是返回的是x而不是映射之后的结果）。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="built_in">min</span>(key=<span class="literal">None</span>)：</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">返回RDD的最小值。</span></span><br><span class="line"><span class="string">key：对RDD中的值进行映射，比较的是key(x)之后的结果（但是返回的是x而不是映射之后的结果）。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">mean()：</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">返回RDD的均值。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">sampleStdev()：</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">计算样本标准差。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">sampleVariance()：</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">计算样本方差。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">stdev()：</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">计算标准差。它与样本标准差的区别在于：分母不同。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">variance()：</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">计算方差。它与样本方差的区别在于：分母不同。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="built_in">sum</span>()：</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">计算总和。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure></p>
<h3 id="Pair-RDD行动操作"><a href="#Pair-RDD行动操作" class="headerlink" title="Pair RDD行动操作"></a>Pair RDD行动操作</h3><p>Pair RDD 可以使用所有标准RDD上的可用的行动操作：由于Pair RDD的元素是二元元组，因此传入的函数应当操作二元元组，而不是独立的元素。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">countByKey()：</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">以字典的形式返回每个键的元素数量。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">collectAsMap()：</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">以字典的形式返回所有的键值对。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">lookup(key)：</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">以列表的形式返回指定键的所有的值。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure></p>
<h2 id="RDD其他方法和属性"><a href="#RDD其他方法和属性" class="headerlink" title="RDD其他方法和属性"></a>RDD其他方法和属性</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 属性</span></span><br><span class="line">.context：返回创建该RDD的SparkContext。</span><br><span class="line"><span class="comment"># 方法</span></span><br><span class="line">.<span class="built_in">id</span>()：返回RDD的ID。</span><br><span class="line">.isEmpty()：当且仅当RDD为空时，它返回<span class="literal">True</span>；否则返回<span class="literal">False</span>。</span><br><span class="line">.name()：返回RDD的名字。</span><br><span class="line">.setName(name)：设置RDD的名字。</span><br><span class="line">.stats()：返回一个StatCounter对象，用于计算RDD的统计值。</span><br><span class="line">.toDebugString()：返回一个RDD的描述字符串，用于调试。</span><br><span class="line">.toLocalIterator()：返回一个迭代器，对它迭代的结果就是对RDD的遍历。</span><br></pre></td></tr></table></figure>
<h2 id="RDD持久化"><a href="#RDD持久化" class="headerlink" title="RDD持久化"></a>RDD持久化</h2><p>如果简单的对RDD调用行动操作，则Spark每次都会重新计算RDD以及它所有的依赖RDD。在迭代算法中，消耗会格外大。因为迭代算法通常会使用同一组数据。</p>
<p>当我们让spark持久化存储一个RDD时，计算出RDD的节点会分别保存它们所求出的分区数据。如果一个拥有持久化数据的节点发生故障，则spark会在需要用到该缓存数据时，重新计算丢失的分区数据。我们也可以将数据备份到多个节点上，从而增加对数据丢失的鲁棒性。</p>
<p>获取当前RDD缓存级别：<code>getStorageLevel()</code>。<br>我们可以为RDD选择不同的持久化级别，这些级别写在<code>pyspark.StorageLevel</code>中：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line">pyspark.StorageLevel(useDisk, useMemory, useOffHeap, deserialized, replication=<span class="number">1</span>)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">useDisk：使用硬盘（外存）。</span></span><br><span class="line"><span class="string">useMemory：使用内存。</span></span><br><span class="line"><span class="string">useOffHeap：使用堆外存。这是Java虚拟机里的概念，堆外存意味着把内存对象分配在Java虚拟机的堆以外的内存，这些内存直接受操作系统管理（不是虚拟机）。这样做的结果是能保持一个较小的堆，以减少垃圾收集对应用的影响。</span></span><br><span class="line"><span class="string">deserialized：反序列化。其逆过程序列化（Serialization）是Java提供的一种机制，将对象表示成一连串的字节；而反序列化就表示将字节恢复为对象的过程。序列化是对象永久化的一种机制，可以将对象及其属性保存起来，并能在反序列化后直接恢复这个对象。</span></span><br><span class="line"><span class="string">replication：备份数（在多个结点上备份）。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="comment"># 定义好的常量</span></span><br><span class="line">DISK_ONLY = StorageLevel(<span class="literal">True</span>, <span class="literal">False</span>, <span class="literal">False</span>, <span class="literal">False</span>, <span class="number">1</span>)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">数据缓存在磁盘中。</span></span><br><span class="line"><span class="string">内存占用：低；CPU 时间：高；是否在内存：否；是否在磁盘中：是。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">DISK_ONLY_2 = StorageLevel(<span class="literal">True</span>, <span class="literal">False</span>, <span class="literal">False</span>, <span class="literal">False</span>, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">MEMORY_AND_DISK = StorageLevel(<span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">False</span>, <span class="literal">False</span>, <span class="number">1</span>)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">数据缓存在内存和硬盘中。</span></span><br><span class="line"><span class="string">内存占用：高；CPU 时间：中等；是否在内存：部分；是否在磁盘中：部分。</span></span><br><span class="line"><span class="string">如果数据在内存中放不下，则溢写到磁盘上。如果数据在内存中放得下，则缓存到内存中。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">MEMORY_AND_DISK_2 = StorageLevel(<span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">False</span>, <span class="literal">False</span>, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">MEMORY_AND_DISK_SER = StorageLevel(<span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">False</span>, <span class="literal">False</span>, <span class="number">1</span>)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">数据经过序列化之后缓存在内存和硬盘中。</span></span><br><span class="line"><span class="string">内存占用：低；CPU 时间：高；是否在内存：部分；是否在磁盘中：部分。</span></span><br><span class="line"><span class="string">如果数据在内存中放不下，则溢写到磁盘上。如果数据在内存中放得下，则缓存到内存中。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">MEMORY_AND_DISK_SER_2 = StorageLevel(<span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">False</span>, <span class="literal">False</span>, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">MEMORY_ONLY = StorageLevel(<span class="literal">False</span>, <span class="literal">True</span>, <span class="literal">False</span>, <span class="literal">False</span>, <span class="number">1</span>)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">数据缓存在内存中。</span></span><br><span class="line"><span class="string">内存占用：高；CPU 时间：低；是否在内存：是；是否在磁盘中：否。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">MEMORY_ONLY_2 = StorageLevel(<span class="literal">False</span>, <span class="literal">True</span>, <span class="literal">False</span>, <span class="literal">False</span>, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">MEMORY_ONLY_SER = StorageLevel(<span class="literal">False</span>, <span class="literal">True</span>, <span class="literal">False</span>, <span class="literal">False</span>, <span class="number">1</span>)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">数据经过序列化之后缓存在内存中。</span></span><br><span class="line"><span class="string">内存占用：低；CPU 时间：高；是否在内存：是；是否在磁盘中：否。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">MEMORY_ONLY_SER_2 = StorageLevel(<span class="literal">False</span>, <span class="literal">True</span>, <span class="literal">False</span>, <span class="literal">False</span>, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">OFF_HEAP = StorageLevel(<span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">False</span>, <span class="number">1</span>)</span><br></pre></td></tr></table></figure><br>持久化方法：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">persist(storageLevel=StorageLevel(<span class="literal">False</span>, <span class="literal">True</span>, <span class="literal">False</span>, <span class="literal">False</span>, <span class="number">1</span>))：</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">对当前RDD进行持久化。默认是将数据以序列化的形式缓存在JVM的堆空间中。</span></span><br><span class="line"><span class="string">该方法调用时，并不会立即执行持久化，它并不会触发求值，而仅仅是对当前RDD做个持久化标记。</span></span><br><span class="line"><span class="string">    一旦该RDD第一次求值时，才会发生持久化。</span></span><br><span class="line"><span class="string">storageLevel：可以手动设置，也可以选择定义好的常量。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">cache()：</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">也是一种持久化调用。它等价于persist(MEMORY_ONLY)，它不可设定缓存级别。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">unpersist()：</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">标记当前RDD是未缓存的，并且将所有该RDD已经缓存的数据从内存、硬盘中清除。</span></span><br><span class="line"><span class="string">当要缓存的数据太多，内存放不下时，spark会利用最近最少使用(LRU)的缓存策略，把最老的分区从内存中移除。</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">对于MEMORY_ONLY、MEMORY_ONLY_SER级别：下一次要用到已经被移除的分区时，这些分区就要重新计算。</span></span><br><span class="line"><span class="string">对于MEMORY_AND_DISK、MEMORY_AND_DISK_SER级别：被移除的分区都会被写入磁盘。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure></p>
<h2 id="RDD分区"><a href="#RDD分区" class="headerlink" title="RDD分区"></a>RDD分区</h2><p>如果使用可控的分区方式，将经常被一起访问的数据放在同一个节点上，那么可以大大减少应用的通信开销。通过正确的分区，可以带来明显的性能提升。为分布式数据集选择正确的分区，类似于为传统的数据集选择合适的数据结构。</p>
<p>分区并不是对所有应用都是有好处的：如果给定的RDD只需要被扫描一次，则我们完全没有必要对其预先进行分区处理。只有当数据集多次在诸如连接基于键的操作中使用时，分区才会有帮助。</p>
<p>Spark 中所有的键值对RDD都可以进行分区。系统会根据一个针对键的函数对元素进行分。Spark 可以确保同一个组的键出现在同一个节点上。</p>
<p>许多Spark操作会自动为结果RDD设定分区，比如<code>sortByKey()</code>会自动生成范围分区的RDD。<code>groupByKey()</code>会自动生成哈希分区的RDD。</p>
<p>许多Spark操作会利用已有的分区信息，如<code>join()</code>、<code>leftOuterJoin()</code>、<code>rightOuterJoin()</code>、<code>cogroup()</code>、<code>groupWith()</code>、<code>groupByKey()</code>、<code>reduceByKey()</code>、<code>combineByKey()</code>、<code>lookup()</code>。这些操作都能从分区中获得收益。任何需要将数据根据键跨节点进行混洗的操作，都能够从分区中获得好处。</p>
<p>对于二元操作，输出数据的分区方式取决于输入RDD的分区方式：<br>（1）默认情况下，结果采用哈希分区。<br>（2）若其中一个输入RDD已经设置过分区方式，则结果就使用该分区方式。<br>（3）如果两个输入RDD都设置过分区方式，则使用第一个输入的分区方式。</p>
<p>对于<code>map()</code>操作，由于理论上它可能改变元素的键，因此其结果不会有固定的分区方式。</p>
<p><strong>通过<code>getNumPartitions()</code>方法或者<code>.getNumPartitions</code>属性查看RDD的分区数。</strong></p>
<p><strong>指定分区：</strong><br>在执行聚合或者分组操作时，可以要求Spark使用指定的分区数（即numPartitions参数）。如果未指定该参数，则Spark根据集群的大小会自动推断出一个有意义的默认值。</p>
<p>如果我们希望在除了聚合/分组操作之外，也能改变RDD的分区。那么Spark提供了<code>repartition()</code>方法：<br>（1）它通过创建更过或更少的分区将数据随机的打散，让数据在不同分区之间相对均匀。这个操作经常是通过网络进行数打散。<br>（2）该方法是代价比较大的操作，你可以通过<code>.coalesce()</code>方法将RDD的分区数减少。它是一个代价相对较小的操作。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">repartition(numPartitions)：</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">返回一个拥有指定分区数量的新的RDD。</span></span><br><span class="line"><span class="string">新的分区数量可能比旧分区数增大，也可能减小。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">coalesce(numPartitions,shuffle=<span class="literal">False</span>)：</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">返回一个拥有指定分区数量的新的RDD。</span></span><br><span class="line"><span class="string">新的分区数量必须比旧分区数减小。</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"># 比如，保存时使用</span></span><br><span class="line"><span class="string">df.coalesce(1).write.format(&quot;csv&quot;)</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">partitionBy(numPartitions, partitionFunc=&lt;function portable_hash at <span class="number">0x7f51f1ac0668</span>&gt;)：</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">返回一个使用指定分区器和分区数量的新的RDD。</span></span><br><span class="line"><span class="string">新的分区数量可能比旧分区数增大，也可能减小。</span></span><br><span class="line"><span class="string">这里partitionFunc是分区函数。注意：如果你想让多个RDD使用同一个分区方式，</span></span><br><span class="line"><span class="string">                              则应该使用同一个分区函数对象（如全局函数），而不要给每个RDD创建一个新的函数对象。</span></span><br></pre></td></tr></table></figure></p>
<h2 id="RDD混洗"><a href="#RDD混洗" class="headerlink" title="RDD混洗"></a>RDD混洗</h2><p>Spark中的某些操作会触发shuffle。<br>shuffle是Spark重新分配数据的一种机制，它使得这些数据可以跨不同区域进行分组。<br>这通常涉及到磁盘IO、数据序列化、网络IO，使得shuffle成为一个复杂的、代价高昂的操作。某些混洗操作会大量消耗堆内存空间，因为混洗操作在数据转换前后，需要使用内存中的数据结构对数据进行组织。</p>
<p>在Spark里，特定的操作需要数据不会跨分区分布。如果跨分区分布，则需要混洗。<br>以reduceByKey操作的过程为例：一个key的所有值不一定在同一个分区里，甚至不一定在同一台机器里。但是它们必须共同被计算。为了所有数据都在单个reduceByKey的reduce任务上运行，需要执行一个all-to-all操作：它必须从所有分区读取所有的key和key对应的所有的值，并且跨分区聚集取计算每个key的结果。这个过程叫做shuffle。</p>
<p>触发混洗的操作包括：<br>（1）重分区操作，如repartition、coalesce。<br>（2）ByKey操作，如groupByKey、reduceByKey。<br>（3）join操作，如cogroup、join。</p>
<h1 id="Spark-SQL"><a href="#Spark-SQL" class="headerlink" title="Spark SQL"></a>Spark SQL</h1><p>Spark SQL是Spark中用于处理结构化数据的一个模块。</p>
<p>Python操作RDD，转换为可执行代码，运行在java虚拟机，涉及两个不同语言引擎之间的切换，进行进程间        通信很耗费性能。而且做某项任务所需要的代码量也不少，尤此出现了Spark SQL，对比下用RDD和Spark SQL（基于DataFrame）两种方式执行同一个任务的代码量：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># RDD</span></span><br><span class="line">sc.textFile(<span class="string">&quot;file:///home/hadoop/derby.log&quot;</span>).flatMap(<span class="keyword">lambda</span> x: x.split(<span class="string">&quot; &quot;</span>))\</span><br><span class="line">.<span class="built_in">map</span>(<span class="keyword">lambda</span> x: (x,<span class="number">1</span>)).reduceByKey(<span class="keyword">lambda</span> a,b:a+b).collect()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Spark SQL支持使用SQL和DataFrame方式处理数据</span></span><br><span class="line">ssc.sql(<span class="string">&#x27;slect world,count(*) as cnt from s group by word&#x27;</span>)</span><br><span class="line"></span><br><span class="line">df.groupBy(<span class="string">&#x27;word&#x27;</span>).count().show()</span><br></pre></td></tr></table></figure><br>使用DataFrame比使用SQL处理数据更加方便简洁。</p>
<p><strong>Spark SQL：是用于操作结构化数据的程序包</strong><br>（1）通过Spark SQL，可以使用SQL/HQL来查询数据，查询结果以Dataset/DataFrame的形式返回。<br>（2）它支持多种数据源，如Hive 表、Parquet 以及 JSON 等。<br>（3）它支持开发者将SQL 和传统的RDD 变成相结合。</p>
<p><strong>Dataset：是一个分布式的数据集合</strong><br>（1）它是Spark 1.6中被添加的新接口<br>（2）它提供了RDD的优点与Spark SQL执行引擎的优点。<br>（1）它在Scala和Java中是可用的。Python不支持Dataset API。但是由于Python的动态特性，许多DataSet API的优点已经可用。</p>
<p><strong>DataFrame：是一个Dataset组成的指定列</strong><br>（1）它的概念等价于一个关系型数据库中的表。<br>（1）在Scala/Python中，DataFrame由DataSet中的<strong>RowS(多个Row)</strong>来表示。</p>
<p><strong>在Spark 2.0之后，SQLContext被SparkSession取代。</strong></p>
<h2 id="SparkSession"><a href="#SparkSession" class="headerlink" title="SparkSession"></a>SparkSession</h2><p>Spark SQL中所有功能的入口点是SparkSession类。它可以用于创建DataFrame、注册DataFrame为表、在表上执行SQL、缓存table、读写文件等等。</p>
<p>要创建一个SparkSession，仅仅使用SparkSession.builder即可：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession</span><br><span class="line"></span><br><span class="line">ssc = SparkSession \</span><br><span class="line">    .builder \</span><br><span class="line">    .appName(<span class="string">&quot;Python Spark SQL&quot;</span>) \</span><br><span class="line">    .config(<span class="string">&quot;spark.some.config.option&quot;</span>, <span class="string">&quot;some-value&quot;</span>) \</span><br><span class="line">    .getOrCreate()</span><br></pre></td></tr></table></figure><br>Builder用于创建SparkSession，它的方法有（这些方法都返回实例）：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">.appName(name)：</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">给程序设定一个名字，用于在Spark web UI中展示。如果未指定，则会随机生成一个。</span></span><br><span class="line"><span class="string">name：一个字符串，表示程序的名字。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">.config(key=<span class="literal">None</span>,value=<span class="literal">None</span>,conf=<span class="literal">None</span>)：</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">通过**键值对**设置，或者使用**已有的SparkConf**设置，如.config(conf=SparkConf())</span></span><br><span class="line"><span class="string">这里设定的配置会直接传递给SparkConf和SparkSession各自的配置。</span></span><br><span class="line"><span class="string">key：一个字符串，表示配置名；</span></span><br><span class="line"><span class="string">value：对应配置的值；</span></span><br><span class="line"><span class="string">conf：一个SparkConf实例。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">.enableHiveSupport()：</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">开启Hive支持。2.0的新接口。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">.master(master)：</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">设置Spark master URL。如：</span></span><br><span class="line"><span class="string">    master=local：表示单机本地运行。</span></span><br><span class="line"><span class="string">    master=local[4]：表示单机本地4核运行。</span></span><br><span class="line"><span class="string">    master=spark://master:7077：表示在一个standalone模式集群运行。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">.getOrCreate()：</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">返回一个已有的SparkSession实例。如果没有则基于当前builder的配置，创建一个新的SparkSession实例。</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">首先检测是否有一个有效的全局默认SparkSession实例。</span></span><br><span class="line"><span class="string">    如果有，则返回它；</span></span><br><span class="line"><span class="string">    如果没有，则创建一个作为全局默认SparkSession实例，并返回它。</span></span><br><span class="line"><span class="string">如果已有一个有效的全局默认SparkSession实例，则当前builder的配置将应用到该实例上。</span></span><br></pre></td></tr></table></figure><br>SparkSession实例的属性：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">.builder：</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">一个Builder实例</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">.catalog：</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">一个接口。用户通过它来create、drop、alter、query底层的数据库、表以及函数等。</span></span><br><span class="line"><span class="string">通过SparkSession.catalog.cacheTable(&#x27;tableName&#x27;)来缓存表；</span></span><br><span class="line"><span class="string">通过SparkSession.catalog.uncacheTable(&#x27;tableName&#x27;)来从缓存中删除该表。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">.conf：</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">spark 的运行时配置接口。通过它，你可以获取、设置spark、hadoop 的配置。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">.read：</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">返回一个DataFrameReader，用于从外部存储系统中读取数据并返回DataFrame。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">.readStream：</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">返回一个DataStreamReader，用于将输入数据流视作一个DataFrame来读取。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">.sparkContext：</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">返回底层的SparkContext。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">.streams：</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">返回一个StreamingQueryManager对象，它管理当前上下文的所有活动的StreamingQuery。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">.udf：</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">返回一个UDFRegistration，用于UDF注册。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">.version：</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">返回当前应用的Spark版本。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><br>SparkSession实例的方法：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">.createDataFrame(data, schema=<span class="literal">None</span>, samplingRatio=<span class="literal">None</span>, verifySchema=<span class="literal">True</span>)：</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">从RDD 、一个列表、或者pandas.DataFrame 中创建一个DataFrame</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">data：输入数据。可以为一个RDD、一个列表、或者一个pandas.DataFrame</span></span><br><span class="line"><span class="string">schema：给出了DataFrame的结构化信息。可以为：</span></span><br><span class="line"><span class="string">        一个字符串的列表：给出了列名信息。此时每一列数据的类型从data中推断。</span></span><br><span class="line"><span class="string">        为None：此时要求data是一个RDD，且元素类型为Row、namedtuple、dict之一。</span></span><br><span class="line"><span class="string">                此时结构化信息从data中推断（推断列名、列类型）</span></span><br><span class="line"><span class="string">        为pyspqrk.sql.types.StructType：此时直接指定了每一列数据的类型。</span></span><br><span class="line"><span class="string">        为pyspark.sql.types.DataType或datatype string：此时直接指定了一列数据的类型，</span></span><br><span class="line"><span class="string">                会自动封装成pyspqrk.sql.types.StructType（只有一列）。此时要求指定的类型与data匹配（否则抛出异常）。</span></span><br><span class="line"><span class="string">samplingRatio：如果需要推断数据类型，则它指定了需要多少比例的行记录来执行推断。如果为None，则只使用第一行来推断。</span></span><br><span class="line"><span class="string">verifySchema：如果为True，则根据schema检验每一行数据。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">.newSession()：</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">返回一个新的SparkSession实例，它拥有独立的SQLConf、已注册的临时视图和UDFs，但是共享同样的SparkContext以及table cache。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">.<span class="built_in">range</span>(start,end=<span class="literal">None</span>,step=<span class="number">1</span>,numPartitions=<span class="literal">None</span>)：</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">创建一个DataFrame，它只有一列。该列的列名为id，类型为pyspark.sql.types.LongType，数值为区间[start,end)，间隔为step。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">.sql(sqlQuery)：</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">查询SQL并以DataFrame的形式返回查询结果。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">.stop()：</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">停止底层的SparkContext。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">.table(tableName)：</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">以DataFrame的形式返回指定的table。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure></p>
<h2 id="DataFrame"><a href="#DataFrame" class="headerlink" title="DataFrame"></a>DataFrame</h2><p>在Spark语义中，DataFrame是一个分布式的<strong>行集合</strong>，可以想象为一个关系型数据库的表，或者一个带有列名的Excel表格。</p>
<p><strong>1、Spark SQL的DataFrame：</strong><br>（1）是RDD为基础的分布式数据集，类似于传统关系型数据库的二维表，DataFrame记录了对应列的名称和类型。<br>（2）DataFrame引入schema（表结构）和off-heap（突破JVM限制内存限制，使用操作系统层面上的内存）：解决了RDD的缺点（序列化和反序列化开销大、频繁的创建和销毁对象造成大量的GC），但也丢失了RDD的优点（RDD编译时进行类型检查、RDD具有面向对象编程的特性）。</p>
<p><strong>2、DataFrame特点（和RDD一样）：</strong><br>（1）不可变的：一旦RDD、DataFrame被创建，就不能更改，只能通过transformation生成新的RDD、DataFrame。<br>（2）惰性：只有action才会触发Transformation的执行。<br>（3）分布式：DataFrame和RDD一样都是分布式的<br>（4）除了DataFrame还有Dataset，DataFrame只是Dataset[ROW]的类型别名。由于Python是弱类型语言，只能使用DataFrame。Dataset适合Java、Scala。</p>
<p><strong>3、DataFrame vs RDD</strong><br>（1）RDD：分布式的对象的集合，Spark并不知道对象的详细模式信息。<br>（2）DataFrame：分布式的Row对象的集合，其提供了由列组成的详细模式信息，使得Spark SQL可以进行某些形式的执行优化。</p>
<p><strong>4、Spark DataFrame vs Pandas DataFrame</strong><br>（1）Cluster Parallel：集群并行执行。<br>（2）Lazy Evaluations: 只有action才会触发Transformation的执行。<br>（3）Immutable：不可更改。<br>（4）Pandas rich API：Pandas DataFrame API 更丰富。</p>
<h3 id="创建"><a href="#创建" class="headerlink" title="创建"></a>创建</h3><p>在一个SparkSession中，应用程序可以从一个已经存在的RDD、Hive表、或者Spark数据源中创建一个DataFrame。</p>
<p>从列表/字典创建：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 从列表创建：</span></span><br><span class="line">l = [(<span class="string">&#x27;Alice&#x27;</span>, <span class="number">1</span>)]</span><br><span class="line"><span class="comment"># 自动分配列名</span></span><br><span class="line">ssc.createDataFrame(l).collect()</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">[Row(_1=&#x27;Alice&#x27;, _2=1)] </span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="comment"># 指定列名</span></span><br><span class="line">ssc.createDataFrame(l, [<span class="string">&#x27;name&#x27;</span>, <span class="string">&#x27;age&#x27;</span>]).collect()</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">[Row(name=&#x27;Alice&#x27;, age=1)]</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 从字典创建（快被移除了）：</span></span><br><span class="line">d = [&#123;<span class="string">&#x27;name&#x27;</span>: <span class="string">&#x27;Alice&#x27;</span>, <span class="string">&#x27;age&#x27;</span>: <span class="number">1</span>&#125;]</span><br><span class="line">ssc.createDataFrame(d).collect()</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">[Row(age=1, name=&#x27;Alice&#x27;)]</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><br>从RDD创建：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 自动分配列名</span></span><br><span class="line">rdd = sc.parallelize([(<span class="string">&#x27;Alice&#x27;</span>, <span class="number">1</span>)])</span><br><span class="line">ssc.createDataFrame(rdd).collect()</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">[Row(_1=&#x27;Alice&#x27;, _2=1)]</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="comment"># 指定列名</span></span><br><span class="line">ssc.createDataFrame(rdd, [<span class="string">&#x27;name&#x27;</span>, <span class="string">&#x27;age&#x27;</span>]).collect()</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">[Row(name=&#x27;Alice&#x27;, age=1)]</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><br>通过Row创建：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> Row</span><br><span class="line"></span><br><span class="line">Person = Row(<span class="string">&#x27;name&#x27;</span>, <span class="string">&#x27;age&#x27;</span>)</span><br><span class="line"><span class="comment"># 或者.map(lambda x: Row(name=x[0],age=x[1]))</span></span><br><span class="line">rdd = sc.parallelize([(<span class="string">&#x27;Alice&#x27;</span>, <span class="number">1</span>)]).<span class="built_in">map</span>(<span class="keyword">lambda</span> x: Person(*x)) </span><br><span class="line">ssc.createDataFrame(rdd, [<span class="string">&#x27;name&#x27;</span>, <span class="string">&#x27;age&#x27;</span>]).collect()</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">[Row(name=&#x27;Alice&#x27;, age=1)]</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="keyword">from</span> pyspark.sql.types <span class="keyword">import</span> *</span><br><span class="line"></span><br><span class="line">schema = StructType([</span><br><span class="line">  StructField(<span class="string">&quot;name&quot;</span>, StringType(), <span class="literal">True</span>),</span><br><span class="line">  StructField(<span class="string">&quot;age&quot;</span>, IntegerType(), <span class="literal">True</span>)])</span><br><span class="line"></span><br><span class="line">rdd = sc.parallelize([(<span class="string">&#x27;Alice&#x27;</span>, <span class="number">1</span>)])</span><br><span class="line"><span class="comment"># 指定schema</span></span><br><span class="line">ssc.createDataFrame(rdd, schema).collect()</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">[Row(name=&#x27;Alice&#x27;, age=1)]</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="comment"># 通过字符串指定schema</span></span><br><span class="line">ssc.createDataFrame(rdd, <span class="string">&quot;a: string, b: int&quot;</span>).collect()</span><br></pre></td></tr></table></figure><br>从pandas.DataFrame创建：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line">df = pd.DataFrame(&#123;<span class="string">&#x27;a&#x27;</span>:[<span class="number">1</span>,<span class="number">3</span>,<span class="number">5</span>],<span class="string">&#x27;b&#x27;</span>:[<span class="number">2</span>,<span class="number">4</span>,<span class="number">6</span>]&#125;)</span><br><span class="line">ssc.createDataFrame(df).collect()  </span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">[Row(a=1, b=2), Row(a=3, b=4), Row(a=5, b=6)]</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><br>从数据源创建：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># .read从数据源创建接口</span></span><br><span class="line"></span><br><span class="line">.<span class="built_in">format</span>()</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">数据源格式。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="comment"># 通用加载</span></span><br><span class="line">.load(path=<span class="literal">None</span>, <span class="built_in">format</span>=<span class="literal">None</span>, schema=<span class="literal">None</span>, **options)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">path：一个字符串，或者字符串的列表。指出了文件的路径</span></span><br><span class="line"><span class="string">format：指出了文件类型。默认为parquet（除非spark.sql.sources.default另有配置）</span></span><br><span class="line"><span class="string">schema：输入数据的schema，一个StructType类型实例</span></span><br><span class="line"><span class="string">options：其他的参数</span></span><br><span class="line"><span class="string">df = ssc.read.format(&#x27;json&#x27;).load(&#x27;./xxx.json&#x27;)</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="comment"># 专用加载</span></span><br><span class="line">.csv(path, schema=<span class="literal">None</span>, sep=<span class="literal">None</span>, encoding=<span class="literal">None</span>, quote=<span class="literal">None</span>, escape=<span class="literal">None</span>, </span><br><span class="line">      comment=<span class="literal">None</span>, header=<span class="literal">None</span>, inferSchema=<span class="literal">None</span>, ignoreLeadingWhiteSpace=<span class="literal">None</span>, </span><br><span class="line">      ignoreTrailingWhiteSpace=<span class="literal">None</span>, nullValue=<span class="literal">None</span>, positiveInf=<span class="literal">None</span>, </span><br><span class="line">      negativeInf=<span class="literal">None</span>, dateFormat=<span class="literal">None</span>, timestampFormat=<span class="literal">None</span>, maxColumns=<span class="literal">None</span>, </span><br><span class="line">      maxCharsPerColumn=<span class="literal">None</span>, maxMalformedLogPerPartition=<span class="literal">None</span>, mode=<span class="literal">None</span>, </span><br><span class="line">      columnNameOfCorruptRecord=<span class="literal">None</span>, multiLine=<span class="literal">None</span>, charToEscapeQuoteEscaping=<span class="literal">None</span>, </span><br><span class="line">      samplingRatio=<span class="literal">None</span>, enforceSchema=<span class="literal">None</span>, emptyValue=<span class="literal">None</span>, locale=<span class="literal">None</span>, lineSep=<span class="literal">None</span>, </span><br><span class="line">      pathGlobFilter=<span class="literal">None</span>, recursiveFileLookup=<span class="literal">None</span>, modifiedBefore=<span class="literal">None</span>, </span><br><span class="line">      modifiedAfter=<span class="literal">None</span>, unescapedQuoteHandling=<span class="literal">None</span>)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">加载csv文件，返回一个DataFrame实例。</span></span><br><span class="line"><span class="string">path：路径</span></span><br><span class="line"><span class="string">header：True表示第一行作为列名，默认为False</span></span><br><span class="line"><span class="string">inferSchema：True表示自动推断字段类型，默认为False</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">.jdbc(url, table, column=<span class="literal">None</span>, lowerBound=<span class="literal">None</span>, upperBound=<span class="literal">None</span>, numPartitions=<span class="literal">None</span>, </span><br><span class="line">      predicates=<span class="literal">None</span>, properties=<span class="literal">None</span>)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">加载数据库中的表。</span></span><br><span class="line"><span class="string">url：一个JDBC URL，格式为：jdbc:subprotocol:subname</span></span><br><span class="line"><span class="string">table：表名</span></span><br><span class="line"><span class="string">column：列名。该列为整数列，用于分区。</span></span><br><span class="line"><span class="string">        如果该参数被设置，那么numPartitions、lowerBound、upperBound将用于分区从而生成where表达式来拆分该列。</span></span><br><span class="line"><span class="string">lowerBound：column的最小值，用于决定分区的步长</span></span><br><span class="line"><span class="string">upperBound：column的最大值（不包含），用于决定分区的步长</span></span><br><span class="line"><span class="string">numPartitions：分区的数量</span></span><br><span class="line"><span class="string">predicates：一系列的表达式，用于where中。每一个表达式定义了DataFrame的一个分区</span></span><br><span class="line"><span class="string">properties：一个字典，用于定义JDBC连接参数。通常至少为：&#123; &#x27;user&#x27;:&#x27;SYSTEM&#x27;,&#x27;password&#x27;:&#x27;mypassword&#x27;&#125;</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">.json(path, schema=<span class="literal">None</span>, primitivesAsString=<span class="literal">None</span>, prefersDecimal=<span class="literal">None</span>, allowComments=<span class="literal">None</span>, </span><br><span class="line">      allowUnquotedFieldNames=<span class="literal">None</span>, allowSingleQuotes=<span class="literal">None</span>, allowNumericLeadingZero=<span class="literal">None</span>, </span><br><span class="line">      allowBackslashEscapingAnyCharacter=<span class="literal">None</span>, mode=<span class="literal">None</span>, columnNameOfCorruptRecord=<span class="literal">None</span>, </span><br><span class="line">      dateFormat=<span class="literal">None</span>, timestampFormat=<span class="literal">None</span>, multiLine=<span class="literal">None</span>)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">加载json文件，返回一个DataFrame实例</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">.orc(path, mergeSchema=<span class="literal">None</span>, pathGlobFilter=<span class="literal">None</span>, recursiveFileLookup=<span class="literal">None</span>, modifiedBefore=<span class="literal">None</span>, modifiedAfter=<span class="literal">None</span>)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">加载ORC文件，返回一个DataFrame实例</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">.parquet(*paths, **options)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">加载Parquet文件，返回一个DataFrame实例</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">.table(tableName)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">从table中创建一个DataFrame</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">.text(paths, wholetext=<span class="literal">False</span>, lineSep=<span class="literal">None</span>, pathGlobFilter=<span class="literal">None</span>, recursiveFileLookup=<span class="literal">None</span>)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">从文本中创建一个DataFrame</span></span><br><span class="line"><span class="string">它不同于.csv()，这里的DataFrame只有一列，每行文本都是作为一个字符串。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><br>从 Hive 表创建：<br>Spark SQL还支持读取和写入存储在Apache Hive中的数据。但是由于Hive具有大量依赖关系，但这些依赖关系不包含在默认Spark版本中。如果在类路径中找到Hive依赖项，则Spark将会自动加载它们；而且这些Hive的依赖关系也必须存在于所有工作节点上。</p>
<p>Hive配置：将hive-site.xml、core-site.html（用于安全配置）、hdfs-site.xml（用户HDFS 配置） 文件放在<code>conf/</code>目录中完成配置。<br><strong>当使用Hive时，必须使用启用Hive支持的SparkSession对象（enableHiveSupport）</strong>。如果未部署Hive，则开启Hive支持不会报错。<br>当hive-site.xml未配置时，上下文会自动在当前目录中创建metastore_db，并创建由spark.sql.warehouse.dir指定的目录。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession</span><br><span class="line">ssec = SparkSession \</span><br><span class="line">    .builder \</span><br><span class="line">    .appName(<span class="string">&quot;Spark SQL Hive&quot;</span>) \</span><br><span class="line">    .config(<span class="string">&quot;spark.sql.warehouse.dir&quot;</span>, <span class="string">&#x27;/home&#x27;</span>) \</span><br><span class="line">    .enableHiveSupport() \</span><br><span class="line">    .getOrCreate()</span><br><span class="line">ssec.sql(<span class="string">&quot;CREATE TABLE IF NOT EXISTS src (key INT, value STRING) USING hive&quot;</span>)</span><br><span class="line">ssec.sql(<span class="string">&quot;LOAD DATA LOCAL INPATH &#x27;./test.txt&#x27; INTO TABLE src&quot;</span>)</span><br><span class="line">ssec.sql(<span class="string">&quot;SELECT * FROM src&quot;</span>).show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建Hive表时，需要定义如何向/从文件系统读写数据，即输入格式、输出格式。</span></span><br><span class="line"><span class="comment"># 还需要定义该表的数据的序列化与反序列化。</span></span><br><span class="line">spark_sess.sql(<span class="string">&quot;CREATE TABLE IF NOT EXISTS src (key INT, value STRING) USING hive OPTIONS(fileFormat &#x27;parquet&#x27;)&quot;</span>)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">可以通过在OPTIONS选项中指定这些属性：</span></span><br><span class="line"><span class="string">（1）fileFormat：文件格式。目前支持6种文件格式：&#x27;sequencefile&#x27;、&#x27;rcfile&#x27;、&#x27;orc&#x27;、&#x27;parquet&#x27;、&#x27;textfile&#x27;、&#x27;avro&#x27;。</span></span><br><span class="line"><span class="string">（2）inputFormat,outputFormat：这两个选项将相应的InputFormat和OutputFormat类的名称指定为字符串文字。</span></span><br><span class="line"><span class="string">     这两个选项必须成对出现</span></span><br><span class="line"><span class="string">     如果已经制定了fileFormat，则无法指定它们</span></span><br><span class="line"><span class="string">（3）serde：该选项指定了serde 类的名称</span></span><br><span class="line"><span class="string">     如果给定的fileFormat 已经包含了serde 信息（如何序列化、反序列化的信息），则不要指定该选项</span></span><br><span class="line"><span class="string">     目前的sequencefile、textfile、rcfile 不包含serde 信息，因此可以使用该选项</span></span><br><span class="line"><span class="string">fieldDelim, escapeDelim, collectionDelim, mapkeyDelim, lineDelim：</span></span><br><span class="line"><span class="string">     这些选项只能与textfile文件格式一起使用，它们定义了如何将分隔的文件读入行。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure></p>
<h3 id="保存"><a href="#保存" class="headerlink" title="保存"></a>保存</h3><p>DataFrame通过DataFrameWriter实例来保存到各种外部存储系统中。可以通过DataFrame.write来访问DataFrameWriter。详细API参考<span class="exturl" data-url="aHR0cDovL3NwYXJrLmFwYWNoZS5vcmcvZG9jcy9sYXRlc3QvYXBpL3B5dGhvbi9yZWZlcmVuY2UvcHlzcGFyay5zcy5odG1sI2lucHV0LWFuZC1vdXRwdXQ=">官网<i class="fa fa-external-link-alt"></i></span></p>
<p>通用保存：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># .write保存</span></span><br><span class="line"></span><br><span class="line">.save(path=<span class="literal">None</span>, <span class="built_in">format</span>=<span class="literal">None</span>, mode=<span class="literal">None</span>, partitionBy=<span class="literal">None</span>, **options)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">保存DataFrame。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">.<span class="built_in">format</span>(source)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">设置数据格式。</span></span><br><span class="line"><span class="string">df.write.format(&#x27;parquet&#x27;).save(&#x27;./df_parquet&#x27;)</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">.mode(saveMode)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">当要保存的目标位置已经有数据时，设置该如何保存。</span></span><br><span class="line"><span class="string">saveMode：可以为：</span></span><br><span class="line"><span class="string">         &#x27;append&#x27;：追加写入</span></span><br><span class="line"><span class="string">         &#x27;overwrite&#x27;：覆写已有数据</span></span><br><span class="line"><span class="string">         &#x27;ignore&#x27;：忽略本次保存操作（不保存）</span></span><br><span class="line"><span class="string">         &#x27;error&#x27;：抛出异常（默认行为）</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">.partitionBy(*cols)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">按照指定的列名来将输出的DataFrame分区。</span></span><br><span class="line"><span class="string">df.write.mode(&#x27;append&#x27;).parquet(&#x27;./data.dat&#x27;)</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><br>专用保存：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line">.csv(path, mode=<span class="literal">None</span>, compression=<span class="literal">None</span>, sep=<span class="literal">None</span>, quote=<span class="literal">None</span>, escape=<span class="literal">None</span>, header=<span class="literal">None</span>, </span><br><span class="line">      nullValue=<span class="literal">None</span>, escapeQuotes=<span class="literal">None</span>, quoteAll=<span class="literal">None</span>, dateFormat=<span class="literal">None</span>, timestampFormat=<span class="literal">None</span>, </span><br><span class="line">      ignoreLeadingWhiteSpace=<span class="literal">None</span>, ignoreTrailingWhiteSpace=<span class="literal">None</span>)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">将DataFrame保存为csv文件。</span></span><br><span class="line"><span class="string">df.write.csv(&#x27;./test.csv&#x27;)</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">insertInto(tableName, overwrite=<span class="literal">False</span>)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">将DataFrame保存在table中。</span></span><br><span class="line"><span class="string">它要求当前的DataFrame 与指定的table 具有同样的schema。其中overwrite 参数指定是否覆盖table 现有的数据。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">.jdbc(url, table, mode=<span class="literal">None</span>, properties=<span class="literal">None</span>)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">将DataFrame保存在数据库中。</span></span><br><span class="line"><span class="string">url：一个JDBC URL，格式为：jdbc:subprotocol:subname</span></span><br><span class="line"><span class="string">table：表名</span></span><br><span class="line"><span class="string">mode：指定当数据表中已经有数据时，如何保存。可以为：</span></span><br><span class="line"><span class="string">      &#x27;append&#x27;：追加写入</span></span><br><span class="line"><span class="string">      &#x27;overwrite&#x27;：覆写已有数据</span></span><br><span class="line"><span class="string">      &#x27;ignore&#x27;：忽略本次保存操作（不保存）</span></span><br><span class="line"><span class="string">      &#x27;error&#x27;：抛出异常（默认行为）</span></span><br><span class="line"><span class="string">properties：一个字典，用于定义JDBC连接参数。通常至少为：&#123;&#x27;user&#x27;:&#x27;SYSTEM&#x27;, &#x27;password&#x27;:&#x27;mypassword&#x27;&#125;</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">.json(path, mode=<span class="literal">None</span>, compression=<span class="literal">None</span>, dateFormat=<span class="literal">None</span>, timestampFormat=<span class="literal">None</span>)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">将DataFrame保存为json文件。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">.orc(path, mode=<span class="literal">None</span>, partitionBy=<span class="literal">None</span>, compression=<span class="literal">None</span>)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">将DataFrame保存为ORC文件。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">.parquet(path, mode=<span class="literal">None</span>, partitionBy=<span class="literal">None</span>, compression=<span class="literal">None</span>)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">将DataFrame保存为Pqrquet格式的文件。</span></span><br><span class="line"><span class="string">对DataFrame进行压缩并转换成一种嵌套的文件格式。</span></span><br><span class="line"><span class="string">仅会缩小数据的整体大小，并且优化数据处理过程中的性能，因为操作针对的是所需的列的子集而不是所有数据。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">.saveAsTable(name, <span class="built_in">format</span>=<span class="literal">None</span>, mode=<span class="literal">None</span>, partitionBy=<span class="literal">None</span>, **options)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">将DataFrame保存为table。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">.text(path, compression=<span class="literal">None</span>)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">将DataFrame保存为文本文件。</span></span><br><span class="line"><span class="string">该DataFrame 必须只有一列，切该列必须为字符串。每一行数据将作为文本的一行。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure></p>
<h3 id="属性"><a href="#属性" class="headerlink" title="属性"></a>属性</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line">.columns</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">以列表的形式返回所有的列名</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">.dtypes</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">以列表的形式返回所有的列的名字和数据类型。形式为：[(col_name1,col_type1),...]</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">.isStreaming</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">如果数据集的数据源包含一个或者多个数据流，则返回True</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">.na</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">返回一个DataFrameNaFunctions对象，用于处理缺失值。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">.rdd</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">返回DataFrame底层的RDD（元素类型为Row）</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">.schema</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">返回DataFrame的schema</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">.stat</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">返回DataFrameStatFunctions对象，用于统计</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">.storageLevel</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">返回当前的缓存级别</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">.write</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">返回一个DataFrameWriter对象，它是no-streaming DataFrame的外部存储接口</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">.writeStream</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">返回一个DataStreamWriter对象，它是streaming DataFrame的外部存储接口</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>
<h3 id="转换操作"><a href="#转换操作" class="headerlink" title="转换操作"></a>转换操作</h3><p>详细API参考<span class="exturl" data-url="aHR0cDovL3NwYXJrLmFwYWNoZS5vcmcvZG9jcy9sYXRlc3QvYXBpL3B5dGhvbi9yZWZlcmVuY2UvYXBpL3B5c3Bhcmsuc3FsLkRhdGFGcmFtZS5odG1sP2hpZ2hsaWdodD1weXNwYXJrJTIwc3FsJTIwZGF0YWZyYW1lI3B5c3Bhcmsuc3FsLkRhdGFGcmFtZQ==">官网<i class="fa fa-external-link-alt"></i></span><br>聚合操作：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">.agg(*exprs)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">在整个DataFrame开展聚合操作。它是df.groupBy.agg()的快捷方式。</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">df.agg(&#123;&quot;age&quot;: &quot;max&quot;&#125;).collect() # 在agg列上聚合</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">from pyspark.sql import functions</span></span><br><span class="line"><span class="string">df.agg(functions.max(df.age)).collect() # 和字典方式等价</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">.<span class="built_in">filter</span>(condition)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">对行进行过滤。它是where()的别名。</span></span><br><span class="line"><span class="string">condition：一个types.BooleanType的Column，或者一个字符串形式的SQL的表达式。</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">df.filter(df[&quot;age&quot;] &gt; 3).collect()</span></span><br><span class="line"><span class="string">df.filter(&quot;age &gt; 3&quot;).collect()</span></span><br><span class="line"><span class="string">df.where(&quot;age = 2&quot;).collect()</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><br>分组：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">.cube(*cols)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">根据当前DataFrame的指定列，创建一个多维的cube，从而方便我们之后的聚合过程。返回一个GroupedData对象。</span></span><br><span class="line"><span class="string">cols：指定的列名或者Column的列表。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">.groupBy(*cols)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">它是groupby的别名。通过指定的列来将DataFrame分组，从而方便我们之后的聚合过程。返回一个GroupedData对象。</span></span><br><span class="line"><span class="string">cols：指定的列名或者Column的列表。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><br>排序：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">.orderBy(*cols, **kwargs)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">返回一个新的DataFrame，它根据旧的DataFrame指定列排序。</span></span><br><span class="line"><span class="string">cols：一个列名或者Column的列表，指定了排序列。</span></span><br><span class="line"><span class="string">ascending：一个布尔值/一个布尔值列表。指定了升序还是降序排序。如果是列表，则必须和cols长度相同。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">.sort(*cols, **kwargs)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">返回一个新的DataFrame，它根据旧的DataFrame指定列排序。</span></span><br><span class="line"><span class="string">cols：一个列名或者Column的列表，指定了排序列。</span></span><br><span class="line"><span class="string">ascending：一个布尔值/一个布尔值列表。指定了升序还是降序排序。如果是列表，则必须和cols长度相同。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">.sortWithinPartitions(*cols, **kwargs)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">返回一个新的DataFrame，它根据旧的DataFrame指定列在每个分区进行排序。</span></span><br><span class="line"><span class="string">cols：一个列名或者Column的列表，指定了排序列。</span></span><br><span class="line"><span class="string">ascending：一个布尔值/一个布尔值列表。指定了升序还是降序排序。如果是列表，则必须和cols长度相同。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><br>调整分区：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">.coalesce(numPartitions)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">返回一个新的DataFrame，拥有指定的numPartitions分区。不会混洗数据。</span></span><br><span class="line"><span class="string">只能缩小分区数量，而无法扩张分区数量。如果numPartitions比当前的分区数量大，则新的DataFrame的分区数与旧DataFrame相同。</span></span><br><span class="line"><span class="string">numPartitions：目标分区数量。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">.repartition(numPartitions, *cols)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">返回一个新的DataFrame，拥有指定的numPartitions分区。</span></span><br><span class="line"><span class="string">可以增加分区数量，也可以缩小分区数量。</span></span><br><span class="line"><span class="string">结果DataFrame是通过hash来分区。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><br>集合操作：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">.crossJoin(other)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">返回一个新的DataFrame，它是输入的两个DataFrame的笛卡儿积。</span></span><br><span class="line"><span class="string">other：另一个DataFrame对象。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">.intersect(other)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">返回两个DataFrame的行的交集。</span></span><br><span class="line"><span class="string">other：另一个DataFrame对象。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">.join(other,on=<span class="literal">None</span>,how=<span class="literal">None</span>)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">返回两个DataFrame的join结果。</span></span><br><span class="line"><span class="string">other：另一个DataFrame对象。</span></span><br><span class="line"><span class="string">on：指定了在哪些列上执行对齐。</span></span><br><span class="line"><span class="string">    可以为字符串/字符串列表/Column（指定单个列）/Column列表（指定多个列）。</span></span><br><span class="line"><span class="string">    要求两个DataFrame都存在这些列。</span></span><br><span class="line"><span class="string">how：指定join的方式，默认为&#x27;inner&#x27;。可选： </span></span><br><span class="line"><span class="string">     inner/cross/outer/full/full_outer/left/left_outer/right/right_outer/left_semi/left_anti</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">.subtract(other)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">返回一个新的DataFrame。AUB-B。</span></span><br><span class="line"><span class="string">other：另一个DataFrame对象。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">.union(other)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">它是unionAll的别名。返回两个DataFrame的行的并集（它并不会去重）。</span></span><br><span class="line"><span class="string">other：另一个DataFrame对象。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><br>统计：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">.crosstab(col1, col2)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">统计两列的成对频率。要求每一列的distinct值数量少于10^2个。最多返回10^6对频率。</span></span><br><span class="line"><span class="string">它是DataFrameStatFunctions.crosstab()的别名。</span></span><br><span class="line"><span class="string">结果的第一列的列名为col1_col2，值就是第一列的元素值。后面的列的列名就是第二列元素值，值就是对应的频率。</span></span><br><span class="line"><span class="string">col1,col2：列名字符串（或者Column）。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">.describe(*cols)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">该函数仅仅用于探索数据规律。计算指定的数值列、字符串列的统计值。</span></span><br><span class="line"><span class="string">统计结果包括：count、mean、stddev、min、max。</span></span><br><span class="line"><span class="string">cols：列名/多个列名字符串（或者Column）。如果未传入任何列名，则计算所有的数值列、字符串列。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">.freqItems(cols,support=<span class="literal">None</span>)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">寻找指定列中频繁出现的值（可能有误报）。</span></span><br><span class="line"><span class="string">它是DataFrameStatFunctions.freqItems()的别名。</span></span><br><span class="line"><span class="string">cols：字符串的列表或者元组，指定了待考察的列。</span></span><br><span class="line"><span class="string">support：指定所谓的频繁的标准（默认是1%）。该数值必须大于10^-4。  </span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><br>移除数据：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">.distinct()</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">返回一个新的DataFrame，去除重复的行。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">.drop(*cols)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">返回一个新的DataFrame，去除重复的列。</span></span><br><span class="line"><span class="string">cols：列名字符串（或者Column）。如果它在旧DataFrame中不存在，也不做任何操作（也不报错）。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">.dropDuplicates(subset=<span class="literal">None</span>)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">返回一个新的DataFrame，去除DataFrame重复的行。</span></span><br><span class="line"><span class="string">它与.distinct()区别在于：它仅仅考虑指定的列来判断是否重复行。</span></span><br><span class="line"><span class="string">.drop_duplicates是.dropDuplicates的别名。</span></span><br><span class="line"><span class="string">subset：列名集合（或者Column的集合）。如果为None，则考虑所有的列。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">.dropna(how=<span class="string">&#x27;any&#x27;</span>, thresh=<span class="literal">None</span>, subset=<span class="literal">None</span>)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">返回一个新的DataFrame，去除null行。</span></span><br><span class="line"><span class="string">它是DataFrameNaFunctions.drop()的别名。</span></span><br><span class="line"><span class="string">how：指定如何判断null行的标准。&#x27;all&#x27;：所有字段都是na时去除；&#x27;any&#x27;：任何字段存在na时去除。</span></span><br><span class="line"><span class="string">thresh：一个整数。当一行中非null的字段数量小于thresh时，认为是空行。如果该参数设置how失效。</span></span><br><span class="line"><span class="string">subset：列名集合，给出了要考察的列。如果为None，则考察所有列。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">.limit(num)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">返回一个新的DataFrame，只显示num行。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><br>采样、拆分：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">.randomSplit(weights, seed=<span class="literal">None</span>)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">返回一组新的DataFrame，随机拆分。</span></span><br><span class="line"><span class="string">weights：一个double的列表。它给出了每个结果DataFrame的相对大小。如果列表的数值之和不等于1.0，则它将被归一化为1.0。</span></span><br><span class="line"><span class="string">seed：随机数种子。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">.sample(withReplacement, fraction, seed=<span class="literal">None</span>)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">返回一个新的DataFrame，它是旧DataFrame的采样。</span></span><br><span class="line"><span class="string">withReplacement：如果为True，则可以重复采样；否则是无放回采样。</span></span><br><span class="line"><span class="string">fractions：新的DataFrame的期望大小（占旧DataFrame的比例）。spark并不保证结果刚好满足这个比例（只是一个期望值）。</span></span><br><span class="line"><span class="string">          如果withReplacement=True：则表示每个元素期望被选择的次数。</span></span><br><span class="line"><span class="string">          如果withReplacement=False：则表示每个元素期望被选择的概率。</span></span><br><span class="line"><span class="string">seed：随机数生成器的种子。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">.sampleBy(col, fractions, seed=<span class="literal">None</span>)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">返回一个新的DataFrame，它是旧DataFrame的采样。它执行的是无放回的分层采样。分层由col列指定。</span></span><br><span class="line"><span class="string">col：列名/Column，它给出了分层的依据。</span></span><br><span class="line"><span class="string">fractions：一个字典，给出了每个分层抽样的比例。如果某层未指定，则其比例视作0。</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">sampled = df.sampleBy(&quot;key&quot;, fractions=&#123;0: 0.1, 1: 0.2&#125;, seed=0)</span></span><br><span class="line"><span class="string"># df[&#x27;key&#x27;] 这一列作为分层依据，0 抽取 10%， 1 抽取 20%</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><br>替换：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">.replace(to_replace, value=<span class="literal">None</span>, subset=<span class="literal">None</span>)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">返回一组新的DataFrame，它是旧DataFrame的数值替代结果。</span></span><br><span class="line"><span class="string">它是DataFrameNaFunctions.replace()的别名。</span></span><br><span class="line"><span class="string">当替换时，value将被类型转换到目标列。</span></span><br><span class="line"><span class="string">to_replace：可以为布尔、整数、浮点数、字符串、列表、字典，给出了被替代的值。如果是字典，则给出了每一列要被替代的值。</span></span><br><span class="line"><span class="string">value：一个整数、浮点数、字符串、列表。给出了替代值。</span></span><br><span class="line"><span class="string">subset：列名的列表。指定要执行替代的列。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">.fillna(value, subset=<span class="literal">None</span>)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">返回一个新的DataFrame，它替换了旧DataFrame中的null值。</span></span><br><span class="line"><span class="string">它是DataFrameNaFunctions.fill()的别名。</span></span><br><span class="line"><span class="string">value：一个整数、浮点数、字符串、字典，用于替换null值。如果是个字典，则忽略subset，字典的键就是列名，指定了该列的null值被替换的值。</span></span><br><span class="line"><span class="string">subset：列名集合，给出了要被替换的列。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><br>选取数据：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">.select(*cols)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">执行一个表达式，将其结果返回为一个DataFrame。</span></span><br><span class="line"><span class="string">cols：一个列名的列表，或者Column表达式。如果列名为*，则扩张到所有的列名。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">.selectExpr(*expr)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">执行一个SQL表达式，将其结果返回为一个DataFrame。</span></span><br><span class="line"><span class="string">expr：一组SQL的字符串描述。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">.toDF(*cols)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">选取指定的列组成一个新的DataFrame。</span></span><br><span class="line"><span class="string">cols：列名字符串的列表。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">.toJSON(use_unicode=<span class="literal">True</span>)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">返回一个新的DataFrame，它将旧的DataFrame转换为RDD（元素为字符串），其中每一行转换为json字符串。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><br>列操作：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">.withColumn(colName, col)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">返回一个新的DataFrame，它将旧的DataFrame增加一列（或者替换现有的列）。</span></span><br><span class="line"><span class="string">colName：一个列名，表示新增的列（如果是已有的列名，则是替换的列）。</span></span><br><span class="line"><span class="string">col：一个Column表达式，表示新的列。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">.withColumnRenamed(existing, new)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">返回一个新的DataFrame，它将旧的DataFrame的列重命名。</span></span><br><span class="line"><span class="string">existing：一个字符串，表示现有的列的列名。</span></span><br><span class="line"><span class="string">col：一个字符串，表示新的列名。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure></p>
<h3 id="行动操作"><a href="#行动操作" class="headerlink" title="行动操作"></a>行动操作</h3><p>查看数据：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">.collect()</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">以Row的列表的形式返回所有的数据。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">.first()</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">返回第一行。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">.head(n=<span class="literal">None</span>)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">返回前面的n行。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">.show(n=<span class="number">20</span>, truncate=<span class="literal">True</span>)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">在终端中打印前n行。</span></span><br><span class="line"><span class="string">它并不返回结果，而是类似print结果。</span></span><br><span class="line"><span class="string">n：打印的行数。</span></span><br><span class="line"><span class="string">truncate：如果为True，则超过20个字符的字符串被截断。如果为一个数字，则长度超过它的字符串将被截断。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">.take(num)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">以Row的列表的形式返回开始的num行数据。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">.toLocalIterator()</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">返回一个迭代器，对它迭代的结果就是DataFrame的每一行数据（Row 对象）</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><br>统计：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">.count()</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">返回当前DataFrame有多少行。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">.corr(col1, col2, method=<span class="literal">None</span>)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">计算两列的相关系数，返回一个浮点数。当前仅支持皮尔逊相关系数。</span></span><br><span class="line"><span class="string">它是DataFrameStatFunctions.corr()的别名。</span></span><br><span class="line"><span class="string">col,col2：为列的名字字符串（或者Column）。</span></span><br><span class="line"><span class="string">method：当前只支持&#x27;pearson&#x27;。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">.cov(col1,col2)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">计算两列的协方差。</span></span><br><span class="line"><span class="string">它是DataFrameStatFunctions.cov()的别名。</span></span><br><span class="line"><span class="string">col,col2：为列的名字字符串（或者Column）。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><br>遍历：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">.foreach(f)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">对DataFrame中的每一行应用f。</span></span><br><span class="line"><span class="string">它是df.rdd.foreach()的快捷方式。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">.toPandas()</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">将DataFrame作为pandas.DataFrame返回。</span></span><br><span class="line"><span class="string">只有当数据较小，可以在驱动器程序中放得下时，才可以用该方法。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><br>其他方法：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line">.cache()</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">使用默认的storage level缓存DataFrame（缓存级别为：MEMORY_AND_DISK）。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">.persist(storageLevel=StorageLevel(<span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">False</span>, <span class="literal">False</span>, <span class="number">1</span>))</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">缓存DataFrame。</span></span><br><span class="line"><span class="string">storageLevel：缓存级别。默认为MEMORY_AND_DISK。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">.unpersist(blocking=<span class="literal">False</span>)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">标记该DataFrame为未缓存的，并且从内存和磁盘冲移除它的缓存块。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">.isLocal()</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">如果collect()和take()方法能本地运行（不需要任何executor节点），则返回True。否则返回False。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">.printSchema()</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">打印DataFrame的schema。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">.createTempView(name)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">创建一个名字为name的临时视图。</span></span><br><span class="line"><span class="string">临时视图是session级别的，会随着session的消失而消失</span></span><br><span class="line"><span class="string">如果指定的临时视图已存在，则抛出异常。</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">df.createTempView(&quot;people&quot;)</span></span><br><span class="line"><span class="string">df2 = spark_session.sql(&quot;select * from people&quot;)</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">.createOrReplaceTempView(name)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">创建一个名字为name的临时视图。如果该视图已存在，则替换它。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">.createGlobalTempView(name)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">创建一个名字为name的临时视图，全局有效。</span></span><br><span class="line"><span class="string">如果希望一个临时视图跨session而存在，则可以建立一个全局临时视图。</span></span><br><span class="line"><span class="string">如果指定的全局临时视图已存在，则抛出异常。</span></span><br><span class="line"><span class="string">全局临时视图存在于系统数据库global_temp中，必须加上库名取引用它。</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">df.createGlobalTempView(&quot;people&quot;)</span></span><br><span class="line"><span class="string">spark_session.sql(&quot;SELECT * FROM global_temp.people&quot;).show()</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">.createOrReplaceGlobalTempView(name)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">创建一个名字为name的临时视图，全局有效。如果该视图已存在，则替换它。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">.explain(extended=<span class="literal">False</span>)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">打印logical plan和physical plan，用于调试模式。</span></span><br><span class="line"><span class="string">extended：如果为False，则仅仅打印physical plan。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure></p>
<h3 id="Row和Column"><a href="#Row和Column" class="headerlink" title="Row和Column"></a>Row和Column</h3><p>一个Row对象代表了DataFrame的一行。<br>访问DataFrame中的一行：<code>row[key]</code>。<br><code>key in row</code>将在Row的键上遍历，而不是值上遍历。</p>
<p>创建Row：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">Row(name=<span class="string">&quot;Alice&quot;</span>, age=<span class="number">11</span>)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">如果某个参数为None，则必须显式指定，而不能忽略。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="comment"># 可以创建一个Row作为一个类来使用，它的作用随后用于创建具体的Row</span></span><br><span class="line">Person = Row(<span class="string">&quot;name&quot;</span>, <span class="string">&quot;age&quot;</span>)</span><br><span class="line">p1 = Person(<span class="string">&quot;Alice&quot;</span>, <span class="number">11</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 其他方法</span></span><br><span class="line">.asDict(recursive=<span class="literal">False</span>)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">以字典的方式返回该Row实例。如果recursive=True，则递归的处理元素中包含的Row。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><br>Column代表了DataFrame的一列。</p>
<p>创建Column：<code>df[&#39;Name&#39;]</code>。</p>
<p>Column的方法：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br></pre></td><td class="code"><pre><span class="line">.alias(*alias, **kwargs)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">创建一个新列，对旧列的重命名（一个/一组名字，如explode表达式会返回多列）。</span></span><br><span class="line"><span class="string">alias：列的别名。</span></span><br><span class="line"><span class="string">metadata：一个字符串，存储在列的metadata属性中。</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">df.age.alias(&quot;age2&quot;)</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">.asc()</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">创建一个新列，它是旧列的升序排序的结果。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">.desc()</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">创建一个新列，它是旧列的降序排序的结果。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">.astype(dataType)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">创建一个新列，它是旧列的数值转换的结果。</span></span><br><span class="line"><span class="string">它是.cast()的别名。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">.between(lowerBound, upperBound)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">创建一个新列，它是一个布尔值。</span></span><br><span class="line"><span class="string">如果旧列的数值在[lowerBound, upperBound]之内，则为True。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">.bitwiseAND(other)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">二进制逻辑与。返回一个新列，是布尔值。</span></span><br><span class="line"><span class="string">other：另一个Column。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">.bitwiseOR(other)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">二进制逻辑或。返回一个新列，是布尔值。</span></span><br><span class="line"><span class="string">other：另一个Column。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">.bitwiseXOR(other)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">二进制逻辑异或。返回一个新列，是布尔值。</span></span><br><span class="line"><span class="string">other：另一个Column。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">.getField(name)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">返回一个新列，是旧列的指定字段组成。</span></span><br><span class="line"><span class="string">name：一个字符串，是字段名。</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">df = sc.parallelize([Row(r=Row(a=1, b=&quot;b&quot;))]).toDF()</span></span><br><span class="line"><span class="string">df.show()</span></span><br><span class="line"><span class="string">+------+</span></span><br><span class="line"><span class="string">|     r|</span></span><br><span class="line"><span class="string">+------+</span></span><br><span class="line"><span class="string">|[1, b]|</span></span><br><span class="line"><span class="string">+------+</span></span><br><span class="line"><span class="string">df.select(df.r.getField(&quot;b&quot;)).show()</span></span><br><span class="line"><span class="string">+---+</span></span><br><span class="line"><span class="string">|r.b|</span></span><br><span class="line"><span class="string">+---+</span></span><br><span class="line"><span class="string">|  b|</span></span><br><span class="line"><span class="string">+---+</span></span><br><span class="line"><span class="string">df.select(df.r.a).show()</span></span><br><span class="line"><span class="string">+---+</span></span><br><span class="line"><span class="string">|r.a|</span></span><br><span class="line"><span class="string">+---+</span></span><br><span class="line"><span class="string">|  1|</span></span><br><span class="line"><span class="string">+---+</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">.getItem(key)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">返回一个新列，是旧列的指定位置（列表），或者指定键（字典）组成。</span></span><br><span class="line"><span class="string">key：一个整数/一个字符串。</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">df = sc.parallelize([([1, 2], &#123;&quot;key&quot;: &quot;value&quot;&#125;)]).toDF([&quot;l&quot;, &quot;d&quot;])</span></span><br><span class="line"><span class="string">df.show()</span></span><br><span class="line"><span class="string">+------+--------------+</span></span><br><span class="line"><span class="string">|     l|             d|</span></span><br><span class="line"><span class="string">+------+--------------+</span></span><br><span class="line"><span class="string">|[1, 2]|[key -&gt; value]|</span></span><br><span class="line"><span class="string">+------+--------------+</span></span><br><span class="line"><span class="string">df.select(df.l.getItem(0), df.d.getItem(&quot;key&quot;)).show()</span></span><br><span class="line"><span class="string">+----+------+</span></span><br><span class="line"><span class="string">|l[0]|d[key]|</span></span><br><span class="line"><span class="string">+----+------+</span></span><br><span class="line"><span class="string">|   1| value|</span></span><br><span class="line"><span class="string">+----+------+</span></span><br><span class="line"><span class="string">df.select(df.l[0], df.d[&quot;key&quot;]).show()</span></span><br><span class="line"><span class="string">+----+------+</span></span><br><span class="line"><span class="string">|l[0]|d[key]|</span></span><br><span class="line"><span class="string">+----+------+</span></span><br><span class="line"><span class="string">|   1| value|</span></span><br><span class="line"><span class="string">+----+------+</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">.isNotNull()</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">返回一个布尔值新列。表示旧列的值是否非null。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">.isNull()</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">返回一个布尔值新列。表示旧列的值是否null。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">.isin(*cols)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">返回一个布尔值新列。表示旧列的值是否在cols中。</span></span><br><span class="line"><span class="string">cols：一个列表或者元组。</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">df[df.name.isin(&quot;Bob&quot;, &quot;Mike&quot;)].collect()</span></span><br><span class="line"><span class="string">df[df.age.isin([1, 2, 3])].collect()</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">.like(other)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">返回一个布尔值新列。表示旧列的值是否like other。它执行的是SQL的like语义。</span></span><br><span class="line"><span class="string">other：一个字符串，是SQL like表达式。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">.rlike(other)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">返回一个布尔值新列。表示旧列的值是否rlike other。它执行的是SQL的rlike语义。</span></span><br><span class="line"><span class="string">other：一个字符串，是SQL rlike表达式。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">.contains(other)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">返回一个布尔值新列。表示是否包含other。other为一个字符串。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">.endswith(other)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">返回一个布尔值新列。表示是否以other结尾。other为一个字符串。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">.startswith(other)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">返回一个布尔值新列。表示是否以other开头。other为一个字符串。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">.substr(startPos, length)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">返回一个新列，它是旧列的子串。</span></span><br><span class="line"><span class="string">startPos：子串开始位置（整数/Column）。</span></span><br><span class="line"><span class="string">length：子串长度（整数/Column）。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">.when(condition, value)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">返回一个新列。对条件进行求值，如果满足条件则返回value，如果不满足：</span></span><br><span class="line"><span class="string">    如果有.otherwise()调用，则返回otherwise的结果。</span></span><br><span class="line"><span class="string">    如果无.otherwise()调用，则返回None。</span></span><br><span class="line"><span class="string">condition：一个布尔型的Column表达式。</span></span><br><span class="line"><span class="string">value：一个字面值/一个Column表达式。</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">from pyspark.sql import functions as F</span></span><br><span class="line"><span class="string">df.select(df.name, F.when(df.age &gt; 4, 1).when(df.age &lt; 3, -1).otherwise(0)).show()</span></span><br><span class="line"><span class="string">+-----+------------------------------------------------------------+</span></span><br><span class="line"><span class="string">| name|CASE WHEN (age &gt; 4) THEN 1 WHEN (age &lt; 3) THEN -1 ELSE 0 END|</span></span><br><span class="line"><span class="string">+-----+------------------------------------------------------------+</span></span><br><span class="line"><span class="string">|Alice|                                                          -1|</span></span><br><span class="line"><span class="string">|  Bob|                                                           1|</span></span><br><span class="line"><span class="string">+-----+------------------------------------------------------------+</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">.otherwise(value)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">value 为一个字面值/一个Column表达式。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure></p>
<h3 id="GroupedData"><a href="#GroupedData" class="headerlink" title="GroupedData"></a>GroupedData</h3><p>GroupedData通常由<code>DataFrame.groupBy()</code>创建，用于分组聚合。</p>
<p>GroupedData的方法：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line">.agg(*exprs)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">聚合并以DataFrame的形式返回聚合的结果。</span></span><br><span class="line"><span class="string">可用的聚合函数包括：avg、max、min、sum、count。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">.avg(*cols)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">统计数值列每一组的均值，以DataFrame的形式返回。</span></span><br><span class="line"><span class="string">它是mean()的别名。</span></span><br><span class="line"><span class="string">cols：列名/列名的列表。</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">df.groupBy().avg(&#x27;age&#x27;)</span></span><br><span class="line"><span class="string">df.groupBy().avg(&#x27;age&#x27;, &#x27;height&#x27;)</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">.count()</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">统计每一组的记录数量，以DataFrame的形式返回。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">.<span class="built_in">max</span>(*cols)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">统计数值列每一组的最大值，以DataFrame的形式返回。</span></span><br><span class="line"><span class="string">cols：列名/列名的列表。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">.<span class="built_in">min</span>(*cols)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">统计数值列每一组的最小值，以DataFrame的形式返回。</span></span><br><span class="line"><span class="string">cols：列名/列名的列表。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">.<span class="built_in">sum</span>(*cols)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">统计数值列每一组的和，以DataFrame的形式返回。</span></span><br><span class="line"><span class="string">cols：列名/列名的列表。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">.pivot(pivot_col, values=<span class="literal">None</span>)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">对指定列进行透视。</span></span><br><span class="line"><span class="string">pivot_col：待分析的列名。</span></span><br><span class="line"><span class="string">values：待分析的列上的待考察的值的列表。如果为空，则spark会首先计算pivot_col的不重复的值。</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">df4.groupBy(&quot;year&quot;).pivot(&quot;course&quot;, [&quot;dotNET&quot;, &quot;Java&quot;]).sum(&quot;earnings&quot;).collect()</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure></p>
<h3 id="functions"><a href="#functions" class="headerlink" title="functions"></a>functions</h3><p>pyspark.sql.functions模块提供了一些内建的函数，它们用于创建Column。<br>它们基本都有公共的参数col，表示列名/Column。<br>它们的返回结果通常都是Column。</p>
<p>数学函数：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 参数col都是数值列。</span></span><br><span class="line"><span class="built_in">abs</span>(col)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">计算绝对值</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">acos(col)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">计算acos</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">cos(col)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">计算cos</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">cosh(col)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">计算cosh</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">asin(col)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">计算asin</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">atan(col)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">计算atan</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">atan2(col1,col2)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">计算从直角坐标(x,y) 到极坐标(r,theta)的角度theta</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">bround(col,scale=<span class="number">0</span>)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">计算四舍五入的结果。如果scale&gt;=0，则使用HALF_EVEN舍入模式；如果scale&lt;0，则将其舍入到整数部分。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">cbrt(col)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">计算立方根</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">ceil(col)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">向上取整</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">floor(col)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">向下取整</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">corr(col1,col2)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">计算两列的皮尔逊相关系数</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">covar_pop(col1,col2)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">计算两列的总体协方差 (公式中的除数是N)</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">covar_samp(col1,col2)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">计算两列的样本协方差 (公式中的除数是N-1)</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">degrees(col)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">将弧度制转换为角度制</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">radians(col)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">将角度制转换为弧度制</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">exp(col)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">计算指数e^x</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">expml(col)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">计算指数e^(x-1)</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">fractorial(col)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">计算阶乘</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="built_in">pow</span>(col1,col2)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">计算幂级数col1^col2</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="built_in">hash</span>(*cols)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">计算指定一些列的hash值，返回一个整数列。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">hypot(col1,col2)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">计算 (a^2+b^2)开根号（没有中间产出的上溢出、下溢出），返回一个数值列</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">log(arg1,arg2=<span class="literal">None</span>)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">计算对数。其中第一个参数为底数。如果只有一个参数，则使用自然底数。</span></span><br><span class="line"><span class="string">arg1：如果有两个参数，则它给出了底数。否则就是对它求自然底数。</span></span><br><span class="line"><span class="string">arg2：如果有两个参数，则对它求对数。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">log10(col)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">计算10为底的对数</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">log1p(col)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">计算ln(x+1)</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">log2(col)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">计算2为底的对数</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">rand(seed=<span class="literal">None</span>)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">从均匀分布U~[0.0,1.0]生成一个独立同分布(i.i.d)的随机列。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">randn(seed=<span class="literal">None</span>)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">从标准正态分布N~(0.0,1.0)生成一个独立同分布(i.i.d)的随机列。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">rint(col)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">返回最接近的整数的double形式。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="built_in">round</span>(col,scale=<span class="number">0</span>)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">四舍五入，如果scale&gt;=0，则使用HALF_UP 的舍入模式；否则直接取参数的整数部分。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">signum(col)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">计算正负号</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">sin(col)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">计算sin</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">sinh(col)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">计算sinh</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">sqrt(col)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">计算平方根</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">tan(col)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">计算tan</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">tanh(col)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">计算tanh</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><br>字符串函数：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">ascii</span>(col)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">返回一个数值列，它是旧列的字符串中的首个字母的ascii值。其中col必须是字符串列。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">base64(col)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">返回一个字符串列，它是旧列（二进制值）的base64编码得到的字符串。其中col必须是二进制列。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="built_in">bin</span>(col)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">返回一个字符串列，它是旧列（二进制值）的字符串表示（如二进制1101的字符串表示为&#x27;1101&#x27;）其中col必须是二进制列。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">cov(col,fromBase,toBase)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">返回一个字符串列，它是一个数字的字符串表达从fromBase转换到toBase。</span></span><br><span class="line"><span class="string">col：一个字符串列，它是数字的表达。如1028。它的基数由fromBase给出。</span></span><br><span class="line"><span class="string">fromBase：一个整数，col中字符串的数值的基数。</span></span><br><span class="line"><span class="string">toBase：一个整数，要转换的数值的基数。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">concat(*cols)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">创建一个新列，它是指定列的字符串拼接的结果（没有分隔符）。</span></span><br><span class="line"><span class="string">cols：列名字符串列表/Column列表。要求这些列具有同样的数据类型。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">concat_ws(sep,*cols)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">创建一个新列，它是指定列的字符串使用指定的分隔符拼接的结果。</span></span><br><span class="line"><span class="string">sep：一个字符串，表示分隔符。</span></span><br><span class="line"><span class="string">cols：列名字符串列表/Column列表。要求这些列具有同样的数据类型。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">decode(col,charset)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">从二进制列根据指定字符集来解码成字符串。</span></span><br><span class="line"><span class="string">col：一个字符串/Column，为二进制列。</span></span><br><span class="line"><span class="string">charset：一个字符串，表示字符集。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">encode(col,charset)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">把字符串编码成二进制格式。</span></span><br><span class="line"><span class="string">col：一个字符串/Column，为二进制列。</span></span><br><span class="line"><span class="string">charset：一个字符串，表示字符集。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">format_number(col,d)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">格式化数值成字符串，根据HALF_EVEN来四舍五入成d位的小数。</span></span><br><span class="line"><span class="string">col：一个字符串/Column，为数值列。</span></span><br><span class="line"><span class="string">d：一个整数，表示d位小数。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">format_string(<span class="built_in">format</span>,*cols)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">返回print风格的格式化字符串。</span></span><br><span class="line"><span class="string">format：print风格的格式化字符串。如%s%d。</span></span><br><span class="line"><span class="string">cols：一组列名/Columns，用于填充format。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="built_in">hex</span>(col)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">计算指定列的十六进制值（以字符串表示）。</span></span><br><span class="line"><span class="string">col：一个字符串/Column，为字符串列、二进制列、整数列。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">initcap(col)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">将句子中每个单词的首字母大写。</span></span><br><span class="line"><span class="string">col：一个字符串/Column，为字符串列。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">input_file_name()</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">为当前的spark task的文件名创建一个字符串列。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">instr(<span class="built_in">str</span>,substr)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">给出substr在str的首次出现的位置。位置不是从0开始，而是从1开始的。</span></span><br><span class="line"><span class="string">如果substr不在str中，则返回0。如果str或者substr为null，则返回null。</span></span><br><span class="line"><span class="string">str：一个字符串/Column，为字符串列。</span></span><br><span class="line"><span class="string">substr：一个字符串。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">locate(substr,<span class="built_in">str</span>,pos=<span class="number">1</span>)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">给出substr在str的pos之后首次出现的位置。位置不是从0开始，而是从1开始的。</span></span><br><span class="line"><span class="string">如果substr不在str中，则返回0。如果str或者substr为null，则返回null。</span></span><br><span class="line"><span class="string">str：一个字符串/Column，为字符串列。</span></span><br><span class="line"><span class="string">substr：一个字符串。</span></span><br><span class="line"><span class="string">pos：：起始位置（基于0开始）。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">length(col)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">计算字符串或者字节的长度。</span></span><br><span class="line"><span class="string">col：一个字符串/Column，为字符串列，或者为字节列。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">levenshtein(left,right)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">计算两个字符串之间的Levenshtein距离。</span></span><br><span class="line"><span class="string">Levenshtein距离：刻画两个字符串之间的差异度。</span></span><br><span class="line"><span class="string">        它是从一个字符串修改到另一个字符串时，其中编辑单个字符串（修改、插入、删除）所需要的最少次数。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">lower(col)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">转换字符串到小写。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">lpad(col,<span class="built_in">len</span>,pad)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">对字符串，向左填充。</span></span><br><span class="line"><span class="string">col：一个字符串/Column，为字符串列。</span></span><br><span class="line"><span class="string">len：预期填充后的字符串长度。</span></span><br><span class="line"><span class="string">pad：填充字符串。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">ltrim(col)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">裁剪字符串的左侧空格。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">md5(col)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">计算指定列的MD5值（一个32字符的十六进制字符串）。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">regexp_extract(<span class="built_in">str</span>,pattern,idx)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">通过正则表达式抽取字符串中指定的子串。</span></span><br><span class="line"><span class="string">str：一个字符串/Column，为字符串列，表示被抽取的字符串。</span></span><br><span class="line"><span class="string">pattern：一个Java正则表达式子串。</span></span><br><span class="line"><span class="string">idx：表示抽取第几个匹配的结果。</span></span><br><span class="line"><span class="string">返回值：如果未匹配到，则返回空字符串。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">.regexp_replace(<span class="built_in">str</span>,pattern,replacement)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">通过正则表达式替换字符串中指定的子串。</span></span><br><span class="line"><span class="string">str：一个字符串/Column，为字符串列，表示被替换的字符串。</span></span><br><span class="line"><span class="string">pattern：一个Java正则表达式子串。</span></span><br><span class="line"><span class="string">replacement：表示替换的子串</span></span><br><span class="line"><span class="string">返回值：如果未匹配到，则返回空字符串。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">repeat(col,n)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">重复一个字符串列n次，结果返回一个新的字符串列。</span></span><br><span class="line"><span class="string">col：一个字符串/Column，为字符串列。</span></span><br><span class="line"><span class="string">n：一个整数，表示重复次数。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">reverse(col)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">返回一个翻转的新的字符串列。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">rpad(col,<span class="built_in">len</span>,pad)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">向右填充字符串到指定长度。</span></span><br><span class="line"><span class="string">col：一个字符串/Column，为字符串列。</span></span><br><span class="line"><span class="string">len：指定的长度。</span></span><br><span class="line"><span class="string">pad：填充字符串 。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">rtrim(col)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">剔除字符串右侧的空格符。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">sha1(col)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">以16进制字符串的形式返回SHA-1的结果。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">sha2(col,numBites)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">以16进制字符串的形式返回SHA-2的结果。</span></span><br><span class="line"><span class="string">numBites：结果的位数（可以为244,256,384,512,0表示256）</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">soundex(col)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">返回字符串的SoundEx编码。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">split(<span class="built_in">str</span>,pattern)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">利用正则表达式拆分字符串。产生一个array列。</span></span><br><span class="line"><span class="string">str：一个字符串/Column，为字符串列。</span></span><br><span class="line"><span class="string">pattern：一个字符串，表示正则表达式。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">substring(<span class="built_in">str</span>,pos,<span class="built_in">len</span>)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">抽取子串。</span></span><br><span class="line"><span class="string">str：一个字符串/Column，为字符串列，或者字节串列。</span></span><br><span class="line"><span class="string">pos：抽取的起始位置。</span></span><br><span class="line"><span class="string">len：抽取的子串长度。</span></span><br><span class="line"><span class="string">返回值：如果str表示字符串列，则返回的是子字符串。如果str是字节串列，则返回的是字节子串。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">substring_index(<span class="built_in">str</span>,delim,count)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">抽取子串。</span></span><br><span class="line"><span class="string">str：一个字符串/Column，为字符串列。</span></span><br><span class="line"><span class="string">delim：一个字符串，表示分隔符。</span></span><br><span class="line"><span class="string">count：指定子串的下标。 </span></span><br><span class="line"><span class="string">       如果为正数，则从左开始，遇到第count个delim时，返回其左侧的内容； </span></span><br><span class="line"><span class="string">       如果为负数，则从右开始，遇到第abs(count)个delim时，返回其右侧的内容；</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">translate(srcCol,matching,replace)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">将srcCol中指定的字符替换成另外的字符。</span></span><br><span class="line"><span class="string">srcCol：一个字符串/Column，为字符串列。</span></span><br><span class="line"><span class="string">matching：一个字符串。只要srcCol中的字符串，有任何字符匹配了它，则执行替换。</span></span><br><span class="line"><span class="string">replace：它一一对应于matching中要替换的字符。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">trim(col)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">除字符串两侧的空格符。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">unbase64(col)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">字符串列执行base4编码，并且返回一个二进制列。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">unhex(col)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">对字符串列执行hex的逆运算。 给定一个十进制数字字符串，将其逆转换为十六进制数字字符串。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">upper(col)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">将字符串列转换为大写格式。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><br>日期函数：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br></pre></td><td class="code"><pre><span class="line">add_months(start, months)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">增加月份。</span></span><br><span class="line"><span class="string">start：列名/Column表达式，指定起始时间。</span></span><br><span class="line"><span class="string">months：指定增加的月份。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">current_data()</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">返回当前日期作为一列。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">current_timestamp()</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">返回当前的时间戳作为一列。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">date_add(start,days)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">增加天数。</span></span><br><span class="line"><span class="string">start：列名/Column表达式，指定起始时间。</span></span><br><span class="line"><span class="string">days：指定增加的天数。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">date_sub(start,days)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">减去天数。</span></span><br><span class="line"><span class="string">start：列名/Column表达式，指定起始时间。</span></span><br><span class="line"><span class="string">days：指定减去的天数。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">date_diff(end,start)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">返回两个日期之间的天数差值。</span></span><br><span class="line"><span class="string">end：列名/Column表达式，指定结束时间。为date/timestamp/string。</span></span><br><span class="line"><span class="string">start：列名/Column表达式，指定起始时间。为date/timestamp/string。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">date_format(date,<span class="built_in">format</span>)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">转换date/timestamp/string到指定格式的字符串。</span></span><br><span class="line"><span class="string">date：一个date/timestamp/string列的列名或者Column。</span></span><br><span class="line"><span class="string">format：一个字符串，指定了日期的格式化形式。支持java.text.SimpleDateFormat的所有格式。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">dayofmonth(col)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">返回日期是当月的第几天（一个整数）。其中col为date/timestamp/string。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">dayofyear(col)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">返回日期是当年的第几天（一个整数）。其中col为date/timestamp/string。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">from_unixtime(timestamp, <span class="built_in">format</span>=<span class="string">&#x27;yyyy-MM-dd HH:mm:ss&#x27;</span>)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">转换unix时间戳到指定格式的字符串。</span></span><br><span class="line"><span class="string">timestamp：时间戳的列。</span></span><br><span class="line"><span class="string">format：时间格式化字符串。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">from_utc_timestamp(timestamp, tz)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">转换unix时间戳到指定时区的日期。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">hour(col)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">从指定时间中抽取小时，返回一个整数列。</span></span><br><span class="line"><span class="string">col：一个字符串/Column。是表示时间的字符串列，或者datetime列。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">minute(col)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">从指定时间中抽取分钟，返回一个整数列。</span></span><br><span class="line"><span class="string">col：一个字符串/Column。是表示时间的字符串列，或者datetime列。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">second(col)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">从指定的日期中抽取秒，返回一个整数列。</span></span><br><span class="line"><span class="string">col：一个字符串/Column。是表示时间的字符串列，或者datetime列。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">month(col)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">从指定时间中抽取月份，返回一个整数列。</span></span><br><span class="line"><span class="string">col：一个字符串/Column。是表示时间的字符串列，或者datetime列。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">quarter(col)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">从指定时间中抽取季度，返回一个整数列。</span></span><br><span class="line"><span class="string">col：一个字符串/Column。是表示时间的字符串列，或者datetime列。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">last_day(date)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">返回指定日期的当月最后一天（一个datetime.date）。</span></span><br><span class="line"><span class="string">col：一个字符串/Column。是表示时间的字符串列，或者datetime列。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">months_between(date1,date2)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">返回date1到date2之间的月份（一个浮点数）。</span></span><br><span class="line"><span class="string">也就是date1-date2的天数的月份数量。如果为正数，表明date1&gt;date2。</span></span><br><span class="line"><span class="string">date1,date2：一个字符串/Column。是表示时间的字符串列，或者datetime列。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">next_day(date,dayOfWeek)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">返回指定天数之后的、且匹配dayOfWeek的那一天。</span></span><br><span class="line"><span class="string">date1：一个字符串/Column。是表示时间的字符串列，或者datetime列。</span></span><br><span class="line"><span class="string">dayOfWeek：指定星期几。是大小写敏感的，可以为：&#x27;Mon&#x27;, &#x27;Tue&#x27;, &#x27;Wed&#x27;, &#x27;Thu&#x27;, &#x27;Fri&#x27;, &#x27;Sat&#x27;, &#x27;Sun&#x27;</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">to_date(col,<span class="built_in">format</span>=<span class="literal">None</span>)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">转换pyspark.sql.types.StringType/TimestampType到pyspark.pysql.types.DateType。</span></span><br><span class="line"><span class="string">col：一个字符串/Column。是表示时间的字符串列，或者datetime列。</span></span><br><span class="line"><span class="string">format：指定的格式。默认为yyyy-MM-dd。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">to_timestamp(col,<span class="built_in">format</span>=<span class="literal">None</span>)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">将StringType,TimestampType转换为DataType。</span></span><br><span class="line"><span class="string">col：一个字符串/Column。是表示时间的字符串列，或者datetime列。</span></span><br><span class="line"><span class="string">format：指定的格式。默认为yyyy-MM-dd HH:mm:ss。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">to_utc_timestamp(timestamp,tz)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">根据给定的时区，将StringType,TimestampType转换为DataType。</span></span><br><span class="line"><span class="string">col：一个字符串/Column。是表示时间的字符串列，或者datetime列。</span></span><br><span class="line"><span class="string">tz：一个字符串，表示时区。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">trunc(date,<span class="built_in">format</span>)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">裁剪日期到指定的格式。未指定格式的会保留，月和日保留为1。</span></span><br><span class="line"><span class="string">date：一个字符串/Column。是表示时间的字符串列，或者datetime列。</span></span><br><span class="line"><span class="string">format：指定的格式。如：&#x27;year&#x27;,&#x27;YYYY&#x27;,&#x27;yy&#x27;,&#x27;month&#x27;,&#x27;mon&#x27;,&#x27;mm&#x27;,&#x27;d&#x27;</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">unix_timestamp(timestamp=<span class="literal">None</span>,<span class="built_in">format</span>=<span class="string">&#x27;yyyy-MM-dd HH:mm:ss&#x27;</span>)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">给定一个unix timestamp（单位为秒），将其转换为指定格式的字符串。使用默认的时区和默认的locale。</span></span><br><span class="line"><span class="string">如果转换失败，返回null。如果timestamp=None，则返回当前的timestamp。</span></span><br><span class="line"><span class="string">timestamp：一个unix时间戳列。</span></span><br><span class="line"><span class="string">format：指定转换的格式。 </span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">weekofyear(col)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">回给定时间是当年的第几周。返回一个整数。</span></span><br><span class="line"><span class="string">col：一个字符串/Column。是表示时间的字符串列，或者datetime列。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">year(col)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">从日期中抽取年份，返回一个整数。</span></span><br><span class="line"><span class="string">col：一个字符串/Column。是表示时间的字符串列，或者datetime列。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><br>聚合函数：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br></pre></td><td class="code"><pre><span class="line">count(col)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">计算每一组的元素的个数。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">avg(col)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">计算指定列的均值。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">approx_count_distinct(col, rsd=<span class="literal">None</span>)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">计指定列有多少个去重的值。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">countDistinct(col,*cols)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">计算一列/一组列中的去重value的数量。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">collect_list(col)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">返回指定列的元素组成的列表（不会去重）。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">collect_set(col)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">返回指定列的元素组成的集合（去重）。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">first(col,ignorenulls=<span class="literal">False</span>)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">返回组内的第一个元素。</span></span><br><span class="line"><span class="string">如果ignorenulls=True，则忽略null值，直到第一个非null值。如果都是null，则返回null。</span></span><br><span class="line"><span class="string">如果ignorenulls=False，则返回组内第一个元素(不管是不是null)。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">ast(col,ignorenulls=<span class="literal">False</span>)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">返回组内的最后一个元素。</span></span><br><span class="line"><span class="string">如果ignorenulls=True，则忽略null值，直到第一个非null值。如果都是null，则返回null。</span></span><br><span class="line"><span class="string">如果ignorenulls=False，则返回组内第一个元素(不管是不是null)。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">grouping(col)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">判断group by list中的指定列是否被聚合。如果被聚合则返回1，否则返回0。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">grouping_id(*cols)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">返回grouping的级别。</span></span><br><span class="line"><span class="string">cols必须严格匹配grouping columns，或者为空（表示所有的grouping columns)。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">kurtosis(col)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">返回一组元素的峰度。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="built_in">max</span>(col)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">返回组内的最大值。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">mean(col)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">回组内的均值。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="built_in">min</span>(col)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">返回组内的最小值。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">skewness(col)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">返回组内的偏度。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">stddev(col)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">返回组内的样本标准差（分母除以N-1）。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">stddev_pop(col)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">返回组内的总体标准差（分母除以N）。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">stddev_samp(col)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">返回组内的标准差，与stddev相同。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="built_in">sum</span>(col)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">返回组内的和。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">sumDistinct(col)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">返回组内去重值的和。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">var_pop(col)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">返回组内的总体方差。分母除以N。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">var_samp(col)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">返回组内的样本方差。分母除以N-1。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">variance(col)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">返回组内的总体方差，与var_pop相同。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><br>按位函数：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">bitwiseNot(col)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">返回一个字符串列，它是旧列的比特级的取反。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">isnan(col)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">返回指定的列是否是NaN。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">isnull(col)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">返回指定的列是否为null。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">shiftLeft(col,numBites)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">按位左移指定的比特位数。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">shiftRight(col,numBites)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">按位右移指定的比特位数。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">shiftRightUnsigned(col,numBites)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">按位右移指定的比特位数。但是无符号移动。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><br>排序、拷贝：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">asc(col)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">返回一个升序排列的Column。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">desc(col)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">返回一个降序排列的Column。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">col(col)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">返回值指定列组成的Column。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">column(col)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">返回值指定列组成的Column。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><br>窗口函数：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line">window(timeColumn,windowDuration,slideDuration=<span class="literal">None</span>,startTime=<span class="literal">None</span>)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">将rows划分到一个/多个窗口中（通过timestamp列）。</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">timeColumn：一个时间列，用于划分window。它必须是pyspark.sql.types.TimestampType。</span></span><br><span class="line"><span class="string">windowDuration：表示时间窗口间隔的字符串。如 &#x27;1 second&#x27;,&#x27;1 day 12 hours&#x27;,&#x27;2 minutes&#x27;。</span></span><br><span class="line"><span class="string">                单位字符串可以为&#x27;week&#x27;,&#x27;day&#x27;,&#x27;hour&#x27;,&#x27;minute&#x27;,&#x27;second&#x27;,&#x27;millisecond&#x27;,&#x27;microsecond&#x27;。</span></span><br><span class="line"><span class="string">slideDuration：表示窗口滑动的间隔，即：下一个窗口移动多少。如果未提供，则窗口为tumbling windows。</span></span><br><span class="line"><span class="string">               单位字符串可以为&#x27;week&#x27;,&#x27;day&#x27;,&#x27;hour&#x27;,&#x27;minute&#x27;,&#x27;second&#x27;,&#x27;millisecond&#x27;,&#x27;microsecond&#x27;。</span></span><br><span class="line"><span class="string">startTime：起始时间。它是1970-01-01 00:00:00 以来的相对偏移时刻。</span></span><br><span class="line"><span class="string">           如你需要在每个小时的15分钟开启小时窗口，则它为15 minutes：12:15-13:15,13:15-14:15,...这样子。</span></span><br><span class="line"><span class="string">返回值：返回一个称作window的struct，它包含[start,end)。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">cume_dist()</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">返回一个窗口中的累计分布概率。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">dense_rank()</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">返回窗口内的排名。不跳跃。</span></span><br><span class="line"><span class="string">它和rank()的区别在于：它的排名没有跳跃（比如有3个排名为1，那么下一个排名是2，而不是下一个排名为4）。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">rank()</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">返回窗口内的排名。跳跃。</span></span><br><span class="line"><span class="string">如有3个排名为1，则下一个排名是 4。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">percent_rank()</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">返回窗口的相对排名（如：百分比）。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">lag(col,count=<span class="number">1</span>,default=<span class="literal">None</span>)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">返回当前行之前偏移行的值。如果当前行之前的行数小于count，则返回default值。</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">col：一个字符串/Column。开窗的列。</span></span><br><span class="line"><span class="string">count：偏移行。</span></span><br><span class="line"><span class="string">default：默认值。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">lead(col,count=<span class="number">1</span>,default=<span class="literal">None</span>)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">返回当前行之后偏移行的值。如果当前行之后的行数小于count，则返回default值。</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">col：一个字符串/Column。开窗的列。</span></span><br><span class="line"><span class="string">count：偏移行。</span></span><br><span class="line"><span class="string">default：默认值。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">ntile(n)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">返回有序窗口分区中的ntile group id（从 1 到 n ）</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">row_number()</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">返回一个序列，从 1 开始，到窗口的长度。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><br>其它函数：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br></pre></td><td class="code"><pre><span class="line">array(*cols)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">创新一个新的array列。</span></span><br><span class="line"><span class="string">cols：列名字符串列表，或者Column列表。要求这些列具有同样的数据类型。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">array_contains(col, value)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">创建一个新列，指示value是否在array中（由col给定）。其中col必须是array类型。而value是一个值/一个Column/列名。</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">判断逻辑：</span></span><br><span class="line"><span class="string">如果array为null，则返回null；</span></span><br><span class="line"><span class="string">如果value位于array中，则返回True；</span></span><br><span class="line"><span class="string">如果value不在array中，则返回False。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">create_map(*cols)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">创建一个map（字典）列。</span></span><br><span class="line"><span class="string">cols：列名字符串列表，或者Column列表。这些列组成了键值对。如(key1,value1,key2,value2,...)。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">broadcast(df)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">标记df这个Dataframe足够小，从而应用于broadcast join。</span></span><br><span class="line"><span class="string">df：一个 Dataframe对象。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">coalesce(*cols)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">返回第一个非null的列组成的Column。如果都为null，则返回null。</span></span><br><span class="line"><span class="string">cols：列名字符串列表，或者Column列表。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">crc32(col)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">计算二进制列的CRC32校验值。要求col是二进制列。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">explode(col)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">将一个array或者map列拆成多行。要求col是一个array或者map列。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">posexplode(col)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">对指定array或者map中的每个元素，依据每个位置返回新的一行。要求col是一个array/map列。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">expr(<span class="built_in">str</span>)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">计算表达式。</span></span><br><span class="line"><span class="string">str：一个表达式。如length(name)。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">from_json(col,schema,options=&#123;&#125;)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">解析一个包含JSON字符串的列。如果遇到无法解析的字符串，则返回null。</span></span><br><span class="line"><span class="string">col：一个字符串列，字符串是json格式。</span></span><br><span class="line"><span class="string">schema：一个StructType（表示解析一个元素），或者StructType的ArrayType（表示解析一组元素）。</span></span><br><span class="line"><span class="string">options：用于控制解析过程。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">get_json_object(col,path)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">从json 字符串中提取指定的字段。如果json字符串无效，则返回null。</span></span><br><span class="line"><span class="string">col：包含json格式的字符串的列。</span></span><br><span class="line"><span class="string">path：json的字段的路径。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">greatest(*cols)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">返回指定的一堆列中的最大值。要求至少包含2列。</span></span><br><span class="line"><span class="string">它会跳过null值。如果都是null值，则返回null。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">least(*cols)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">返回指定的一堆列中的最小值。要求至少包含2列。</span></span><br><span class="line"><span class="string">它会跳过null值。如果都是null值，则返回null。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">json_tuple(col,*fields)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">从json列中抽取字段组成新列（抽取n个字段，则生成n列）。</span></span><br><span class="line"><span class="string">col：一个json字符串列。</span></span><br><span class="line"><span class="string">fields：一组字符串，给出了json中待抽取的字段。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">lit(col)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">创建一个字面量值的列。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">monotonically_increasing_id()</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">创建一个单调递增的id列（64位整数）。</span></span><br><span class="line"><span class="string">它可以确保结果是单调递增的，并且是unique的，但是不保证是连续的。</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">它隐含两个假设：</span></span><br><span class="line"><span class="string">假设dataframe分区数量少于1 billion。</span></span><br><span class="line"><span class="string">假设每个分区的记录数量少于8 billion。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">nanvl(col1,col2)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">如果col1不是NaN，则返回col1；否则返回col2。</span></span><br><span class="line"><span class="string">要求col1和col2都是浮点列（DoubleType或者 FloatType）。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">size(col)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">计算array/map列的长度（元素个数）。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">sort_array(col,asc=<span class="literal">True</span>)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">对array列中的array进行排序（排序的方式是自然的顺序）。</span></span><br><span class="line"><span class="string">col：一个字符串/Column，指定一个array列。</span></span><br><span class="line"><span class="string">asc： 如果为True，则是升序；否则是降序。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">spark_partition_id()</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">返回一个partition ID 列。</span></span><br><span class="line"><span class="string">该方法产生的结果依赖于数据划分和任务调度，因此是未确定结果的。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">struct(*cols)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">创建一个新的struct列。</span></span><br><span class="line"><span class="string">cols：一个字符串列表（指定了列名），或者一个Column列表。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">to_json(col,options=&#123;&#125;)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">将包含StructType/Arrytype的StructType转换为json字符串。如果遇到不支持的类型，则抛出异常。</span></span><br><span class="line"><span class="string">col：一个字符串/Column，表示待转换的列。</span></span><br><span class="line"><span class="string">options：转换选项。它支持和json datasource同样的选项。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">udf(f=<span class="literal">None</span>,returnType=StringType)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">根据用户定义函数(UDF)来创建一列。</span></span><br><span class="line"><span class="string">f：一个python函数，它接受一个参数。</span></span><br><span class="line"><span class="string">returnType：一个pyspqrk.sql.types.DataType类型，表示udf的返回类型。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">when(condition,value)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">对一系列条件求值，返回其中匹配的哪个结果。</span></span><br><span class="line"><span class="string">如果Column.otherwise()未被调用，则当未匹配时，返回None；</span></span><br><span class="line"><span class="string">如果Column.otherwise()被调用，则当未匹配时，返回otherwise()的结果。</span></span><br><span class="line"><span class="string">condition：一个布尔列。</span></span><br><span class="line"><span class="string">value：一个字面值，或者一个Column。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure></p>
<h3 id="反射推断"><a href="#反射推断" class="headerlink" title="反射推断"></a>反射推断</h3><p>Spark SQL 支持两种不同的方法用于转换已存在的 RDD 成为 DataFrame。<br>（1）使用<strong>反射去推断</strong>一个包含指定的对象类型的 RDD 的 Schema。这个基于<strong>方法的反射</strong>可以让你的代码更简洁。<br>（2）用于创建 DataFrame 的方法，是通过一个允许你构造一个 Schema 然后把它应用到一个已存在的 RDD 的编程接口。然而这种方法更繁琐, 当列和它们的类型直到运行时都是未知时，它允许你去构造 DataFrame。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 有一个txt文件，查看它的内容</span></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;./data/people.txt&#x27;</span>,<span class="string">&#x27;r+&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">    line = f.readline()</span><br><span class="line">    <span class="keyword">while</span> line:</span><br><span class="line">        print(line,end=<span class="string">&#x27;&#x27;</span>)</span><br><span class="line">        line = f.readline()</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">Michael, 29</span></span><br><span class="line"><span class="string">Andy, 30</span></span><br><span class="line"><span class="string">Justin, 19</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="comment"># 反射推断</span></span><br><span class="line">%config ZMQInteractiveShell.ast_node_interactivity=<span class="string">&#x27;all&#x27;</span> </span><br><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> Row </span><br><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession</span><br><span class="line"><span class="keyword">from</span> pyspark <span class="keyword">import</span> SparkContext </span><br><span class="line"><span class="keyword">from</span> pyspark <span class="keyword">import</span> SparkConf </span><br><span class="line"></span><br><span class="line">spark = SparkSession \</span><br><span class="line">    .builder \</span><br><span class="line">    .appName(<span class="string">&quot;Python Spark SQL&quot;</span>) \</span><br><span class="line">    .config(<span class="string">&quot;spark.some.config.option&quot;</span>, <span class="string">&quot;some-value&quot;</span>) \</span><br><span class="line">    .getOrCreate()</span><br><span class="line"></span><br><span class="line">sc = spark.sparkContext</span><br><span class="line">lines = sc.textFile(<span class="string">&quot;./data/people.txt&quot;</span>)</span><br><span class="line">parts = lines.<span class="built_in">map</span>(<span class="keyword">lambda</span> l: l.split(<span class="string">&quot;,&quot;</span>)) </span><br><span class="line"><span class="comment"># [&#x27;Michael&#x27;, &#x27; 29&#x27;], [&#x27;Andy&#x27;, &#x27; 30&#x27;], [&#x27;Justin&#x27;, &#x27; 19&#x27;]</span></span><br><span class="line">people = parts.<span class="built_in">map</span>(<span class="keyword">lambda</span> p: Row(name=p[<span class="number">0</span>], age=<span class="built_in">int</span>(p[<span class="number">1</span>]))) </span><br><span class="line"><span class="comment"># Row(age=29, name=&#x27;Michael&#x27;),Row(age=30, name=&#x27;Andy&#x27;),Row(age=19, name=&#x27;Justin&#x27;)</span></span><br><span class="line"></span><br><span class="line">schemaPeople = spark.createDataFrame(people) <span class="comment"># 用people创建DataFrame</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 依照已有的DataFrame，创建一个临时的表(相当于mysql数据库中的一个表)，这样就可以用纯sql语句进行数据操作，这就是反射推断</span></span><br><span class="line">schemaPeople.createOrReplaceTempView(<span class="string">&quot;people&quot;</span>)</span><br><span class="line"></span><br><span class="line">teenagers = spark.sql(<span class="string">&quot;SELECT name FROM people WHERE age &gt;= 13 AND age &lt;= 19&quot;</span>) <span class="comment"># 使用sql查询</span></span><br><span class="line"><span class="built_in">type</span>(teenagers)</span><br><span class="line"><span class="built_in">type</span>(teenagers.rdd) <span class="comment"># DataFrame转rdd</span></span><br><span class="line">teenagers.rdd.first()</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">pyspark.sql.dataframe.DataFrame</span></span><br><span class="line"><span class="string">pyspark.rdd.RDD</span></span><br><span class="line"><span class="string">Row(name=&#x27;Justin&#x27;)</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">teenNames = teenagers.rdd.<span class="built_in">map</span>(<span class="keyword">lambda</span> p: <span class="string">&quot;Name: &quot;</span> + p.name).collect()</span><br><span class="line"><span class="keyword">for</span> name <span class="keyword">in</span> teenNames:</span><br><span class="line">    print(name)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">Name: Justin</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><br>不用反射，如何进行程序指定Schema，通过以下的方式去初始化一个 DataFrame：<br>（1）RDD转成一个RDD的toples或者一个列表。<br>（2）创建 Schema 结构，字段用 StructField 创建， 再用StructType 组成整体结构，之后匹配 RDD 中的结构。<br>（3）通过 SparkSession 提供的 createDataFrame 方法应用 Schema 到 RDD 。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.sql.types <span class="keyword">import</span> *</span><br><span class="line"></span><br><span class="line">sc = spark.sparkContext</span><br><span class="line"></span><br><span class="line">lines = sc.textFile(<span class="string">&quot;./data/people.txt&quot;</span>)</span><br><span class="line">parts = lines.<span class="built_in">map</span>(<span class="keyword">lambda</span> l: l.split(<span class="string">&quot;,&quot;</span>))</span><br><span class="line">people = parts.<span class="built_in">map</span>(<span class="keyword">lambda</span> p: (p[<span class="number">0</span>], p[<span class="number">1</span>].strip()))<span class="comment">#</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># The schema is encoded in a string.</span></span><br><span class="line">schemaString = <span class="string">&quot;name age&quot;</span> <span class="comment"># 定义表结构</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义字段结构StructField(名字,类型,是否可以为空)</span></span><br><span class="line"><span class="comment"># [StructField(name,StringType,true), StructField(age,StringType,true)]</span></span><br><span class="line">fields = [StructField(field_name, StringType(), <span class="literal">True</span>) <span class="keyword">for</span> field_name <span class="keyword">in</span> schemaString.split()]</span><br><span class="line">schema = StructType(fields) <span class="comment"># 按照字段做成表</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Apply the schema to the RDD.</span></span><br><span class="line">schemaPeople = spark.createDataFrame(people, schema) <span class="comment"># 数据和结构放入表中</span></span><br><span class="line"></span><br><span class="line">schemaPeople.createOrReplaceTempView(<span class="string">&quot;people&quot;</span>)</span><br><span class="line">results = spark.sql(<span class="string">&quot;SELECT * FROM people&quot;</span>)</span><br><span class="line">results.show()</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">+-------+---+</span></span><br><span class="line"><span class="string">|   name|age|</span></span><br><span class="line"><span class="string">+-------+---+</span></span><br><span class="line"><span class="string">|Michael| 29|</span></span><br><span class="line"><span class="string">|   Andy| 30|</span></span><br><span class="line"><span class="string">| Justin| 19|</span></span><br><span class="line"><span class="string">+-------+---+</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">jsonString = [</span><br><span class="line"><span class="string">&quot;&quot;&quot;&#123; &quot;id&quot; : &quot;01001&quot;, &quot;city&quot; : &quot;AGAWAM&quot;,  &quot;pop&quot; : 15338, &quot;state&quot; : &quot;MA&quot; &#125;&quot;&quot;&quot;</span>,</span><br><span class="line"><span class="string">&quot;&quot;&quot;&#123; &quot;id&quot; : &quot;01002&quot;, &quot;city&quot; : &quot;CUSHMAN&quot;, &quot;pop&quot; : 36963, &quot;state&quot; : &quot;MA&quot; &#125;&quot;&quot;&quot;</span></span><br><span class="line">]</span><br><span class="line"></span><br><span class="line">jsonRDD = sc.parallelize(jsonString)</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> pyspark.sql.types <span class="keyword">import</span> *</span><br><span class="line"></span><br><span class="line"><span class="comment">#定义结构类型</span></span><br><span class="line"><span class="comment">#StructType：schema的整体结构，表示JSON的对象结构</span></span><br><span class="line"><span class="comment">#XXXStype:指的是某一列的数据类型</span></span><br><span class="line">jsonSchema = StructType() \</span><br><span class="line">  .add(<span class="string">&quot;id&quot;</span>, StringType(),<span class="literal">True</span>) \</span><br><span class="line">  .add(<span class="string">&quot;city&quot;</span>, StringType()) \</span><br><span class="line">  .add(<span class="string">&quot;pop&quot;</span> , LongType()) \</span><br><span class="line">  .add(<span class="string">&quot;state&quot;</span>,StringType())</span><br><span class="line"></span><br><span class="line">jsonSchema = StructType() \</span><br><span class="line">  .add(<span class="string">&quot;id&quot;</span>, LongType(),<span class="literal">True</span>) \</span><br><span class="line">  .add(<span class="string">&quot;city&quot;</span>, StringType()) \</span><br><span class="line">  .add(<span class="string">&quot;pop&quot;</span> , DoubleType()) \</span><br><span class="line">  .add(<span class="string">&quot;state&quot;</span>,StringType())</span><br><span class="line"></span><br><span class="line">jsonDF= spark.read.schema(jsonSchema).json(jsonRDD)</span><br><span class="line"></span><br><span class="line">jsonDF.printSchema()</span><br><span class="line">jsonDF.show()</span><br></pre></td></tr></table></figure>
<h3 id="综合示例"><a href="#综合示例" class="headerlink" title="综合示例"></a>综合示例</h3><div class="tabs" id="f3"><ul class="nav-tabs"><li class="tab active"><a href="#f3-1">常用方法</a></li><li class="tab"><a href="#f3-2">去重</a></li><li class="tab"><a href="#f3-3">缺失值处理</a></li><li class="tab"><a href="#f3-4">异常值处理</a></li></ul><div class="tab-content"><div class="tab-pane active" id="f3-1"><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br><span class="line">302</span><br><span class="line">303</span><br><span class="line">304</span><br><span class="line">305</span><br><span class="line">306</span><br><span class="line">307</span><br><span class="line">308</span><br><span class="line">309</span><br><span class="line">310</span><br><span class="line">311</span><br><span class="line">312</span><br><span class="line">313</span><br><span class="line">314</span><br><span class="line">315</span><br><span class="line">316</span><br><span class="line">317</span><br><span class="line">318</span><br><span class="line">319</span><br><span class="line">320</span><br><span class="line">321</span><br><span class="line">322</span><br><span class="line">323</span><br><span class="line">324</span><br><span class="line">325</span><br><span class="line">326</span><br><span class="line">327</span><br><span class="line">328</span><br><span class="line">329</span><br><span class="line">330</span><br><span class="line">331</span><br><span class="line">332</span><br><span class="line">333</span><br><span class="line">334</span><br><span class="line">335</span><br><span class="line">336</span><br><span class="line">337</span><br><span class="line">338</span><br><span class="line">339</span><br><span class="line">340</span><br><span class="line">341</span><br><span class="line">342</span><br><span class="line">343</span><br><span class="line">344</span><br><span class="line">345</span><br><span class="line">346</span><br><span class="line">347</span><br><span class="line">348</span><br><span class="line">349</span><br><span class="line">350</span><br><span class="line">351</span><br><span class="line">352</span><br><span class="line">353</span><br><span class="line">354</span><br><span class="line">355</span><br><span class="line">356</span><br><span class="line">357</span><br><span class="line">358</span><br><span class="line">359</span><br><span class="line">360</span><br><span class="line">361</span><br><span class="line">362</span><br><span class="line">363</span><br><span class="line">364</span><br><span class="line">365</span><br><span class="line">366</span><br><span class="line">367</span><br><span class="line">368</span><br><span class="line">369</span><br><span class="line">370</span><br><span class="line">371</span><br><span class="line">372</span><br><span class="line">373</span><br><span class="line">374</span><br><span class="line">375</span><br><span class="line">376</span><br><span class="line">377</span><br><span class="line">378</span><br><span class="line">379</span><br><span class="line">380</span><br><span class="line">381</span><br><span class="line">382</span><br><span class="line">383</span><br><span class="line">384</span><br><span class="line">385</span><br><span class="line">386</span><br><span class="line">387</span><br><span class="line">388</span><br><span class="line">389</span><br><span class="line">390</span><br><span class="line">391</span><br><span class="line">392</span><br><span class="line">393</span><br><span class="line">394</span><br><span class="line">395</span><br><span class="line">396</span><br><span class="line">397</span><br><span class="line">398</span><br><span class="line">399</span><br><span class="line">400</span><br><span class="line">401</span><br><span class="line">402</span><br><span class="line">403</span><br><span class="line">404</span><br><span class="line">405</span><br><span class="line">406</span><br><span class="line">407</span><br><span class="line">408</span><br><span class="line">409</span><br><span class="line">410</span><br><span class="line">411</span><br><span class="line">412</span><br><span class="line">413</span><br><span class="line">414</span><br><span class="line">415</span><br><span class="line">416</span><br><span class="line">417</span><br><span class="line">418</span><br><span class="line">419</span><br><span class="line">420</span><br><span class="line">421</span><br><span class="line">422</span><br><span class="line">423</span><br><span class="line">424</span><br><span class="line">425</span><br><span class="line">426</span><br><span class="line">427</span><br><span class="line">428</span><br><span class="line">429</span><br><span class="line">430</span><br><span class="line">431</span><br><span class="line">432</span><br><span class="line">433</span><br><span class="line">434</span><br><span class="line">435</span><br><span class="line">436</span><br><span class="line">437</span><br><span class="line">438</span><br><span class="line">439</span><br><span class="line">440</span><br><span class="line">441</span><br><span class="line">442</span><br><span class="line">443</span><br><span class="line">444</span><br><span class="line">445</span><br><span class="line">446</span><br><span class="line">447</span><br><span class="line">448</span><br><span class="line">449</span><br><span class="line">450</span><br><span class="line">451</span><br><span class="line">452</span><br><span class="line">453</span><br><span class="line">454</span><br><span class="line">455</span><br><span class="line">456</span><br><span class="line">457</span><br><span class="line">458</span><br><span class="line">459</span><br><span class="line">460</span><br><span class="line">461</span><br><span class="line">462</span><br><span class="line">463</span><br><span class="line">464</span><br><span class="line">465</span><br><span class="line">466</span><br><span class="line">467</span><br><span class="line">468</span><br><span class="line">469</span><br><span class="line">470</span><br><span class="line">471</span><br><span class="line">472</span><br><span class="line">473</span><br><span class="line">474</span><br><span class="line">475</span><br><span class="line">476</span><br><span class="line">477</span><br><span class="line">478</span><br><span class="line">479</span><br><span class="line">480</span><br><span class="line">481</span><br><span class="line">482</span><br><span class="line">483</span><br><span class="line">484</span><br><span class="line">485</span><br><span class="line">486</span><br><span class="line">487</span><br><span class="line">488</span><br><span class="line">489</span><br><span class="line">490</span><br><span class="line">491</span><br><span class="line">492</span><br><span class="line">493</span><br><span class="line">494</span><br><span class="line">495</span><br><span class="line">496</span><br><span class="line">497</span><br><span class="line">498</span><br><span class="line">499</span><br><span class="line">500</span><br><span class="line">501</span><br><span class="line">502</span><br><span class="line">503</span><br><span class="line">504</span><br><span class="line">505</span><br><span class="line">506</span><br><span class="line">507</span><br><span class="line">508</span><br><span class="line">509</span><br><span class="line">510</span><br><span class="line">511</span><br><span class="line">512</span><br><span class="line">513</span><br><span class="line">514</span><br><span class="line">515</span><br><span class="line">516</span><br><span class="line">517</span><br><span class="line">518</span><br><span class="line">519</span><br><span class="line">520</span><br><span class="line">521</span><br><span class="line">522</span><br><span class="line">523</span><br><span class="line">524</span><br><span class="line">525</span><br><span class="line">526</span><br><span class="line">527</span><br><span class="line">528</span><br><span class="line">529</span><br><span class="line">530</span><br><span class="line">531</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession</span><br><span class="line"></span><br><span class="line"><span class="comment"># appName为sparksession命名</span></span><br><span class="line"><span class="comment"># config配置</span></span><br><span class="line"><span class="comment"># getOrCreate有此sparksession拿来用，没有新建。</span></span><br><span class="line">spark = SparkSession \</span><br><span class="line">    .builder \</span><br><span class="line">    .appName(<span class="string">&quot;Python Spark SQL&quot;</span>) \</span><br><span class="line">    .config(<span class="string">&quot;spark.some.config.option&quot;</span>, <span class="string">&quot;some-value&quot;</span>) \</span><br><span class="line">    .getOrCreate()</span><br><span class="line"></span><br><span class="line">df = spark.createDataFrame([(<span class="number">5</span>,<span class="number">10</span>)], [<span class="string">&#x27;a&#x27;</span>,<span class="string">&#x27;b&#x27;</span>])</span><br><span class="line">df.show()</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">+---+---+</span></span><br><span class="line"><span class="string">|  a|  b|</span></span><br><span class="line"><span class="string">+---+---+</span></span><br><span class="line"><span class="string">|  5| 10|</span></span><br><span class="line"><span class="string">+---+---+</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="keyword">from</span> pyspark.sql.functions <span class="keyword">import</span> lit</span><br><span class="line">df.select(lit(<span class="number">5</span>).alias(<span class="string">&#x27;height&#x27;</span>)).withColumn(<span class="string">&#x27;spark_user&#x27;</span>, lit(<span class="literal">True</span>)).show()</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">+------+----------+</span></span><br><span class="line"><span class="string">|height|spark_user|</span></span><br><span class="line"><span class="string">+------+----------+</span></span><br><span class="line"><span class="string">|     5|      true|</span></span><br><span class="line"><span class="string">+------+----------+</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="comment"># 重命名列名，把a改成s</span></span><br><span class="line">df.withColumnRenamed(<span class="string">&#x27;a&#x27;</span>, <span class="string">&#x27;s&#x27;</span>).show()</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">+---+---+</span></span><br><span class="line"><span class="string">|  s|  b|</span></span><br><span class="line"><span class="string">+---+---+</span></span><br><span class="line"><span class="string">|  5| 10|</span></span><br><span class="line"><span class="string">+---+---+</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="comment"># 有一个json文件，先来它的看看内容</span></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;./data/people.json&#x27;</span>,<span class="string">&#x27;r+&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">    line = f.readline()</span><br><span class="line">    <span class="keyword">while</span> line:</span><br><span class="line">        print(line,end=<span class="string">&#x27;&#x27;</span>)</span><br><span class="line">        line = f.readline()</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">&#123;&quot;name&quot;:&quot;Michael&quot;&#125;</span></span><br><span class="line"><span class="string">&#123;&quot;name&quot;:&quot;Andy&quot;, &quot;age&quot;:30&#125;</span></span><br><span class="line"><span class="string">&#123;&quot;name&quot;:&quot;Justin&quot;, &quot;age&quot;:19&#125;</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="comment"># Spark读取json文件</span></span><br><span class="line"><span class="comment"># 等价于spark.read.format(&#x27;json&#x27;).load(&#x27;xxx.json&#x27;)</span></span><br><span class="line"><span class="comment"># 还可以指定schema，方便使用SQL：spark.read.schema(jsonSchema).json(&#x27;xxx.json&#x27;)</span></span><br><span class="line">df = spark.read.json(<span class="string">&quot;./data/people.json&quot;</span>)</span><br><span class="line">df.show()</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">+----+-------+</span></span><br><span class="line"><span class="string">| age|   name|</span></span><br><span class="line"><span class="string">+----+-------+</span></span><br><span class="line"><span class="string">|null|Michael|</span></span><br><span class="line"><span class="string">|  30|   Andy|</span></span><br><span class="line"><span class="string">|  19| Justin|</span></span><br><span class="line"><span class="string">+----+-------+</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="comment"># 查看表结构，相当于df.info()</span></span><br><span class="line">df.printSchema() </span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">root</span></span><br><span class="line"><span class="string"> |-- age: long (nullable = true)</span></span><br><span class="line"><span class="string"> |-- name: string (nullable = true)</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="comment"># select相当于loc，选择列</span></span><br><span class="line">df.select(<span class="string">&quot;name&quot;</span>).show() </span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">+-------+</span></span><br><span class="line"><span class="string">|   name|</span></span><br><span class="line"><span class="string">+-------+</span></span><br><span class="line"><span class="string">|Michael|</span></span><br><span class="line"><span class="string">|   Andy|</span></span><br><span class="line"><span class="string">| Justin|</span></span><br><span class="line"><span class="string">+-------+</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="comment"># 选择多个列</span></span><br><span class="line">df.select([<span class="string">&quot;name&quot;</span>,<span class="string">&#x27;age&#x27;</span>]).show()</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">+-------+----+</span></span><br><span class="line"><span class="string">|   name| age|</span></span><br><span class="line"><span class="string">+-------+----+</span></span><br><span class="line"><span class="string">|Michael|null|</span></span><br><span class="line"><span class="string">|   Andy|  30|</span></span><br><span class="line"><span class="string">| Justin|  19|</span></span><br><span class="line"><span class="string">+-------+----+</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="comment"># 列数据处理后组成新列</span></span><br><span class="line">df.select(df[<span class="string">&#x27;name&#x27;</span>], df[<span class="string">&#x27;age&#x27;</span>] + <span class="number">1</span>).show() </span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">+-------+---------+</span></span><br><span class="line"><span class="string">|   name|(age + 1)|</span></span><br><span class="line"><span class="string">+-------+---------+</span></span><br><span class="line"><span class="string">|Michael|     null|</span></span><br><span class="line"><span class="string">|   Andy|       31|</span></span><br><span class="line"><span class="string">| Justin|       20|</span></span><br><span class="line"><span class="string">+-------+---------+</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">df = spark.createDataFrame([(<span class="string">&#x27;abcd&#x27;</span>,<span class="string">&#x27;123&#x27;</span>)], [<span class="string">&#x27;s&#x27;</span>, <span class="string">&#x27;d&#x27;</span>])</span><br><span class="line">df.show()</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">+----+---+</span></span><br><span class="line"><span class="string">|   s|  d|</span></span><br><span class="line"><span class="string">+----+---+</span></span><br><span class="line"><span class="string">|abcd|123|</span></span><br><span class="line"><span class="string">+----+---+</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="comment"># 列的合并</span></span><br><span class="line"><span class="keyword">from</span> pyspark.sql.functions <span class="keyword">import</span> concat</span><br><span class="line">df.select(concat(df.s, df.d).alias(<span class="string">&#x27;b&#x27;</span>)).show()</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">+-------+</span></span><br><span class="line"><span class="string">|      b|</span></span><br><span class="line"><span class="string">+-------+</span></span><br><span class="line"><span class="string">|abcd123|</span></span><br><span class="line"><span class="string">+-------+</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="comment"># 列的过滤</span></span><br><span class="line">df.<span class="built_in">filter</span>(df[<span class="string">&#x27;age&#x27;</span>] &gt; <span class="number">21</span>).show() </span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">+---+----+</span></span><br><span class="line"><span class="string">|age|name|</span></span><br><span class="line"><span class="string">+---+----+</span></span><br><span class="line"><span class="string">| 30|Andy|</span></span><br><span class="line"><span class="string">+---+----+</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="comment"># 列的分组聚合</span></span><br><span class="line">df.groupBy(<span class="string">&quot;age&quot;</span>).count().show() </span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">+----+-----+</span></span><br><span class="line"><span class="string">| age|count|</span></span><br><span class="line"><span class="string">+----+-----+</span></span><br><span class="line"><span class="string">|  19|    1|</span></span><br><span class="line"><span class="string">|null|    1|</span></span><br><span class="line"><span class="string">|  30|    1|</span></span><br><span class="line"><span class="string">+----+-----+</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> Row</span><br><span class="line">df = spark.createDataFrame([Row(name=<span class="string">&#x27;Tom&#x27;</span>, age=<span class="number">15</span>)])</span><br><span class="line">df.show()</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">+---+----+</span></span><br><span class="line"><span class="string">|age|name|</span></span><br><span class="line"><span class="string">+---+----+</span></span><br><span class="line"><span class="string">| 15| Tom|</span></span><br><span class="line"><span class="string">+---+----+</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="comment"># 类型转换</span></span><br><span class="line">df.select(df.age.cast(<span class="string">&quot;string&quot;</span>).alias(<span class="string">&#x27;ages&#x27;</span>)).collect()</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">[Row(ages=&#x27;15&#x27;)]</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="comment"># 格式转换</span></span><br><span class="line"><span class="keyword">from</span> pyspark.sql.functions <span class="keyword">import</span> avg, format_number</span><br><span class="line">spark.createDataFrame([(<span class="number">5</span>,)], [<span class="string">&#x27;a&#x27;</span>]).select(format_number(<span class="string">&#x27;a&#x27;</span>, <span class="number">4</span>).alias(<span class="string">&#x27;v&#x27;</span>)).collect()</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">[Row(v=&#x27;5.0000&#x27;)]</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="comment"># 类型和格式转换</span></span><br><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession,Row </span><br><span class="line">spark = SparkSession.Builder().appName(<span class="string">&#x27;Exercise&#x27;</span>).getOrCreate()</span><br><span class="line"></span><br><span class="line">df = spark.createDataFrame([Row(name=<span class="string">&#x27;Tom&#x27;</span>, age=<span class="number">15.345</span>, height=<span class="number">175</span>),Row(name=<span class="string">&#x27;Jerry&#x27;</span>, age=<span class="number">12.456</span>, height=<span class="number">125</span>)])</span><br><span class="line">df.show()</span><br><span class="line"></span><br><span class="line">df.select([<span class="string">&#x27;name&#x27;</span>]+[format_number(df[c].cast(<span class="string">&#x27;float&#x27;</span>), <span class="number">2</span>).alias(c) <span class="keyword">for</span> c <span class="keyword">in</span> df.columns[:<span class="number">2</span>]]).show()</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">+------+------+-----+</span></span><br><span class="line"><span class="string">|   age|height| name|</span></span><br><span class="line"><span class="string">+------+------+-----+</span></span><br><span class="line"><span class="string">|15.345|   175|  Tom|</span></span><br><span class="line"><span class="string">|12.456|   125|Jerry|</span></span><br><span class="line"><span class="string">+------+------+-----+</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">+-----+-----+------+</span></span><br><span class="line"><span class="string">| name|  age|height|</span></span><br><span class="line"><span class="string">+-----+-----+------+</span></span><br><span class="line"><span class="string">|  Tom|15.35|175.00|</span></span><br><span class="line"><span class="string">|Jerry|12.46|125.00|</span></span><br><span class="line"><span class="string">+-----+-----+------+</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="comment"># 类型和格式转换</span></span><br><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession</span><br><span class="line">spark = SparkSession.Builder().appName(<span class="string">&#x27;Exercise&#x27;</span>).getOrCreate()</span><br><span class="line">df = spark.read.csv(<span class="string">r&#x27;./data/walmart_stock.csv&#x27;</span>,inferSchema=<span class="literal">True</span>, header=<span class="literal">True</span>)</span><br><span class="line">desc = df.describe() </span><br><span class="line">desc.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 转成float且保留2位小数</span></span><br><span class="line">desc.select([<span class="string">&#x27;summary&#x27;</span>]+[format_number(desc[c].cast(<span class="string">&#x27;float&#x27;</span>), <span class="number">2</span>).alias(c) <span class="keyword">for</span> c <span class="keyword">in</span> desc.columns[<span class="number">2</span>:]]).show()</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">+-------+------------------+-----------------+-----------------+-----------------+-----------------+-----------------+</span></span><br><span class="line"><span class="string">|summary|              Open|             High|              Low|            Close|           Volume|        Adj Close|</span></span><br><span class="line"><span class="string">+-------+------------------+-----------------+-----------------+-----------------+-----------------+-----------------+</span></span><br><span class="line"><span class="string">|  count|              1258|             1258|             1258|             1258|             1258|             1258|</span></span><br><span class="line"><span class="string">|   mean| 72.35785375357709|72.83938807631165| 71.9186009594594|72.38844998012726|8222093.481717011|67.23883848728146|</span></span><br><span class="line"><span class="string">| stddev|  6.76809024470826|6.768186808159218|6.744075756255496|6.756859163732991|  4519780.8431556|6.722609449996857|</span></span><br><span class="line"><span class="string">|    min|56.389998999999996|        57.060001|        56.299999|        56.419998|          2094900|        50.363689|</span></span><br><span class="line"><span class="string">|    max|         90.800003|        90.970001|            89.25|        90.470001|         80898100|84.91421600000001|</span></span><br><span class="line"><span class="string">+-------+------------------+-----------------+-----------------+-----------------+-----------------+-----------------+</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">+-------+--------+--------+--------+-------------+---------+</span></span><br><span class="line"><span class="string">|summary|    High|     Low|   Close|       Volume|Adj Close|</span></span><br><span class="line"><span class="string">+-------+--------+--------+--------+-------------+---------+</span></span><br><span class="line"><span class="string">|  count|1,258.00|1,258.00|1,258.00|     1,258.00| 1,258.00|</span></span><br><span class="line"><span class="string">|   mean|   72.84|   71.92|   72.39| 8,222,093.50|    67.24|</span></span><br><span class="line"><span class="string">| stddev|    6.77|    6.74|    6.76| 4,519,781.00|     6.72|</span></span><br><span class="line"><span class="string">|    min|   57.06|   56.30|   56.42| 2,094,900.00|    50.36|</span></span><br><span class="line"><span class="string">|    max|   90.97|   89.25|   90.47|80,898,096.00|    84.91|</span></span><br><span class="line"><span class="string">+-------+--------+--------+--------+-------------+---------+</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession,Row </span><br><span class="line">spark = SparkSession.Builder().appName(<span class="string">&#x27;Exercise&#x27;</span>).getOrCreate()</span><br><span class="line"></span><br><span class="line">df = spark.createDataFrame([Row(name=<span class="string">&#x27;Tom&#x27;</span>, age=<span class="number">15.345</span>, height=<span class="number">175</span>),Row(name=<span class="string">&#x27;Jerry&#x27;</span>, age=<span class="number">12.456</span>, height=<span class="number">125</span>),\</span><br><span class="line">                            Row(name=<span class="string">&#x27;Dog&#x27;</span>, age=<span class="number">16.326</span>, height=<span class="number">135</span>)])</span><br><span class="line">df.show()</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">+------+------+-----+</span></span><br><span class="line"><span class="string">|   age|height| name|</span></span><br><span class="line"><span class="string">+------+------+-----+</span></span><br><span class="line"><span class="string">|15.345|   175|  Tom|</span></span><br><span class="line"><span class="string">|12.456|   125|Jerry|</span></span><br><span class="line"><span class="string">|16.326|   135|  Dog|</span></span><br><span class="line"><span class="string">+------+------+-----+</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="comment"># 排序，sort和orderBy都能做</span></span><br><span class="line"><span class="comment"># 默认升序</span></span><br><span class="line">df.orderBy(<span class="string">&#x27;age&#x27;</span>).show()</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">+------+------+-----+</span></span><br><span class="line"><span class="string">|   age|height| name|</span></span><br><span class="line"><span class="string">+------+------+-----+</span></span><br><span class="line"><span class="string">|12.456|   125|Jerry|</span></span><br><span class="line"><span class="string">|15.345|   175|  Tom|</span></span><br><span class="line"><span class="string">|16.326|   135|  Dog|</span></span><br><span class="line"><span class="string">+------+------+-----+</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="comment"># 降序</span></span><br><span class="line">df.sort(df.age.desc()).show()</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">+------+------+-----+</span></span><br><span class="line"><span class="string">|   age|height| name|</span></span><br><span class="line"><span class="string">+------+------+-----+</span></span><br><span class="line"><span class="string">|16.326|   135|  Dog|</span></span><br><span class="line"><span class="string">|15.345|   175|  Tom|</span></span><br><span class="line"><span class="string">|12.456|   125|Jerry|</span></span><br><span class="line"><span class="string">+------+------+-----+</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="comment"># 降序</span></span><br><span class="line">df.orderBy(df.age.desc()).show()</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">+------+------+-----+</span></span><br><span class="line"><span class="string">|   age|height| name|</span></span><br><span class="line"><span class="string">+------+------+-----+</span></span><br><span class="line"><span class="string">|16.326|   135|  Dog|</span></span><br><span class="line"><span class="string">|15.345|   175|  Tom|</span></span><br><span class="line"><span class="string">|12.456|   125|Jerry|</span></span><br><span class="line"><span class="string">+------+------+-----+</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession,Row </span><br><span class="line">spark = SparkSession.Builder().appName(<span class="string">&#x27;Exercise&#x27;</span>).getOrCreate()</span><br><span class="line"></span><br><span class="line">df = spark.createDataFrame([Row(name=<span class="string">&#x27;Tom&#x27;</span>, age=<span class="number">15.345</span>, height=<span class="number">175</span>),Row(name=<span class="string">&#x27;Jerry&#x27;</span>, age=<span class="number">12.456</span>, height=<span class="number">125</span>),\</span><br><span class="line">                            Row(name=<span class="string">&#x27;Dog&#x27;</span>, age=<span class="number">16.326</span>, height=<span class="number">135</span>)])</span><br><span class="line">df.show()</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">+------+------+-----+</span></span><br><span class="line"><span class="string">|   age|height| name|</span></span><br><span class="line"><span class="string">+------+------+-----+</span></span><br><span class="line"><span class="string">|15.345|   175|  Tom|</span></span><br><span class="line"><span class="string">|12.456|   125|Jerry|</span></span><br><span class="line"><span class="string">|16.326|   135|  Dog|</span></span><br><span class="line"><span class="string">+------+------+-----+</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="comment"># mean和avg都是计算均值</span></span><br><span class="line"><span class="keyword">from</span> pyspark.sql.functions <span class="keyword">import</span> avg,mean</span><br><span class="line">df.select(avg(<span class="string">&#x27;age&#x27;</span>)).show()</span><br><span class="line">df.select(mean(<span class="string">&#x27;age&#x27;</span>)).show()</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">+------------------+</span></span><br><span class="line"><span class="string">|          avg(age)|</span></span><br><span class="line"><span class="string">+------------------+</span></span><br><span class="line"><span class="string">|14.709000000000001|</span></span><br><span class="line"><span class="string">+------------------+</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">+------------------+</span></span><br><span class="line"><span class="string">|          avg(age)|</span></span><br><span class="line"><span class="string">+------------------+</span></span><br><span class="line"><span class="string">|14.709000000000001|</span></span><br><span class="line"><span class="string">+------------------+</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="comment"># 计算最大最小值</span></span><br><span class="line"><span class="keyword">from</span> pyspark.sql.functions <span class="keyword">import</span> <span class="built_in">min</span>, <span class="built_in">max</span></span><br><span class="line">df.select(<span class="built_in">max</span>(df[<span class="string">&#x27;age&#x27;</span>]), <span class="built_in">min</span>(df[<span class="string">&#x27;age&#x27;</span>])).show()</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">+--------+--------+</span></span><br><span class="line"><span class="string">|max(age)|min(age)|</span></span><br><span class="line"><span class="string">+--------+--------+</span></span><br><span class="line"><span class="string">|  16.326|  12.456|</span></span><br><span class="line"><span class="string">+--------+--------+</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="comment"># age小于16的有多少人</span></span><br><span class="line"><span class="keyword">from</span> pyspark.sql.functions <span class="keyword">import</span> countDistinct</span><br><span class="line">df.<span class="built_in">filter</span>(df[<span class="string">&#x27;age&#x27;</span>] &lt; <span class="number">16</span>).select(countDistinct(df[<span class="string">&#x27;name&#x27;</span>])).show()</span><br><span class="line">df.<span class="built_in">filter</span>(df[<span class="string">&#x27;age&#x27;</span>] &lt; <span class="number">16</span>).count()</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">+--------------------+</span></span><br><span class="line"><span class="string">|count(DISTINCT name)|</span></span><br><span class="line"><span class="string">+--------------------+</span></span><br><span class="line"><span class="string">|                   2|</span></span><br><span class="line"><span class="string">+--------------------+</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">2</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="comment"># height&gt;135的人数，占数据总人数的百分之多少？</span></span><br><span class="line">df.<span class="built_in">filter</span>(df[<span class="string">&#x27;height&#x27;</span>] &gt; <span class="number">135</span>).count() / df.count() * <span class="number">100</span> <span class="comment"># 百分制乘以100</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">33.33333333333333</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="comment"># 求age和height的皮尔逊相关性</span></span><br><span class="line"><span class="keyword">from</span> pyspark.sql.functions <span class="keyword">import</span> corr</span><br><span class="line">df.select(corr(<span class="string">&#x27;age&#x27;</span>, <span class="string">&#x27;height&#x27;</span>)).show()</span><br><span class="line">df.corr(<span class="string">&#x27;age&#x27;</span>, <span class="string">&#x27;height&#x27;</span>)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">+-------------------+</span></span><br><span class="line"><span class="string">|  corr(age, height)|</span></span><br><span class="line"><span class="string">+-------------------+</span></span><br><span class="line"><span class="string">|0.45060091587683804|</span></span><br><span class="line"><span class="string">+-------------------+</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">0.450600915876838</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession,Row </span><br><span class="line"><span class="keyword">import</span> datetime</span><br><span class="line">spark = SparkSession.Builder().appName(<span class="string">&#x27;Exercise&#x27;</span>).getOrCreate()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 添加时间，方法1</span></span><br><span class="line">df1 = spark.createDataFrame([Row(ns=datetime.datetime(<span class="number">1997</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>))])</span><br><span class="line">df1.show()</span><br><span class="line">df1.printSchema()</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">+-------------------+</span></span><br><span class="line"><span class="string">|                 ns|</span></span><br><span class="line"><span class="string">+-------------------+</span></span><br><span class="line"><span class="string">|1997-01-01 00:00:00|</span></span><br><span class="line"><span class="string">+-------------------+</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">root</span></span><br><span class="line"><span class="string"> |-- ns: timestamp (nullable = true)</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="comment"># 添加时间，方法2</span></span><br><span class="line">df2 = spark.createDataFrame([Row(ns=datetime.date(<span class="number">2015</span>, <span class="number">5</span>, <span class="number">8</span>),h=<span class="number">10</span>),\</span><br><span class="line">                             Row(ns=datetime.date(<span class="number">2015</span>, <span class="number">4</span>, <span class="number">8</span>),h=<span class="number">12</span>),\</span><br><span class="line">                             Row(ns=datetime.date(<span class="number">2016</span>, <span class="number">5</span>, <span class="number">8</span>),h=<span class="number">13</span>),\</span><br><span class="line">                             Row(ns=datetime.date(<span class="number">2016</span>, <span class="number">4</span>, <span class="number">8</span>),h=<span class="number">11</span>)])</span><br><span class="line">df2.show()</span><br><span class="line">df2.printSchema()</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">+---+----------+</span></span><br><span class="line"><span class="string">|  h|        ns|</span></span><br><span class="line"><span class="string">+---+----------+</span></span><br><span class="line"><span class="string">| 10|2015-05-08|</span></span><br><span class="line"><span class="string">| 12|2015-04-08|</span></span><br><span class="line"><span class="string">| 13|2016-05-08|</span></span><br><span class="line"><span class="string">| 11|2016-04-08|</span></span><br><span class="line"><span class="string">+---+----------+</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">root</span></span><br><span class="line"><span class="string"> |-- h: long (nullable = true)</span></span><br><span class="line"><span class="string"> |-- ns: date (nullable = true)</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="comment"># 取值：年、月、日</span></span><br><span class="line"><span class="keyword">from</span> pyspark.sql.functions <span class="keyword">import</span> year,month,dayofmonth,dayofweek,dayofyear</span><br><span class="line">df = df2.withColumn(<span class="string">&#x27;year&#x27;</span>, year(df2.ns))\</span><br><span class="line">    .withColumn(<span class="string">&#x27;month&#x27;</span>, month(df2.ns))\</span><br><span class="line">    .withColumn(<span class="string">&#x27;dayofmonth&#x27;</span>, dayofmonth(df2.ns))\</span><br><span class="line">    .withColumn(<span class="string">&#x27;dayofweek&#x27;</span>, dayofweek(df2.ns))\</span><br><span class="line">    .withColumn(<span class="string">&#x27;dayofyear&#x27;</span>, dayofyear(df2.ns))</span><br><span class="line"></span><br><span class="line">df.show()</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">+---+----------+----+-----+----------+---------+---------+</span></span><br><span class="line"><span class="string">|  h|        ns|year|month|dayofmonth|dayofweek|dayofyear|</span></span><br><span class="line"><span class="string">+---+----------+----+-----+----------+---------+---------+</span></span><br><span class="line"><span class="string">| 10|2015-05-08|2015|    5|         8|        6|      128|</span></span><br><span class="line"><span class="string">| 12|2015-04-08|2015|    4|         8|        4|       98|</span></span><br><span class="line"><span class="string">| 13|2016-05-08|2016|    5|         8|        1|      129|</span></span><br><span class="line"><span class="string">| 11|2016-04-08|2016|    4|         8|        6|       99|</span></span><br><span class="line"><span class="string">+---+----------+----+-----+----------+---------+---------+</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="comment"># 分组聚合：按照年分组，取每组h最大值</span></span><br><span class="line">df.groupBy(<span class="string">&#x27;year&#x27;</span>).<span class="built_in">max</span>().select([<span class="string">&#x27;year&#x27;</span>, <span class="string">&#x27;max(h)&#x27;</span>]).show()</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">+----+------+</span></span><br><span class="line"><span class="string">|year|max(h)|</span></span><br><span class="line"><span class="string">+----+------+</span></span><br><span class="line"><span class="string">|2015|    12|</span></span><br><span class="line"><span class="string">|2016|    13|</span></span><br><span class="line"><span class="string">+----+------+</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">df = spark.createDataFrame([(<span class="string">&#x27;1997-02-28&#x27;</span>,)], [<span class="string">&#x27;d&#x27;</span>])</span><br><span class="line">df.show()</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">+----------+</span></span><br><span class="line"><span class="string">|         d|</span></span><br><span class="line"><span class="string">+----------+</span></span><br><span class="line"><span class="string">|1997-02-28|</span></span><br><span class="line"><span class="string">+----------+</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="comment"># 截取date到指定格式</span></span><br><span class="line"><span class="keyword">from</span> pyspark.sql.functions <span class="keyword">import</span> trunc</span><br><span class="line"><span class="comment"># 截取到年</span></span><br><span class="line">df.select(trunc(df.d, <span class="string">&#x27;year&#x27;</span>).alias(<span class="string">&#x27;year&#x27;</span>)).show()</span><br><span class="line"><span class="comment"># 截取到月</span></span><br><span class="line">df.select(trunc(df.d, <span class="string">&#x27;mon&#x27;</span>).alias(<span class="string">&#x27;month&#x27;</span>)).show()</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">+----------+</span></span><br><span class="line"><span class="string">|      year|</span></span><br><span class="line"><span class="string">+----------+</span></span><br><span class="line"><span class="string">|1997-01-01|</span></span><br><span class="line"><span class="string">+----------+</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">+----------+</span></span><br><span class="line"><span class="string">|     month|</span></span><br><span class="line"><span class="string">+----------+</span></span><br><span class="line"><span class="string">|1997-02-01|</span></span><br><span class="line"><span class="string">+----------+</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="comment"># udf可参考https://blog.csdn.net/crazybean_lwb/article/details/87006752</span></span><br><span class="line"><span class="keyword">from</span> pyspark.sql.functions <span class="keyword">import</span> udf</span><br><span class="line"><span class="keyword">from</span> pyspark.sql.types <span class="keyword">import</span> IntegerType, StringType</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用udf定义函数</span></span><br><span class="line"><span class="comment"># 需指定 自定义函数、返回值类型</span></span><br><span class="line">slen = udf(<span class="keyword">lambda</span> s: <span class="built_in">len</span>(s), IntegerType())</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">to_upper</span>(<span class="params">s</span>):</span></span><br><span class="line">    <span class="keyword">if</span> s <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        <span class="keyword">return</span> s.upper()</span><br><span class="line">to_upper = udf(to_upper,returnType=StringType())  </span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">add_one</span>(<span class="params">x</span>):</span></span><br><span class="line">     <span class="keyword">if</span> x <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        <span class="keyword">return</span> x + <span class="number">1</span></span><br><span class="line">add_one = udf(add_one,returnType=IntegerType())  </span><br><span class="line">    </span><br><span class="line"><span class="comment"># 测试</span></span><br><span class="line">df = spark.createDataFrame([(<span class="number">1</span>, <span class="string">&quot;John Doe&quot;</span>, <span class="number">21</span>)], (<span class="string">&quot;id&quot;</span>, <span class="string">&quot;name&quot;</span>, <span class="string">&quot;age&quot;</span>))</span><br><span class="line">df.show()</span><br><span class="line"></span><br><span class="line">df.select(slen(<span class="string">&quot;name&quot;</span>).alias(<span class="string">&quot;slen(name)&quot;</span>), to_upper(<span class="string">&quot;name&quot;</span>), add_one(<span class="string">&quot;age&quot;</span>)).show()</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">+---+--------+---+</span></span><br><span class="line"><span class="string">| id|    name|age|</span></span><br><span class="line"><span class="string">+---+--------+---+</span></span><br><span class="line"><span class="string">|  1|John Doe| 21|</span></span><br><span class="line"><span class="string">+---+--------+---+</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">+----------+--------------+------------+</span></span><br><span class="line"><span class="string">|slen(name)|to_upper(name)|add_one(age)|</span></span><br><span class="line"><span class="string">+----------+--------------+------------+</span></span><br><span class="line"><span class="string">|         8|      JOHN DOE|          22|</span></span><br><span class="line"><span class="string">+----------+--------------+------------+</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="comment"># 还可以使用Pandas UDF，在速度和处理耗时方面比Spark UDF更好</span></span><br><span class="line"><span class="keyword">from</span> pyspark.sql.functions <span class="keyword">import</span> pandas_udf</span><br><span class="line"><span class="keyword">from</span> pyspark.sql.types <span class="keyword">import</span> IntegerType</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">remaining_yrs</span>(<span class="params">age</span>):</span></span><br><span class="line">    yrs_left = <span class="number">100</span>-age</span><br><span class="line">    <span class="keyword">return</span> yrs_left</span><br><span class="line">length_udf = pandas_udf(remaining_yrs,IntegerType())</span><br><span class="line"><span class="comment"># 应用到age列，并且创建一个新列yrs_left</span></span><br><span class="line">df.withColumn(<span class="string">&quot;yrs_left&quot;</span>, length_udf(df[<span class="string">&#x27;age&#x27;</span>])).show()</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">+---+--------+---+--------+</span></span><br><span class="line"><span class="string">| id|    name|age|yrs_left|</span></span><br><span class="line"><span class="string">+---+--------+---+--------+</span></span><br><span class="line"><span class="string">|  1|John Doe| 21|      79|</span></span><br><span class="line"><span class="string">+---+--------+---+--------+</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="comment"># 当需要用matplotlib画图时候，需要把spark DataFrame转成pandas DataFrame</span></span><br><span class="line">df = spark.createDataFrame([(<span class="number">1</span>, <span class="string">&quot;John Doe&quot;</span>, <span class="number">21</span>)], (<span class="string">&quot;id&quot;</span>, <span class="string">&quot;name&quot;</span>, <span class="string">&quot;age&quot;</span>))</span><br><span class="line">df.show()</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">+---+--------+---+</span></span><br><span class="line"><span class="string">| id|    name|age|</span></span><br><span class="line"><span class="string">+---+--------+---+</span></span><br><span class="line"><span class="string">|  1|John Doe| 21|</span></span><br><span class="line"><span class="string">+---+--------+---+</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="comment"># 转成pandas DataFrame</span></span><br><span class="line">df.toPandas()</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">  id      name  age</span></span><br><span class="line"><span class="string">0  1  John Doe   21</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="comment"># 转成rdd</span></span><br><span class="line">df.rdd.collect()</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">[Row(id=1, name=&#x27;John Doe&#x27;, age=21)]</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="comment"># rdd转spark DataFrame</span></span><br><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> Row</span><br><span class="line">df = sc.parallelize([ \</span><br><span class="line">     Row(name=<span class="string">&#x27;Alice&#x27;</span>, age=<span class="number">5</span>, height=<span class="number">80</span>), \</span><br><span class="line">     Row(name=<span class="string">&#x27;Alice&#x27;</span>, age=<span class="number">5</span>, height=<span class="number">80</span>), \</span><br><span class="line">     Row(name=<span class="string">&#x27;Alice&#x27;</span>, age=<span class="number">10</span>, height=<span class="number">80</span>)]).toDF()</span><br><span class="line"></span><br><span class="line">df.show()</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">+---+------+-----+</span></span><br><span class="line"><span class="string">|age|height| name|</span></span><br><span class="line"><span class="string">+---+------+-----+</span></span><br><span class="line"><span class="string">|  5|    80|Alice|</span></span><br><span class="line"><span class="string">|  5|    80|Alice|</span></span><br><span class="line"><span class="string">| 10|    80|Alice|</span></span><br><span class="line"><span class="string">+---+------+-----+</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="comment"># 转成字典</span></span><br><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> Row</span><br><span class="line">Row(name=<span class="string">&quot;Alice&quot;</span>, age=<span class="number">11</span>).asDict()</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">&#123;&#x27;age&#x27;: 11, &#x27;name&#x27;: &#x27;Alice&#x27;&#125;</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure></div><div class="tab-pane" id="f3-2"><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">1.删除重复数据</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">groupby().count()：可以看到数据的重复情况</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">df = spark.createDataFrame([</span><br><span class="line">  (<span class="number">1</span>, <span class="number">144.5</span>, <span class="number">5.9</span>, <span class="number">33</span>, <span class="string">&#x27;M&#x27;</span>),</span><br><span class="line">  (<span class="number">2</span>, <span class="number">167.2</span>, <span class="number">5.4</span>, <span class="number">45</span>, <span class="string">&#x27;M&#x27;</span>),</span><br><span class="line">  (<span class="number">3</span>, <span class="number">124.1</span>, <span class="number">5.2</span>, <span class="number">23</span>, <span class="string">&#x27;F&#x27;</span>),</span><br><span class="line">  (<span class="number">4</span>, <span class="number">144.5</span>, <span class="number">5.9</span>, <span class="number">33</span>, <span class="string">&#x27;M&#x27;</span>),</span><br><span class="line">  (<span class="number">5</span>, <span class="number">133.2</span>, <span class="number">5.7</span>, <span class="number">54</span>, <span class="string">&#x27;F&#x27;</span>),</span><br><span class="line">  (<span class="number">3</span>, <span class="number">124.1</span>, <span class="number">5.2</span>, <span class="number">23</span>, <span class="string">&#x27;F&#x27;</span>),</span><br><span class="line">  (<span class="number">5</span>, <span class="number">129.2</span>, <span class="number">5.3</span>, <span class="number">42</span>, <span class="string">&#x27;M&#x27;</span>),</span><br><span class="line">], [<span class="string">&#x27;id&#x27;</span>, <span class="string">&#x27;weight&#x27;</span>, <span class="string">&#x27;height&#x27;</span>, <span class="string">&#x27;age&#x27;</span>, <span class="string">&#x27;gender&#x27;</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看重复记录</span></span><br><span class="line"><span class="comment">#无意义重复数据去重：数据中行与行完全重复</span></span><br><span class="line"><span class="comment"># 1.首先删除完全一样的记录</span></span><br><span class="line">df2 = df.dropDuplicates()</span><br><span class="line"></span><br><span class="line"><span class="comment">#有意义去重：删除除去无意义字段之外的完全重复的行数据</span></span><br><span class="line"><span class="comment"># 2.其次，关键字段值完全一模一样的记录（在这个例子中，是指除了id之外的列一模一样）</span></span><br><span class="line"><span class="comment"># 删除某些字段值完全一样的重复记录，subset参数定义这些字段</span></span><br><span class="line">df3 = df2.dropDuplicates(subset = [c <span class="keyword">for</span> c <span class="keyword">in</span> df2.columns <span class="keyword">if</span> c!=<span class="string">&#x27;id&#x27;</span>])</span><br><span class="line"><span class="comment"># 3.有意义的重复记录去重之后，再看某个无意义字段的值是否有重复（在这个例子中，是看id是否重复）</span></span><br><span class="line"><span class="comment"># 查看某一列是否有重复值</span></span><br><span class="line"><span class="keyword">import</span> pyspark.sql.functions <span class="keyword">as</span> fn</span><br><span class="line">df3.agg(fn.count(<span class="string">&#x27;id&#x27;</span>).alias(<span class="string">&#x27;id_count&#x27;</span>),fn.countDistinct(<span class="string">&#x27;id&#x27;</span>).alias(<span class="string">&#x27;distinct_id_count&#x27;</span>)).collect()</span><br><span class="line"><span class="comment"># 4.对于id这种无意义的列重复，添加另外一列自增id</span></span><br><span class="line"></span><br><span class="line">df3.withColumn(<span class="string">&#x27;new_id&#x27;</span>,fn.monotonically_increasing_id()).show()</span><br></pre></td></tr></table></figure></div><div class="tab-pane" id="f3-3"><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">2.处理缺失值</span></span><br><span class="line"><span class="string">2.1 对缺失值进行删除操作(行，列)</span></span><br><span class="line"><span class="string">2.2 对缺失值进行填充操作(列的均值)</span></span><br><span class="line"><span class="string">2.3 对缺失值对应的行或列进行标记</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">df_miss = spark.createDataFrame([</span><br><span class="line">(<span class="number">1</span>, <span class="number">143.5</span>, <span class="number">5.6</span>, <span class="number">28</span>,<span class="string">&#x27;M&#x27;</span>, <span class="number">100000</span>),</span><br><span class="line">(<span class="number">2</span>, <span class="number">167.2</span>, <span class="number">5.4</span>, <span class="number">45</span>,<span class="string">&#x27;M&#x27;</span>, <span class="literal">None</span>),</span><br><span class="line">(<span class="number">3</span>, <span class="literal">None</span> , <span class="number">5.2</span>, <span class="literal">None</span>, <span class="literal">None</span>, <span class="literal">None</span>),</span><br><span class="line">(<span class="number">4</span>, <span class="number">144.5</span>, <span class="number">5.9</span>, <span class="number">33</span>, <span class="string">&#x27;M&#x27;</span>, <span class="literal">None</span>),</span><br><span class="line">(<span class="number">5</span>, <span class="number">133.2</span>, <span class="number">5.7</span>, <span class="number">54</span>, <span class="string">&#x27;F&#x27;</span>, <span class="literal">None</span>),</span><br><span class="line">(<span class="number">6</span>, <span class="number">124.1</span>, <span class="number">5.2</span>, <span class="literal">None</span>, <span class="string">&#x27;F&#x27;</span>, <span class="literal">None</span>),</span><br><span class="line">(<span class="number">7</span>, <span class="number">129.2</span>, <span class="number">5.3</span>, <span class="number">42</span>, <span class="string">&#x27;M&#x27;</span>, <span class="number">76000</span>),],</span><br><span class="line"> [<span class="string">&#x27;id&#x27;</span>, <span class="string">&#x27;weight&#x27;</span>, <span class="string">&#x27;height&#x27;</span>, <span class="string">&#x27;age&#x27;</span>, <span class="string">&#x27;gender&#x27;</span>, <span class="string">&#x27;income&#x27;</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1.计算每条记录的缺失值情况</span></span><br><span class="line"></span><br><span class="line">df_miss.rdd.<span class="built_in">map</span>(<span class="keyword">lambda</span> row:(row[<span class="string">&#x27;id&#x27;</span>],<span class="built_in">sum</span>([c==<span class="literal">None</span> <span class="keyword">for</span> c <span class="keyword">in</span> row]))).collect()</span><br><span class="line">[(<span class="number">1</span>, <span class="number">0</span>), (<span class="number">2</span>, <span class="number">1</span>), (<span class="number">3</span>, <span class="number">4</span>), (<span class="number">4</span>, <span class="number">1</span>), (<span class="number">5</span>, <span class="number">1</span>), (<span class="number">6</span>, <span class="number">2</span>), (<span class="number">7</span>, <span class="number">0</span>)]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2.计算各列的缺失情况百分比</span></span><br><span class="line">df_miss.agg(*[(<span class="number">1</span> - (fn.count(c) / fn.count(<span class="string">&#x27;*&#x27;</span>))).alias(c + <span class="string">&#x27;_missing&#x27;</span>) <span class="keyword">for</span> c <span class="keyword">in</span> df_miss.columns]).show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3、删除缺失值过于严重的列</span></span><br><span class="line"><span class="comment"># 其实是先建一个DF，不要缺失值的列</span></span><br><span class="line">df_miss_no_income = df_miss.select([</span><br><span class="line">c <span class="keyword">for</span> c <span class="keyword">in</span> df_miss.columns <span class="keyword">if</span> c != <span class="string">&#x27;income&#x27;</span></span><br><span class="line">])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 4、按照缺失值删除行（threshold是根据一行记录中，缺失字段的百分比的定义）</span></span><br><span class="line">df_miss_no_income.dropna(thresh=<span class="number">3</span>).show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 5、填充缺失值，可以用fillna来填充缺失值，</span></span><br><span class="line"><span class="comment"># 对于bool类型、或者分类类型，可以为缺失值单独设置一个类型，missing</span></span><br><span class="line"><span class="comment"># 对于数值类型，可以用均值或者中位数等填充</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># fillna可以接收两种类型的参数：</span></span><br><span class="line"><span class="comment"># 一个数字、字符串，这时整个DataSet中所有的缺失值都会被填充为相同的值。</span></span><br><span class="line"><span class="comment"># 也可以接收一个字典｛列名：值｝这样</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 先计算均值，并组织成一个字典</span></span><br><span class="line">means = df_miss_no_income.agg( *[fn.mean(c).alias(c) <span class="keyword">for</span> c <span class="keyword">in</span> df_miss_no_income.columns <span class="keyword">if</span> c != <span class="string">&#x27;gender&#x27;</span>])\</span><br><span class="line">       .toPandas().to_dict(<span class="string">&#x27;records&#x27;</span>)[<span class="number">0</span>]</span><br><span class="line"><span class="comment"># 然后添加其它的列</span></span><br><span class="line">means[<span class="string">&#x27;gender&#x27;</span>] = <span class="string">&#x27;missing&#x27;</span></span><br><span class="line"></span><br><span class="line">df_miss_no_income.fillna(means).show()</span><br></pre></td></tr></table></figure></div><div class="tab-pane" id="f3-4"><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">3、异常值处理</span></span><br><span class="line"><span class="string">异常值：不属于正常的值 包含：缺失值，超过正常范围内的较大值或较小值</span></span><br><span class="line"><span class="string">分位数去极值</span></span><br><span class="line"><span class="string">中位数绝对偏差去极值</span></span><br><span class="line"><span class="string">正态分布去极值</span></span><br><span class="line"><span class="string">上述三种操作的核心都是：通过原始数据设定一个正常的范围，超过此范围的就是一个异常值</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">df_outliers = spark.createDataFrame([</span><br><span class="line">(<span class="number">1</span>, <span class="number">143.5</span>, <span class="number">5.3</span>, <span class="number">28</span>),</span><br><span class="line">(<span class="number">2</span>, <span class="number">154.2</span>, <span class="number">5.5</span>, <span class="number">45</span>),</span><br><span class="line">(<span class="number">3</span>, <span class="number">342.3</span>, <span class="number">5.1</span>, <span class="number">99</span>),</span><br><span class="line">(<span class="number">4</span>, <span class="number">144.5</span>, <span class="number">5.5</span>, <span class="number">33</span>),</span><br><span class="line">(<span class="number">5</span>, <span class="number">133.2</span>, <span class="number">5.4</span>, <span class="number">54</span>),</span><br><span class="line">(<span class="number">6</span>, <span class="number">124.1</span>, <span class="number">5.1</span>, <span class="number">21</span>),</span><br><span class="line">(<span class="number">7</span>, <span class="number">129.2</span>, <span class="number">5.3</span>, <span class="number">42</span>),</span><br><span class="line">], [<span class="string">&#x27;id&#x27;</span>, <span class="string">&#x27;weight&#x27;</span>, <span class="string">&#x27;height&#x27;</span>, <span class="string">&#x27;age&#x27;</span>])</span><br><span class="line"><span class="comment"># 设定范围 超出这个范围的 用边界值替换</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># approxQuantile方法接收三个参数：</span></span><br><span class="line"><span class="comment"># 列名；</span></span><br><span class="line"><span class="comment"># 想要计算的分位点，可以是一个点，也可以是一个列表（0和1之间的小数）</span></span><br><span class="line"><span class="comment"># 能容忍的误差，如果是0，代表百分百精确计算</span></span><br><span class="line"></span><br><span class="line">cols = [<span class="string">&#x27;weight&#x27;</span>, <span class="string">&#x27;height&#x27;</span>, <span class="string">&#x27;age&#x27;</span>]</span><br><span class="line"></span><br><span class="line">bounds = &#123;&#125;</span><br><span class="line"><span class="keyword">for</span> col <span class="keyword">in</span> cols:</span><br><span class="line">    quantiles = df_outliers.approxQuantile(col, [<span class="number">0.25</span>, <span class="number">0.75</span>], <span class="number">0.05</span>)</span><br><span class="line">    IQR = quantiles[<span class="number">1</span>] - quantiles[<span class="number">0</span>]</span><br><span class="line">    bounds[col] = [</span><br><span class="line">        quantiles[<span class="number">0</span>] - <span class="number">1.5</span> * IQR,</span><br><span class="line">        quantiles[<span class="number">1</span>] + <span class="number">1.5</span> * IQR</span><br><span class="line">        ]</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>bounds</span><br><span class="line">&#123;<span class="string">&#x27;age&#x27;</span>: [<span class="number">-11.0</span>, <span class="number">93.0</span>], <span class="string">&#x27;height&#x27;</span>: [<span class="number">4.499999999999999</span>, <span class="number">6.1000000000000005</span>], <span class="string">&#x27;weight&#x27;</span>: [<span class="number">91.69999999999999</span>, <span class="number">191.7</span>]&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 为异常值字段打标志</span></span><br><span class="line">outliers = df_outliers.select(*[<span class="string">&#x27;id&#x27;</span>] + [( (df_outliers[c] &lt; bounds[c][<span class="number">0</span>]) | </span><br><span class="line">                                        (df_outliers[c] &gt; bounds[c][<span class="number">1</span>]) ).alias(c + <span class="string">&#x27;_o&#x27;</span>) <span class="keyword">for</span> c <span class="keyword">in</span> cols ])</span><br><span class="line">outliers.show()</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">+---+--------+--------+-----+</span></span><br><span class="line"><span class="string">| id|weight_o|height_o|age_o|</span></span><br><span class="line"><span class="string">+---+--------+--------+-----+</span></span><br><span class="line"><span class="string">|  1|   false|   false|false|</span></span><br><span class="line"><span class="string">|  2|   false|   false|false|</span></span><br><span class="line"><span class="string">|  3|    true|   false| true|</span></span><br><span class="line"><span class="string">|  4|   false|   false|false|</span></span><br><span class="line"><span class="string">|  5|   false|   false|false|</span></span><br><span class="line"><span class="string">|  6|   false|   false|false|</span></span><br><span class="line"><span class="string">|  7|   false|   false|false|</span></span><br><span class="line"><span class="string">+---+--------+--------+-----+</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="comment"># 再回头看看这些异常值的值，重新和原始数据关联</span></span><br><span class="line">df_outliers = df_outliers.join(outliers, on=<span class="string">&#x27;id&#x27;</span>)</span><br><span class="line">df_outliers.<span class="built_in">filter</span>(<span class="string">&#x27;weight_o&#x27;</span>).select(<span class="string">&#x27;id&#x27;</span>, <span class="string">&#x27;weight&#x27;</span>).show()</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">+---+------+</span></span><br><span class="line"><span class="string">| id|weight|</span></span><br><span class="line"><span class="string">+---+------+</span></span><br><span class="line"><span class="string">|  3| 342.3|</span></span><br><span class="line"><span class="string">+---+------+</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">df_outliers.<span class="built_in">filter</span>(<span class="string">&#x27;age_o&#x27;</span>).select(<span class="string">&#x27;id&#x27;</span>, <span class="string">&#x27;age&#x27;</span>).show()</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">+---+---+</span></span><br><span class="line"><span class="string">| id|age|</span></span><br><span class="line"><span class="string">+---+---+</span></span><br><span class="line"><span class="string">|  3| 99|</span></span><br><span class="line"><span class="string">+---+---+</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure></div></div></div>
<h2 id="Spark-SQL-1"><a href="#Spark-SQL-1" class="headerlink" title="Spark SQL"></a>Spark SQL</h2><p>Spark SQL 是 Spark 处理结构化数据的一个模块，与基础的 Spark RDD API 不同，Spark SQL 提供了查询结构化数据及计算结果等信息的接口。在内部，Spark SQL 使用这个额外的信息去执行额外的优化。有几种方式可以跟 Spark SQL 进行交互，包括 SQL 和 Dataset API。当使用相同执行引擎进行计算时，无论使用哪种 API/语言 都可以快速的计算。</p>
<p>Spark SQL 的功能之一是执行 SQL 查询，Spark SQL 也能够被用于从已存在的 Hive 环境中读取数据。当以另外的编程语言运行SQL 时，查询结果将以 Dataset/DataFrame 的形式返回，也可以使用 命令行或者通过JDBC/ODBC 与 SQL 接口交互。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># df构建一个临时视图/表</span></span><br><span class="line">df.createOrReplaceTempView(<span class="string">&quot;people&quot;</span>) </span><br><span class="line"></span><br><span class="line"><span class="comment"># 可以用sparksession的sql查询，返回结果还是DataFrame格式</span></span><br><span class="line">sqlDF = spark.sql(<span class="string">&quot;SELECT * FROM people&quot;</span>)show显示</span><br><span class="line">sqlDF.show()</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">+----+-------+</span></span><br><span class="line"><span class="string">| age|   name|</span></span><br><span class="line"><span class="string">+----+-------+</span></span><br><span class="line"><span class="string">|null|Michael|</span></span><br><span class="line"><span class="string">|  30|   Andy|</span></span><br><span class="line"><span class="string">|  19| Justin|</span></span><br><span class="line"><span class="string">+----+-------+</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure></p>
<p><strong>DataFrame vs SQL</strong><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession</span><br><span class="line"></span><br><span class="line">spark = SparkSession \</span><br><span class="line">    .builder \</span><br><span class="line">    .appName(<span class="string">&quot;Python Spark SQL&quot;</span>) \</span><br><span class="line">    .config(<span class="string">&quot;spark.some.config.option&quot;</span>, <span class="string">&quot;some-value&quot;</span>) \</span><br><span class="line">    .getOrCreate()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 构建json对象</span></span><br><span class="line">stringJSONRDD = spark.sparkContext.parallelize((<span class="string">&quot;&quot;&quot; </span></span><br><span class="line"><span class="string">  &#123; &quot;id&quot;: &quot;123&quot;,</span></span><br><span class="line"><span class="string">    &quot;name&quot;: &quot;Katie&quot;,</span></span><br><span class="line"><span class="string">    &quot;age&quot;: 19,</span></span><br><span class="line"><span class="string">    &quot;eyeColor&quot;: &quot;brown&quot;</span></span><br><span class="line"><span class="string">  &#125;&quot;&quot;&quot;</span>,</span><br><span class="line">   <span class="string">&quot;&quot;&quot;&#123;</span></span><br><span class="line"><span class="string">    &quot;id&quot;: &quot;234&quot;,</span></span><br><span class="line"><span class="string">    &quot;name&quot;: &quot;Michael&quot;,</span></span><br><span class="line"><span class="string">    &quot;age&quot;: 22,</span></span><br><span class="line"><span class="string">    &quot;eyeColor&quot;: &quot;green&quot;</span></span><br><span class="line"><span class="string">  &#125;&quot;&quot;&quot;</span>, </span><br><span class="line">  <span class="string">&quot;&quot;&quot;&#123;</span></span><br><span class="line"><span class="string">    &quot;id&quot;: &quot;345&quot;,</span></span><br><span class="line"><span class="string">    &quot;name&quot;: &quot;Simone&quot;,</span></span><br><span class="line"><span class="string">    &quot;age&quot;: 23,</span></span><br><span class="line"><span class="string">    &quot;eyeColor&quot;: &quot;blue&quot;</span></span><br><span class="line"><span class="string">  &#125;&quot;&quot;&quot;</span>)</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 构建DataFrame</span></span><br><span class="line">swimmersJSON = spark.read.json(stringJSONRDD)</span><br><span class="line"><span class="comment"># 创建临时表</span></span><br><span class="line">swimmersJSON.createOrReplaceTempView(<span class="string">&quot;swimmersJSON&quot;</span>)</span><br><span class="line"><span class="comment"># DataFrame信息</span></span><br><span class="line">swimmersJSON.show()</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">+---+--------+---+-------+</span></span><br><span class="line"><span class="string">|age|eyeColor| id|   name|</span></span><br><span class="line"><span class="string">+---+--------+---+-------+</span></span><br><span class="line"><span class="string">| 19|   brown|123|  Katie|</span></span><br><span class="line"><span class="string">| 22|   green|234|Michael|</span></span><br><span class="line"><span class="string">| 23|    blue|345| Simone|</span></span><br><span class="line"><span class="string">+---+--------+---+-------+</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">spark.sql(<span class="string">&quot;select * from swimmersJSON&quot;</span>).show()</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">+---+--------+---+-------+</span></span><br><span class="line"><span class="string">|age|eyeColor| id|   name|</span></span><br><span class="line"><span class="string">+---+--------+---+-------+</span></span><br><span class="line"><span class="string">| 19|   brown|123|  Katie|</span></span><br><span class="line"><span class="string">| 22|   green|234|Michael|</span></span><br><span class="line"><span class="string">| 23|    blue|345| Simone|</span></span><br><span class="line"><span class="string">+---+--------+---+-------+</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="comment"># 执行SQL请求</span></span><br><span class="line">spark.sql(<span class="string">&quot;select * from swimmersJSON&quot;</span>).collect()</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string"># 执行SQL请求</span></span><br><span class="line"><span class="string">spark.sql(&quot;select * from swimmersJSON&quot;).collect()</span></span><br><span class="line"><span class="string"># 执行SQL请求</span></span><br><span class="line"><span class="string">spark.sql(&quot;select * from swimmersJSON&quot;).collect()</span></span><br><span class="line"><span class="string">[Row(age=19, eyeColor=&#x27;brown&#x27;, id=&#x27;123&#x27;, name=&#x27;Katie&#x27;),</span></span><br><span class="line"><span class="string"> Row(age=22, eyeColor=&#x27;green&#x27;, id=&#x27;234&#x27;, name=&#x27;Michael&#x27;),</span></span><br><span class="line"><span class="string"> Row(age=23, eyeColor=&#x27;blue&#x27;, id=&#x27;345&#x27;, name=&#x27;Simone&#x27;)]</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="comment"># 输出数据表的格式</span></span><br><span class="line">swimmersJSON.printSchema()</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">root</span></span><br><span class="line"><span class="string"> |-- age: long (nullable = true)</span></span><br><span class="line"><span class="string"> |-- eyeColor: string (nullable = true)</span></span><br><span class="line"><span class="string"> |-- id: string (nullable = true)</span></span><br><span class="line"><span class="string"> |-- name: string (nullable = true)</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="comment"># 执行SQL</span></span><br><span class="line">spark.sql(<span class="string">&quot;select count(1) from swimmersJSON&quot;</span>)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">DataFrame[count(1): bigint]</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">spark.sql(<span class="string">&quot;select count(1) from swimmersJSON&quot;</span>).show()</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">+--------+</span></span><br><span class="line"><span class="string">|count(1)|</span></span><br><span class="line"><span class="string">+--------+</span></span><br><span class="line"><span class="string">|       3|</span></span><br><span class="line"><span class="string">+--------+</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="comment"># DataFrame的写法</span></span><br><span class="line">swimmersJSON.select(<span class="string">&quot;id&quot;</span>, <span class="string">&quot;age&quot;</span>).<span class="built_in">filter</span>(<span class="string">&quot;age = 22&quot;</span>).show()</span><br><span class="line"><span class="comment"># SQL的写法</span></span><br><span class="line">spark.sql(<span class="string">&quot;select id, age from swimmersJSON where age = 22&quot;</span>).show()</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">+---+---+</span></span><br><span class="line"><span class="string">| id|age|</span></span><br><span class="line"><span class="string">+---+---+</span></span><br><span class="line"><span class="string">|234| 22|</span></span><br><span class="line"><span class="string">+---+---+</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">+---+---+</span></span><br><span class="line"><span class="string">| id|age|</span></span><br><span class="line"><span class="string">+---+---+</span></span><br><span class="line"><span class="string">|234| 22|</span></span><br><span class="line"><span class="string">+---+---+</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="comment"># DataFrame的写法</span></span><br><span class="line">swimmersJSON.select(<span class="string">&quot;name&quot;</span>, <span class="string">&quot;eyeColor&quot;</span>).<span class="built_in">filter</span>(<span class="string">&quot;eyeColor like &#x27;b%&#x27;&quot;</span>).show()</span><br><span class="line"><span class="comment"># SQL的写法</span></span><br><span class="line">spark.sql(<span class="string">&quot;select name, eyeColor from swimmersJSON where eyeColor like &#x27;b%&#x27;&quot;</span>).show()</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">+------+--------+</span></span><br><span class="line"><span class="string">|  name|eyeColor|</span></span><br><span class="line"><span class="string">+------+--------+</span></span><br><span class="line"><span class="string">| Katie|   brown|</span></span><br><span class="line"><span class="string">|Simone|    blue|</span></span><br><span class="line"><span class="string">+------+--------+</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">+------+--------+</span></span><br><span class="line"><span class="string">|  name|eyeColor|</span></span><br><span class="line"><span class="string">+------+--------+</span></span><br><span class="line"><span class="string">| Katie|   brown|</span></span><br><span class="line"><span class="string">|Simone|    blue|</span></span><br><span class="line"><span class="string">+------+--------+</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure></p>
<h1 id="Spark-Streaming"><a href="#Spark-Streaming" class="headerlink" title="Spark-Streaming"></a>Spark-Streaming</h1><p><span class="exturl" data-url="aHR0cDovL3NwYXJrLmFwYWNoZS5vcmcvZG9jcy8yLjQuMC9zdHJlYW1pbmctcHJvZ3JhbW1pbmctZ3VpZGUuaHRtbA==">官方指导手册<i class="fa fa-external-link-alt"></i></span></p>
<p>Spark-Core和Spark-SQL都是处理属于离线批处理任务，数据一般都是在固定位置上，通常我们写好一个脚本，每天定时去处理数据，计算，保存数据结果。这类任务通常是T+1(一天一个任务)，对实时性要求不高。</p>
<p>但在企业中存在很多实时性处理的需求，例如：双十一的京东阿里，通常会做一个实时的数据大屏，显示实时订单。这种情况下，对数据实时性要求较高，仅仅能够容忍到延迟1分钟或几秒钟。</p>
<p><strong>实时计算框架对比</strong><br>Storm<br>（1）流式计算框架<br>（2）以record为单位处理数据<br>（3）也支持micro-batch方式（Trident）</p>
<p>Spark<br>（1）批处理计算框架<br>（2）以RDD为单位处理数据<br>（3）支持micro-batch流式处理数据（Spark Streaming）</p>
<p>对比：<br>（1）吞吐量：Spark Streaming优于Storm，能处理的数据量更大。<br>（2）延迟：Spark Streaming差于Storm，Spark Streaming每隔一段时间处理一批数据，而Storm是来一条处理一条。</p>
<h2 id="组件"><a href="#组件" class="headerlink" title="组件"></a>组件</h2><p>Streaming Context：</p>
<ul>
<li>流上下文 通过Streaming Context 可以连接数据源获取数据</li>
<li>创建Streaming Context 需要指定一个时间间隔（micro batch）</li>
<li>对DStream处理的逻辑要写在Streaming Context开启（start()方法）之前， 一旦开启了，就不能再添加新的数据处理逻辑</li>
<li>一旦一个Streaming Context已经停止（stop()方法）,不能重新启动（不能再次调start()）</li>
<li>在JVM（java虚拟机）中, 同一时间只能有一个Streaming Context处于活跃状态, 一个SparkContext只能创建一个Streaming Context</li>
<li>Streaming Context停止（Stop()方法）, 也会关闭SparkContext对象, 如果只想仅关闭Streaming Context对象,设置stop()的可选参数为false</li>
<li>一个SparkContext对象可以重复利用去创建多个Streaming Context对象（不关闭SparkContext前提下）, 但是需要关一个再开下一个</li>
</ul>
<p>DStream (离散流)：</p>
<ul>
<li>Streaming Context 连接到不同的数据源获取到的数据 抽象成DStream模型</li>
<li>代表一个连续的数据流，由一系列连续的RDD组成，每个RDD都包含确定时间间隔内的数据</li>
<li>任何对DStreams的操作都转换成了对DStreams隐含的RDD的操作</li>
<li>数据源<ul>
<li>基本源<ul>
<li>TCP/IP Socket</li>
<li>FileSystem</li>
</ul>
</li>
<li>高级源<ul>
<li>Kafka</li>
<li>Flume</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="编码实践"><a href="#编码实践" class="headerlink" title="编码实践"></a>编码实践</h2><p><strong>Spark Streaming编码步骤：</strong><br>1，创建一个StreamingContext<br>2，从StreamingContext中创建一个数据对象<br>3，对数据对象进行Transformations操作<br>4，输出结果<br>5，开始和停止</p>
<p><strong>利用Spark Streaming实现WordCount</strong><br>需求：监听某个端口上的网络数据，实时统计出现的不同单词个数。<br>1，需要安装一个nc工具：<code>sudo yum install -y nc</code><br>2，执行指令：<code>nc -lk 9999 -v</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="comment"># 配置spark driver和pyspark运行时，所使用的python解释器路径</span></span><br><span class="line">PYSPARK_PYTHON = <span class="string">&quot;/home/hadoop/miniconda3/envs/datapy365spark23/bin/python&quot;</span></span><br><span class="line">JAVA_HOME=<span class="string">&#x27;/home/hadoop/app/jdk1.8.0_191&#x27;</span></span><br><span class="line">SPARK_HOME = <span class="string">&quot;/home/hadoop/app/spark-2.3.0-bin-2.6.0-cdh5.7.0&quot;</span></span><br><span class="line"><span class="comment"># 当存在多个版本时，不指定很可能会导致出错</span></span><br><span class="line">os.environ[<span class="string">&quot;PYSPARK_PYTHON&quot;</span>] = PYSPARK_PYTHON</span><br><span class="line">os.environ[<span class="string">&quot;PYSPARK_DRIVER_PYTHON&quot;</span>] = PYSPARK_PYTHON</span><br><span class="line">os.environ[<span class="string">&#x27;JAVA_HOME&#x27;</span>]=JAVA_HOME</span><br><span class="line">os.environ[<span class="string">&quot;SPARK_HOME&quot;</span>] = SPARK_HOME</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> pyspark <span class="keyword">import</span> SparkContext</span><br><span class="line"><span class="keyword">from</span> pyspark.streaming <span class="keyword">import</span> StreamingContext</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    </span><br><span class="line">    sc = SparkContext(<span class="string">&quot;local[2]&quot;</span>,appName=<span class="string">&quot;NetworkWordCount&quot;</span>)</span><br><span class="line">    <span class="comment">#参数2：指定执行计算的时间间隔</span></span><br><span class="line">    ssc = StreamingContext(sc, <span class="number">1</span>)</span><br><span class="line">    <span class="comment">#监听ip，端口上的上的数据</span></span><br><span class="line">    lines = ssc.socketTextStream(<span class="string">&#x27;localhost&#x27;</span>,<span class="number">9999</span>)</span><br><span class="line">    <span class="comment">#将数据按空格进行拆分为多个单词</span></span><br><span class="line">    words = lines.flatMap(<span class="keyword">lambda</span> line: line.split(<span class="string">&quot; &quot;</span>))</span><br><span class="line">    <span class="comment">#将单词转换为(单词，1)的形式</span></span><br><span class="line">    pairs = words.<span class="built_in">map</span>(<span class="keyword">lambda</span> word:(word,<span class="number">1</span>))</span><br><span class="line">    <span class="comment">#统计单词个数</span></span><br><span class="line">    wordCounts = pairs.reduceByKey(<span class="keyword">lambda</span> x,y:x+y)</span><br><span class="line">    <span class="comment">#打印结果信息，会使得前面的transformation操作执行</span></span><br><span class="line">    wordCounts.pprint()</span><br><span class="line">    <span class="comment">#启动StreamingContext</span></span><br><span class="line">    ssc.start()</span><br><span class="line">    <span class="comment">#等待计算结束</span></span><br><span class="line">    ssc.awaitTermination()</span><br></pre></td></tr></table></figure>
<p>可视化查看效果：<span class="exturl" data-url="aHR0cDovLzE5Mi4xNjguMTk5LjE4ODo0MDQwLueCueWHu3N0cmVhbWluZ++8jOafpeeci+aViOaenOOAgg==">http://192.168.199.188:4040.点击streaming，查看效果。<i class="fa fa-external-link-alt"></i></span></p>
<h2 id="状态操作"><a href="#状态操作" class="headerlink" title="状态操作"></a>状态操作</h2><p>在Spark Streaming中存在两种状态操作：<br>1, UpdateStateByKey<br>2, Windows操作</p>
<p>使用有状态的transformation，需要开启Checkpoint：<br>1, Spark Streaming 的容错机制<br>2, 它将足够多的信息checkpoint到某些具备容错性的存储系统如hdfs上，以便出错时能够迅速恢复</p>
<p><strong>updateStateByKey</strong><br>Spark Streaming实现的是一个实时批处理操作，每隔一段时间将数据进行打包，封装成RDD，是无状态的（指的是每个时间片段的数据之间是没有关联的）。</p>
<p>需求：想要将一个大时间段（1天），即多个小时间段的数据内的数据持续进行累积操作（一般超过一天都是用RDD或Spark SQL来进行离线批处理）。</p>
<p>Spark Streaming中提供状态保护机制UpdateStateByKey，如果没有它，我们需要将每一秒的数据计算好放入mysql中取，再用mysql来进行统计计算。</p>
<p>步骤：<br>1, 要定义一个state，可以是任意的数据类型<br>2, 要定义state更新函数，指定一个函数如何使用之前的state和新值来更新state<br>3, 对于每个batch，Spark都会为每个之前已经存在的key去应用一次state更新函数，无论这个key在batch中是否有新的数据。如果state更新函数返回none，那么key对应的state就会被删除<br>4, 对于每个新出现的key，也会执行state更新函数</p>
<p>举例：词统计。</p>
<p>需求：监听网络端口的数据，获取到每个批次的出现的单词数量，并且需要把每个批次的信息保留下来<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="comment"># 配置spark driver和pyspark运行时，所使用的python解释器路径</span></span><br><span class="line">PYSPARK_PYTHON = <span class="string">&quot;/home/hadoop/miniconda3/envs/datapy365spark23/bin/python&quot;</span></span><br><span class="line">JAVA_HOME=<span class="string">&#x27;/home/hadoop/app/jdk1.8.0_191&#x27;</span></span><br><span class="line">SPARK_HOME = <span class="string">&quot;/home/hadoop/app/spark-2.3.0-bin-2.6.0-cdh5.7.0&quot;</span></span><br><span class="line"><span class="comment"># 当存在多个版本时，不指定很可能会导致出错</span></span><br><span class="line">os.environ[<span class="string">&quot;PYSPARK_PYTHON&quot;</span>] = PYSPARK_PYTHON</span><br><span class="line">os.environ[<span class="string">&quot;PYSPARK_DRIVER_PYTHON&quot;</span>] = PYSPARK_PYTHON</span><br><span class="line">os.environ[<span class="string">&#x27;JAVA_HOME&#x27;</span>]=JAVA_HOME</span><br><span class="line">os.environ[<span class="string">&quot;SPARK_HOME&quot;</span>] = SPARK_HOME</span><br><span class="line"><span class="keyword">from</span> pyspark.streaming <span class="keyword">import</span> StreamingContext</span><br><span class="line"><span class="keyword">from</span> pyspark.sql.session <span class="keyword">import</span> SparkSession</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建SparkContext</span></span><br><span class="line">spark = SparkSession.builder.master(<span class="string">&quot;local[2]&quot;</span>).getOrCreate()</span><br><span class="line">sc = spark.sparkContext</span><br><span class="line"></span><br><span class="line">ssc = StreamingContext(sc, <span class="number">3</span>)</span><br><span class="line"><span class="comment">#开启检查点</span></span><br><span class="line">ssc.checkpoint(<span class="string">&quot;checkpoint&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#定义state更新函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">updateFunc</span>(<span class="params">new_values, last_sum</span>):</span></span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">sum</span>(new_values) + (last_sum <span class="keyword">or</span> <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">lines = ssc.socketTextStream(<span class="string">&quot;localhost&quot;</span>, <span class="number">9999</span>)</span><br><span class="line"><span class="comment"># 对数据以空格进行拆分，分为多个单词</span></span><br><span class="line">counts = lines.flatMap(<span class="keyword">lambda</span> line: line.split(<span class="string">&quot; &quot;</span>)) \</span><br><span class="line">    .<span class="built_in">map</span>(<span class="keyword">lambda</span> word: (word, <span class="number">1</span>)) \</span><br><span class="line">    .updateStateByKey(updateFunc=updateFunc)<span class="comment">#应用updateStateByKey函数</span></span><br><span class="line">    </span><br><span class="line">counts.pprint()</span><br><span class="line"></span><br><span class="line">ssc.start()</span><br><span class="line">ssc.awaitTermination()</span><br></pre></td></tr></table></figure></p>
<h2 id="Windows"><a href="#Windows" class="headerlink" title="Windows"></a>Windows</h2><p>参考Spark Streaming手册的Window Operations小节。</p>
<p>窗口长度L：运算的数据量<br>滑动间隔G：控制每隔多长时间做一次运算</p>
<p>每隔G秒，统计最近L秒的数据</p>
<p><strong>操作细节</strong><br>1, Window操作是基于窗口长度和滑动间隔来工作的<br>2, 窗口的长度控制考虑前几批次数据量<br>3, 默认为批处理的滑动间隔来确定计算结果的频率</p>
<p><strong>相关函数</strong><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">window(windowLength, slideInterval)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">对每个滑动窗口数据执行自定义的计算</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">countByWindow(windowLength, slideInterval)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">对每个滑动窗口的数据执行count</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">reduceByWindow(func, windowLength, slideInterval)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">对每个滑动窗口的数据执行reduce操作</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">reduceByKeyAndWindow(func, windowLength, slideInterval, [numTasks])</span><br><span class="line">reduceByKeyAndWindow(func,invFunc,windowLength,slideInterval,[num,Tasks])</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">对每个滑动窗口的数据执行reduceByKey操作</span></span><br><span class="line"><span class="string">func:正向操作，类似于updateStateByKey</span></span><br><span class="line"><span class="string">invFunc：反向操作</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">countByValueAndWindow(windowLength, slideInterval, [numTasks])</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">对每个滑动窗口的数据执行countByValue操作</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><br>例如在热词时，在上一个窗口中可能是热词，这个一个窗口中可能不是热词，就需要在这个窗口中把该次剔除掉。</p>
<p>典型案例：热点搜索词滑动统计，每隔10秒，统计最近60秒钟的搜索词的搜索频次，并打印出最靠前的3个搜索词出现次数。</p>
<p>监听网络端口的数据，每隔3秒统计前6秒出现的单词数量：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="comment"># 配置spark driver和pyspark运行时，所使用的python解释器路径</span></span><br><span class="line">PYSPARK_PYTHON = <span class="string">&quot;/home/hadoop/miniconda3/envs/datapy365spark23/bin/python&quot;</span></span><br><span class="line">JAVA_HOME=<span class="string">&#x27;/home/hadoop/app/jdk1.8.0_191&#x27;</span></span><br><span class="line">SPARK_HOME = <span class="string">&quot;/home/hadoop/app/spark-2.3.0-bin-2.6.0-cdh5.7.0&quot;</span></span><br><span class="line"><span class="comment"># 当存在多个版本时，不指定很可能会导致出错</span></span><br><span class="line">os.environ[<span class="string">&quot;PYSPARK_PYTHON&quot;</span>] = PYSPARK_PYTHON</span><br><span class="line">os.environ[<span class="string">&quot;PYSPARK_DRIVER_PYTHON&quot;</span>] = PYSPARK_PYTHON</span><br><span class="line">os.environ[<span class="string">&#x27;JAVA_HOME&#x27;</span>]=JAVA_HOME</span><br><span class="line">os.environ[<span class="string">&quot;SPARK_HOME&quot;</span>] = SPARK_HOME</span><br><span class="line"><span class="keyword">from</span> pyspark <span class="keyword">import</span> SparkContext</span><br><span class="line"><span class="keyword">from</span> pyspark.streaming <span class="keyword">import</span> StreamingContext</span><br><span class="line"><span class="keyword">from</span> pyspark.sql.session <span class="keyword">import</span> SparkSession</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_countryname</span>(<span class="params">line</span>):</span></span><br><span class="line">    country_name = line.strip()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> country_name == <span class="string">&#x27;usa&#x27;</span>:</span><br><span class="line">        output = <span class="string">&#x27;USA&#x27;</span></span><br><span class="line">    <span class="keyword">elif</span> country_name == <span class="string">&#x27;ind&#x27;</span>:</span><br><span class="line">        output = <span class="string">&#x27;India&#x27;</span></span><br><span class="line">    <span class="keyword">elif</span> country_name == <span class="string">&#x27;aus&#x27;</span>:</span><br><span class="line">        output = <span class="string">&#x27;Australia&#x27;</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        output = <span class="string">&#x27;Unknown&#x27;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> (output, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">	<span class="comment">#定义处理的时间间隔</span></span><br><span class="line">    batch_interval = <span class="number">1</span> <span class="comment"># base time unit (in seconds)</span></span><br><span class="line">    <span class="comment">#定义窗口长度</span></span><br><span class="line">    window_length = <span class="number">6</span> * batch_interval</span><br><span class="line">    <span class="comment">#定义滑动时间间隔</span></span><br><span class="line">    frequency = <span class="number">3</span> * batch_interval</span><br><span class="line"></span><br><span class="line">    <span class="comment">#获取StreamingContext</span></span><br><span class="line">    spark = SparkSession.builder.master(<span class="string">&quot;local[2]&quot;</span>).getOrCreate()</span><br><span class="line">	sc = spark.sparkContext</span><br><span class="line">	ssc = StreamingContext(sc, batch_interval)</span><br><span class="line">    </span><br><span class="line">    <span class="comment">#需要设置检查点</span></span><br><span class="line">    ssc.checkpoint(<span class="string">&quot;checkpoint&quot;</span>)</span><br><span class="line"></span><br><span class="line">    lines = ssc.socketTextStream(<span class="string">&#x27;localhost&#x27;</span>, <span class="number">9999</span>)</span><br><span class="line">    addFunc = <span class="keyword">lambda</span> x, y: x + y</span><br><span class="line">    invAddFunc = <span class="keyword">lambda</span> x, y: x - y</span><br><span class="line">    <span class="comment">#调用reduceByKeyAndWindow，来进行窗口函数的调用</span></span><br><span class="line">    window_counts = lines.<span class="built_in">map</span>(get_countryname) \</span><br><span class="line">        .reduceByKeyAndWindow(addFunc, invAddFunc, window_length, frequency)</span><br><span class="line">	<span class="comment">#输出处理结果信息</span></span><br><span class="line">    window_counts.pprint()</span><br><span class="line"></span><br><span class="line">    ssc.start()</span><br><span class="line">    ssc.awaitTermination()</span><br></pre></td></tr></table></figure></p>
<h1 id="累加器"><a href="#累加器" class="headerlink" title="累加器"></a>累加器</h1><p>在集群中执行代码时，一个难点是：理解变量和方法的范围、生命周期。下面是一个闭包的例子：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">counter = <span class="number">0</span></span><br><span class="line">rdd = sc.parallelize(data)</span><br><span class="line">​</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">increment_counter</span>(<span class="params">x</span>):</span></span><br><span class="line">    <span class="keyword">global</span> counter</span><br><span class="line">    counter += x</span><br><span class="line"></span><br><span class="line">rdd.foreach(increment_counter)</span><br><span class="line">print(<span class="string">&quot;Counter value: &quot;</span>, counter)</span><br></pre></td></tr></table></figure><br>述代码的行为是不确定的，并且无法按照预期正常工作。</p>
<p>在执行作业时，Spark会分解RDD操作到每个executor 的task中。在执行之前，Spark计算任务的闭包。<br>（1）所谓闭包：指的是executor要在RDD上进行计算时，必须对执行节点可见的那些变量和方法<br>（2）闭包被序列化，并被发送到每个executor。</p>
<p>在上述代码中，闭包的变量的副本被发送给每个executor，当counter被foreach函数引用时，它已经不再是驱动器节点的counter了。<br>（1）虽然驱动器程序中，仍然有一个counter在内存中；但是对于executors，它是不可见的。<br>（2）executor看到的只是序列化的闭包的一个副本。所有对counter的操作都是在executor的本地进行。<br>（3）要想正确实现预期目标，则需要使用累加器。</p>
<p><strong>Accumulator</strong><br>一个累加器（Accumulator）变量只支持累加操作。</p>
<p>工作节点和驱动器程序对它都可以执行<code>+=</code>操作，但是只有驱动器程序可以访问它的值。在工作节点上，累加器对象看起来就像是一个只写的变量。<br>工作节点对它执行的任何累加，都将自动的传播到驱动器程序中。</p>
<p>SparkContext的累加器变量只支持基本的数据类型，如int、float等。可以通过AccumulatorParam来实现自定义的累加器。</p>
<p>Accumulator的方法：<code>.add(term)</code>：向累加器中增加值term。</p>
<p>Accumulator的属性：<code>.value</code>：获取累加器的值。只可以在驱动器程序中使用。</p>
<p>通常使用累加器的流程为：<br>（1）在驱动器程序中调用<code>SparkContext.accumulator(init_value)</code>来创建出带有初始值的累加器；<br>（2）在执行器的代码中使用累加器的<code>+=</code>方法或者<code>.add(term)</code>方法来增加累加器的值；<br>（3）在驱动器程序中使用累加器的<code>.value</code>属性来访问累加器的值。</p>
<p>累加器示例：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">counter = <span class="number">0</span></span><br><span class="line">​sc.accumulator(counter)</span><br><span class="line">rdd = sc.parallelize(data)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">increment_counter</span>(<span class="params">x</span>):</span></span><br><span class="line">    <span class="keyword">global</span> counter</span><br><span class="line">    counter += x</span><br><span class="line"></span><br><span class="line">rdd.foreach(increment_counter)</span><br><span class="line">print(<span class="string">&quot;Counter value: &quot;</span>, counter)</span><br></pre></td></tr></table></figure></p>
<p>累加器与容错性：<br>Spark中同一个任务可能被运行多次：<br>（1）如果工作节点失败了，则Spark会在另一个节点上重新运行该任务。<br>（2）如果工作节点处理速度比别的节点慢很多，则Spark也会抢占式的在另一个节点上启动一个投机性的任务副本，甚至有时候Spark需要重新运行任务来获取缓存中被移出内存的数据。</p>
<p>当Spark同一个任务被运行多次时，任务中的累加器的处理规则：<br>（1）在行动操作中使用的累加器，Spark确保每个任务对各累加器修改应用一次。因此，如果想要一个无论在失败还是重新计算时，都绝对可靠的累加器，我们必须将它放在<code>foreach()</code>这样的行动操作中。<br>（2）在转化操作中使用的累加器，无法保证只修改应用一次。转化操作中累加器可能发生不止一次更新。在转化操作中，累加器通常只用于调试目的。</p>
<h1 id="广播变量"><a href="#广播变量" class="headerlink" title="广播变量"></a>广播变量</h1><p>广播变量可以让程序高效的向所有工作节点发送一个较大的只读值。</p>
<p>Spark会自动的把闭包中所有引用到的变量都发送到工作节点上。虽然这很方便，但是也很低效：<br>（1）默认的任务发射机制是专门为小任务进行优化的。<br>（2）事实上，你很可能在多个并行操作中使用同一个变量。但是Spark会为每个操作分别发送。</p>
<p><strong>Broadcast</strong><br>Broadcast变量的value中存放着广播的值，该值只会被发送到各节点一次。</p>
<p>Broadcast的方法：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">.destroy()</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">销毁当前Broadcast变量的所有数据和所有metadata。</span></span><br><span class="line"><span class="string">注意：一旦一个Boradcast变量被销毁，那么它就再也不能被使用。该方法将阻塞直到销毁完成。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">.dump(value,f)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">保存Broadcast变量。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">.load(path)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">加载Broadcast变量。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">.unpersist(blocking=<span class="literal">False</span>)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">删除Broadcast变量在executor上的缓存备份。</span></span><br><span class="line"><span class="string">如果在此之后，该Broadcast被使用，则需要从驱动器程序重新发送Broadcast变量到executor。</span></span><br><span class="line"><span class="string">blocking：如果为True，则阻塞直到unpersist完成。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure></p>
<p>Broadcast的属性：<code>.value</code>：返回Broadcast变量的值。</p>
<p>使用Broadcast 的流程：<br>（1）通过<code>SparkCotext.broadcast(xx)</code>创建一个Broadcast变量。<br>（2）通过<code>.value</code>属性访问该对象的值。<br>（3）该变量只会被发送到各节点一次，应该作为只读值来处理（修改这个值并不会影响到其他的节点）。</p>
<p>示例参考<strong>RDD创建</strong>中的讲解例子。</p>
<p><strong>广播的优化</strong><br>当广播一个较大的值时，选择既快又好的序列化格式非常重要：如果序列化对象的时间较长，或者传送花费的时间太久，则这个时间很容易成为性能瓶颈。</p>
<p>Spark中的Java API和Scala API默认使用的序列化库为Java序列化库，它对于除了基本类型的数组以外的任何对象都比较低效。可以使用spark.serializer属性来选择另一个序列化库来优化序列化过程。</p>
<h1 id="简答题"><a href="#简答题" class="headerlink" title="简答题"></a>简答题</h1><p><strong>1、简述Hadoop的优点有哪些？Spark与之相比又有哪些优点？</strong><br><strong>hadoop具有如下优点</strong> ：  </p>
<ul>
<li>hadoop是一个适合大数据的分布式存储和计算的平台。</li>
<li>低成本:hadoop本身是运行在普通PC服务器组成的集群中进行大数据的分发及处理工作的，这些服务器集群是可以支持数千个节点的。   </li>
<li>高效性:这也是hadoop的核心竞争优势所在，接受到客户的数据请求后，hadoop可以在数据所在的集群节点上并发处理。   </li>
<li>可靠性:通过分布式存储，hadoop可以自动存储多份副本，当数据处理请求失败后，会自动重新部署计算任务。   </li>
<li>扩展性:hadoop的分布式存储和分布式计算是在集群节点完成的，这也决定了hadoop可以扩展至更多的集群节点。   </li>
<li>容错性:hadoop能够自动保存数据的多个副本，并且能够自动将失败的任务重新分配   </li>
</ul>
<p><strong>Hadoop的局限性和不足</strong>：</p>
<ul>
<li>抽象层次低，需要手动编写mapper和reducer逻辑，使用上复杂</li>
<li>只提供Map和reduce两个操作，表达力欠缺</li>
<li>处理逻辑隐藏在代码细节中，没有整理逻辑</li>
<li>中间结果也存放在HDFS上，IO和通信开销大</li>
<li>时延高，只适用batch数据处理，对于交互式数据处理和实时数据处理的支持不够</li>
<li>对于迭代式数据处理性能比较差</li>
</ul>
<p><strong>Spark相比hadoop的优点有</strong>：</p>
<ul>
<li>丰富的API，而后者只有map和reduce。</li>
<li>中间结果不存磁盘，而后者需要把中间结果写磁盘。</li>
<li>线程池模型减少task启动开销，而后者任务调度和启动开销很大。</li>
<li>能充分利用内存速度快优势，减少IO操作，而后者不能。</li>
<li>可以避免排序操作，而hadoop中map和reduce都需要排序。</li>
<li>适合迭代计算（机器学习算法），而后者不适合。</li>
</ul>
<p><strong>2、简述对MAP-REDUCE这一编程模型的理解？</strong><br>Hadoop包括分布式文件系统hdfs,和分布式计算框架，MapReduce是用于数据处理的一种编程模型,是hadoop的核心组件之一，可以通过mapreduce很容易在hadoop平台上进行分布式的计算编程。 </p>
<p>MapRedeuce其处理过程主要分为两个步骤：<br>（1）<strong>映射</strong>(Mapping)函数以Key/Value数据对作为输入，将输入数据经过业务逻辑计算产生若干仍旧以Key/Value形式表达的中间数。 MapReduce计算框架会自动将中间结果中具有相同Key值的记录聚合在一起，并将数据传送给Reduce函数内定义好的处理逻辑作为其输入值。<br>（2）<strong>聚合</strong>(Reducing)函数接收到Map阶段传过来的某个Key值及其对应的若干Value值等中间数据，函数逻辑对这个Key对应的Value内容进行处理，一般是对其进行累加、过滤、转换等操作，生成Key/Value形式的结果，这就是最终的业务计算结果。</p>
<p><strong>3、简述RDD的含义，并写出针对RDD的两类操作（transformation与action)，每类下至少三种的操作</strong><br>RDD(Resilient Distributed Datasets),弹性分布式数据集是一个容错的、可以被并行操作的元素集合弹性分布数据集。是Spark的核心，也是整个Spark的架构基础。Spark是以RDD概念为中心运行的。</p>
<p><strong>RDD的分布式存储，其最大的好处是可以让数据在不同工作节点并行存储，以便在需要数据时并行运算。</strong><br><strong>RDD的弹性指其在节点存储时，既可以使用内存，也可已使用外存，为使用者进行大数据处理提供方便</strong></p>
<p>它的特性可以总结如下：</p>
<ul>
<li>它是不变的数据结构存储</li>
<li>只读特性，维护DAG以便通过重新计算获得容错性</li>
<li>它是支持跨集群的分布式数据结构</li>
<li>可以根据数据记录的key对结构进行分区</li>
<li>提供了粗粒度的操作，且这些操作都支持分区</li>
<li>它将数据存储在内存中，从而提供了低延迟性</li>
</ul>
<p>常用的transformation操作：  </p>
<ul>
<li><strong><code>map(func)</code> 对调用map的RDD数据集中的每个element都使用func，然后返回一个新的RDD,这个返回的数据集是分布式的数据集</strong></li>
<li><strong><code>filter(func)</code> 对调用filter的RDD数据集中的每个元素都使用func，然后返回一个包含使func为true的元素构成的RDD</strong></li>
<li><strong><code>flatMap(func)</code> 和map差不多，但是flatMap生成的是多个结果,返回值是一个Seq(一个List)</strong></li>
<li><strong><code>sample(withReplacement, fraction, seed)</code> 从RDD中的item中按fraction百分比采样，有放回或者无放回</strong></li>
<li><strong><code>takeSample(withReplacement, num, seed=None)</code> 从RDD中的item中采样num个，有放回或者无放回</strong></li>
<li><strong><code>union(otherDataset)</code> 返回一个新的dataset，包含源dataset和给定dataset的元素的集合</strong>  </li>
<li><strong><code>distinct([numTasks]))</code> 对RDD中的item去重</strong></li>
<li><strong><code>groupByKey([numTasks])</code>: 返回(K,Seq[V])，也就是hadoop中reduce函数接受的key-valuelist</strong></li>
<li><strong><code>reduceByKey(func, [numTasks])</code>: 就是用一个给定的reduce func再作用在groupByKey产生的(K,Seq[V]),比如求和，求平均数</strong></li>
<li><strong><code>sortByKey([ascending], [numTasks])</code> 按照key来进行排序（lambda x中x是第一个元素），ascending是升序还是降序</strong>  </li>
<li><strong><code>sortBy(keyfunc, ascending=True, numPartitions=None)</code>: 按照keyfunc排序</strong></li>
<li><strong><code>join(otherDataset, [numTasks])</code> 当有两个KV的dataset(K,V)和(K,W)，返回的是(K,(V,W))的dataset,numTasks为并发的任务数</strong> </li>
<li><strong><code>cartesian(otherDataset)</code> 笛卡尔积就是m*n</strong>  </li>
<li><strong><code>intersection(otherDataset)</code>: 交集</strong></li>
<li><strong><code>substract(otherDataset)</code>: 差集</strong></li>
</ul>
<p>常用的action操作：</p>
<ul>
<li><strong><code>reduce(func)</code>: 对RDD中的items做聚合</strong>  </li>
<li><strong><code>collect()</code>: 计算所有的items并返回所有的结果到driver端，接着 <code>collect()</code>会以Python list的形式返回结果</strong>  </li>
<li><strong><code>count()</code>: 返回的是dataset中的element的个数</strong>  </li>
<li><strong><code>first()</code>: 和上面是类似的，不过只返回第1个item</strong>  </li>
<li><strong><code>take(n)</code>: 类似，但是返回n个item</strong>  </li>
<li><strong><code>top(n)</code>: 返回头n个items，按照自然结果排序</strong>  </li>
<li><strong><code>countByKey()</code>:  返回的是key对应的个数的一个map，作用于一个RDD</strong></li>
<li><strong><code>foreach()</code>: 对dataset中的每个元素都使用func</strong></li>
<li><strong><code>takeSample()</code>: 指定采样个数，返回相应的数目</strong></li>
<li><strong><code>saveAsTextFile(path)</code>: 把dataset写到一个text file中，或者hdfs，或者hdfs支持的文件系统中，spark把每条记录都转换为一行记录，然后写到file中</strong></li>
</ul>
<p><strong>4、Spark内置了机器学习库mllib，请写出使用该库完成一项机器学习任务的通用步骤</strong></p>
<ul>
<li>1、数据探索<blockquote>
<p>拿到数据以后，一般不会急于创建模型、训练模型，在这之前，需要对数据、对需求或机器学习的目标进行分析，尤其对数据进行一些必要的探索，如了解数据的大致结构、数据量、各特征的统计信息、整个数据质量情况、数据的分布情况等。为了更好体现数据分布情况，数据可视化是一个不错方法。</p>
</blockquote>
</li>
<li>2、预处理数据  <blockquote>
<p>通过对数据探索后，可能发现不少问题：如存在缺失数据、数据不规范、数据分布不均衡、存在奇异数据、有很多非数值数据、存在很多无关或不重要的数据等等。这些问题的存在直接影响数据质量，为此，数据预处理工作应该就是接下来的重点工作，数据预处理是机器学习过程中必不可少的重要步骤，特别是在生产环境中的机器学习，数据往往是原始、为加工和处理过，数据预处理常常占据整个机器学习过程的大部分时间。数据预处理过程中，一般包括数据清理、数据转换、规范数据、特征选择等等工作。</p>
</blockquote>
</li>
<li>3、训练模型  <blockquote>
<p>在模型选择时，一般不存在某种对任何情况都表现很好的算法（这种现象又称为没有免费的午餐）。因此在实际选择时，一般会选用几种不同方法来训练模型，然后比较它们的性能，从中选择最优的这个。当然，在比较不同模型之前，需要先确认衡量性能的指标，对分类问题常用的是准确率或ROC曲线，对回归连续性目标值问题一般采用误差来评估。训练模型前，一般会把数据集分为训练集和测试集，或对训练集再细分为训练集和验证集，从而对模型的泛化能力进行评估。</p>
</blockquote>
</li>
<li>4、评估模型与优化模型<blockquote>
<p>使用训练数据构建模型后，通常使用测试数据对模型进行测试，测试模型对新数据的测试。如果对模型的测试结果满意，就可以用此模型对以后的进行预测；如果测试结果不满意，可以优化模型，优化的方法很多，其中网格搜索参数是一种有效方法，当然也可以采用手工调节参数等方法。如果出现过拟合，尤其是回归类问题，可以考虑正则化方法来降低模型的泛化误差。</p>
</blockquote>
</li>
</ul>

    </div>

    
    
    
      
  <div class="popular-posts-header">相关文章</div>
  <ul class="popular-posts">
    <li class="popular-posts-item">
      <div class="popular-posts-title"><a href="\2020\04\20\Machine Learning\00.ML-机器学习？\" rel="bookmark">ML-机器学习？</a></div>
    </li>
    <li class="popular-posts-item">
      <div class="popular-posts-title"><a href="\2020\06\10\Machine Learning\11.ML-数据分析\" rel="bookmark">ML-数据分析</a></div>
    </li>
    <li class="popular-posts-item">
      <div class="popular-posts-title"><a href="\2020\09\20\Machine Learning\16.Introduction\" rel="bookmark">Introduction</a></div>
    </li>
    <li class="popular-posts-item">
      <div class="popular-posts-title"><a href="\2020\09\21\Machine Learning\17.MathBasics\" rel="bookmark">MathBasics</a></div>
    </li>
    <li class="popular-posts-item">
      <div class="popular-posts-title"><a href="\2020\09\25\Machine Learning\18.LinearRegression\" rel="bookmark">LinearRegression</a></div>
    </li>
  </ul>


    <footer class="post-footer">
          

<div class="post-copyright">
<ul>
  <li class="post-copyright-author">
      <strong>本文作者： </strong>SoundMemories
  </li>
  <li class="post-copyright-link">
      <strong>本文链接：</strong>
      <a href="https://soundmemories.github.io/2020/05/10/Machine%20Learning/07.ML-Spark/" title="ML-Spark">https://soundmemories.github.io/2020/05/10/Machine Learning/07.ML-Spark/</a>
  </li>
  <li class="post-copyright-license">
    <strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <span class="exturl" data-url="aHR0cHM6Ly9jcmVhdGl2ZWNvbW1vbnMub3JnL2xpY2Vuc2VzL2J5LW5jLXNhLzQuMC8="><i class="fab fa-fw fa-creative-commons"></i>BY-NC-SA</span> 许可协议。转载请注明出处！
  </li>
</ul>
</div>

          <div class="post-tags">
              <a href="/tags/Machine-Learning/" rel="tag"><i class="fa fa-tag"></i> Machine Learning</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2020/05/07/Machine%20Learning/06.ML-Hive%E3%80%81Hbase/" rel="prev" title="ML-Hive、Hbase">
                  <i class="fa fa-chevron-left"></i> ML-Hive、Hbase
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2020/05/18/Machine%20Learning/08.ML-scikit-learn/" rel="next" title="ML-scikit-learn">
                  ML-scikit-learn <i class="fa fa-chevron-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






      

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      const activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      const commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

    </div>
  </main>

  <footer class="footer">
    <div class="footer-inner">
      

      

<div class="copyright">
  
  &copy; 2019 – 
  <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">SoundMemories</span>
</div>
  <div class="powered-by">由 <span class="exturl theme-link" data-url="aHR0cHM6Ly9oZXhvLmlv">Hexo</span> & <span class="exturl theme-link" data-url="aHR0cHM6Ly90aGVtZS1uZXh0LmpzLm9yZy9tdXNlLw==">NexT.Muse</span> 强力驱动
  </div>

    </div>
  </footer>

  
  <script src="//cdn.jsdelivr.net/npm/animejs@3.2.1/lib/anime.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/@next-theme/pjax@0.4.0/pjax.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/jquery@3.5.1/dist/jquery.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js"></script>
<script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/muse.js"></script><script src="/js/next-boot.js"></script><script src="/js/bookmark.js"></script>
  <script>
var pjax = new Pjax({
  selectors: [
    'head title',
    '.page-configurations',
    '.main-inner',
    '.post-toc-wrap',
    '.languages',
    '.pjax'
  ],
  analytics: false,
  cacheBust: false,
  scrollRestoration: false,
  scrollTo: !CONFIG.bookmark.enable
});

document.addEventListener('pjax:success', () => {
  pjax.executeScripts(document.querySelectorAll('script[data-pjax], .pjax script'));
  NexT.boot.refresh();
  // Define Motion Sequence & Bootstrap Motion.
  if (CONFIG.motion.enable) {
    NexT.motion.integrator
      .init()
      .add(NexT.motion.middleWares.subMenu)
      .add(NexT.motion.middleWares.postList)
      .bootstrap();
  }
  const hasTOC = document.querySelector('.post-toc');
  document.querySelector('.sidebar-inner').classList.toggle('sidebar-nav-active', hasTOC);
  document.querySelector(hasTOC ? '.sidebar-nav-toc' : '.sidebar-nav-overview').click();
  NexT.utils.updateSidebarPosition();
});
</script>


  




  <script src="/js/local-search.js"></script>








<script data-pjax>
document.querySelectorAll('.pdfobject-container').forEach(element => {
  const url = element.dataset.target;
  const pdfOpenParams = {
    navpanes : 0,
    toolbar  : 0,
    statusbar: 0,
    pagemode : 'thumbs',
    view     : 'FitH'
  };
  const pdfOpenFragment = '#' + Object.entries(pdfOpenParams).map(([key, value]) => `${key}=${encodeURIComponent(value)}`).join('&');
  const fullURL = `/lib/pdf/web/viewer.html?file=${encodeURIComponent(url)}${pdfOpenFragment}`;

  if (NexT.utils.supportsPDFs()) {
    element.innerHTML = `<embed class="pdfobject" src="${url + pdfOpenFragment}" type="application/pdf" style="height: ${element.dataset.height};">`;
  } else {
    element.innerHTML = `<iframe src="${fullURL}" style="height: ${element.dataset.height};" frameborder="0"></iframe>`;
  }
});
</script>


<script data-pjax>
if (document.querySelectorAll('.mermaid').length) {
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/mermaid@8.8.2/dist/mermaid.min.js', () => {
    mermaid.init({
      theme    : 'neutral',
      logLevel : 3,
      flowchart: { curve     : 'linear' },
      gantt    : { axisFormat: '%m/%d/%Y' },
      sequence : { actorMargin: 50 }
    }, '.mermaid');
  }, window.mermaid);
}
</script>





  








    <div class="pjax">
  

  
      <script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              const target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    const script = document.createElement('script');
    script.src = '//cdn.jsdelivr.net/npm/mathjax@3.1.2/es5/tex-mml-chtml.js';
    script.defer = true;
    document.head.appendChild(script);
  } else {
    MathJax.startup.document.state(0);
    MathJax.typesetClear();
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  
  <script src="//cdn.jsdelivr.net/npm/quicklink@2.0.0/dist/quicklink.umd.js"></script>
  <script>
      window.addEventListener('load', () => {
      quicklink.listen({
        timeout : 3000,
        priority: true,
        ignores : [uri => uri.includes('#'),uri => uri === 'https://soundmemories.github.io/2020/05/10/Machine%20Learning/07.ML-Spark/',]
      });
      });
  </script>

    </div>
</body>
</html>
