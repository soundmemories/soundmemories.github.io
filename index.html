<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.2.0">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="https://fonts.loli.net/css?family=Monda:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">

<link rel="stylesheet" href="//cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.1/css/all.min.css">
  <link rel="stylesheet" href="//cdn.jsdelivr.net/npm/animate.css@3.1.1/animate.min.css">
  <link rel="stylesheet" href="//cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.css">

<script class="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"soundmemories.github.io","root":"/","images":"/images","scheme":"Muse","version":"8.0.2","exturl":true,"sidebar":{"position":"left","display":"always","padding":18,"offset":12},"copycode":true,"bookmark":{"enable":true,"color":"#222","save":"auto"},"fancybox":true,"mediumzoom":true,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"},"path":"/search.xml","localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false}};
  </script>

  <meta name="description" content="今日事，今日毕">
<meta property="og:type" content="website">
<meta property="og:title" content="SoundMemories">
<meta property="og:url" content="https://soundmemories.github.io/index.html">
<meta property="og:site_name" content="SoundMemories">
<meta property="og:description" content="今日事，今日毕">
<meta property="og:locale" content="zh_CN">
<meta property="article:author" content="SoundMemories">
<meta name="twitter:card" content="summary">


<link rel="canonical" href="https://soundmemories.github.io/">


<script data-pjax class="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'zh-CN'
  };
</script>

  <title>SoundMemories</title>
  






  <noscript>
  <style>
  body { margin-top: 2rem; }

  .use-motion .menu-item,
  .use-motion .sidebar,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header {
    visibility: visible;
  }

  .use-motion .header,
  .use-motion .site-brand-container .toggle,
  .use-motion .footer { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle,
  .use-motion .custom-logo-image {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line {
    transform: scaleX(1);
  }

  .search-pop-overlay, .sidebar-nav { display: none; }
  .sidebar-panel { display: block; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <h1 class="site-title">SoundMemories</h1>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu">
        <li class="menu-item menu-item-主页">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>主页</a>

  </li>
        <li class="menu-item menu-item-分类">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a>

  </li>
        <li class="menu-item menu-item-标签">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a>

  </li>
        <li class="menu-item menu-item-归档">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
        <li class="menu-item menu-item-关于">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" maxlength="80"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="search-result-container no-result">
  <div class="search-result-icon">
    <i class="fa fa-spinner fa-pulse fa-5x"></i>
  </div>
</div>

    </div>
  </div>

</div>
        
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>

  <aside class="sidebar">

    <div class="sidebar-inner sidebar-overview-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>
	   
		  
      <div class="sidebar-panel-container">
        <!--noindex-->
        <section class="post-toc-wrap sidebar-panel">
        </section>
        <!--/noindex-->

        <section class="site-overview-wrap sidebar-panel">
          <div class="site-author site-overview-item animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="SoundMemories"
      src="https://i.loli.net/2020/11/04/6JhNuwtBe4adylS.png">
  <p class="site-author-name" itemprop="name">SoundMemories</p>
  <div class="site-description" itemprop="description">今日事，今日毕</div>
</div>
<div class="site-state-wrap site-overview-item animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives">
          <span class="site-state-item-count">85</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
        <span class="site-state-item-count">7</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
        <span class="site-state-item-count">7</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author site-overview-item animated">
      <span class="links-of-author-item">
        <span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL3NvdW5kbWVtb3JpZXM=" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;soundmemories"><i class="fab fa-github fa-fw"></i>GitHub</span>
      </span>
      <span class="links-of-author-item">
        <span class="exturl" data-url="bWFpbHRvOnNvdW5kbWVtb3JpZXNAMTYzLmNvbQ==" title="E-Mail → mailto:soundmemories@163.com"><i class="fa fa-envelope fa-fw"></i>E-Mail</span>
      </span>
  </div>
  <div class="cc-license site-overview-item animated" itemprop="license">
    <span class="exturl cc-opacity" data-url="aHR0cHM6Ly9jcmVhdGl2ZWNvbW1vbnMub3JnL2xpY2Vuc2VzL2J5LW5jLXNhLzQuMC8="><img src="/images/cc-by-nc-sa.svg" alt="Creative Commons"></span>
  </div>



        </section>
      </div>
    </div>
  </aside>
  <div class="sidebar-dimmer"></div>


    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>
  <a role="button" class="book-mark-link book-mark-link-fixed"></a>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


    <div class="main-inner index posts-expand">
      

      
    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://soundmemories.github.io/2021/06/25/Paper/02.XGBoost%20A%20Scalable%20Tree%20Boosting%20System/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="https://i.loli.net/2020/11/04/6JhNuwtBe4adylS.png">
      <meta itemprop="name" content="SoundMemories">
      <meta itemprop="description" content="今日事，今日毕">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="SoundMemories">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/06/25/Paper/02.XGBoost%20A%20Scalable%20Tree%20Boosting%20System/" class="post-title-link" itemprop="url">XGBoost A Scalable Tree Boosting System</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2021-06-25 00:00:00" itemprop="dateCreated datePublished" datetime="2021-06-25T00:00:00+08:00">2021-06-25</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Paper/" itemprop="url" rel="index"><span itemprop="name">Paper</span></a>
        </span>
    </span>

  
      </div>
      <div class="post-meta">
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>4.7k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>4 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>提出名为XGBoost的树提升系统。<br>提出一种新颖的稀疏数据感知算法用于稀疏数据，一种带权值的分位数略图(weighted quantile sketch) 来近似实现树的学习。<br>提出有关缓存访问模式，数据压缩和分片的见解，以构建有延展性的提升树系统。</p>
<h1 id="导读"><a href="#导读" class="headerlink" title="导读"></a>导读</h1><p>机器学习方法里，GradientTree Boosting（GBDT）是一个在很多应用里都很出彩的技术。提升树方法在很多有标准分类基准的情况下表现很出色。本文提出了一个可扩展的提升树机器学习系统（XGBoost）。XGBoost在2015年的29场比赛获胜队伍中，有17个都使用了XGBoost。</p>
<p>主要贡献：<br>1、设计和构建高度可扩展的端到端提升树系统。（树的个数能灵活的增加或减少）<br>2、提出了一个理论上合理的加权分位法。（推荐分割点的时候用，能不用遍历所有的点，只用部分点就行）<br>3、引入了一种新颖的稀疏感知算法用于并行树学习。（令缺失值有默认方向，稀疏数据处理方法和并行计算）<br>4、提出了一个有效的用于核外树形学习的缓存感知块结构。（有效使用缓存块处理数据）</p>
<h1 id="内容"><a href="#内容" class="headerlink" title="内容"></a>内容</h1><h2 id="Regularized-Learning-Objective"><a href="#Regularized-Learning-Objective" class="headerlink" title="Regularized Learning Objective"></a>Regularized Learning Objective</h2><p>给定一个数据集$\mathcal{D}$，$ n$个样本，每个样本有$ m$个特征：</p>
<script type="math/tex; mode=display">
\mathcal{D}= \{(x_i,y_i)\}(|\mathcal{D}|= n,x_i\in \Bbb{R}^m, y_i\in \Bbb{R})</script><p>第 $ k$ 棵树对于输入 $ x_i$ 的样本预测结果为$ f_k(x_i)$：</p>
<script type="math/tex; mode=display">
 \hat{y}_i=\varnothing(x_i)=\sum\limits_{k=1}^{K}f_k(x_i),\quad f_k\in \mathcal{F}</script><p>其中，$\mathcal{F}= \{f(x)=w_{q(x)}\}(q:\Bbb{R}^m\to T,w\in \Bbb{R}^T)$ 是CART回归树。$ q(x)$ 表示映射样本 $ x$ 到叶子节点的下标。$ T$是叶子节点数量。$ w$ 是叶子节点最优解（$ w_i$表示第$i$个叶子节点的最优解）。每一棵树都有独立的 $ q$  和 $ w$。</p>
<h2 id="Gradient-Tree-Boosting"><a href="#Gradient-Tree-Boosting" class="headerlink" title="Gradient Tree Boosting"></a>Gradient Tree Boosting</h2><p>以上定义了树模型的预测函数，那么接下来定义整个损失函数：</p>
<script type="math/tex; mode=display">
\mathcal{L}(\varnothing)= \sum\limits_i l(\hat{y}_i,y_i)+\sum\limits_k \Omega(f_k)\\</script><script type="math/tex; mode=display">
\text{where} \quad \Omega( f)=\gamma  T + \frac{1}{2}\lambda||w||^2</script><p>其中，$ l$ 函数是一个可导的凸函数，用来表示预测值 $\hat{y}_i$ 和真实值 $ y_i$ 之间的差异。$\Omega$ 是是惩罚项（正则化），用来防止树的结构过于复杂。<br>损失函数 $\mathcal{L}$ 参数中包含了函数，所以不能用传统的优化算法来优化。假设 $\hat{y}_i^{t}$ 是 $ x_i$ 在第 $ t$ 次迭代中的预测值。那么则有：</p>
<script type="math/tex; mode=display">
\mathcal{L}^{( t)}= \sum\limits_{i= 1}^{n}l(y_i,\hat{y}_i^{(t-1)}+f_t(x_i))+\Omega(f_t)</script><p>把 $ y_i,\hat{y}_i^{(t-1)}$ 看成 $x$ ，把 $ f_t(x_i))$ 看成 $\Delta x$ ，对上式近似为二阶泰勒级展开：</p>
<script type="math/tex; mode=display">
\mathcal{L}^{( t)}\simeq \sum\limits_{i= 1}^{n}[l(y_i,\hat{y}_i^{(t-1)})+g_if_t(x_i)+\frac{1}{2} h_if_t^{ 2}(x_i)]+\Omega(f_t)</script><script type="math/tex; mode=display">
\text{where} \quad  g_i=\partial_{\hat{y}^{(t-1)}}l(y_i,\hat{y}_i^{(t-1)})\quad \text{and} \quad h_i=\partial_{\hat{y}^{(t-1)}}^{ 2}l(y_i,\hat{y}_i^{(t-1)})</script><p>其中，$ g_i$$ h_i$是一阶偏导和二阶偏导。由于$ l(y_i,\hat{y}_i^{(t-1)})$为常数项，可以去除。<br>定义 $ I_j=\{i|q(x_i)=j\}$ 作为样本 $ x_i$ 被分割到第 $ j$ 个叶子节点下的样本下标集合（样本集合）。那么上面公式可以由<strong>对每棵树的样本求和</strong>转成<strong>对每棵树的叶子节点求和</strong>：</p>
<script type="math/tex; mode=display">
\begin{aligned}
\tilde{\mathcal{L}}^{( t)}&= \sum\limits_{i= 1}^{n}[g_if_t(x_i)+\frac{1}{2} h_if_t^{ 2}(x_i)]+\Omega(f_t)\\
&= \sum\limits_{i= 1}^{n}[g_if_t(x_i)+\frac{1}{2} h_if_t^{ 2}(x_i)]+\gamma  T + \frac{1}{2}\lambda \sum\limits_{ j= 1}^{T}w_j^{ 2}\\
&= \sum\limits_{i= 1}^{n}[g_iw_{q(x_i)}+\frac{1}{2} h_i(w_{q(x_i)})^{ 2}]+\gamma  T + \frac{1}{2}\lambda \sum\limits_{ j= 1}^{T}w_j^{ 2}\\
&= \sum\limits_{j= 1}^{T}[\sum\limits_{i\in I_j}g_iw_{j}+\frac{1}{2}\sum\limits_{i\in  I_j} h_i(w_{j})^{ 2}]+\gamma  T + \frac{1}{2}\lambda \sum\limits_{ j= 1}^{T}w_j^{ 2}\\
&= \sum\limits_{j= 1}^{T}[(\sum\limits_{i\in I_j}g_i)w_{j}+\frac{1}{2}(\sum\limits_{i\in  I_j} h_i+\lambda)w_{j}^{ 2}]+\gamma  T 
\end{aligned}</script><p><strong>1、如何求出每个叶子节点的最优解？</strong><br>对上式求极值点（它是凸函数，求的是极小值），即对 $ w_j$ 求一阶导数等于零（也可以看成二元一次方程求解），解得：</p>
<script type="math/tex; mode=display">
 w_j^*=-\frac{\sum_{i\in I_j}g_i}{\sum_{i\in I_j}h_i+\lambda}</script><p>带入 $\tilde{\mathcal{L}}^{( t)}$ 求得最优值（最小值）：</p>
<script type="math/tex; mode=display">
 \tilde{\mathcal{L}}^{( t)}=-\frac{1}{2}\sum\limits_{j= 1}^{T}\frac{(\sum_{i\in I_j}g_i)^{ 2}}{\sum_{i\in I_j}h_i+\lambda}+\gamma  T</script><p>可以用这个公式来来衡量决策树的质量。有点像决策树信息熵一个道理。</p>
<p><strong>2、对当前决策树做子树分裂时，如何选择哪个特征和特征值进行分裂，使损失函数最小？</strong><br>如果想要划分后损失函数得到最小值，意味这在做每次划分的时候，要尽量保证划分后的score比划分前的score要更小。那么应该找到 (划分前的score - 划分后的score) 这个差最大的切分点作为我们这一次的划分点。假设 $ I_L$和 $ I_R$ 为一个节点 $ I$ 划分后的左子集和右子集，节点 $ I=I_L \cup I_R$ ，则得到以下公式：</p>
<script type="math/tex; mode=display">
\begin{aligned}
 \mathcal{L}_{split}&=- \dfrac{1}{2}[\frac{(\sum_{i\in I}g_i)^{ 2}}{\sum_{i\in I}h_i+\lambda}-\frac{(\sum_{i\in I_L}g_i)^{ 2}}{\sum_{i\in I_L}h_i+\lambda}-\frac{(\sum_{i\in I_R}g_i)^{ 2}}{\sum_{i\in I_R}h_i+\lambda}]-\gamma\\
&= \dfrac{1}{2}  [\frac{(\sum_{i\in I_L}g_i)^{ 2}}{\sum_{i\in I_L}h_i+\lambda}+\frac{(\sum_{i\in I_R}g_i)^{ 2}}{\sum_{i\in I_R}h_i+\lambda}-\frac{(\sum_{i\in I}g_i)^{ 2}}{\sum_{i\in I}h_i+\lambda}]-\gamma
\end{aligned}</script><p>可以用这个公式来衡量是否当前节点是否再应该继续划分下去。每次用不同的特征，计算分数，然后用最大的值那个特征，作为当前树节点的划分点。</p>
<p>相对于GBDT，XGBoost一次性求解出<strong>最优解叶子节点区域</strong>和<strong>每个叶子节点区域最优解</strong>。而GBDT是基于残差（一阶泰勒）拟合一颗CART书，得到<strong>最优叶子节点区域</strong>，再求出<strong>每个叶子节点区域最优解</strong>。</p>
<h2 id="Shrinkage-and-Column-Subsampling"><a href="#Shrinkage-and-Column-Subsampling" class="headerlink" title="Shrinkage and Column Subsampling"></a>Shrinkage and Column Subsampling</h2><p>除了之前提过的添加正则化的项来防止模型的过拟合之外，还可以用两种方式来防止过拟合：<br>（1）添加类似梯度下降优化问题中的学习率 $ \eta$ ，这个可以收缩每棵树的权重，让每棵树的生长更加稳定。<br>（2）对样本的特征子采样（随机森林用到过）。每次生成树的时候，只用其中一部分抽样的特征。这样子也能降低过拟合的风险。</p>
<p><strong>精准的贪心算法</strong><br>贪心算法就是每次都希望找到最优的结果，但这样需每次都遍历所有的特征，对每个特征，又遍历所有划分的可能。然后通过 $ \mathcal{L}_{split}$ 的分数，计算每次划分后的score，取最大的score的对应的特征来进行划分。<br><img src="/images/XGB/01.png" width="60%"></p>
<p><strong>近似的贪心算法</strong><br>用上面贪心算法来寻找最佳划分点，准确度非常不错，但是时间复杂度和空间复杂度都太高了，特别是对于连续值的变量来说，简直是一个大灾难。作者提出一种近似法分位法，先对数据进行分桶(Bucket)，然后桶内的数据相加起来，作为一个代表来进行计算。</p>
<p>那我们应该在什么时候对数据进行分桶呢？有两种方式。<br>（1）全局分桶（Global Bucket），可以一开始就对全部的数据进行分桶。后面进行划分的时候只需要使用分桶数据就可以了。<br>（2）局部分桶（Local Bucket），每次需要对当前leaf node进行划分的时候，对当前节点里面的数据进行分桶。然后再划分。当然这个时间复杂度也会变的比较高。</p>
<p>具体划分的伪代码如下：<br>（1）先计算1到M个特征，找出每个特征的分桶的候选点。<br>（2）然后将候选点之间的数据的g和h求和，装入到Bucket里面，代表这些数据。<br>（3）后面流程就跟精确的弹性分割算法一样。只是将每个Bucket看成一个x。<br><img src="/images/XGB/02.png" width="60%"></p>
<p>下面是作者测试对几种不同的切分算法的AUC结果比较图。可以看得出，当eps=0.05，也就是将数据分成20个Bucket的时候，AUC的分数跟精准的贪心算法一样。<br><img src="/images/XGB/03.png" width="60%"></p>
<h2 id="SPLIT-FINDING-ALGORITHMS"><a href="#SPLIT-FINDING-ALGORITHMS" class="headerlink" title="SPLIT FINDING ALGORITHMS"></a>SPLIT FINDING ALGORITHMS</h2><p>提出一种新的分桶的方法<strong>Weighted Quantile Sketch</strong>（加权分位法）。</p>
<p><strong>加权分位法</strong><br>上面我们讨论了对数据分成Bucket，再来计算他的节点的split分数。来减少我们的计算量。那么我们如何对数据分桶，才能够比较合理呢？</p>
<p>作者按照对loss的影响权重来进行分桶，让数据分桶后，每个桶对loss的影响权重相同。<br>定义一个数据集 $ \mathcal{D}_k=\{(x_{1k},h_1),(x_{2k},h_2),…,(x_{nk},h_n)\}$ ，$ k$ 为样本 $x$ 的特征数。<br>定义一个排序函数 $ r_k:\Bbb{R}\sim [0,\infty)$，则：</p>
<script type="math/tex; mode=display">
 r_k(z)=\frac{1}{\sum_{(x,h)\in \mathcal{D}_k}h}\sum_{(x,h)\in \mathcal{D}_k,x<z}h</script><p>切分点集合 $ \{s_{k1}, s_{k2},…,s_{kl}\}$，满足：</p>
<script type="math/tex; mode=display">
|r_k(s_{k,j})-r_k(s_{k,j+1})|<\epsilon, \quad s_{k1}=\min\limits_i x_{ik},s_{kl}=\max\limits_i x_{ik}</script><p>这里 $\epsilon$ 用来衡量划分区间的大小（每个Bucket不能太大，以免错过最优切分点）。这个意味这数据集大约被分为 $\frac{1}{\epsilon}$ 个候选点。</p>
<p>那么 $h_i$ 为什么能够代表 $x_i$ 的权重来进行排序呢？将公式 ${\tilde{\mathcal{L}}}^{(t)}$ 对 $ h_i$ 进行提取，可得到：</p>
<script type="math/tex; mode=display">
\begin{aligned}
\tilde{\mathcal{L}}^{( t)}&= \sum\limits_{i= 1}^{n}[g_if_t(x_i)+\frac{1}{2} h_if_t^{ 2}(x_i)]+\Omega(f_t)\\
&= \sum\limits_{i= 1}^{n}[\frac{1}{2} h_i\frac{ 2* g_if_t(x_i)}{h_i}+\frac{1}{2} h_if_t^{ 2}(x_i)]+\Omega(f_t)\\
&= \sum\limits_{i= 1}^{n}\frac{1}{2} h_i[ 2\times \frac{g_i}{h_i}f_t(x_i)+f_t^{ 2}(x_i)]+\Omega(f_t)\\
&= \sum\limits_{i= 1}^{n}\frac{1}{2} h_i[ 2\times \frac{g_i}{h_i}f_t(x_i)+f_t^{ 2}(x_i)+(\frac{g_i}{h_i})^2-(\frac{g_i}{h_i})^2]+\Omega(f_t)\\
&= \sum\limits_{i= 1}^{n}\frac{1}{2} h_i(f_t(x_i)+\frac{g_i}{h_i})^{ 2}+\Omega(f_t)\\
&= \sum\limits_{i= 1}^{n}\frac{1}{2} h_i(f_t(x_i)-(-\frac{g_i}{h_i}))^{ 2}+\Omega(f_t)+\text{constant}
\end{aligned}</script><p>发现 $ h_i$ 对结果影响最大，所以用它来进行排序。</p>
<h2 id="Sparsity-aware-Split-Finding"><a href="#Sparsity-aware-Split-Finding" class="headerlink" title="Sparsity-aware Split Finding"></a>Sparsity-aware Split Finding</h2><p>对稀疏数据的处理，重点是针对特征为空时的处理，对<strong>精准的贪心算法</strong>做了改进：<br><img src="/images/XGB/04.png" width="60%"></p>
<p>简单来讲，通过两轮遍历可以确保稀疏值位于左子树和右子树的情形，就是对该特征划分为左节点还是右节点做了一次比较，哪个效果好就把它放在哪。</p>
<h2 id="SYSTEM-DESIGN"><a href="#SYSTEM-DESIGN" class="headerlink" title="SYSTEM DESIGN"></a>SYSTEM DESIGN</h2><p>重点是优化：<br>（1）<strong>预排序</strong>：这个算法大量时间消耗在排序上。只需在最开始对每个特征排一次序即可。<br>这里XGB将所有的列数据都预先排了序。以压缩形式分别存到block里，不同的block可以分布式存储，甚至存到硬盘里。在特征选择的时候，可以并行的处理这些列数据，XGB就是在这实现的并行化，用多线程来实现加速。同时这里还用cache加了一个底层优化：当数据排序后，索引值是乱序的，可能指向了不同的内存地址，找的时候数据是不连续的，这里加了个缓存，让以后找的时候能找到小批量的连续地址，以实现加速！这里是在每个线程里申请了一个internal buffer来实现的！这个优化在小数据下看不出来，数据越多越明显。</p>
<p>（2）<strong>预取</strong>：尽可能的把数据保存在缓存中，这样不用去磁盘进行读取，减少时间开销。并且给出了对比结果：<br><img src="/images/XGB/05.png" width="100%"></p>
<p>（3）内存块的大小也会影响缓存速率。<br><img src="/images/XGB/06.png" width="50%"></p>
<p>针对磁盘存储优化。磁盘block不大的情况下：<br>1.把block数据进行压缩，让它没有那么大。<br>2.把block数据放在多个磁盘，增大磁盘带宽，让读取速度更。</p>
<h1 id="信息论"><a href="#信息论" class="headerlink" title="信息论"></a>信息论</h1><h2 id="熵"><a href="#熵" class="headerlink" title="熵"></a>熵</h2><p>如果$\textbf{X}$是一个离散型随机变量，取空间值为$\Bbb{R}$，其概率分布为$ p(x)=P(\textbf{X}=x),x\in\Bbb{R}$。那么，$\textbf{X}$的熵$H(\textbf{X})$定义为：</p>
<script type="math/tex; mode=display">
H(\textbf{X})=-\sum\limits_{x\in\Bbb{R}} p(x) log p(x)</script><p>其中，$H(\textbf{X})$可以写成$H( p)$。</p>
<p><strong>熵又称为子信息（self-information），可以视为描述一个随机变量的不确定性的数量</strong>。它表示信源$\textbf{X}$每发一个符号（不论发什么符号）所提供的平均信息量。<strong>一个随机变量的熵越大，它的不确定性越大，那么，正确估计其值的可能性就越小。越不确定的随机变量越需要大的信息量用以确定其值。</strong></p>
<p><strong>熵定义了随机变量的不确定性，当熵最大时，随机变量最不确定，最难准确地预测其行为。也就是说，在已知部分知识的前提下，关于未知分布最合理的推断应该是符合已知知识最不确定或最大随机的推断。</strong> </p>
<h2 id="联合熵和条件熵"><a href="#联合熵和条件熵" class="headerlink" title="联合熵和条件熵"></a>联合熵和条件熵</h2><p>如果$\textbf{X},\textbf{Y}$是一对离散型随机变量$\textbf{X},\textbf{Y}\sim p(x,y)$，$\textbf{X},\textbf{Y}$的联合熵（joint entropy）$H(\textbf{X},\textbf{Y})$定义为：</p>
<script type="math/tex; mode=display">
H(\textbf{X},\textbf{Y})=-\sum\limits_{ x\in\textbf{X}}\sum\limits_{ y\in\textbf{Y}} p(x,y) log p(x,y)</script><p>联合熵实际上就是描述一对随机变量平均所需要的信息量。</p>
<p>给定随机变量$\textbf{X}$的情况下， 随机变量$\textbf{Y}$的条件熵（conditionalentropy）：</p>
<script type="math/tex; mode=display">
\begin{aligned}
H(\textbf{Y}|\textbf{X})&=\sum\limits_{ x\in\textbf{X}} p(x) H(\textbf{Y}|\textbf{X}= x)\\
&=\sum\limits_{ x\in\textbf{X}} p(x)[-\sum\limits_{ y\in\textbf{Y}} p(y|x) log p(y|x)]\\
&=-\sum\limits_{ x\in\textbf{X}}\sum\limits_{ y\in\textbf{Y}} p(x,y) log p(y|x)
\end{aligned}</script><p>将其中的联合概率$log p(x,y)$展开，可得：</p>
<script type="math/tex; mode=display">
\begin{aligned}
H(\textbf{X},\textbf{Y})&=-\sum\limits_{ x\in\textbf{X}}\sum\limits_{ y\in\textbf{Y}} p(x,y) log[ p(x)p(y|x)]\\
&=-\sum\limits_{ x\in\textbf{X}}\sum\limits_{ y\in\textbf{Y}} p(x,y) [log p(x) + log p(y|x)]\\
&=-\sum\limits_{ x\in\textbf{X}}\sum\limits_{ y\in\textbf{Y}} p(x,y) log p(x)-\sum\limits_{ x\in\textbf{X}}\sum\limits_{ y\in\textbf{Y}} p(x,y) log p(y|x)\\
&=-\sum\limits_{ x\in\textbf{X}} p(x) log p(x)-\sum\limits_{ x\in\textbf{X}}\sum\limits_{ y\in\textbf{Y}} p(x,y) log p(y|x)\\
&=H(\textbf{X})+H(\textbf{Y}|\textbf{X})
\end{aligned}</script><p>我们称上式为熵的连锁规则。推广到一般情况，有：</p>
<script type="math/tex; mode=display">
H(\textbf{X}_1,\textbf{X}_2,...,\textbf{X}_n)=H(\textbf{X}_1)+H(\textbf{X}_2|\textbf{X}_1)+...+H(\textbf{X}_n|\textbf{X}_1,\textbf{X}_2,...,\textbf{X}_{n-1})</script><h2 id="互信息"><a href="#互信息" class="headerlink" title="互信息"></a>互信息</h2><p>根据熵的连锁规则， 有：</p>
<script type="math/tex; mode=display">
H(\textbf{X},\textbf{Y})=H(\textbf{X})+H(\textbf{Y}|\textbf{X})=H(\textbf{Y})+H(\textbf{X}|\textbf{Y})</script><p>因此：</p>
<script type="math/tex; mode=display">
H(\textbf{X})-H(\textbf{X}|\textbf{Y})=H(\textbf{Y})-H(\textbf{Y}|\textbf{X})</script><p>这个差叫做$\textbf{X}$和$\textbf{Y}$的互信息（mutual information, MI），记作$I(\textbf{X};\textbf{Y})$。<br>或者定义为：如果$(\textbf{X},\textbf{Y})\sim  p(x,y)$，则$\textbf{X},\textbf{Y}$之间的互信息$I(\textbf{X};\textbf{Y})=H(\textbf{X})-H(\textbf{X}|\textbf{Y})$。</p>
<p><strong>$I(\textbf{X};\textbf{Y})$反映的是在知道了$\textbf{Y}$的值以后$\textbf{X}$的不确定性的减少量。可以理解为$\textbf{Y}$的值透露了多少关于$\textbf{X}$的信息量。</strong><br>互信息和熵之间的关系：<br><img src="/images/XGB/互信息.png" width="40%"></p>
<p>如果将定义中的$H(\textbf{X})$和$H(\textbf{X}|\textbf{Y})$展开，可得：</p>
<script type="math/tex; mode=display">
\begin{aligned}
I(\textbf{X};\textbf{Y})&=H(\textbf{X})-H(\textbf{X}|\textbf{Y})\\
&=H(\textbf{X})+H(\textbf{Y})-H(\textbf{X},\textbf{Y})\\
&=\sum\limits_{ x} p(x) log \frac{1}{p(x)}  + \sum\limits_{ y} p(y) log \frac{1}{p(y)}  + \sum\limits_{ x,y} p(x,y) log p(x,y) \\
&=\sum\limits_{ x,y} p(x,y) log \frac{p(x,y)}{p(x)p(y)} \\
\end{aligned}</script><p>由于$H(\textbf{X}|\textbf{X})=0$， 因此，</p>
<script type="math/tex; mode=display">
H(\textbf{X})=H(\textbf{X})-H(\textbf{X}|\textbf{X})=I(\textbf{X};\textbf{X})</script><p>这一方面说明了为什么熵又称为自信息，另一方面说明了两个完全相互依赖的变量之间的互信息并不是一个常量，而是取决于它们的熵。</p>
<p>实际上，互信息体现了两变量之间的依赖程度：<br>如果$I(\textbf{X};\textbf{Y})≫0$，表明$\textbf{X}$和$\textbf{Y}$是高度相关的；<br>如果$I(\textbf{X};\textbf{Y})=0$，表明$\textbf{X}$和$\textbf{Y}$是相互独立的；<br>如果$I(\textbf{X};\textbf{Y})≪0$，表明$\textbf{Y}$的出现不但未使$\textbf{X}$的不确定性减小，反而增大了$\textbf{X}$的不确定性，是非常是不利的。</p>
<h2 id="相对熵"><a href="#相对熵" class="headerlink" title="相对熵"></a>相对熵</h2><p>相对熵（relative entropy）又称Kullback-Leibler差异（KullbackLeibler divergence），或简称KL距离，是<strong>衡量相同事件空间里两个概率分布相对差距的测度</strong>。两个概率分布$ p(x)$和$ q(x)$的相对熵定义为：</p>
<script type="math/tex; mode=display">
\begin{aligned}
D( p||q)&=\sum\limits_{ x\in\textbf{X}} p(x) log\frac{ p(x)}{ q(x)}\\
&=\sum\limits_{ x\in\textbf{X}} p(x) log  p(x)  - \sum\limits_{ x\in\textbf{X}} p(x) log q(x)\\
&=-H( p)+H( p,q)
\end{aligned}</script><p>其中，$H( p)$恒不变，只需考虑交叉熵$H( p,q)$即可。$ q(x)$ 分布越接近 $ p(x)$，那么散度值越小。<br>有时会将KL散度称为KL距离，但它并不满足距离的性质：KL散度不是对称的；KL散度不满足三角不等式。</p>
<h2 id="交叉熵"><a href="#交叉熵" class="headerlink" title="交叉熵"></a>交叉熵</h2><p>根据前面熵的定义，知道熵是一个不确定性的测度，也就是说，我们对于某件事情知道得越多，那么，熵就越小，因而对于试验的结果我们越不感到意外。<strong>交叉熵的概念就是用来衡量估计模型与真实概率分布之间差异情况的。</strong><br>如果一个随机变量$\textbf{X}\sim p(x)$，$ q(x)$为用于近似$ p(x)$的概率分布，那么，随机变量$\textbf{X}$和模型$ p(x)$之间的交叉熵（cross entropy）定义为：</p>
<script type="math/tex; mode=display">
\begin{aligned}
H( p,q)&=H( p)+D( p||q)\\
&=-\sum\limits_{ x\in\textbf{X}} p(x) log  q(x)\\
\end{aligned}</script><h1 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h1><p><span class="exturl" data-url="aHR0cHM6Ly9kbC5hY20ub3JnL2RvaS9wZGYvMTAuMTE0NS8yOTM5NjcyLjI5Mzk3ODU=">XGBoost: A Scalable Tree Boosting System<i class="fa fa-external-link-alt"></i></span><br><span class="exturl" data-url="aHR0cHM6Ly93d3cuY25ibG9ncy5jb20vcGluYXJkL3AvMTA5Nzk4MDguaHRtbA==">XGBoost算法原理小结<i class="fa fa-external-link-alt"></i></span><br><span class="exturl" data-url="aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FkYnN6c2ovYXJ0aWNsZS9kZXRhaWxzLzc5NjE1NzEy">XGBoost 论文翻译+个人注释<i class="fa fa-external-link-alt"></i></span><br><span class="exturl" data-url="aHR0cHM6Ly96aHVhbmxhbi56aGlodS5jb20vcC84OTU4OTIyMg==">XGBoost论文详解<i class="fa fa-external-link-alt"></i></span><br>[统计自然语言处理（第二版），宗成庆]</p>

      
    </div>

    
    
    
      


    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://soundmemories.github.io/2021/06/20/Paper/01.Unsupervised%20Data%20Augmentation%20for%20Consistency%20Training/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="https://i.loli.net/2020/11/04/6JhNuwtBe4adylS.png">
      <meta itemprop="name" content="SoundMemories">
      <meta itemprop="description" content="今日事，今日毕">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="SoundMemories">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/06/20/Paper/01.Unsupervised%20Data%20Augmentation%20for%20Consistency%20Training/" class="post-title-link" itemprop="url">Unsupervised Data Augmentation for Consistency Training</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2021-06-20 00:00:00" itemprop="dateCreated datePublished" datetime="2021-06-20T00:00:00+08:00">2021-06-20</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Paper/" itemprop="url" rel="index"><span itemprop="name">Paper</span></a>
        </span>
    </span>

  
      </div>
      <div class="post-meta">
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>5.4k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>5 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>背景：深度学习的模型训练通常依赖大量的标签数据（如Bert、XLNet），在只有少量数据上通常表现不好。由此产生了数据增强，但以前的研究都是基于监督学习的，并且效果不是特别理想。</p>
<p>贡献：本文提出了一种对无监督（无标签）数据增强方式（半监督学习中无标签数据的增强），简称UDA。UDA方法生成的无监督数据与原始无监督数据具备<strong>分布的一致性</strong>，而以前的方法通常只是应用高斯噪声和dropout噪声（无法保证一致性）。</p>
<p>效果：使用这种数据增强方法，在极少量数据集上，六种语言任务和三种视觉任务都得到了明显的提升。IMDb数据分类任务上，仅仅使用20个带标签数据加UDA方法，就超过了25000个带标签数据的训练模型，错误率达到了4.2%。在CIFAR-10上仅用4000张标签图片就达到了2.7%的错误率。在SVHN任务上，仅仅用250个标签数据就达到了2.85%的错误率，这相当于用全数据集才能达到的正确率，而它们的数量级差别达到了1或2（差10倍或100倍）。在大量标签数据集上，UDA同样表现优秀，在ImageNet任务上，使用10%带标签数据，UDA方法就将Top1和Top5的准确率分别由55.1%提高到77.3%，68.7提高到88.5%。在全数据集上，则分别由78.3%提高到94.4%，79%提高到94.5%。</p>
<h1 id="导读"><a href="#导读" class="headerlink" title="导读"></a>导读</h1><p>深度学习需要大量带标签数据，但是实际工程中很难满足，这就需要数据标注，但数据标注是一项耗时耗力的工作。所以，充分利用未标注数据是一个很有意义的研究方向。而半监督方法，是最有前景的方法之一，当前半监督方法可归结为三类：<br>（1）基于图卷积和图嵌入的图标签传播方法。<br>（2）将目标数据作为潜变量进行预测。<br>（3）强制一致/平滑。这种方法在许多任务中被证明具有较好的效果。</p>
<p>强制平滑方法只是使得模型对应较小的噪声不那么敏感。常用方法就是：对于一个样本，添加一些噪声（例如高斯噪声）然后强制让模型对于加噪和不加噪的数据的输出尽量的相似。直观而言就是一个好的模型，应该能够适应各种小的、不改变样本性质的扰动。通常由于扰动函数的不同会有各种不同的方案。</p>
<p>本文在Sajjadi、 Laine等人的研究的基础上，从有监督数据中学习扰动函数，从而得到最优的数据增强方法。良好的数据增强方法能够大大提高模型的结果，并且数据增强方法能应用于各领域。<strong>本文使用的优化方法是最小化增强数据与真实数据之间的KL散度</strong>。虽然有监督数据的数据增强取得了很多成功，但是大量的无监督数据使得UDA这种无监督数据增强方法拥有更广阔前景。</p>
<p>主要贡献：<br>（1）提出一种TSA方法，该方法能够在无标签数据大于标签数据的时候防止过拟合。<br>（2）证明<strong>针对性的数据增强</strong>（如AutoAugment）效果明显优于无针对性的数据增强。<br>（3）验证了本文方法在NLP任务上（如Bert）上的有效性。<br>（4）在CV和NLP任务中，本文方法都表现优异。<br>（5）研究一种能应用于分类数据中有标签数据和无标签数据不匹配情况的方法（数据不平衡处理方法）。</p>
<h1 id="内容"><a href="#内容" class="headerlink" title="内容"></a>内容</h1><h2 id="有监督数据增强"><a href="#有监督数据增强" class="headerlink" title="有监督数据增强"></a>有监督数据增强</h2><p>在保持标签相同（同一类别）的情况下，通过某种转换方法扩充出类似于真实数据的训练数据。简单而言就是，有一个样本$x$，通过转换函数$q(x)$生成新数据$\hat{x}$，新旧数据有相同的数据标签$y(\hat{x})=y(x)$。通常为了得到的增强数据与原始数据相似，使用的是最大似然估计方法。</p>
<p>数据增强方法可以看成是从有标签数据中扩充出更多的有标签数据，然后用扩充数据进行模型训练。因此，扩充数据相对于原始数据必须是有效的变换（例如图片缩放对图片识别可能有效，图片旋转可能无效）。也因此，如何设计转换函数至关重要。</p>
<p>目前，针对NLP任务的有监督数据增强方法已经取得了很大进展。虽然有成果，但是它通常被比喻成“蛋糕上的樱桃”，只是提高有限的性能，这是由于监督数据通常都是少量的。因此，本文研究了一种基于大量数据的无监督数据增强方法。</p>
<h2 id="无监督数据增强"><a href="#无监督数据增强" class="headerlink" title="无监督数据增强"></a>无监督数据增强</h2><p>本文研究了一种利用无监督数据的强制平滑方法（类似VAT）。工作流程如下：<br><img src="/images/UDA/UDA.png" width="90%"></p>
<p>（1）监督学习部分，使用交叉熵损失函数，模型是$p_\theta(y|x)$。<br>（2）无监督学习部分，使用强制平滑损失函数，对无标签数据进行数据增强，使增强前和增强后的数据分布越相近越好。增强前模型$p_{\tilde{\theta}}(y|x)$，增强后模型$p_\theta(y|\hat{x})$。<br>（3）最后，同时使用有标签和无标签数据，把二者模型结合起来，得到Final Loss。</p>
<p>本文使用最小化<strong>增强后的无标签数据</strong>和<strong>增强前无标签数据</strong>的KL散度。公式如下：<br><img src="/images/UDA/UDA_loss1.png" width="60%"></p>
<p>为了同时使用带标签数据和无标签数据，作者在计算带标签数据时上加上交叉熵损失和权重$\lambda$。<br><img src="/images/UDA/UDA_loss2.png" width="40%"></p>
<p>其中$\it q(\hat{x}|x)$是数据增强变换，$\tilde{\theta}$是当前参数$\theta$的固定副本，表明梯度像Miyato等人所建议的那样，不是通过$\tilde{\theta}$传播的。这里使用的数据增强与监督数据增强中使用的增强方法相同。由于数据增强耗时比较大，所以数据增强是离线生成的，单个原始样本会生成多个增强样本。</p>
<p>在无监督学习时，使用了针对性的数据增强：<br>（1）<strong>Back-translation</strong>：回译能够在保证语义不变的情况下，生成多样的句式。实验证明，在QANet上，这种策略取得了良好的效果。因此作者在情感分类问题等数据集，如IMDb，Yelp-2，Yelp-5，Amazon-2，Amazon-5上采用了这种策略，同时，他们发现，句式的多样性比较有效性更重要。所以使用了<strong>RandAugument</strong>。<br>（2）<strong>RandAugument</strong>：随机抽样增强，加入噪声。采用随机抽样代替集束搜索策略（一种贪心策略）。具体而言，作者使用WMT14语料库来训练英语到法语和法语到英语的翻译模型，并对每个句子执行回译，而不是整个段落，因为WMT14中的并行数据是用于句子级翻译，而情感分类语料库中的输入类型是段落。<br>（3）<strong>TF-IDF word replacement</strong>：虽然回译能够很好的进行数据扩充，但是它并不能保证扩充的句子包含关键词。而对于某些任务，如DBPedia任务，它的目标是预测某些句子属于维基百科的哪个词条。因此关键字非常重要，本文研究了一种在保留TF-IDF高的关键字，用其他非关键字替代TF-IDF分数低的非关键字扩充方案，详细见论文附录B。<br>增强结果如图所示：<br><img src="/images/UDA/trans.png" width="80%"></p>
<p>当然，对CV任务用了<strong>AutoAugument</strong>：用强化学习来搜索图像增强的“最优”组合，其性能明显优于任何人工设计的优化方法。作者使用已发现的增强策略，在CIFAR-10， SVHN和ImageNet上进行了实验，并在CIFAR-10，SVHN上组合应用了Cutout技术。<br>增强结果如图所示：<br><img src="/images/UDA/trans2.png" width="80%"></p>
<h2 id="数据增强在多样性和有效性上的平衡"><a href="#数据增强在多样性和有效性上的平衡" class="headerlink" title="数据增强在多样性和有效性上的平衡"></a>数据增强在多样性和有效性上的平衡</h2><p>虽然在一些非常优秀的数据增强方法中，能够得到很好的多样性和有效性。但是，由于多样性是通过改变原始数据得到的，所以，它存在改变数据类别的风险，所以，多样性和有效性是存在一定矛盾的。</p>
<p>对于图像分类，AutoAugment算法在有监督的环境下，根据验证集的性能进行优化，从而自动找到多样性和有效性之间的最佳点。</p>
<p>对于文本分类，作者调整随机抽样的强度。一方面，当强度为0时，随机抽样解码退化为贪婪方法，产生完全有效但完全相同的样本。另一方面，当作者使用1的强度时，随机抽样会产生非常不同但几乎不可读的样本。作者发现，设置Softmax强度为0.7、0.8或0.9的表现最好。</p>
<h2 id="训练技巧"><a href="#训练技巧" class="headerlink" title="训练技巧"></a>训练技巧</h2><p>要介绍一些针对不同问题，不同场景下的训练技巧。</p>
<p><strong>Training Signal Annealing（TSA）</strong>：针对标签数据与未标签数据不平衡时的场景。由于有大量的未标签数据需要UDA处理，所以需要一个较大模型，但是由于较大模型很容易在少量标签数据下过拟合，所以，提出了本方法用于解决该问题。<br>TSA原理就是在训练过程中，随着未标签数据的增加，逐步去除带标签数据，从而避免模型过拟合到带标签的训练数据。具体而言，就是在训练的$t$时刻，设置一个阈值$\eta_t$，当$\frac{1}{k}\leqslant\eta_t\leqslant 1$，其中$k$是类别数。当某个标签计算的$p_\theta(y^*|x)$大于阈值$\eta_t$，就将该标签数据移除出计算损失的过程，而只计算miniBatch里面的其余数据。假定miniBatch样本记作B，那么该策略计算损失如下：<br><img src="/images/UDA/TSA.png" width="40%"><br>过滤后的样本集合：<br><img src="/images/UDA/TSA2.png" width="40%"></p>
<p>阈值$\eta_t$用于防止模型过拟合到标签数据。随着$\eta_t$向1靠近，模型只能缓慢地从标注的实例中得到监督，大大缓解了过拟合问题。假设T是总训练步数，t是当前的训练步数。为了考虑未标记数据和标记数据的不同比率，有以下三种$\eta_t$更新计算方式：<br><img src="/images/UDA/TSA3.png" width="90%"></p>
<p>对于数据量少，容易过拟合的情况，使用指数形式比较好。对于标签数据不容易过拟合的情况，比如标签数据比较多或者使用了有效的正则化手段时，使用对数形式会比较好。使用不同更新方式的效果：<br><img src="/images/UDA/TSA4.png" width="50%"></p>
<p><strong>Sharpening Predictions</strong><br>当标签数据很少时，未标签数据和预测的未标签数据分布会很平坦。因此，在计算KL散度时，主要贡献的部分来自于标签数据。例如在Imagenet任务中，使用10%标签数据下，未标签数据的分布明显比标签数据的分布更加平坦。而比较丰富的数据分布是比较有利于模型训练的，因此，提出以下三种锐化方案：<br>（1）基于置信度的mask：对模型预测效果不好的，预测的概率小于一定阈值的标签，不计算一致性损失。<br>（2）最小化熵：最小化熵就是使得预测的增广数据能够拥有一个较低的熵，因此，需要在计算损失时，加上熵的计算。<br>（3）Softmax控制：通过调整Softmax控制输出， $p_{\tilde{\theta}}(y|x)$通过$Softmax(l(x)/\tau)$计算，其中$l(x)$表示结果逻辑分布概率，$\tau$表示强度。$\tau$越小，分布越锐化。</p>
<p><strong>Domain-relevance Data Filtering</strong><br>通常，作者希望能够运用领域外的数据，因为它比较容易获取。但是，一般领域外的数据和领域内的数据不匹配。由于数据分布的不匹配，使用领域外的数据往往对模型是有负面影响的。为了获取与当前任务相关的域数据，本文采用一种通用的检测领域外数据的技术。作者用领域内的数据训练了一个模型，让后用它去评估领域外的数据，然后过滤掉置信度低的数据。具体说就是，对于分类任务，对所有领域外数据进行概率计算，只使用其中分类正确且概率高的数据。</p>
<h2 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h2><p>本文对文本分类和视觉相关任务，运用UDA进行了实验。包括六项文本分类任务和三项图片分类任务。</p>
<h3 id="文本分类"><a href="#文本分类" class="headerlink" title="文本分类"></a>文本分类</h3><p>实验是基于Bert进行的，因为它在许多NLP任务中表现都很好。具体实验设置请看原始论文，实验结果如下：<br><img src="/images/UDA/01.png" width="80%"></p>
<p>实验结果表明，运用UDA后，基本都取得了较大的提高。同时，作者还实验了<strong>不同数量的标签</strong>对结果的影响，结果如下：<br><img src="/images/UDA/02.png" width="80%"></p>
<p>作者实验对比了UDA与半监督方法，结果显示，UDA结果明显更优。<br><img src="/images/UDA/03.png" width="90%"></p>
<p>同时，作者还对比实验了不同模型的情况：<br><img src="/images/UDA/04.png" width="80%"></p>
<h3 id="图像任务"><a href="#图像任务" class="headerlink" title="图像任务"></a>图像任务</h3><p>ImageNet之所以要单独拿出来，是因为它是一个很有挑战性的任务，而且数据量很大。作者使用10%标签数据和全数据分别做了对比（图片尺寸224）。<strong>10%标签数据</strong>，ImageNet对比实验结果：<br><img src="/images/UDA/05.png" width="50%"></p>
<p><strong>全数据</strong>，ImageNet对比实验结果：<br><img src="/images/UDA/06.png" width="50%"></p>
<p>作者做了<strong>使用不同训练策略</strong>下的情况，TSA对比实验结果：<br><img src="/images/UDA/07.png" width="50%"></p>
<p>最后，作者做了<strong>消融实验，对比不同策略的重要性</strong>。不同模块的消融实验结果：<br><img src="/images/UDA/08.png" width="50%"></p>
<h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>本文提供了一种无监督数据（无标签）数据增强方式，通过<strong>Back-translation</strong>、<strong>RandAugument</strong>、<strong>TF-IDF word replacement</strong>方法对无监督文本数据增强，使用<strong>AutoAugument</strong>对图像数据进行增强，最后使用KL散度使新生成的样本数据和原样本数据分布一致，最后结合有监督数据（有标签）形成最终的损失函数，通过<strong>TSA</strong>处理了无标签数据大于有标签数据的过拟合问题。</p>
<p>本文重要的是使用了针对性的数据增强，并且效果很好，不同于传统的高斯噪声、dropout噪声、或者简单的仿射变换，这种针对性的增强能生成更有效的噪声。并且对扰动的有效性和多样性进行了平衡。</p>
<p>这种针对性的思想值得学习，并且考虑分布影响，结合可以利用的增强方式，比如EDA中提到的同义词替换（synonym replacement）和随机插入（random Insertion，RI）。</p>
<h1 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h1><p><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzE5MDQuMTI4NDh2Mi5wZGY=">Unsupervised Data Augmentation for Consistency Training<i class="fa fa-external-link-alt"></i></span><br><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzE5MDQuMTI4NDgucGRmP3JlZj1oYWNrZXJub29uLmNvbQ==">Unsupervised Data Augmentation for Consistency Training<i class="fa fa-external-link-alt"></i></span><br><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzE5MDEuMTExOTYucGRm">EDA: Easy Data Augmentation Techniques for Boosting Performance on Text Classification Tasks<i class="fa fa-external-link-alt"></i></span><br><span class="exturl" data-url="aHR0cHM6Ly9zZXZlbm9sZC5naXRodWIuaW8vMjAyMC8wNi90ZXh0X0VEQS8=">自然语言处理之文本数据增强<i class="fa fa-external-link-alt"></i></span><br>Github：<span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL2dvb2dsZS1yZXNlYXJjaC91ZGE=">uda<i class="fa fa-external-link-alt"></i></span><br>Github：<span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL3poYW5sYW9iYW4vRURBX05MUF9mb3JfQ2hpbmVzZQ==">EDA_NLP_for_Chinese<i class="fa fa-external-link-alt"></i></span><br><span class="exturl" data-url="aHR0cHM6Ly93d3cuamlhbnNodS5jb20vcC81ZDRlMThiOGRlMDQ=">谷歌惊艳的无监督数据增强方法—Unsupervised Data Augmentation for Consistency Training<i class="fa fa-external-link-alt"></i></span></p>

      
    </div>

    
    
    
      


    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://soundmemories.github.io/2021/06/20/NLP/01.%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%92%8C%E8%AF%8D%E5%90%91%E9%87%8F/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="https://i.loli.net/2020/11/04/6JhNuwtBe4adylS.png">
      <meta itemprop="name" content="SoundMemories">
      <meta itemprop="description" content="今日事，今日毕">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="SoundMemories">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/06/20/NLP/01.%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%92%8C%E8%AF%8D%E5%90%91%E9%87%8F/" class="post-title-link" itemprop="url">语言模型和词向量</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2021-06-20 00:00:00" itemprop="dateCreated datePublished" datetime="2021-06-20T00:00:00+08:00">2021-06-20</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/NLP/" itemprop="url" rel="index"><span itemprop="name">NLP</span></a>
        </span>
    </span>

  
      </div>
      <div class="post-meta">
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>6.7k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>6 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="语言模型"><a href="#语言模型" class="headerlink" title="语言模型"></a>语言模型</h1><p><strong>模型</strong>指的是对事物的数学抽象，那么<strong>语言模型</strong>指的就是对语言现象的数学抽象。准确的讲，给定一个句子 $w$ ，语言模型就是计算句子的出现概率 $p(w)$ 的模型，而统计的对象就是人工标注而成的语料库。</p>
<p>假设构建如下的小型语料库：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">商品 和 服务</span><br><span class="line">商品 和服 物美价廉</span><br><span class="line">服务 和 货币</span><br></pre></td></tr></table></figure><br>每个句子出现的概率都是 $\footnotesize\dfrac{1}{3}$，因为样本空间为 3 ，这 3 次基数平均分给了 3 个句子，所以它们的概率都为$\footnotesize\dfrac{1}{3}$，既然它们的概率之和为 1 ，那么其他句子的概率自然为 0 了，这就是语言模型。然而 $p(w)$ 的计算非常难：句子数量无穷无尽，无法枚举。即便是大型语料库，也只能“枚举”有限的数百万个句子。实际遇到的句子大部分都在语料库之外，意味着它们的概率都被当作 0，这种现象被称为<strong>数据稀疏</strong>。枚举不可行，我们需要一种可计算的、更合理的概率估计方法。</p>
<p>考虑到句子由单词构成，句子无限，单词有限。于是我们从单词构成句子的角度出发去建模句子，把句子表示为单词列表 $\textbf{w}=w_1w_2…w_k$，每个 $w_t,\footnotesize t\in[1,k]$都是一个单词，然后定义<strong>语言模型</strong>：</p>
<script type="math/tex; mode=display">
\begin{aligned}
p(\textbf{w})&=p(w_1w_2...w_k)\\&=p(w_1|w_0)\times p(w_2|w_0w_1)\times...\times p(w_{k+1}|w_0w_1...w_k)\\&=
\prod\limits _{t=1}^{k+1}p(w_{t}|w_0w_1...w_{t-1})
\end{aligned}</script><p>其中，$w_0$=BOS（begin of sentence，或\<s\>），$w_{k+1}$=EOS（end of sentence，或\&lt;/s>），用来标记句子首尾两个特殊“单词”。<br>也就是说，语言模型模拟说话顺序：给定已经说出口的词语序列，预测下一个词语的后验概率。一个单词一个单词地乘上后验概率，我们就能估计任意一句话的概率。以极大似然估计来计算每个后验概率，即：</p>
<script type="math/tex; mode=display">
\begin{aligned}
p(w_{t}|w_0w_1...w_{t-1})=p_{MLE}(w_{t}|w_0w_1...w_{t-1})=\dfrac{c(w_0...w_t)}{c(w_0...w_{t-1})}
\end{aligned}</script><p>其中，$c(w_0…w_t)$表示$w_0…w_t$的计数。</p>
<p>以上面小型语料库为例，计算$p(\textnormal{\footnotesize商品 和 服务})$出现的概率？<br>（1）$p(\textnormal{\footnotesize商品|BOS}) =\frac{2}{3}$（“商品”作为第一个词出现的次数为2，所有单词作为第一个词出现的次数为3）；<br>（2）$p(\textnormal{\footnotesize和|BOS 商品}) =\frac{1}{2}$（“BOS 商品 和”出现的次数为1，“BOS 商品”出现的次数为2）；<br>（3）$p(\textnormal{\footnotesize 服务|BOS 商品 和}) =\frac{1}{1}$（“BOS 商品 和 服务”出现的次数为1，“BOS 商品 和”出现的次数为1）；<br>（4）$p(\textnormal{\footnotesize EOS|BOS 商品 和 服务}) =\frac{1}{1}$（“BOS 商品 和 服务 EOS”出现的次数为1，“BOS 商品 和 服务”出现的次数为1）；<br>整个句子的概率是4者乘积：$p(\textnormal{\footnotesize商品 和 服务})=\frac{2}{3}\times \frac{1}{2}\times \frac{1}{1}\times \frac{1}{1}=\frac{1}{3}$。</p>
<p>但是随着句子长度增大，语言模型会遇到如下问题：<br>（1）<strong>数据稀疏</strong>。指长度越大的句子越难出现，语料库中极有可能统计不到长句子的频次，导致$p(w_{t}|w_0w_1…w_{t-1})$为0。<br>（2）<strong>计算代价大</strong>。t越大，需要存储的$p(w_{t}|w_0w_1…w_{t-1})$就越多。</p>
<h2 id="n元语法"><a href="#n元语法" class="headerlink" title="n元语法"></a>n元语法</h2><p>为了解决上面两个问题，使用<strong>马尔可夫假设</strong>（Markov Assumption）来简化语言模型：给定时间线上有一串事件顺序发生，假设每个事件的发生概率只取决于前一个事件，那么这串事件构成的因果链被称作<strong>马尔可夫链</strong>。</p>
<p>在语言模型中，第t个事件指的是$w_t$作为第t个单词出现。也就是说，马尔科夫链假设每个单词出现的概率只取决于前一个单词：</p>
<script type="math/tex; mode=display">
\begin{aligned}
p(w_{t}|w_0w_1...w_{t-1})=p(w_t|w_{t-1})
\end{aligned}</script><p>基于此假设，需要计算的量一下子减少了不少，由于每次计算只涉及连续两个单词的二元接续，所以此语言模型称为<strong>二元语法</strong>（bi-gram），也叫<strong>一阶马尔科夫链</strong>：</p>
<script type="math/tex; mode=display">
\begin{aligned}
p(\textbf{w})&=p(w_1w_2...w_k)\\&=p(w_1|w_0)\times p(w_2|w_1)\times...\times p(w_{k+1}|w_k)\\&=
\prod\limits _{t=1}^{k+1}p(w_{t}|w_{t-1})
\end{aligned}</script><p>那么根据这个思路推广下，可以得到<strong>n元语法</strong>（n-gram）的定义：每个单词出现的概率，仅取决于该单词之前n个单词，即：</p>
<script type="math/tex; mode=display">
\begin{aligned}
p(\textbf{w})=\prod\limits_{t=1}^{k+n-1}p(w_{t}|w_{t-(n-1)}...w_{t-1})
\end{aligned}</script><p>当$\text{n=1}$时（即出现在第t位的词$w_t$独立于历史时），称为<strong>一元语法</strong>（uni-gram）；当$\text{n=2}$时（即出现在第t位的词$w_t$仅与它前面的一个历史词$w_{t-1}$有关），称为<strong>二元语法</strong>（bi-gram），也叫<strong>一阶马尔科夫链</strong>；当$\text{n=3}$时（即出现在第t位的词$w_t$仅与它前面的两个历史词$w_{t-1}w_{t-2}$有关），称为<strong>三元语法</strong>（tri-gram），也叫<strong>二阶马尔科夫链</strong>。当$n\geqslant 4$时数据稀疏和计算代价又变的显著了，实际工程中几乎不使用。另外，深度学习带了一种递归神经网络语言模型（RNN Language Model），理论上可以记忆无限个单词，可以看作“无穷元语法”（∞-gram）。</p>
<p>以<strong>二元语法</strong>（bi-gram）为例，计算$p(\textnormal{\footnotesize商品 和 服务})$出现的概率？<br>（1）$p(\textnormal{\footnotesize商品|BOS}) =\frac{2}{3}$；<br>（2）$p(\textnormal{\footnotesize和|商品}) =\frac{1}{2}$；<br>（3）$p(\textnormal{\footnotesize 服务|和}) =\frac{1}{2}$；<br>（4）$p(\textnormal{\footnotesize EOS|服务}) =\frac{1}{1}$；<br>整个句子的概率是4者乘积：$p(\textnormal{\footnotesize商品 和 服务})=\frac{2}{3}\times \frac{1}{2}\times \frac{1}{2}\times \frac{1}{1}=\frac{1}{6}$。</p>
<p>这次的概率比上次的$\footnotesize\dfrac{1}{3}$要小一半，剩下的概率到哪里去了呢？来算算语料库之外的新句子$p(\textnormal{\footnotesize商品 和 货币})$就知道了：<br>（1）$p(\textnormal{\footnotesize商品|BOS}) =\frac{2}{3}$；<br>（2）$p(\textnormal{\footnotesize和|商品}) =\frac{1}{2}$；<br>（3）$p(\textnormal{\footnotesize 货币|和}) =\frac{1}{2}$；<br>（4）$p(\textnormal{\footnotesize EOS|货币}) =\frac{1}{1}$；<br>整个句子的概率是4者乘积：$p(\textnormal{\footnotesize商品 和 货币})=\frac{2}{3}\times \frac{1}{2}\times \frac{1}{2}\times \frac{1}{1}=\frac{1}{6}$。<br>原来剩下的$\footnotesize\dfrac{1}{6}$分配给了语料库之外的句子，它们的概率终于不是0了，这样就缓解了一部分数据稀疏的问题。</p>
<h2 id="模型评估：困惑度"><a href="#模型评估：困惑度" class="headerlink" title="模型评估：困惑度"></a>模型评估：困惑度</h2><p>在理想情况下，对两个语言模型A，B进行评估，选定一个特定的任务比如拼写纠错系统，把两个模型A，B都应用在此任务中，最后比较准确率，从而判断A，B的表现。这种评估方法是以应用为中心的度量方法，通过在下游任务中的性能来进行评估。那有没有更简单的评估方法？不需要放在特定的任务中验证？——————<strong>困惑度</strong>（Perplexity）。</p>
<p><strong>困惑度</strong>（Perplexity）是一种信息论测度，用来测量一个概率模型预测样本的好坏，困惑度越低越好。给定一个包含k个词的文本预料$\textbf{w}=w_1w_2…w_k$，和一个基于历史行为的语言模型，其预测结果为$p(\textbf{w})$，则这个语言模型在这个语料的困惑度是：</p>
<script type="math/tex; mode=display">
\begin{aligned}
pp(\textbf{w})=2^{-\footnotesize\dfrac{1}{k}\log p(\textbf{w})}
\end{aligned}</script><p>以二元语法（bi-gram）为例，此时困惑度可以表示为：</p>
<script type="math/tex; mode=display">
\begin{aligned}
pp(\textbf{w})&=2^{-\footnotesize\dfrac{1}{k}\log p(\textbf{w})}\\&=2^{-\footnotesize\dfrac{1}{k}\log \prod\limits _{t=1}^{k+1}p(w_{t}|w_{t-1})}\\&=
2^{-\footnotesize\dfrac{1}{k}\sum\limits_{t=1}^{k+1}\log p(w_{t}|w_{t-1})}
\end{aligned}</script><p>计算”商品 和 服务”的困惑度？<br>（1）$p(\textnormal{\footnotesize商品|BOS}) =\frac{2}{3}$；<br>（2）$p(\textnormal{\footnotesize和|商品}) =\frac{1}{2}$；<br>（3）$p(\textnormal{\footnotesize 服务|和}) =\frac{1}{2}$；<br>（4）$p(\textnormal{\footnotesize EOS|服务}) =\frac{1}{1}$；<br>整个句子的困惑度：$pp(\textnormal{\footnotesize商品 和 服务})=2^{-\footnotesize\dfrac{1}{3}(log\frac{2}{3}+log\frac{1}{2}+log\frac{1}{2}+log\frac{1}{1})}$。</p>
<h2 id="数据平滑"><a href="#数据平滑" class="headerlink" title="数据平滑"></a>数据平滑</h2><p>n元语法虽然有效，但它有一大不足，以二元语法为例，如果$c(w_tw_{t-1})\text{=0}$，因为计算句子概率时的乘法计算，导致整个语料的0-概率分配。0概率会造成非常大的困惑度，这是一种很糟糕的情况。一种避免0-概率事件的方法是使用平滑技术，确保为每个可能的情况都分配一个概率（可能非常小）。</p>
<h3 id="加法平滑"><a href="#加法平滑" class="headerlink" title="加法平滑"></a>加法平滑</h3><p>最简单的一类方法是<strong>加法平滑</strong>（Additive Smoothing），以下公式都以二元语法（bi-gram）为例。</p>
<p>不加平滑时：</p>
<script type="math/tex; mode=display">
\begin{aligned}
p_{\textnormal{\tiny MLE}}(w_tw_{t-1})=\dfrac{c(w_{t-1}w_t)}{c(w_t)}
\end{aligned}</script><p><strong>加一平滑</strong>（Add-one Smoothing/Laplace Smoothing）：</p>
<script type="math/tex; mode=display">
\begin{aligned}
p_{\textnormal{\tiny add-one}}(w_t|w_{t-1})=\dfrac{c(w_{t-1}w_t)+\textcolor{red}{1}}{c(w_t)+\textcolor{red}{V}}, \quad \textnormal{\textcolor{red}{V}\footnotesize是词表大小，即语料库中所有单词去重的总数}
\end{aligned}</script><p><strong>加K平滑</strong>（Add-K Smoothing/Laplace Smoothing）：</p>
<script type="math/tex; mode=display">
\begin{aligned}
p_{\textnormal{\tiny add-k}}(w_t|w_{t-1})=\dfrac{c(w_{t-1}w_t)+\textcolor{red}{k}}{c(w_t)+\textcolor{red}{kV}}, \quad \textnormal{\textcolor{red}{V}\footnotesize是词表大小，即语料库中所有单词去重的总数}
\end{aligned}</script><h3 id="插值法"><a href="#插值法" class="headerlink" title="插值法"></a>插值法</h3><p>另外一类方法使用back-off策略，即如果没有观测到n元语法，那么就基于n-1元语法计算，利用低阶n元语法平滑高阶n元语法，这就产生了很多方案，最简单的一种是<strong>线性插值法</strong>（Linear Interpolation）。</p>
<p>三元语法（tri-gram）的线性插值法：</p>
<script type="math/tex; mode=display">
\begin{aligned}
p_{\textnormal{\tiny Int}}(w_t|w_{t-2}w_{t-1})=\lambda_1 p(w_t|w_{t-2}w_{t-1})+\lambda_2p(w_t|w_{t-1})+\lambda_3p(w_t),\quad \lambda_1+\lambda_2+\lambda_3\textnormal{=1}
\end{aligned}</script><p>二元语法（bi-gram）的线性插值法：</p>
<script type="math/tex; mode=display">
\begin{aligned}
p_{\textnormal{\tiny Int}}(w_t|w_{t-1})=\lambda_1 p(w_t|w_{t-1})+\lambda_2 p(w_t),\quad \lambda_1+\lambda_2\textnormal{=1}
\end{aligned}</script><p>一元语法（uni-gram）的线性插值法：</p>
<script type="math/tex; mode=display">
\begin{aligned}
p_{\textnormal{\tiny Int}}(w_t)=\lambda_1 p(w_t)+\lambda_2\frac{1}{V},\quad \lambda_1+\lambda_2\textnormal{=1}
\end{aligned}</script><p>其中，V是词表大小，即语料库中所有单词去重的总数。</p>
<h3 id="古德-图灵平滑"><a href="#古德-图灵平滑" class="headerlink" title="古德-图灵平滑"></a>古德-图灵平滑</h3><p><strong>古德-图灵平滑</strong>（Good-Turing Smoothing）：对于任何一个出现 $r$ 次n元语法，都假设它出现了$r^*$次：</p>
<script type="math/tex; mode=display">
\begin{aligned}
r^*=\frac{(r+1)N_{r+1}}{N_r} ,\quad  N_r\textnormal{\footnotesize表示训练预料中出现}r\textnormal{\footnotesize次的n元语法的数目}
\end{aligned}</script><p>要把整个统计数转化为概率，只需要进行归一化处理：对于统计数为$r$的n元语法，其概率为：</p>
<script type="math/tex; mode=display">
\begin{aligned}
p_r=\frac{r^*}{N}=\frac{(r+1)N_{r+1}}{N_r*N},\quad \footnotesize \textnormal{N=}\sum\limits_{r=1}^{\infty}r^* N_r
\end{aligned}</script><p>注意到：</p>
<script type="math/tex; mode=display">
\begin{aligned}
\textnormal{N=}\sum\limits_{r=1}^{\infty}r^* N_r=\sum\limits_{r=1}^{\infty}(r+1)N_{r+1}=\sum\limits_{r=1}^{\infty}rN_r
\end{aligned}</script><p>也就是说，N等于整个分布中的最初的计数。这样，样本中所有事件的概率之和为：</p>
<script type="math/tex; mode=display">
\begin{aligned}
\sum\limits_{r>0}N_rp_r=1-\frac{N_1}{N}<1
\end{aligned}</script><p>因此，有$\footnotesize N_1/N$的概率剩余量可以分配给所有未见事件（$\textnormal{r=0}$的事件）。</p>
<p>但古德-图灵平滑也有其缺陷，比如某一个 $\footnotesize N_{r+1}$ 为0，此时就无法计算 $p_{r}$ 了，一般这种情况，我们使用机器学习算法去拟合 $\footnotesize N_{r}$，这样就可以把缺失的部分补上。</p>
<h1 id="词向量"><a href="#词向量" class="headerlink" title="词向量"></a>词向量</h1><p>主要考虑单词或句子，甚至是文章的表示，一般把它们进行向量化处理。</p>
<h2 id="相似度"><a href="#相似度" class="headerlink" title="相似度"></a>相似度</h2><h3 id="距离"><a href="#距离" class="headerlink" title="距离"></a>距离</h3><p>单词/句子用向量表示后，可以计算它们的<strong>距离</strong>来判断相似度（距离越大，相似度越低），$i$为向量下标：<br>（1）<strong>欧氏距离</strong>：$d=||A-B||_2=\sqrt{\sum \limits_{i=1}^n(A_{i} - B_{i})^2}$，两个点的直线距离。<br>（2）<strong>曼哈顿距离</strong>：$d=||A-B||_1=\sum \limits_{i=1}^{n}|A_{i}-B_{i}|$，各个维度的长度差进行累加。常用计算城市间到达距离计算。<br>（3）<strong>闵科夫斯基距离</strong>：$d=||A-B||_P=\sqrt[p]{\sum \limits_{i=1}^n|A_{i}-B_{i}|^p}$，可以根据p来决定距离，如果p=1就是曼哈顿距离；p=2就是欧氏距离；当p趋近无穷时，就会变为长度差最大那个距离。</p>
<h3 id="方向"><a href="#方向" class="headerlink" title="方向"></a>方向</h3><p>但是向量不光有大小还有<strong>方向</strong>的，两个向量之间是有夹角的，从这个角度发现了<strong>余弦相似度</strong>（值越大，相似度越高），$i$为向量下标：</p>
<script type="math/tex; mode=display">
\begin{aligned}
cos(A,B)=\frac{A \cdot B}{|A||B|}=\frac{\sum \limits_{i=1}^{n}A_iB_i}{\sqrt{\sum \limits_{i=1}^{n}A_i^2}\sqrt{\sum \limits_{i=1}^{n}B_i^2}}
\end{aligned}</script><p>余弦相似度的<strong>取值范围是[-1, 1]，相同的两个向量之间的相似度为1</strong>。</p>
<p>当一对文本相似度的长度差距很大、但内容相近时，如果使用词频/词向量作为特征：<br>（1）如果使用欧氏距离的话，它们在特定空间中的欧氏距离通常很大，因而相似度低；<br>（2）而如果使用余弦相似度的话，它们之间的夹角可能很小，因而相似度高。<br>在高维情况下：<br>（1）余弦相似度依然保持“<strong>相同时为1，正交时为0，相反时为-1</strong>”的性质；<br>（2）而欧式距离的数值则受维度的影响，范围不固定，并且含义也比较模糊。</p>
<p>如果希望得到类似于距离的表示，使用$\textbf{1-cos(A,B)}$即为<strong>余弦距离</strong>，<strong>其取值范围是[0, 2]，相同的两个向量余弦距离为0</strong>。<br>在一些场景，比如word2vec中，其向量的模长是经过归一化的，此时欧式距离与余弦距离有着单调的关系：</p>
<script type="math/tex; mode=display">
\begin{aligned}
||A-B||_2=\sqrt{2(1-cos(A,B))}
\end{aligned}</script><p>其中，$||A-B||_2$表示欧氏距离，$cos(A,B)$表示余弦相似度，$1-cos(A,B)$表示余弦距离。此时，如果选择距离小的（相似度最大）的近邻，那么使用余弦相似度和欧氏距离的结果是相同的。<br>总的来说：<strong>欧氏距离体现数值上的绝对差异，余弦距离体现方向上的相对差异。</strong></p>
<h2 id="One-Hot"><a href="#One-Hot" class="headerlink" title="One-Hot"></a>One-Hot</h2><p><strong>词袋模型</strong>（Bag-of-words model）：将所有词语装进一个袋子里，不考虑其词法和语序的问题，即每个词语都是独立的。最简单的一种就是<strong>独热表示</strong>（One-Hot Representation）。<br>假设，词典：[是，天空，蓝色，的]。<strong>每个单词的表示</strong>：<br>“是”　——&gt;[1, 0, 0, 0]<br>“天空”——&gt;[0, 1, 0, 0]<br>“蓝色”——&gt;[0, 0, 1, 0]<br>“的”　——&gt;[0, 0, 0, 1]<br>向量的维度等于词典的的大小。<br><div class="note info"><p>利用One-Hot表示法无法表达<strong>单词</strong>之间的相似度！不管用欧氏距离（任意两个词的相似度计算结果都相同）还是余弦相似度（任意两个词的相似度计算结果都是0）。</p>
<p>One-Hot表示单词/句子的缺点：<br>（1）<strong>稀疏性</strong>（Sparsity）：如果词典非常大，维度就会很大，而一个句子可能只有很少的词，导致出现很多0，造成稀疏问题。核心问题是维度太大。<br>（2）<strong>弱语义</strong>（Semantically Weak）：无法表达词与词之间（语义）的相似度，因为One-Hot表示的单词向量是正交的。核心问题是每个单词的向量只能有一个有效值（local representation），且取值只能是{0,1}。</p>
</div></p>
<h3 id="boolean"><a href="#boolean" class="headerlink" title="boolean"></a>boolean</h3><p><strong>每个句子的表示（boolean）</strong>，向量对应的下标与字典的下标相匹配，其值为该词语出现为1，没出现为0：<br>“天空 是 蓝色”——&gt;[1, 1, 1, 0]<br>“蓝色 是 蓝色”——&gt;[1, 0, 1, 0]</p>
<h3 id="count"><a href="#count" class="headerlink" title="count"></a>count</h3><p><strong>每个句子的表示（count）</strong>，向量对应的下标与字典的下标相匹配，其值为该词语出现的次数：<br>“天空 是 蓝色”——&gt;[1, 1, 1, 0]<br>“蓝色 是 蓝色”——&gt;[1, 0, 2, 0]</p>
<h3 id="TF-IDF"><a href="#TF-IDF" class="headerlink" title="TF-IDF"></a>TF-IDF</h3><p>一句话中每个词的重要程度是不同的，但boolean（每个单词权重相同）和count（出现次数越多不一定越重要）都不合理。由此考虑到新的计算方式————<strong>TF-IDF</strong>。<br><strong>TF-IDF</strong>（term frequency–inverse document frequency）是一种用于信息检索与文本挖掘的常用加权技术。用以评估一个词，对于一个文件集或一个语料库中的其中一份文件的重要程度。词的重要性随着它在文件中出现的次数成正比增加，但同时会随着它在语料库中出现的频率成反比下降。其公式为：</p>
<script type="math/tex; mode=display">
\begin{aligned}
\text{TF-IDF(t,d)}=\text{TF(t,d)}\times \text{IDF(t)}
\end{aligned}</script><p>在一份给定的文件里，<strong>词频</strong>（term frequency，TF）指的是某一个给定的词语在该文件中出现的频率。这个数字是对<strong>词数</strong>（term count）的归一化，以防止它偏向长的文件。对于某一特定文件 $d_j$ 里的词语 $t_i$ 来说，它的词频（在本文件的重要程度）可表示为：</p>
<script type="math/tex; mode=display">
\begin{aligned}
\text{TF}(t_i,d_j)=\frac{n_{i,j}}{\sum_k n_{k,j}}
\end{aligned}</script><p>其中，$n_{i,j}$ 是该词在文件 $d_j$ 中的出现次数，而分母则是在文件 $d_j$ 中所有字词的出现次数之和。<br>有时 $\footnotesize\text{TF}(t_i,d_j)$ 也可以直接采用词频 $n_{i,j}$ 计算，不进行归一化处理。</p>
<p><strong>逆向文件频率</strong>（inverse document frequency，IDF）是一个词语普遍重要性的度量（在整体文件的重要程度，和文件频率反比关系）。某一特定词语的IDF———总文件数目除以包含该词语的文件数目，再取对数（防止它的值过大）：</p>
<script type="math/tex; mode=display">
\begin{aligned}
\text{IDF}(t_i)=log\frac{|D|}{|1+\{j:t_i\in d_j\}|}
\end{aligned}</script><p>其中，$|D|$ 是语料库中文件总数，$\{j:t_i\in d_j\}$ 是包含词语 $t_i$ 的文件数目（如果词语不存在资料库中，按 1 处理）。</p>
<p>假设，词典：[是，天空，蓝色，的]，语料库：[“天空 是 蓝色”, “蓝色 是 蓝色”]。<br><strong>每个句子的表示（TF-IDF）</strong>，向量对应的下标与字典的下标相匹配，其值为该词语的TF-IDF（TF按词频计算）：<br>“天空 是 蓝色”——&gt;$[1·\log\frac{2}{1}, 1·\log\frac{2}{2}, 1·\log\frac{2}{2}, 0]$<br>“蓝色 是 蓝色”——&gt;$[1\log\frac{2}{1}, 0, 2·\log\frac{2}{2}, 0]$</p>
<h2 id="word2vec"><a href="#word2vec" class="headerlink" title="word2vec"></a>word2vec</h2><p>之前我们说了One-Hot表示方法有<strong>稀疏性</strong>、<strong>弱语义</strong>缺点，那么如何解决这些问题？————分布式表示。</p>
<p><strong>分布式表示</strong>（Distributed Representation）的思路是：通过训练，将每个词用<strong>低维度</strong>的向量表示（解决稀疏性/高维度问题，维度不再依赖字典长度），并且每个单词的向量<strong>有多个有效值</strong>（global representation），每个维度上的有效值不再是{0,1}，而是介于[0,1]的值（解决弱语义问题，可计算相似度）。这种把词映射到低维的向量表示，也叫做<strong>词嵌入</strong>（word embedding）。对于句子表示，可以使用平均策略，即句子中所有词的向量求和，再取均值。</p>
<p><strong><span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL3RtaWtvbG92L3dvcmQydmVj">word2vec<i class="fa fa-external-link-alt"></i></span></strong> 就是分布式表示方法的一种，它将词的语义表示为训练语料库中上下文的向量。它根据输入和输出的不同分为<strong>CBOW</strong>（Continuous Bag-Of-Words）和<strong>Skip-Gram</strong>两种模型。而且word2vec对这两种方法进行了优化，从而得到<strong>Hierarchical Softmax</strong>模型和<strong>Negative Sampling</strong>模型。<br><img src="/images/语言模型和词向量/CBOW和Skip-gram.png" width="80%" height="80%"></p>
<p><strong>CBOW</strong>：基于上下文词（输入）预测中心词（输出）。<br><strong>Skip-Gram</strong>：基于中心词（输入）预测上下文词（输出）。</p>
<h3 id="Skip-Gram"><a href="#Skip-Gram" class="headerlink" title="Skip-Gram"></a>Skip-Gram</h3><p><strong>Skip-Gram</strong>核心思想是：用中心词（输入）预测上下文词（输出）。输入的中心词使用One-Hot向量表示，引入一个大小为 $c$ 的窗口，那么上下文词就是由中心词左 $c$ 个词和右 $c$ 个词构成（一般用$2c$表示上下文）我们希望模型输出的就是这些上下文词，而通过<strong>神经网络输出+softmax</strong>计算得到的是所有词的概率，那么只需要优化上下文词的概率最大即可达到我们的目的。</p>
<p>把这个核心思想转化成数学表示，假设有如下一句话：</p>
<script type="math/tex; mode=display">
[w_1...w_{t-1}w_tw_{t+1}...w_V]</script><p>其中 $w_t$ 代表第 $t$ 个词，总共有$\footnotesize V$个词。我们要计算的就是：</p>
<script type="math/tex; mode=display">
\prod\limits_{t=1}^Vp(\text{context}(w_t)|w_t)</script><p>每一个 $p(\text{context}(w_t)|w_t)$ 是相互独立的。<br>此时引入窗口参数 $i\in \text{[-c,c]}$，$c$ 为窗口大小。中心词 $w_t$ 的上下文词 $\text{context}(w_t)$ 就是其左 $c$ 个词和右 $c$ 个词构成，上式变为：</p>
<script type="math/tex; mode=display">
\begin{aligned}
\prod\limits_{t=1}^V \prod\limits_{i=-c}^c p(w_{t+i}|w_t)
\end{aligned}</script><p>每一个 $p(\text{context}(w_i)|w_t)$ 是相互独立且同分布的。<br>为了方便计算，转成$log$（其实就是对数损失函数），并且取均值：</p>
<script type="math/tex; mode=display">
\begin{aligned}
\dfrac{1}{\text{V}} \sum\limits_{t=1}^V \sum\limits_{i=-c}^c \log p(w_{t+i}|w_t)
\end{aligned}</script><p>我们的目标函数就是引入参数 $\theta$，使整个式子最大化：</p>
<script type="math/tex; mode=display">
\begin{aligned}
\mathop{argmax}\limits_{\theta}\dfrac{1}{\text{V}} \sum\limits_{t=1}^V \sum\limits_{i=-c}^c \log p(w_{t+i}|w_t;\theta)
=\mathop{argmin}\limits_{\theta}-\dfrac{1}{\text{V}} \sum\limits_{t=1}^V \sum\limits_{i=-c}^c \log p(w_{t+i}|w_t;\theta)
\end{aligned}</script><p>上式其实就是我们的优化函数$J(\theta)$，这里为了方便优化计算，最大化转成了最小化，而引入的参数 $\theta$ 其实就是我们要找的<strong>词向量</strong>。</p>
<hr>
<p>那么如何计算呢？一般采用的方法是一个三层的神经网络结构，分为输入层，隐藏层和输出层（softmax层）。<br>这个神经网络计算的是<strong>一个词</strong>（输入）预测<strong>所有词</strong>（输出）的情况：<br><img src="/images/语言模型和词向量/one-word.png" width="60%" height="60%"></p>
<p>$\text{V}$：词汇表的长度;<br>$\text{N}$：隐层神经元个数（词向量维度，需要我们自己指定）;<br>$\text{W}$：输入层到隐层的权重矩阵（词向量矩阵，每一行代表一个词的词向量），维度是$\small [V,N]$;<br>$\text{W}^\prime$：隐层到输出层的权重矩阵（词向量矩阵，每一列代表一个词的词向量），维度是$\small [N,V]$;</p>
<p>我们需要做的是用输入的词去预测输出的词（方便书写这里用行向量表示一个词）：<br>（1）输入层的一个单词 $w_t$ 使用One-Hot表示：</p>
<script type="math/tex; mode=display">
w_t=[x_1...x_t...x_V]</script><p>其中，只有 $x_t$ 为1，其余为0，其中t是输入单词在词汇表中的索引下标，它的维度是$\small [1,V]$。<br>（2）输入的词 $w_t$ 和词向量矩阵 $\footnotesize W$ 相乘，得到一个维度为$\small [1,N]$的隐层向量 $h$。此过程可看作从词向量矩阵 $\footnotesize W$取对应的词向量。</p>
<script type="math/tex; mode=display">
\begin{aligned}
h=w_t\cdot\text{W}
\end{aligned}</script><p>（3）隐层向量 $h$ 和 词向量矩阵 $\footnotesize W^\prime$ 相乘，得到一个维度为$\small [1,V]$的输出向量 $y$。此过程可看作计算当前词向量和所有词向量的相似度。从这个过程可看出<strong>word2vec中隐藏层没有用激活函数</strong>。</p>
<script type="math/tex; mode=display">
\begin{aligned}
y=h\cdot\text{W}^\prime
\end{aligned}</script><p>（4）输出向量 $y$ 再通过softmax计算，从而得到概率。此过程可看作把相似度转成概率，即向量 $y$ 的每个值是当前词向量和另一个词向量相似的概率。下面公式是预测一个词的概率：</p>
<script type="math/tex; mode=display">
\normalsize p(w_{i,i\ne k}|w_t)=p(w_{i,i\ne k}|y)=\dfrac{e^{y_i}}{\sum\limits_{j=1}^V e^{y_j}}</script><p>其中，$w_i$ 代表词表中第 $i$ 个词且$i\mathbb{\ne} t$（非中心词）。$y_i$为在原始输出向量$y$中，与单词$w_i$所对应的维度取值。$y$向量通过softamx计算出来就是当前词$w_t$和所有词的相似概率。<br>为什么是softmax？因为其值域是$[0,1]$，且所有结果的和为$1$。符合我们想要得到概率的目的。</p>
<hr>
<p><img src="/images/语言模型和词向量/skip-gram.png" width="40%"></p>
<p>那么<strong>Skip-Gram</strong>是怎么计算的呢？回到核心思想：用<strong>中心词</strong>（输入）预测<strong>上下文词</strong>（输出）。<br>它引入了窗口 $c$ ，上下文词就是$2c$（左 $c$ 个词和右 $c$ 个词构成），目标是给定一个词$w_t$预测上下文词的概率最大化。以下是一个词$w_t$的优化公式：</p>
<script type="math/tex; mode=display">
\normalsize p(w_{t+i,i\in[-c,c]}|w_t)=\dfrac{e^{y_i}}{\sum\limits_{j=1}^V e^{y_j}}</script><script type="math/tex; mode=display">
\normalsize \mathop{argmax}\limits_{W,W^{\prime}} \prod\limits_{i=-c}^{c} p(w_{t+i}|w_t;\scriptsize{W,W^{\prime}})\normalsize
=\mathop{min}\limits_{W,W^{\prime}} \sum\limits_{i=-c}^{c} -logp(w_{t+i}|w_t;\scriptsize{W,W^{\prime}})</script><p>把所有中心词都训练一遍，就是以下公式：</p>
<script type="math/tex; mode=display">
\begin{aligned}
\mathop{argmax}\limits_{W,W^{\prime}}\dfrac{1}{\text{V}} \sum\limits_{t=1}^V \sum\limits_{i=-c}^c \log p(w_{t+i}|w_t;\scriptsize{W,W^{\prime}})\normalsize
=\mathop{argmin}\limits_{W,W^{\prime}}-\dfrac{1}{\text{V}} \sum\limits_{t=1}^V \sum\limits_{i=-c}^c \log p(w_{t+i}|w_t;\scriptsize{W,W^{\prime}})
\end{aligned}</script><p>其中，损失函数选择对数损失函数（$log$），全局损失定义为所有训练样本上的平均损失。<br>上式损失函数优化过程就可以通过反向传播方法优化（基于梯度的优化），这里不再赘述。所有的中心词都训练一遍后，得到的$\footnotesize\text{W}$和$\footnotesize\text{W}^\prime$就是我们需要的词向量，可选其中一个作为V个词的N维向量表示（word2vec中一般选择$\footnotesize\text{W}$作为词向量）。</p>
<h3 id="CBOW"><a href="#CBOW" class="headerlink" title="CBOW"></a>CBOW</h3><p><img src="/images/语言模型和词向量/cbow.png" width="40%"></p>
<p><strong>CBOW</strong>核心思想：用上下文词（输入）预测中心词（输出）。<br>（1）输入是多个词（上下文词）的One-Hot表示。在计算隐层的向量前，对输入向量和取均值即可。</p>
<script type="math/tex; mode=display">
\begin{aligned}
h=\frac{\sum\limits_{i=-c}^c w_{t+i}\cdot W}{2c}=\frac{\sum\limits_{i=-c}^c h_{t+i}}{2c}
\end{aligned}</script><p>（2）输出是上下文词向量和某一个词向量相似的概率，使中心词概率最大即可。以下是一个词$w_t$的优化公式：</p>
<script type="math/tex; mode=display">
\normalsize \mathop{argmax}\limits_{W,W^{\prime}} p(w_t|w_{t+i,i\in[-c,c]};\scriptsize{W,W^{\prime}})\normalsize
=\mathop{argmin}\limits_{W,W^{\prime}} -logp(w_t|w_{t+i,i\in[-c,c]};\scriptsize{W,W^{\prime}})</script><p>把所有中心词的上下文词都训练一遍，就是以下公式：</p>
<script type="math/tex; mode=display">
\begin{aligned}
\mathop{argmax}\limits_{W,W^{\prime}}\dfrac{1}{\text{V}} \sum\limits_{t=1}^V \sum\limits_{i=-c}^c \log p(w_t|w_{t+i};\scriptsize{W,W^{\prime}})\normalsize
=\mathop{argmin}\limits_{W,W^{\prime}}-\dfrac{1}{\text{V}} \sum\limits_{t=1}^V \sum\limits_{i=-c}^c \log p(w_t|w_{t+i};\scriptsize{W,W^{\prime}})
\end{aligned}</script><h3 id="Hierarchical-Softmax"><a href="#Hierarchical-Softmax" class="headerlink" title="Hierarchical Softmax"></a>Hierarchical Softmax</h3><p>word2vec也是用了CBOW与Skip-Gram来训练模型与得到词向量，但没有使用神经网络结构，而是使用<strong>霍夫曼树</strong>（Huffman）来替代隐藏层到输出层的过程。</p>
<hr>
<p>我们先来复习下<strong>霍夫曼树</strong>，其特点是<strong>带权路径最短</strong>。首先明确一些概念：<br>（1）<strong>路径</strong>：指从树种一个结点到另一个结点的分支所构成的路线。<br>（2）<strong>路径长度</strong>：指路径上的分支数目。<br>（3）<strong>树的路径长度</strong>：指从根到每个结点的路径长度之和。<br>（4）<strong>带权路径长度</strong>：结点具有权值，从该结点到根之间的路径长度乘以结点的权值，就是该结点的带权路径长度。<br>（5）<strong>树的带权路径长度</strong>（WPL）：指树中所有叶子结点的带权路径长度之和。</p>
<p><strong>霍夫曼树的构造方法</strong>（霍夫曼树可以是n叉树，我们主要以二叉树为例）<br>给定$n$个权值，用这$n$个权值构造霍夫曼树的算法如下：<br>（1）将这个$n$个权值分别看作只有根节点的n棵二叉树，这些二叉树构成的集合记为$\small F$。<br>（2）从$\small F$中选出两棵根节点的权值最小的数（假设为$a$、$b$），作为左、右子树，构造一棵新的二叉树（假设为$c$），新的二叉树的根节点权值为左、右子树根节点权值之和。<br>（3）从F中删除$a$、$b$，加入新构造的树$c$。<br>（4）重复（2）（3）两步，直到$\small F$中只剩下一棵树为止，这棵树就是霍夫曼树。</p>
<p>一个简单的例子：“this is an example of a huffman tree” 中得到的字母频率（权重）来建构霍夫曼树。<br><img src="/images/语言模型和词向量/huffman.png" width="60%"></p>
<p><strong>霍夫曼树特点</strong>：<br>（1）权重（频率）越大的结点，距离根结点越近。<br>（2）树的带权路径长度最短。</p>
<p><strong>霍夫曼编码</strong>：<br>一般得到霍夫曼树后我们会对叶子节点进行霍夫曼编码，由于权重高的叶子节点越靠近根节点，而权重低的叶子节点会远离根节点，这样我们的高权重节点编码值较短，而低权重值编码值较长。这保证的树的带权路径最短，也符合我们的信息论，即我们希望越常用的词拥有更短的编码。如何编码呢？一般对于一个霍夫曼树的节点（根节点除外），可以约定<strong>左子树编码为0</strong>，<strong>右子树编码为1</strong>。如下图所示：<br><img src="/images/语言模型和词向量/huffman1.png" width="60%"></p>
<hr>
<p><strong>word2vec中，霍夫曼编码方式和正常的相反，即约定沿着左子树走编码为1（负类），沿着右子树走编码为0（正类），同时约定左子树的权重不小于右子树的权重。</strong></p>
<p><strong>Hierarchical Softmax</strong>的<strong>隐藏层</strong>到<strong>输出概率</strong>的计算过程（CBOW）：<br>首先按照<strong>词频</strong>建立一棵霍夫曼树，叶子结点就是词典中的每个单词，但顺序和词典中不一定相同。假设预测的词（叶子节点）为$w$，定义一些符号：<br>（1）$p^w$：从根节点到$w$对应叶子节点的路径。<br>（2）$n^w$：路径$p^w$中包含结点个数。<br>（3）$p_1^w,p_2^w,…,p_{n^w}^w$：路径$p^w$中的$n^w$个结点，$p_1^w$表示根节点，$p_{n^w}^w$表示词$w$对应的叶子结点。<br>（4）$d_2^w,d_3^w,…,d_{n^w}^w$：路径$p^w$中的$n^w$个结点的编码，总共有$n^w\text{-1}$个，根结点不对应编码，每个编码值为$\{0,1\}$。<br>（5）$\theta_1^w,\theta_2^w,…,\theta_{n^w-1}^w$：路径$p^w$中的$n^w\text{-1}$个结点的向量（维度为$N,1$），不包括叶子结点。</p>
<p>对于词典中任意的词$w$，霍夫曼树中必存在一条从根结点到词$w$叶子节点的路径$p^w$（路径唯一）。路径$p^w$上存在$n^w\text{-1}$个分支，每个分支看做一次二分类，每一次分类就产生一个概率，将这些概率乘起来就是预测词$w$的概率，即$p(w|context(w))$。<br>由此可以给出$w$的条件概率：</p>
<script type="math/tex; mode=display">
p(w|context(w))=\prod\limits_{j=2}^{n^w}p(d_j^w|h_w;\theta_{j-1}^w)</script><p>从根节点到叶节点经过了$n^w\text{-1}$个节点，编码从下标2开始（根节点无编码），对应的参数向量下标从1开始（根节点为1）。<br>其中<strong>每个</strong>$\small p(d_j^w|h_w;\theta_{j-1}^w)$都是一个逻辑回归二分类：</p>
<script type="math/tex; mode=display">
p(d_j^w|h_w;\theta_{j-1}^w)=
\begin{cases}
   \sigma(h_w\cdot\theta_{j-1}^w), &d_j^w=0\quad\text{(正类)}\\
   1-\sigma(h_w\cdot\theta_{j-1}^w), &d_j^w=1\quad\text{(负类)}
\end{cases}</script><p>其中$h_w$是隐藏层向量，$\sigma$是sigmoid函数。<br>考虑到$d$只有0和1两种取值，我们可以用指数形式方便地将其写到一起：</p>
<script type="math/tex; mode=display">
p(d_j^w|h_w;\theta_{j-1}^w)=[\sigma(h_w\cdot\theta_{j-1}^w)]^{1-d_j^w}\cdot[1-\sigma(h_w\cdot\theta_{j-1}^w)]^{d_j^w}</script><p>所以对于$p(w|context(w))$，目标函数取对数似然，引入窗口$C$代表$context(w)$：</p>
<script type="math/tex; mode=display">
\begin{aligned}
p(w|context(w))&=\sum\limits_{w\in C}log\prod\limits_{j=2}^{n^w}\{[\sigma(h_w\cdot\theta_{j-1}^w)]^{1-d_j^w}\cdot[1-\sigma(h_w\cdot\theta_{j-1}^w)]^{d_j^w}\}\\
&=\sum\limits_{w\in C}\sum\limits_{j=2}^{n^w}\{(1-d_j^w)\cdot log[\sigma(h_w\cdot\theta_{j-1}^w)]+d_j^w\cdot log[1-\sigma(h_w\cdot\theta_{j-1}^w)]\}
\end{aligned}</script><p>其中$C$是上下文单词。接下来只需要对$h_w$和$\theta_{j-1}^w$求梯度，然后用随机梯度上升法优化即可，这个过程和逻辑回归梯度优化类似。</p>
<p>以 <strong>CBOW：上下文词（输入）预测中心词（输出）</strong> 为例，从输入层到隐藏层计算方式不变，最后得到维度为$[1,N]$的隐藏层向量$h$：<br><img src="/images/语言模型和词向量/hs.png"></p>
<p>上图中$w_2$不一定就是词典中的$w_2$。<br>使用$w$表示$w_2$，计算过程如下：<br>第1次：$p(d_2^w|h_w;\theta_1^w)=1-\sigma(h_w\cdot\theta_1^w)$<br>第2次：$p(d_3^w|h_w;\theta_2^w)=1-\sigma(h_w\cdot\theta_2^w)$<br>第3次：$p(d_4^w|h_w;\theta_3^w)=\sigma(h_w\cdot\theta_3^w)$</p>
<p><strong>基于Hierarchical Softmax的CBOW</strong>模型算法流程，梯度迭代使用了随机梯度上升法：<br>输入：基于CBOW的语料训练样本，词典大小$V$，中心词$w$，词向量的维度大小$N$，上下文$C$大小$2c$，步长$η$。<br>输出：霍夫曼树的内部节点模型参数$θ$，所有的词向量$W=w_1,w_2,…,w_V$。<br>（1）基于语料训练样本建立霍夫曼树。<br>（2）随机初始化所有的模型参数$θ$和所有的词向量$W$。<br>（3）进行梯度上升迭代过程，对于训练集中的每一个样本$(context(w),w)$做如下处理：</p>
<ul>
<li>e=0，计算$h_w=\frac{1}{2c}\sum\limits_{i=-c}^cw_{i}$</li>
<li>for $\text{j=2}$ to $n^w$，计算：<ul>
<li>$f=\sigma(h_w\theta_{j-1}^w)$</li>
<li>$g=η(1-d_j^w-f)$</li>
<li>$e=e+g\cdot\theta_{j-1}^w$</li>
<li>$\theta_{j-1}^w=\theta_{j-1}^w+g\cdot h_w$</li>
</ul>
</li>
<li>对于$(context(w),w)$中的每一个词向量$w_i$（共$2c$个）进行更新：<ul>
<li>$w_i=w_i+e$</li>
</ul>
</li>
<li>如果梯度收敛，则结束梯度迭代，否则回到（3）继续迭代。</li>
</ul>
<p><strong>基于Hierarchical Softmax的Skip-Gram</strong>模型算法流程，梯度迭代使用了随机梯度上升法：<br>输入：基于Skip-Gram的语料训练样本，词典大小$V$，中心词$w$，词向量的维度大小$N$，上下文$C$大小$2c$，步长$η$。<br>输出：霍夫曼树的内部节点模型参数$θ$，所有的词向量$W=w_1,w_2,…,w_V$。<br>（1）基于语料训练样本建立霍夫曼树。<br>（2）随机初始化所有的模型参数$θ$和所有的词向量$W$。<br>（3）进行梯度上升迭代过程，对于训练集中的每一个样本$(context(w),w)$做如下处理：</p>
<ul>
<li>for $s=context(w)$，计算：<ul>
<li>e=0，$h_w=w_i$</li>
<li>for $\text{j=2}$ to $n^w$，计算：<ul>
<li>$f=\sigma(h_w\theta_{j-1}^s)$</li>
<li>$g=η(1-d_j^s-f)$</li>
<li>$e=e+g\cdot\theta_{j-1}^s$</li>
<li>$\theta_{j-1}^s=\theta_{j-1}^s+g\cdot h_w$</li>
</ul>
</li>
</ul>
</li>
<li>对于$(context(w),w)$中的每一个词向量$w_i$（共$2c$个）进行更新：<ul>
<li>$w_i=w_i+e$</li>
</ul>
</li>
<li>如果梯度收敛，则结束梯度迭代，否则回到（3）继续迭代。</li>
</ul>
<p><strong>优点</strong>：在前面的<strong>CBOW</strong>和<strong>Skip-gram</strong>模型中，softmax计算时分母时需要对所有词的值进行计算求和，word2vec的<strong>Hierarchical Softmax</strong>采用了霍夫曼二叉树来替代从隐藏层到输出softmax层的过程，<strong>之前softmax计算量为$V$，现在为$log_2V$</strong>。</p>
<p><strong>为什么word2vec中用$\footnotesize\text{W}$作为词向量？</strong><br>在之前讲的三层网络中我们可以选择词向量是$\{\footnotesize\text{W},\text{W}^\prime\normalsize\}$，word2vec这中$\footnotesize\text{W}^\prime$替换成$\large\theta$了，所以word2vec一般采用$\footnotesize\text{W}$作为词向量而不用$\footnotesize\text{W}^\prime$作为词向量。除此原因外，输入矩阵$\footnotesize\text{W}$和输出矩阵$\footnotesize\text{W}^\prime$可以看作<strong>所有词作为中心词</strong>或<strong>所有词作为上下文词</strong>而产生的词向量，它们侧重点不同，在不同算法作用也不同，比如在Skip-gram中$\footnotesize\text{W}$可看作所有词作为中心词而产生的词向量，在CBOW中$\footnotesize\text{W}$可看作所有词作为上下文词产生的词向量。对于<strong>于Hierarchical Softmax</strong>和后面的<strong>Negative Sampling</strong>都代替了$\footnotesize\text{W}^\prime$，从而都是选择$\footnotesize\text{W}$作为词向量，而$\footnotesize\text{W}$作为中心词而得到词向量是Skip-gram中实现，所以word2vec中选择Skip-gram效果会更好。</p>
<h3 id="Negative-Sampling"><a href="#Negative-Sampling" class="headerlink" title="Negative Sampling"></a>Negative Sampling</h3><p>在讲基于<strong>Negative Sampling</strong>的word2vec模型前，我们先看看<strong>Hierarchical Softmax</strong>的的缺点。HS使用了霍夫曼树，不难发现对于词频高的词计算很快，但对于词频低的词计算很慢。如果我们的训练样本里的中心词$w$是一个很生僻的词，那么就得在霍夫曼树中辛苦的向下走很久了。能不能不用搞这么复杂的一颗霍夫曼树，将模型变的更加简单呢？这就是<strong>Negative Sampling</strong>（负采样）。</p>
<p><strong>Negative Sampling</strong>就是这么一种求解word2vec模型的方法，它<strong>摒弃了霍夫曼树</strong>，采用了Negative Sampling（负采样）的方法来求解，下面我们就来讲述NS中下预测一个词过程：<br>（1）已知词$w$的上下文$context(w)$，需要预测$w$，那么认为词$w$作为中心词就是一个正样本$(context(w),w)$，其他词作为中心词就是负样本$(context(w),w_i)$，其中$i\in[1,\text{neg}]$。通过<strong>负采样</strong>得到 $\text{neg}$ （自己指定）个负样本 + 一个正样本，用$u$表示它们的集合。<br>（2）利用这一个正例$(context(w),w)$和 $\text{neg}$ 个负例$(context(w),w_i)$进行逻辑回归二分类，我们希望正样本分类概率最大化，可以通过梯度优化完成。这个就是预测一个词的过程。</p>
<p>整个过程要明白两个核心问题：1）如何利用$u$来做逻辑回归二分类？ 2）如何进行负采样？</p>
<hr>
<p><strong>我们通过基于Negative Sampling的CBOW来解答第一个问题。</strong><br>预测一个词时优化的目标函数$g(w)$表示为：</p>
<script type="math/tex; mode=display">
g(w)=\prod\limits_{u\in\{w\}\cup\{w_1...w_{neg}\}}p(u|context(w))</script><p>其中：</p>
<script type="math/tex; mode=display">
\begin{aligned}
p(u|context(w))&=
\begin{cases}
   \sigma(h_w\cdot\theta^{u}), &y_u=1\quad\text{(正类)}\\
   1-\sigma(h_w\cdot\theta^{u}), &y_u=0\quad\text{(负类)}
\end{cases}
~\\
\\&=[\sigma(h_w\cdot\theta^u)]^{y_u}\cdot[1-\sigma(h_w\cdot\theta^u)]^{1-y_u}
\end{aligned}</script><p>上式，代入$g(w)$中：</p>
<script type="math/tex; mode=display">
g(w)=\prod\limits_{u\in\{w\}\cup\{w_1...w_{neg}\}}[\sigma(h_w\cdot\theta^u)]^{y_u}\cdot[1-\sigma(h_w\cdot\theta^u)]^{1-y_u}</script><p>这就是预测一次的优化函数了。此时引入窗口$C=2c$，整体的优化函数就是：</p>
<script type="math/tex; mode=display">
\begin{aligned}
L=log\prod\limits_{C}g(w)=\sum\limits_{C}log(g(w))&=\sum\limits_{C}log\prod\limits_{u\in\{w\}\cup\{w_1...w_{neg}\}}\{[\sigma(h_w\cdot\theta^u)]^{y_u}\cdot[1-\sigma(h_w\cdot\theta^u)]^{1-y_u}\}\\
&=\sum\limits_{C}\sum\limits_{u\in\{w\}\cup\{w_1...w_{neg}\}}\{y_u\cdot log[\sigma(h_w\cdot\theta^u)]+(1-y_u)\cdot log[1-\sigma(h_w\cdot\theta^u)]\}
\end{aligned}</script><p>之后使用随机梯度上升优化即可。</p>
<p><strong>基于Negative Sampling的CBOW</strong>：<br>输入：基于CBOW的语料训练样本，词典大小$V$，中心词$w$，词向量的维度大小$N$，上下文$C$大小$2c$，步长$η$。<br>输出：词汇表每个词对应的模型参数$θ$，所有的词向量$W=x_1,x_2,…,x_V$（避免和下面负样本混淆）。<br>（1）随机初始化所有的模型参数$θ$和所有的词向量$W$。<br>（2）对于每个训练样本$(context(w),w)$，负采样出$neg$个负样本$(context(w),w_i)$，其中$i\in[1,\text{neg}]$。<br>（3）进行梯度上升迭代过程，对于训练集中的每一个样本$(context(w),w,w_1…w_{neg})$做如下处理：</p>
<ul>
<li>e=0，计算$h_w=\frac{1}{2c}\sum\limits_{i=-c}^cx_{i}$</li>
<li>for $u=\{w\}\cup\{w_1…w_{neg}\}$，计算：<ul>
<li>$f=\sigma(h_w\theta^u)$</li>
<li>$g=η(y^u-f)$</li>
<li>$e=e+g\cdot\theta^u$</li>
<li>$\theta^u=\theta^u+g\cdot h_w$</li>
</ul>
</li>
<li>对于$context(w)$中的每一个词向量$x_i$（共$2c$个）进行更新：<ul>
<li>$x_i=x_i+e$</li>
</ul>
</li>
<li>如果梯度收敛，则结束梯度迭代，否则回到（3）继续迭代。</li>
</ul>
<p><strong>基于Negative Sampling的Skip-gram</strong>：<br>输入：基于CBOW的语料训练样本，词典大小$V$，中心词$w$，词向量的维度大小$N$，上下文$C$大小$2c$，步长$η$。<br>输出：词汇表每个词对应的模型参数$θ$，所有的词向量$W=x_1,x_2,…,x_V$（避免和下面负样本混淆）。<br>（1）随机初始化所有的模型参数$θ$和所有的词向量$W$。<br>（2）对于每个训练样本$(context(w),w)$，负采样出$neg$个负样本$(context(w),w_i)$，其中$i\in[1,\text{neg}]$。<br>（3）进行梯度上升迭代过程，对于训练集中的每一个样本$(context(w),w,w_1…w_{neg})$做如下处理：</p>
<ul>
<li>for $s=context(w)$，计算：<ul>
<li>e=0，$h_w=x_i$</li>
<li>for $u=\{w\}\cup\{w_1…w_{neg}\}$，计算：<ul>
<li>$f=\sigma(h_w^s\theta^u)$</li>
<li>$g=η(y^u-f)$</li>
<li>$e=e+g\cdot\theta^u$</li>
<li>$\theta^u=\theta^u+g\cdot h_w^s$</li>
</ul>
</li>
</ul>
</li>
<li>对于$context(w)$中的每一个词向量$x_i$（共$2c$个）进行更新：<ul>
<li>$x_i^s=x_i^s+e$</li>
</ul>
</li>
<li>如果梯度收敛，则结束梯度迭代，否则回到（3）继续迭代。</li>
</ul>
<hr>
<p><strong>第二个问题：负采样如何做？</strong><br>对于<strong>Negative Sampling</strong>模型，负采样是一个很重要的环节，对于一个中心词$w$，如何生成$neg$个负样本呢？由于词典中的词在预料中出现的频次不同，我们希望那些<strong>高频词被选为负样本的概率大</strong>，<strong>低频词被选为负样本的概率低</strong>，这就是负采样的本质要求————<strong>带权采样问题</strong>。</p>
<p>通过一段通俗的描述来帮助理解带权采样的机理：<br>设大小为$V$的词典$D$中每一个词对应一个线段$l(w)$，长度为：</p>
<script type="math/tex; mode=display">
len(w)=\frac{count(w)}{\sum\limits_{u\in D}count(u)}</script><p>其中，分子表示一个词在语料中出现的次数（分母中的求和项用来做归一化）。将这些线段连接起来（共$V$个线段），形成一个长度为 1 的单位线段。如果随机的往这个线段上打点，则其中长度越长的线段（对应高频词）被打中的概率越大。</p>
<p>word2vec中词典$D$的词设置权值时，不是直接使用$count(w)$，而是对其做了$\alpha$次幂，其中$\alpha=\large\frac{3}{4}$，即上式变为：</p>
<script type="math/tex; mode=display">
len(w)=\frac{[count(w)]^{\large\frac{3}{4}}}{\normalsize\sum\limits_{u\in D}[count(u)]^{\large\frac{3}{4}}}</script><p>word2vec中具体做法是：把上面长度为 1 的单位线段分成<strong>等距离</strong>的M份（$\footnotesize \text{M&gt;&gt;V}$），把这M份映射到前面讲的<strong>非等距离</strong>的V份中去。然后每次生成一个$[1,M]$间的随机整数，代表选择$M$份中的一份，然后按映射找到对应$V$份中的一份，此时它对应的单词就是我们选择的负样本了。这样重复取$neg$次，就得到所有负样本了。word2vec中$M=10^8$（对应源码中变量table_size）。</p>
<h3 id="a-good-word-embedding"><a href="#a-good-word-embedding" class="headerlink" title="a good word embedding"></a>a good word embedding</h3><p><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzE1MDcuMDU1MjMucGRmJTIwaHR0cDovL2llZWV4cGxvcmUuaWVlZS5vcmcvZG9jdW1lbnQvNzQ3ODQxNy8=">How to Generate a Good Word Embedding?<i class="fa fa-external-link-alt"></i></span>论文中对比了不同模型，总结了选择word embedding经验。<br>不同模型之间主要区别有两点：<br>1、目标词和上下文关系。上下文来预测目标词（这类模型更能够捕获单词之间的可替代关系）、目标词来预测上下文<br>2、上下文表示方法。<br><img src="/images/语言模型和词向量/good_embedding1.png"></p>
<p>不同模型之间上下文表示：<br><img src="/images/语言模型和词向量/good_embedding2.png"></p>
<p>据研究估计，<strong>文本含义信息的20%来自于词序，剩下的来自于词的选择</strong>。所以忽略词序信息的模型，将会损失约20%的信息。</p>
<p>本文做了三类对比实验：<br>1、<strong>研究词向量的语义特性</strong>。该类实验是为了对比词向量的语义特性，包括：WordSim353，TOEFL，analogy task：semantic和syntactic。<br>2、<strong>将词向量作为特征</strong>。该类实验是为了对比词向量作为处理其他任务的特征时，对该任务性能的提升。包括：文本分类和命名实体识别。前者将词向量加权平均得到文本向量来分类，权值是词频，数据集用的是IMDB；后者用CoNLL03数据集做NER任务。<br>3、<strong>用词向量来初始化神经网络模型</strong>。该类实验是为了研究词向量作为神经网络的初始值，对NN模型的提升。包括：CNN文本分类和词性标注。前者用了我们之前提到过的Kim的CNN模型，将句子表示成矩阵作为CNN的输入得到句子的表示，进行情感分类，数据集是Stanford Sentiment Treebank；后者用Wall Street Journal数据集进行了POS tagging任务。</p>
<p>该文对比了6种模型，并得到如下结论：<br>Q：<strong>哪个模型最好？如何选择c和w的关系以及c的表示方法？</strong><br>A：对于一个小型数据集来说，类似skip-gram这样越简单的模型效果越好，对于一个大型数据集来说，稍微复杂一点的模型，比如cbow和order就可以取得非常好的效果。真实数据中，skip-gram、cbow和order这样的模型足够了。在语义任务中，通过c来预测w的模型要更优于C&amp;W这种将c和w都放在输入层的模型。</p>
<p>Q：<strong>数据集的规模和所属领域对词向量的效果有哪些影响？</strong><br>A：数据集的<strong>领域远比规模重要</strong>，给定一个任务之后，选择任务相关的领域数据将会提升性能，相反，如果数据并不相关，将会导致更差的性能。当然，如果数据都属于同一领域，规模越大性能越好。</p>
<p>Q：<strong>在训练模型时迭代多少次可以有效地避免过拟合？</strong><br>A：通常的做法是在测试数据集上观察误差，当误差开始上升时即可停止训练，但经过本文的研究，这种方法并不能得到最好的task结果：因为训练词向量的目标是尽可能精确地预测目标词，这个优化目标和实际任务并不一致。因此最好的做法是：直接用实际任务的验证集来挑选迭代次数，即用task data作为early stopping的数据。如果实际任务非常耗时，则可以随机挑选某个简单任务（如：情感分类）及其验证集来挑选迭代次数。</p>
<p>Q：<strong>词向量的维度与效果之间的关系？</strong><br>A：做词向量语义分析任务时，一般维度越大，效果越好。做具体NLP任务时（用作输入特征、或者网络初始化），50维之后效果提升就比较少了。</p>
<p>本文作者做了大量的工作，针对当前词向量模型的方方面面问题进行了研究，并且给出了许多有意义的结果。对此想了解更多，可以看一下作者的<span class="exturl" data-url="aHR0cDovL2xpY3N0YXIubmV0L2FyY2hpdmVzLzYyMA==">《How to Generate a Good Word Embedding?》导读<i class="fa fa-external-link-alt"></i></span>。</p>
<hr>
<p><strong>word2vec结果评估</strong>：<br>1、通过kmeans聚类，查看聚类的簇分布。<br>2、通过词向量计算单词之间的相似度，查看相似词。<br>3、通过类比：a之于b等价于c之于d。<br>4、使用tsne降维可视化查看词的分布。</p>
<p><strong>word2vec输入向量和输出向量</strong>（<span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzE2MDguMDU4NTkucGRm">Using the Output Embedding to Improve Language Models<i class="fa fa-external-link-alt"></i></span>）：<br>1、在skip-gram模型中，在常见的衡量词向量的指标上，输出向量略微弱于输入向量。<br>2、<strong>在基于RNN的语言模型中，输出向量反而强于输入向量</strong>。<br>3、强制输入向量的转置作为输出向量，这可以使得输入向量等于输出向量。这种方式得到的词向量能够提升语言模型的困惑度perplexity。</p>
<p><strong>word2vec计算句子相似度</strong>（<span class="exturl" data-url="aHR0cDovL25scC50b3duL2Jsb2cvc2VudGVuY2Utc2ltaWxhcml0eS8=">Comparing Sentence Similarity Methods<i class="fa fa-external-link-alt"></i></span>）：<br>1、无监督方法：<br>（1）对句子中所有的词的词向量求平均，获得句子embedding。<br>（2）对句子中所有的词的词向量加权平均，每个词的权重为tf-idf，获得句子embedding。<br>（3）对句子中所有的词的词向量加权平均，每个词的权重为smooth inverse frequency:SIF（$\frac{a}{a+p(w)}$，$a$为超参数通常取0.001，$p(w)$为数据集中单词$w$的词频）；然后考虑所有的句子，并执行主成分分析；最后对每个句子的词向量加权平均减去first principal componet，获得句子embedding。<br>（4）通过 Word Mover’s Distance:WMD ，直接度量句子之间的相似度。WMD：使用两个句子中单词的词向量来衡量一个句子中的单词需要在语义空间中移动到另一个句子中的单词的最小距离。<br>2、有监督方法：<br>（5）通过分类任务来训练一个文本分类器，取最后一个hidden layer的输出作为句子embedding。就是使用文本分类器的前几层作为encoder。<br>（6）直接训练一对句子的相似性，其优点是可以直接得到句子embeding。<br><strong>最终结论是：简单加权的词向量平均已经可以作为一个较好的baseline。</strong></p>
<h1 id="softmax"><a href="#softmax" class="headerlink" title="softmax"></a>softmax</h1><script type="math/tex; mode=display">
\begin{aligned}
S_i=\dfrac{e^{V_i}}{\sum_j e^{V_j}}
\end{aligned}</script><p>其中，一个向量$\footnotesize V$共有$j$个值，$V_i$表示第$i$个值。<br>首先对所有值进行$e^x$计算，保证所有值都是大于0的。其次进行归一化，保证所有值的和为1。这些特点非常符合概率的要求，所以经常把softmax处理后的值当成概率。</p>
<h1 id="sigmoid"><a href="#sigmoid" class="headerlink" title="sigmoid"></a>sigmoid</h1><script type="math/tex; mode=display">
\sigma(x)=\dfrac{1}{1+e^{-x}}</script><p>定义域为$(-\infty,+\infty)$，值域为$(0,1)$，下图给出了sigmoid的图像：<br><img src="/images/语言模型和词向量/sigmoid.png" width="40%"></p>
<p>sigmoid函数<strong>导函数</strong>具有性质：</p>
<script type="math/tex; mode=display">
\sigma^\prime(x)=\sigma(x)[1-\sigma(x)]</script><p>由此可知：</p>
<script type="math/tex; mode=display">
[log\sigma(x)]^\prime=1-\sigma(x)</script><script type="math/tex; mode=display">
[log(1-\sigma(x))]^\prime=-\sigma(x)</script><p>sigmoid的每一次计算是相互独立的，是对当前事件的一次独立判断。我们把它用作二分类，是因为每一次判断的结果都可以根据阈值划分为两类，比如阈值为t，那么计算结果大于t的为一类，低于t的为另一类。也可以把计算结果看作二分类中一类的概率，比如计算结果为p，那么事件是一类的概率就是p，另一个类概率就是1-p。</p>
<h1 id="gensim"><a href="#gensim" class="headerlink" title="gensim"></a>gensim</h1><p>Word2Vec：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 4.0</span></span><br><span class="line">gensim.models.word2vec.Word2Vec(sentences=<span class="literal">None</span>, corpus_file=<span class="literal">None</span>, vector_size=<span class="number">100</span>, </span><br><span class="line">alpha=<span class="number">0.025</span>, window=<span class="number">5</span>, min_count=<span class="number">5</span>, max_vocab_size=<span class="literal">None</span>, sample=<span class="number">0.001</span>, seed=<span class="number">1</span>, workers=<span class="number">3</span>, </span><br><span class="line">min_alpha=<span class="number">0.0001</span>, sg=<span class="number">0</span>, hs=<span class="number">0</span>, negative=<span class="number">5</span>, ns_exponent=<span class="number">0.75</span>, cbow_mean=<span class="number">1</span>, hashfxn=&lt;built-<span class="keyword">in</span> function <span class="built_in">hash</span>&gt;, </span><br><span class="line">epochs=<span class="number">5</span>, null_word=<span class="number">0</span>, trim_rule=<span class="literal">None</span>, sorted_vocab=<span class="number">1</span>, batch_words=<span class="number">10000</span>, </span><br><span class="line">compute_loss=<span class="literal">False</span>, callbacks=(), comment=<span class="literal">None</span>, max_final_vocab=<span class="literal">None</span>) </span><br><span class="line"><span class="comment"># 3.8</span></span><br><span class="line">gensim.models.word2vec.Word2Vec(sentences=<span class="literal">None</span>, corpus_file=<span class="literal">None</span>, size=<span class="number">100</span>, </span><br><span class="line">alpha=<span class="number">0.025</span>, window=<span class="number">5</span>, min_count=<span class="number">5</span>, max_vocab_size=<span class="literal">None</span>, sample=<span class="number">0.001</span>, seed=<span class="number">1</span>, workers=<span class="number">3</span>, </span><br><span class="line">min_alpha=<span class="number">0.0001</span>, sg=<span class="number">0</span>, hs=<span class="number">0</span>, negative=<span class="number">5</span>, ns_exponent=<span class="number">0.75</span>, cbow_mean=<span class="number">1</span>, hashfxn=&lt;built-<span class="keyword">in</span> function <span class="built_in">hash</span>&gt;, </span><br><span class="line"><span class="built_in">iter</span>=<span class="number">5</span>, null_word=<span class="number">0</span>, trim_rule=<span class="literal">None</span>, sorted_vocab=<span class="number">1</span>, batch_words=<span class="number">10000</span>, </span><br><span class="line">compute_loss=<span class="literal">False</span>, callbacks=(), max_final_vocab=<span class="literal">None</span>)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">sentences：(iterable of iterables, optional) 要分析的语料，可以是一个列表，或者从文件中遍历读出。</span></span><br><span class="line"><span class="string">          大语料库，建议直接从磁盘/网络流迭代传输句子。参阅word2vec模块中的BrownCorpus，Text8Corpus或LineSentence。</span></span><br><span class="line"><span class="string">corpus_file：(str, optional。LineSentence) 格式的语料库文件路径。</span></span><br><span class="line"><span class="string">size/vector_size：(int, optional) 词向量的维度，默认值是100。</span></span><br><span class="line"><span class="string">      这个维度的取值一般与我们的语料的大小相关，如果是不大的语料，比如小于100M的文本语料，则使用默认值一般就可以了。</span></span><br><span class="line"><span class="string">      如果是超大的语料，建议增大维度。</span></span><br><span class="line"><span class="string">window：(int, optional) 即词向量上下文最大距离。</span></span><br><span class="line"><span class="string">        这个参数在讲解中标记为c，值越大，则和某一词较远的词也会产生上下文关系。默认值为5。</span></span><br><span class="line"><span class="string">        如果是小语料则这个值可以设的更小。对于一般的语料这个值推荐在[5,10]之间。</span></span><br><span class="line"><span class="string">min_count：(int, optional) 忽略词频小于此值的单词。这个值可以去掉一些很生僻的低频词，默认是5。</span></span><br><span class="line"><span class="string">           如果是小语料，可以调低这个值。</span></span><br><span class="line"><span class="string">workers：(int, optional) 训练模型时使用的线程数。</span></span><br><span class="line"><span class="string">sg：(&#123;0, 1&#125;, optional) word2vec两个模型选择。0：CBOW模型。1：Skip-Gram模。默认是0即CBOW模型。</span></span><br><span class="line"><span class="string">hs：(&#123;0, 1&#125;, optional) word2vec两个解法选择。0：Negative Sampling。1：Hierarchical Softmax。默认是0即Negative Sampling。</span></span><br><span class="line"><span class="string">negative：(int, optional) 即使用Negative Sampling时负采样的个数，默认是5。推荐在[3,10]之间。这个参数在讲解中标记为neg。</span></span><br><span class="line"><span class="string">ns_exponent：(float, optional) 负采样分布指数。1.0样本值与频率成正比，0.0样本所有单词均等，负值更多地采样低频词。</span></span><br><span class="line"><span class="string">cbow_mean：(&#123;0, 1&#125;, optional) 仅用于CBOW在做投影的时候。</span></span><br><span class="line"><span class="string">           为0，则算法中的h为上下文的词向量之和，为1则为上下文的词向量的平均值。</span></span><br><span class="line"><span class="string">           在讲解中是按照词向量的平均值来描述的。</span></span><br><span class="line"><span class="string">alpha：(float, optional) 在随机梯度下降法中迭代的初始步长。讲解中标记为η，默认是0.025。</span></span><br><span class="line"><span class="string">min_alpha：(float, optional) 随着训练的进行，学习率线性下降到min_alpha。</span></span><br><span class="line"><span class="string">           由于算法支持在迭代的过程中逐渐减小步长，min_alpha给出了最小的迭代步长值。</span></span><br><span class="line"><span class="string">           随机梯度下降中每轮的迭代步长可以由iter，alpha， min_alpha一起得出。</span></span><br><span class="line"><span class="string">           对于大语料，需要对alpha, min_alpha,iter一起调参，来选择合适的三个值。</span></span><br><span class="line"><span class="string">seed：(int, optional) 随机数发生器种子。</span></span><br><span class="line"><span class="string">max_vocab_size：(int, optional) 词汇构建期间RAM的限制。</span></span><br><span class="line"><span class="string">                如果有更多的独特单词，则修剪不常见的单词。每1000万个类型的字需要大约1GB的RAM。</span></span><br><span class="line"><span class="string">max_final_vocab：(int, optional) 自动选择匹配的min_count将词汇限制为目标词汇大小。</span></span><br><span class="line"><span class="string">sample：(float, optional) 高频词随机下采样的配置阈值，范围是(0,1e-5)。</span></span><br><span class="line"><span class="string">hashfxn：(function, optional) 哈希函数用于随机初始化权重，以提高训练的可重复性。</span></span><br><span class="line"><span class="string">iter/epochs：随机梯度下降法中迭代的最大次数，默认是5。对于大语料，可以增大这个值。</span></span><br><span class="line"><span class="string">trim_rule：(function, optional) 词汇修剪规则，指定某些词语是否应保留在词汇表中，修剪掉或使用默认值处理。</span></span><br><span class="line"><span class="string">sorted_vocab：(&#123;0, 1&#125;, optional) 如果为1，则在分配单词索引前按降序对词汇表进行排序。</span></span><br><span class="line"><span class="string">batch_words：(int, optional) 每一个batch传递给线程单词的数量。</span></span><br><span class="line"><span class="string">compute_loss：(bool, optional) 如果为True，则计算并存储可使用get_latest_training_loss()检索的损失值。</span></span><br><span class="line"><span class="string">callbacks：(iterable of CallbackAny2Vec, optional) 在训练中特定阶段执行回调序列。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="comment"># 建立模型后</span></span><br><span class="line">build_vocab(sentences)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">遍历一次语料库建立词典</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="comment"># 3.8</span></span><br><span class="line">train(sentences=<span class="literal">None</span>, corpus_file=<span class="literal">None</span>, total_examples=<span class="literal">None</span>, total_words=<span class="literal">None</span>, </span><br><span class="line">epochs=<span class="literal">None</span>, start_alpha=<span class="literal">None</span>, end_alpha=<span class="literal">None</span>, word_count=<span class="number">0</span>, queue_factor=<span class="number">2</span>, </span><br><span class="line">report_delay=<span class="number">1.0</span>, compute_loss=<span class="literal">False</span>, callbacks=())</span><br><span class="line"><span class="comment"># 4.0</span></span><br><span class="line">train(corpus_iterable=<span class="literal">None</span>, corpus_file=<span class="literal">None</span>, total_examples=<span class="literal">None</span>, total_words=<span class="literal">None</span>, </span><br><span class="line">epochs=<span class="literal">None</span>, start_alpha=<span class="literal">None</span>, end_alpha=<span class="literal">None</span>, word_count=<span class="number">0</span>, queue_factor=<span class="number">2</span>, </span><br><span class="line">report_delay=<span class="number">1.0</span>, compute_loss=<span class="literal">False</span>, callbacks=(), **kwargs)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">第二次遍历语料库建立神经网络模型</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="comment"># 方法</span></span><br><span class="line">similar_by_word()</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">某一个词向量最相近的词集合</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">similarity()</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">两个词向量的相近程度</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">doesnt_match()</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">找出不同类的词</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">save_word2vec_format(<span class="string">&#x27;./model.txt&#x27;</span>,binary = <span class="literal">False</span>)</span><br><span class="line">save_word2vec_format(<span class="string">&#x27;./model.bin&#x27;</span>,binary = <span class="literal">True</span>)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">第一种，保存了训练的全部信息，可以在读取后追加训练</span></span><br><span class="line"><span class="string">第二种，保存为word2vec文本格式但是保存时丢失了词汇树等部分信息，不能追加训练</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">KeyedVectors.load_word2vec_format(<span class="string">&#x27;./model.bin&#x27;</span>, binary=<span class="literal">False</span>)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">加载模型</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><br>LdaMulticore：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 4.0/3.8</span></span><br><span class="line">gensim.models.ldamulticore.LdaMulticore(corpus=<span class="literal">None</span>, num_topics=<span class="number">100</span>, </span><br><span class="line">id2word=<span class="literal">None</span>, workers=<span class="literal">None</span>, chunksize=<span class="number">2000</span>, passes=<span class="number">1</span>, batch=<span class="literal">False</span>, alpha=<span class="string">&#x27;symmetric&#x27;</span>, </span><br><span class="line">eta=<span class="literal">None</span>, decay=<span class="number">0.5</span>, offset=<span class="number">1.0</span>, eval_every=<span class="number">10</span>, iterations=<span class="number">50</span>, gamma_threshold=<span class="number">0.001</span>, </span><br><span class="line">random_state=<span class="literal">None</span>, minimum_probability=<span class="number">0.01</span>, minimum_phi_value=<span class="number">0.01</span>, </span><br><span class="line">per_word_topics=<span class="literal">False</span>, dtype=&lt;<span class="class"><span class="keyword">class</span> &#x27;<span class="title">numpy</span>.<span class="title">float32</span>&#x27;&gt;)</span></span><br><span class="line"><span class="class">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><br>词典：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line">gensim.corpora.dictionary.Dictionary(documents=<span class="literal">None</span>, prune_at=<span class="number">2000000</span>)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">根据文档生成词典</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="comment"># 属性</span></span><br><span class="line">token2id</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">词典：&#123;token:tokenId&#125;</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">dfs</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">单词出现的频率</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="comment"># 方法</span></span><br><span class="line">doc2bow(document, allow_update=<span class="literal">False</span>, return_missing=<span class="literal">False</span>)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">将文档转成词袋格式：一个列表，列表中每个元素为(token_id, token_count)</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">filter_n_most_frequent(remove_n)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">过滤掉出现频率最高的remove_n个单词</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">filter_extremes(no_below=<span class="number">5</span>, no_above=<span class="number">0.5</span>, keep_n=<span class="number">100000</span>) </span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">去掉出现次数低于no_below的。</span></span><br><span class="line"><span class="string">去掉出现次数高于no_above的（百分数）。</span></span><br><span class="line"><span class="string">在上面基础上，保留出现频率前keep_n的单词。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">filter_tokens(bad_ids=<span class="literal">None</span>, good_ids=<span class="literal">None</span>)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">两种用法:</span></span><br><span class="line"><span class="string">(1)去掉bad_id对应的词。</span></span><br><span class="line"><span class="string">(2)保留good_id对应的词而去掉其他词。</span></span><br><span class="line"><span class="string">注意，这里bad_ids和good_ids都是列表形式。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">compacity() </span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">执行完过滤操作以后，可能会造成单词的序号之间有空隙。</span></span><br><span class="line"><span class="string">可以使用该函数来对词典来进行重新排序，去掉这些空隙。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure></p>
<h1 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h1><p><span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL2hhbmtjcy9IYW5MUA==">HanLP: Han Language Processing<i class="fa fa-external-link-alt"></i></span><br>统计自然语言处理 第二版 (宗成庆著)<br><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzEzMDEuMzc4MS5wZGY=">Efficient Estimation of Word Representations in Vector Space<i class="fa fa-external-link-alt"></i></span><br><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzEzMTAuNDU0Ni5wZGY=">Distributed Representations ofWords and Phrases and their Compositionality<i class="fa fa-external-link-alt"></i></span><br><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzE0MTEuMjczOC5wZGY=">word2vec Parameter Learning Explained<i class="fa fa-external-link-alt"></i></span><br><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzE1MDcuMDU1MjMucGRmJTIwaHR0cDovL2llZWV4cGxvcmUuaWVlZS5vcmcvZG9jdW1lbnQvNzQ3ODQxNy8=">How to Generate a Good Word Embedding?<i class="fa fa-external-link-alt"></i></span><br><span class="exturl" data-url="aHR0cDovL25scC50b3duL2Jsb2cvc2VudGVuY2Utc2ltaWxhcml0eS8=">Comparing Sentence Similarity Methods<i class="fa fa-external-link-alt"></i></span><br><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzE2MDguMDU4NTkucGRm">Using the Output Embedding to Improve Language Models<i class="fa fa-external-link-alt"></i></span><br><span class="exturl" data-url="aHR0cHM6Ly96aC53aWtpcGVkaWEub3JnL3dpa2kvJUU5JTlDJThEJUU1JUE0JUFCJUU2JTlCJUJDJUU3JUJDJTk2JUU3JUEwJTgx">维基百科：霍夫曼编码<i class="fa fa-external-link-alt"></i></span><br><span class="exturl" data-url="aHR0cHM6Ly93d3cuY25ibG9ncy5jb20vcGluYXJkL3AvNzI0MzUxMy5odG1s">word2vec原理(二) 基于Hierarchical Softmax的模型<i class="fa fa-external-link-alt"></i></span><br><span class="exturl" data-url="aHR0cHM6Ly93d3cuY25ibG9ncy5jb20vcGluYXJkL3AvNzI0OTkwMy5odG1s">word2vec原理(三) 基于Negative Sampling的模型<i class="fa fa-external-link-alt"></i></span><br><span class="exturl" data-url="aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2l0cGx1cy9hcnRpY2xlL2RldGFpbHMvMzc5Njk5Nzk=">word2vec 中的数学原理详解（四）基于 Hierarchical Softmax 的模型<i class="fa fa-external-link-alt"></i></span><br><span class="exturl" data-url="aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2l0cGx1cy9hcnRpY2xlL2RldGFpbHMvMzc5OTg3OTc=">word2vec 中的数学原理详解（五）基于 Negative Sampling 的模型<i class="fa fa-external-link-alt"></i></span><br><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzE0MDIuMzcyMi5wZGY=">word2vec Explained: Deriving Mikolov et al.’sNegative-Sampling Word-Embedding Method<i class="fa fa-external-link-alt"></i></span></p>

      
    </div>

    
    
    
      


    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://soundmemories.github.io/2021/06/19/Paper/00.Paper%E9%98%85%E8%AF%BB%E6%8A%80%E5%B7%A7/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="https://i.loli.net/2020/11/04/6JhNuwtBe4adylS.png">
      <meta itemprop="name" content="SoundMemories">
      <meta itemprop="description" content="今日事，今日毕">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="SoundMemories">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/06/19/Paper/00.Paper%E9%98%85%E8%AF%BB%E6%8A%80%E5%B7%A7/" class="post-title-link" itemprop="url">Paper阅读技巧</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2021-06-19 00:00:00" itemprop="dateCreated datePublished" datetime="2021-06-19T00:00:00+08:00">2021-06-19</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Paper/" itemprop="url" rel="index"><span itemprop="name">Paper</span></a>
        </span>
    </span>

  
      </div>
      <div class="post-meta">
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>5k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>5 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="Paper"><a href="#Paper" class="headerlink" title="Paper"></a>Paper</h1><p>Paper一般会发表在期刊和顶会中，在期刊上发表Paper需要花费大量时间和精力不断修改完善论文，所以周期很长，尤其是顶级期刊。而在顶会中发表论文周期会短些，所以很多重要的成果都先在顶会中出现。一般工作后，重点关注顶会即可。<br>
          <!--noindex-->
            <div class="post-button">
              <a class="btn" href="/2021/06/19/Paper/00.Paper%E9%98%85%E8%AF%BB%E6%8A%80%E5%B7%A7/#more" rel="contents">
                阅读全文 &raquo;
              </a>
            </div>
          <!--/noindex-->
        
      
    </div>

    
    
    
      


    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://soundmemories.github.io/2021/06/19/NLP/00.NLP%E5%85%A5%E9%97%A8/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="https://i.loli.net/2020/11/04/6JhNuwtBe4adylS.png">
      <meta itemprop="name" content="SoundMemories">
      <meta itemprop="description" content="今日事，今日毕">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="SoundMemories">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/06/19/NLP/00.NLP%E5%85%A5%E9%97%A8/" class="post-title-link" itemprop="url">NLP入门</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2021-06-19 00:00:00" itemprop="dateCreated datePublished" datetime="2021-06-19T00:00:00+08:00">2021-06-19</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/NLP/" itemprop="url" rel="index"><span itemprop="name">NLP</span></a>
        </span>
    </span>

  
      </div>
      <div class="post-meta">
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>10k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>10 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="什么是NLP？"><a href="#什么是NLP？" class="headerlink" title="什么是NLP？"></a>什么是NLP？</h1><p><strong>NLP（Natural Language Processing，自然语言处理）</strong>，是融合了计算机科学、人工智能、语言学的交叉学科。其主要目的是让计算机学会处理人类的语言，甚至实现终极目标——————理解人类语言。</p>
<p>NLP可以概括为<strong>NLP=NLU+NLG</strong>：<br><strong>NLU（Natural Language Understand，自然语言理解）</strong>：语音或文本 ——&gt; 结构化的语义。<br><strong>NLG（Natural Language Generation，自然语言生成）</strong>：把结构化的语义 ——&gt; 文本或语音。<br>
          <!--noindex-->
            <div class="post-button">
              <a class="btn" href="/2021/06/19/NLP/00.NLP%E5%85%A5%E9%97%A8/#more" rel="contents">
                阅读全文 &raquo;
              </a>
            </div>
          <!--/noindex-->
        
      
    </div>

    
    
    
      


    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://soundmemories.github.io/2021/02/05/Graph/01.GCN/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="https://i.loli.net/2020/11/04/6JhNuwtBe4adylS.png">
      <meta itemprop="name" content="SoundMemories">
      <meta itemprop="description" content="今日事，今日毕">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="SoundMemories">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/02/05/Graph/01.GCN/" class="post-title-link" itemprop="url">Graph Convolutional Network</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2021-02-05 00:00:00" itemprop="dateCreated datePublished" datetime="2021-02-05T00:00:00+08:00">2021-02-05</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Graph/" itemprop="url" rel="index"><span itemprop="name">Graph</span></a>
        </span>
    </span>

  
      </div>
      <div class="post-meta">
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>2.9k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>3 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="图论"><a href="#图论" class="headerlink" title="图论"></a>图论</h1><p>最开始是1707年由 Seven bridges 问题而来的，欧拉把这个问题转成图解决。近代图论1959年 Erdos 和 Renyi 发表的 Random Graph 一系列论文建立起来的全新领域。目前学术界和工业界是基于1999年 brabasi 提出的 Scale-free Network 进行的。</p>
<h2 id="degree-distribution"><a href="#degree-distribution" class="headerlink" title="degree-distribution"></a>degree-distribution</h2><p><strong>度分布，即每个节点按边的数量分类，每类节点占总结点比例</strong>。一般 Random Graph 的度分布是 <strong>泊松分布</strong>（poisson，类似正态分布）。但基于 Scale-free Network 的提出，我们发现很多网络结构是 <strong>幂律分布</strong>（power-law）。</p>
<h2 id="adjacency-matrix"><a href="#adjacency-matrix" class="headerlink" title="adjacency matrix"></a>adjacency matrix</h2><p>邻接矩阵。阶为$n$的图$G$的邻接矩阵$A$是$n\times n$的。将$G$的顶点标签为$v_{1},v_{2},…,v_{n}$。若$(v_{i},v_{j})\in E(G)$，$A_{ij}=1$，否则$A_{ij}=0$。也可以用大于0的值表示边的权值，例如可以用边权值表示一个点到另一个点的距离。</p>
<h2 id="Distance-Hilbert-Space"><a href="#Distance-Hilbert-Space" class="headerlink" title="Distance(Hilbert Space)"></a>Distance(Hilbert Space)</h2><p>每个节点具有多维features，可以看成多维空间，计算Distance也可看作Similarity。为什么在 Hilbert Space ，因为要方便后面约束优化。</p>
<p>GCN要求：<br>（1）weights $\geq$ 0<br>（2）linear calculation<br>（3）Inner product</p>
<p>Hilbert Space：<br>（1）对称性：$&lt;\vec{y}, \vec{x}&gt;$ = $&lt;\vec{x}, \vec{y}&gt;$ </p>
<p>（2）线性：$&lt; a\vec{x_1}+b \vec{x_2}, \vec{y}&gt;$ = $a&lt;\vec{x_1}, \vec{y}&gt;+b&lt;\vec{x_2}, \vec{y}&gt;$</p>
<p>（3）半正定性：$&lt;\vec{x}, \vec{x}&gt; \geq 0$，if $\vec{x}=\vec{0}$，$&lt;\vec{x}, \vec{x}&gt; = 0$</p>
<p>Distance：$d(\vec{x}, \vec{y})=||\vec{x}-\vec{y}||=\sqrt{&lt;\vec{x}-\vec{y}, \vec{x}-\vec{y}&gt;}$</p>
<h2 id="Clustering-coefficient"><a href="#Clustering-coefficient" class="headerlink" title="Clustering coefficient"></a>Clustering coefficient</h2><p>聚类系数，一个图中的顶点之间结集成团的程度的系数。集聚系数分为整体与局部两种。整体集聚系数可以给出一个图中整体的集聚程度的评估，而局部集聚系数则可以测量图中每一个结点附近的集聚程度。</p>
<p>分子：闭三点组（邻近三点组成“三角形”）数量。分母：闭三点组（邻近三点组成“三角形”）数量 + 开三点组（邻近三点组成“缺一条边的三角形”）数量。</p>
<p>整体聚类系数：对每个节点的聚类系数求和取均值。如果该值很大，表示图是很稠密的，节点和节点之间联系紧密。</p>
<h2 id="Betweenness"><a href="#Betweenness" class="headerlink" title="Betweenness"></a>Betweenness</h2><p>也叫Betweenness centrality，分node Betweenness 和 edge Betweenness。这个值很大，表示此node或edge很重要，因为很多最短路径都要经过它，表现流通性。</p>
<p>node Betweenness：计算经过一个点的最短路径的数量占所有最短路径数量比例。两点一组，遍历所有组，计算每组中经过一个点的最短路径的数量占该组最短路径数量比例，最后求和。<br>edge Betweenness：node Betweenness 换成边即可。</p>
<p>缺点：需要遍历所有点找到所有最短路径，一般的算法有 迪杰斯特拉算法（Dijkstra） 和 弗洛伊德算法（Floyd），时间复杂度都为 $O(n^3)$。</p>
<h1 id="GCN"><a href="#GCN" class="headerlink" title="GCN"></a>GCN</h1><p>GCN（Graph Convolutional Networks，图卷积神经网络），实际上跟CNN的作用一样，就是一个特征提取器，只不过它的对象是图数据。GCN精妙地设计了一种从图数据中提取特征的方法，从而让我们可以使用这些特征去对图数据进行节点分类（node classification）、图分类（graph classification）、边预测（link prediction），还可以顺便得到图的嵌入表示（graph embedding）。</p>
<p>简单说一下GCN发展历史，那么肯定绕不过下面三篇论文：<br><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzEzMTIuNjIwMy5wZGY=">Spectral Networks and Deep Locally Connected Networks on Graphs<i class="fa fa-external-link-alt"></i></span> 2014年<br><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzE2MDYuMDkzNzUucGRm">Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering<i class="fa fa-external-link-alt"></i></span> 2016年<br><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzE2MDkuMDI5MDcucGRm">Semi-Supervised Classification with Graph Convolutional Networks<i class="fa fa-external-link-alt"></i></span> 2017年</p>
<p>在计算机科学领域、理论物理复杂网络领域的研究者在图（Graph）的空间域（spatial domain）和频谱域（spectral domain）分别提出了不同形式的图神经网络，并最终在2017年实现了空间域模型和频谱域模型的融合，即目前我们使用的第三代GCN。</p>
<p>对于其中理论和公式非常感兴趣的参考<span class="exturl" data-url="aHR0cDovL3h0ZjYxNS5jb20vMjAxOS8wMi8yNC9nY24v">图卷积神经网络理论基础<i class="fa fa-external-link-alt"></i></span>。</p>
<h2 id="Fourier-transform"><a href="#Fourier-transform" class="headerlink" title="Fourier transform"></a>Fourier transform</h2><p>时域上的卷积-&gt;频域上的相乘。从而方便计算，可以看作一种Mapping方式，把时域信号转成频域信号处理。</p>
<script type="math/tex; mode=display">
F(w) = \frac{1}{2\pi}\int_{-\infty}^{+\infty} f(t)e^{-j\omega t}\, {\rm d}t</script><p>$F(\omega)$ 就是<strong>傅里叶变换</strong>，得到的就是<strong>频域曲线</strong>。每个频率$\omega$下都有对应的振幅$F(\omega)$。从几何上来看，$f(t)$ 以 $e^{-j\omega t}$ 为基函数投影，$F(w)$ 就是以频率 $\omega$ 对应基上的投影的坐标。</p>
<p>从数学角度来看，$f(x)$ 是函数 $f$ 在 $t$ 处的取值，所有基都对该处取值有贡献，即把每个$F(w)$ 投影到 $e^{-j\omega t}$ 基方向上分量累加起来，得到的就是该点处的函数值。</p>
<script type="math/tex; mode=display">
f(t) = \int_{-\infty}^{+\infty}F(w)e^{-j\omega t}\, {\rm d}\omega=\sum_{\omega}F(w)e^{-j\omega t}</script><p>上面简化了一下，用 $w$ 代表频率。这个公式也叫做<strong>逆傅里叶变换</strong>。</p>
<h1 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h1><p><span class="exturl" data-url="aHR0cHM6Ly96aC53aWtpcGVkaWEub3JnL3dpa2kvJUU5JTlBJThGJUU2JTlDJUJBJUU1JTlCJUJF">随机图<i class="fa fa-external-link-alt"></i></span>，<span class="exturl" data-url="aHR0cHM6Ly96aC53aWtpcGVkaWEub3JnL3dpa2kvJUU2JTk3JUEwJUU1JUIwJUJBJUU1JUJBJUE2JUU3JUJEJTkxJUU3JUJCJTlD">无尺度网络<i class="fa fa-external-link-alt"></i></span>，<span class="exturl" data-url="aHR0cHM6Ly93d3cuY25ibG9ncy5jb20vbGVlengvcC85NDM2ODIwLmh0bWw=">Scale Free Network<i class="fa fa-external-link-alt"></i></span></p>
<p><span class="exturl" data-url="aHR0cHM6Ly96aC53aWtpcGVkaWEub3JnL3dpa2kvJUU1JUJBJUE2JUU1JTg4JTg2JUU1JUI4JTgz">度分布<i class="fa fa-external-link-alt"></i></span><br><span class="exturl" data-url="aHR0cHM6Ly9lbi53aWtpcGVkaWEub3JnL3dpa2kvQ2x1c3RlcmluZ19jb2VmZmljaWVudA==">Clustering coefficient<i class="fa fa-external-link-alt"></i></span><br><span class="exturl" data-url="aHR0cHM6Ly9lbi53aWtpcGVkaWEub3JnL3dpa2kvQmV0d2Vlbm5lc3M=">Betweenness<i class="fa fa-external-link-alt"></i></span><br><span class="exturl" data-url="aHR0cHM6Ly93d3cuemhpaHUuY29tL3F1ZXN0aW9uLzE5OTY3Nzc4">如何理解希尔伯特空间？<i class="fa fa-external-link-alt"></i></span>，<span class="exturl" data-url="aHR0cHM6Ly96aC53aWtpcGVkaWEub3JnL3dpa2kvJUU1JUI4JThDJUU1JUIwJTk0JUU0JUJDJUFGJUU3JTg5JUI5JUU3JUE5JUJBJUU5JTk3JUI0">希尔伯特空间<i class="fa fa-external-link-alt"></i></span><br><span class="exturl" data-url="aHR0cHM6Ly9lbi53aWtpcGVkaWEub3JnL3dpa2kvRm91cmllcl90cmFuc2Zvcm0=">Fourier transform<i class="fa fa-external-link-alt"></i></span>，<span class="exturl" data-url="aHR0cHM6Ly96aC53aWtpcGVkaWEub3JnL3dpa2kvJUU1JTgyJTg1JUU5JTg3JThDJUU1JThGJUI2JUU1JThGJTk4JUU2JThEJUEy">傅里叶变换<i class="fa fa-external-link-alt"></i></span></p>
<p><span class="exturl" data-url="aHR0cHM6Ly93d3cuc29odS5jb20vYS8zNDI2MzQyOTFfNjUxODkz">跳出公式，看清全局，图神经网络（GCN）原理详解<i class="fa fa-external-link-alt"></i></span><br><span class="exturl" data-url="aHR0cHM6Ly96aHVhbmxhbi56aGlodS5jb20vcC8zODYxMjg2Mw==">图卷积网络(Graph Convolutional networks, GCN) 简述<i class="fa fa-external-link-alt"></i></span><br><span class="exturl" data-url="aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2hvbmdiaW5feHUvYXJ0aWNsZS9kZXRhaWxzLzg5NjcwMDk2">论文笔记：Semi-Supervised Classification with Graph Convolutional Networks<i class="fa fa-external-link-alt"></i></span><br><span class="exturl" data-url="aHR0cHM6Ly93d3cuY25ibG9ncy5jb20vbnhmLXJhYmJpdDc1L3AvMTEzMDYxOTguaHRtbA==">GCN总结<i class="fa fa-external-link-alt"></i></span><br><span class="exturl" data-url="aHR0cDovL3h0ZjYxNS5jb20vMjAxOS8wMi8yNC9nY24v">图卷积神经网络理论基础<i class="fa fa-external-link-alt"></i></span><br><span class="exturl" data-url="aHR0cDovL3RraXBmLmdpdGh1Yi5pby9ncmFwaC1jb252b2x1dGlvbmFsLW5ldHdvcmtzLw==">GRAPH CONVOLUTIONAL NETWORKS<i class="fa fa-external-link-alt"></i></span><br><span class="exturl" data-url="aHR0cHM6Ly96aHVhbmxhbi56aGlodS5jb20vcC83NjAwMTA4MA==">GNN综述——从入门到入门<i class="fa fa-external-link-alt"></i></span></p>

      
    </div>

    
    
    
      


    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://soundmemories.github.io/2021/02/03/Graph/00.%E5%9B%BE%E6%B7%B1%E5%BA%A6%E8%A1%A8%E7%A4%BA/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="https://i.loli.net/2020/11/04/6JhNuwtBe4adylS.png">
      <meta itemprop="name" content="SoundMemories">
      <meta itemprop="description" content="今日事，今日毕">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="SoundMemories">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/02/03/Graph/00.%E5%9B%BE%E6%B7%B1%E5%BA%A6%E8%A1%A8%E7%A4%BA/" class="post-title-link" itemprop="url">图深度表示</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2021-02-03 00:00:00" itemprop="dateCreated datePublished" datetime="2021-02-03T00:00:00+08:00">2021-02-03</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Graph/" itemprop="url" rel="index"><span itemprop="name">Graph</span></a>
        </span>
    </span>

  
      </div>
      <div class="post-meta">
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>4.7k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>4 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="图结构"><a href="#图结构" class="headerlink" title="图结构"></a>图结构</h1><p>在现实中，很多情景的构成是不规则的图结构，比如社交网络、金融网络、化学分子结构等等。</p>
<h1 id="什么是图表示学习？"><a href="#什么是图表示学习？" class="headerlink" title="什么是图表示学习？"></a>什么是图表示学习？</h1><p>简单讲就是把图结构映射到向量空间（也叫graph embedding），或者由向量空间映射到图结构（也叫 graph generate）。</p>
<p>我们为什么这么做呢？或者说将图映射到向量空间的优势是什么？<br>（1）向量表示相对传统图表示（邻接矩阵、邻接表）对现有机器学习算法更友好。<br>（2）可以更好的将拓扑信息和节点本身特征结合。</p>
<h1 id="基于图结构的表示学习"><a href="#基于图结构的表示学习" class="headerlink" title="基于图结构的表示学习"></a>基于图结构的表示学习</h1><p>图论、数据挖掘角度：如何在学习到向量的表示中保留尽可能多的图拓扑结构的信息。</p>
<p>节点的向量表示只来源于图的拓扑结构（nxn 的邻接矩阵表达的图结构），只是对图结构的单一表示，缺乏对图节点特征消息的表示。下图d远远小于n。</p>
<p><img src="/images/图深度表示/图表示1.png" width="100%"></p>
<h2 id="传统方法"><a href="#传统方法" class="headerlink" title="传统方法"></a>传统方法</h2><p>第一个想法就是<strong>降维</strong>（nxn -&gt; nxd），利用现有的降维方法实现，比如PCA、LDA等等。致命缺点：时间复杂度高；<strong>不能保留图中节点和节点之间的拓扑信息</strong>。</p>
<ul>
<li>这里有个问题：什么叫做<strong>保留拓扑信息</strong>？</li>
<li><p>目标：<strong>在拓扑域里面‘邻近’&lt;==&gt; 在向量域‘邻近’</strong>。</p>
</li>
<li><p>问题：<strong>如何建模节点之间的邻近信息？ -&gt; 如何定义’邻近’？</strong></p>
</li>
<li>定义邻近的方法很多：<strong>共同出现、高阶邻近（n-hop邻居）、团体邻近（属于某一个团体）</strong>。</li>
</ul>
<p>所以现有算法都是围绕着 <strong>定义‘邻近’</strong> 和 <strong>求解‘邻近’</strong> 这两个点展开。</p>
<h2 id="Deepwalk"><a href="#Deepwalk" class="headerlink" title="Deepwalk"></a>Deepwalk</h2><ul>
<li><strong>动机</strong>：如何动态的建模邻居信息？</li>
<li><strong>1-hop建模</strong>：对于某个节点只看其邻居节点。两个相邻的结点就可以定义为邻近。<ul>
<li>太局部，忽略了图上的一些全局信息。</li>
</ul>
</li>
<li><strong>n-hop建模</strong>：对于某个节点考虑其n步邻居节点。两个n阶临近的结点也可以定义为邻近。<ul>
<li>组合爆炸，复杂度高，且没必要。</li>
</ul>
</li>
<li>解决方法：<ul>
<li>考虑有限步的情况，例如只考虑1，2 hop，即 LINE 2015。</li>
<li>使用采样的方式————随机游走（Random Walk）思想的 Deepwalk 2014。</li>
</ul>
</li>
</ul>
<p><strong>Deepwalk</strong>：<strong>利用随机游走采样生成的序列去定义节点间的邻近关系</strong>。<strong>在足够多的采样情况下，可以很好的刻画节点之间的邻近信息</strong>。<strong>这样就把图信息，转成了序列信息，通过Word2Vec把序列向量化即可（每个点看成词）</strong>。<br>总结：<br>（1）使用定长的随机游走去采样图中节点的邻近关系。<br>（2）节点-&gt;词语，随机游走序列-&gt;句子。<br>（3）使用自然语言处理相关模型（例如word2vec）对随机游走得到的序列进行表示学习。</p>
<p>基于Random Walk的思路出现了很多 XXX2vec 的论文，基本套路都一样。</p>
<h2 id="Node2vec"><a href="#Node2vec" class="headerlink" title="Node2vec"></a>Node2vec</h2><p><strong>动机</strong>：简单的随机游走采样不够好（不能体现出BFS/DFS性质）。<br><strong>核心思想</strong>：等概率跳 -&gt; 人工设计概率来跳。</p>
<p>当从结点 t 跳跃到结点 v 之后，算法下一步从结点 v 向邻居结点跳跃的概率是不同的。<br><img src="/images/图深度表示/node2vec.png" width="50%"></p>
<p>从结点 v 回跳到上一个结点 t 的 $\alpha$ 为 $\frac{1}{p}$，从结点 v 跳到 t、v 的公共邻居结点的 $\alpha$ 为 1，从结点 v 跳到其他邻居的 $\alpha$ 为 $\frac{1}{q}$。<br><img src="/images/图深度表示/node2vec2.png" width="50%"></p>
<p>我们发现，当 p 比较小的时候，结点间的跳转类似于 BFS，结点间的“接近”就可以理解为结点在<strong>邻接关系</strong>上“接近”；当 q 比较小的时候，结点间的跳转类似于 DFS，节点间的“接近”就可以视作是<strong>结构上相似</strong>。<br><img src="/images/图深度表示/node2vec3.png" width="50%"></p>
<h2 id="Struc2vec"><a href="#Struc2vec" class="headerlink" title="Struc2vec"></a>Struc2vec</h2><p><strong>动机</strong>：保留局部结构一致性。<br><strong>核心思想</strong>：在原来的图上构建一个新图。</p>
<h2 id="Metapath2vec"><a href="#Metapath2vec" class="headerlink" title="Metapath2vec"></a>Metapath2vec</h2><p><strong>动机</strong>：异构图上存在不同类型的节点，这些节点不能等同看待，其间关系可能存在一些固定模式。<br><strong>核心思路</strong>：使用预定义的Meta-Path来进行Random Walk。</p>
<h1 id="基于图特征的学习（图神经网络）"><a href="#基于图特征的学习（图神经网络）" class="headerlink" title="基于图特征的学习（图神经网络）"></a>基于图特征的学习（图神经网络）</h1><p>节点的向量表示既包含了图的拓扑信息（nxn 的邻接矩阵表达的图结构）也包含了节点的特征向量集合（nxf 的特征向量）。<br><img src="/images/图深度表示/图表示2.png" width="100%"></p>
<p>机器学习、特征工程角度：如何通过有效利用图拓扑结构信息结合现有的特征向量得到新的特征。<br>比如：图像-&gt;向量，视频-&gt;向量…。可以不严谨的说<strong>所有深度学习问题都可以归结为表示学习的问题</strong>。<br><strong>挑战</strong>：如何利用我们在图片/视频上取得的成功经验来应对图特征的表示学习问题？</p>
<p><strong>卷积神经网络</strong>（Convolutional Neural Network）：表示学习利器。<br>从图的角度看图像上的CNN：在欧式空间上的格点图（平移不变性、多尺度结构）。<br><strong>目标</strong>：将在欧式空间上的CNN扩展到拓扑空间————<strong>图卷积</strong>。</p>
<h2 id="GCN"><a href="#GCN" class="headerlink" title="GCN"></a>GCN</h2><p>GCN（Graph Convolutional Networks，图卷积神经网络）：</p>
<ul>
<li><strong>输入</strong>：邻接矩阵（节点数×节点数），特征矩阵（节点数×输入特征数）。</li>
<li><strong>输出</strong>：新的特征矩阵（节点数×输出特征数）。</li>
<li>网络层面：多层网络可以叠加。</li>
<li>节点层面：节点<strong>自身特征</strong>和其<strong>邻域特征</strong>的聚合。<br><img src="/images/图深度表示/GCN1.png" width="100%"></li>
</ul>
<p>公式如下：<br><img src="/images/图深度表示/GCN2.png" width="40%"></p>
<p>$\tilde{A}=A+I_N$：带自环的邻接矩阵。<br>$\tilde{D}=\sum_j \tilde{A}_{ij}$：度矩阵。<br>$H$：特征矩阵。<br>$W$：模型参数。<br>$\sigma(.)$：激活函数。</p>
<p><strong>两层GCN构造&amp;损失函数</strong>：<br><img src="/images/图深度表示/GCN3.png" width="50%"><br><img src="/images/图深度表示/GCN4.png" width="25%"></p>
<p><strong>GCN的推导思路</strong>：在图的拓扑空间近似在谱空间中的图滤波的操作，减少可学习参数。</p>
<p><strong>从另一个角度理解GCN</strong>：对<strong>邻居节点</strong>特征的<strong>带权重</strong>（$\tilde{D}^{-\frac{1}{2}}$）的<strong>聚合</strong>（$\tilde{A}H^{(l)}$）。</p>
<h2 id="GraphSAGE"><a href="#GraphSAGE" class="headerlink" title="GraphSAGE"></a>GraphSAGE</h2><p>对<strong>聚合</strong>和<strong>邻居节点</strong>进行了扩展定义：<br>（1）<strong>聚合</strong>：Mean Pooling/Max Pooling/LSTM，etc。<br>（2）<strong>邻居节点</strong>：Fix-length sample -&gt; 可以用来加速GCN计算。</p>
<p><img src="/images/图深度表示/GraphSAGE.png" width="100%"></p>
<h2 id="GAT"><a href="#GAT" class="headerlink" title="GAT"></a>GAT</h2><p>GAT（GRAPH ATTENTION NETWORKS，图注意力网络）：对<strong>权重</strong>（$\tilde{D}^{-\frac{1}{2}}$）进行了扩展。<br>（1）GCN中使用的邻接矩阵权重是提前给定的$\tilde{D}^{-\frac{1}{2}}$。<br>（2）图注意力网络引入了<strong>自注意力机制</strong>，利用当前节点的特征以及其邻居节点的特征计算邻居节点的重要性，把该重要性作为新的邻接矩阵进行卷积计算。<br>（3）有势：利用节点特征的相似性更能反映邻接信息。<br><img src="/images/图深度表示/GAT1.png" width="80%"></p>
<p><img src="/images/图深度表示/GAT2.png" width="50%"><br><img src="/images/图深度表示/GAT3.png" width="40%"></p>
<h1 id="图学习面临的挑战"><a href="#图学习面临的挑战" class="headerlink" title="图学习面临的挑战"></a>图学习面临的挑战</h1><h2 id="如何将图神经网络模型做到更大的图上（如何做大）？"><a href="#如何将图神经网络模型做到更大的图上（如何做大）？" class="headerlink" title="如何将图神经网络模型做到更大的图上（如何做大）？"></a>如何将图神经网络模型做到更大的图上（如何做大）？</h2><p>因为$\tilde{D}^{-\frac{1}{2}}\tilde{A}\tilde{D}^{-\frac{1}{2}}$中邻接矩阵用到所有节点，难以处理超大图。即<strong>对所有邻接节点进行聚合并不高效</strong>。</p>
<p><strong>思路</strong>：采样，采用一部分点/边来进行运算。<br><strong>FastGCN</strong>：<br>（1）把图节点特征看作有一个隐含概率分布产生，利用该分布对每一层的所有节点<strong>整体采样</strong>，避免了采样点个数的指数增加。<br>（2）采样的目标是尽量减少采样的方差-&gt;基于节点degree的采样。<br>（3）<strong>缺点</strong>：没有考虑层间点和点的关系。</p>
<p>为了克服这个缺点出现了层间采样的方法：<br><strong>ASGCN</strong>：FastGCN采样方式并不合理，在图极大而采样比例极少时，层间连接会急剧减少。<br>（1）自顶向下Layer-dependent的采样方式。<br>（2）在控制每层采样个数的同时，确保上下两层之间的连接是密集。<br>（3）通过公式证明了可以保证采样无偏和减小采样方差。<br>（4）扩展：加入了残差连接，能考虑二阶邻居的信息传播。在采样设置下，实现了注意力机制。</p>
<h2 id="如何有效训练更复杂的图神经网络模型（如何做深）？"><a href="#如何有效训练更复杂的图神经网络模型（如何做深）？" class="headerlink" title="如何有效训练更复杂的图神经网络模型（如何做深）？"></a>如何有效训练更复杂的图神经网络模型（如何做深）？</h2><p>为什么不能做深？<br>（1）过拟合（Overfitting）：参数数量过多造成的泛化性降低。<br>（2）<strong>过平滑</strong>（Over-Smoothing）：<strong>多层的邻居聚合造成的特征均化</strong>。</p>
<p>Over-Smoothing的定义：经过L层特征聚合后特征收敛到一个和输入特征无关的子空间M的现象。<br><img src="/images/图深度表示/Dropedge2.png" width="20%"></p>
<p><strong>挑战</strong>：如何减弱Over-Smoothing？<br><strong>DropEdge</strong>：<strong>在每个epoch训练前，随机丢掉一定比例的边</strong>。<br><img src="/images/图深度表示/Dropedge.png" width="20%"></p>
<p>为什么DropEdge可以减弱Over-Smoothing？<br>（1）<strong>DropEdge可以减缓收敛到子空间M的速度</strong>。<img src="/images/图深度表示/Dropedge3.png" width="20%"><br>（2）<strong>DropEdge可以减少收敛过程中的信息损失</strong>。<img src="/images/图深度表示/Dropedge4.png" width="30%"></p>
<p>由此通过减弱Over-Smoothing的影响，可以使我们可以成功在更复杂更深层的图神经网络上进行训练，并且提升精度。</p>
<h1 id="应用"><a href="#应用" class="headerlink" title="应用"></a>应用</h1><p>药物属性预测和可解释性问题：DualMPNN。所有属性预测数据上大幅超越SOTA算法并提供了模型的可解释性。<br>复杂层次图学习问题：SEAL算法。基于复杂层次图结构的GNN模型。应用于安全场景中的群分类任务。<br>社交网络谣言检测：Bi-GCN。首创双向GCN结构并将其应用于谣言检测问题。<br>统一黑盒攻击框架：GF-Attacker。首个可以对于多种图模型进行黑盒攻击的攻击框架。</p>
<h2 id="SEAL"><a href="#SEAL" class="headerlink" title="SEAL"></a>SEAL</h2><p><strong>背景</strong>：在实际数据中，图相互之间的关系可以建模成图，即层级图结构。比如QQ群和QQ群的关系、学术论文引用（不同领域间的引用构成层次图，领域内的文章引用构成实例图）。<br><img src="/images/图深度表示/SEAL.png" width="70%"></p>
<p><strong>问题</strong>：如何预测实例图的分类标签？<br><strong>挑战</strong>：<br>（1）<strong>如何利用统一长度的向量来表示具有不同大小的实例图</strong>？</p>
<ul>
<li>在不同层级下学习图的表示：<ul>
<li>节点层级：$G(V,E) -&gt; H^{n×v}$</li>
<li>层图级：$G(V,E) -&gt; \it e^{v}$</li>
</ul>
</li>
<li>自注意力图表示学习（Self-Attentive Graph Embedding）<ul>
<li>图大小不变性————自注意力机制（$\it e \in R^{r×v}$）</li>
<li>节点重要性——————自注意力机制</li>
<li>排列不变性——————GCN Smoothing</li>
</ul>
</li>
</ul>
<p><img src="/images/图深度表示/SEAL2.png" width="80%"></p>
<p>（2）<strong>如何在不同层级去融合实例图和层次图的信息</strong>？</p>
<ul>
<li><strong>实例图层次</strong>（Instance Classifier）：Graph Level Learning （SEGA）。</li>
<li><strong>层次图层次</strong>（Hierarchical Classifier）：Node Level Learning（GCN）。</li>
<li><strong>特征共享</strong>：将实例图的输出作为层次图模型的输入。<br><img src="/images/图深度表示/SEAL3.png" width="90%"></li>
</ul>
<h1 id="时间线"><a href="#时间线" class="headerlink" title="时间线"></a>时间线</h1><p><img src="/images/图深度表示/GNN.png" width="70%"></p>
<h1 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h1><p><span class="exturl" data-url="aHR0cHM6Ly96aC53aWtpcGVkaWEub3JnL3dpa2kvJUU1JTlCJUJFXyglRTYlOTUlQjAlRTUlQUQlQTY=">图 (数学)<i class="fa fa-external-link-alt"></i></span>)<br><span class="exturl" data-url="aHR0cHM6Ly96aC53aWtpcGVkaWEub3JnL3dpa2kvJUU1JTlCJUJFJUU4JUFFJUJB">图论<i class="fa fa-external-link-alt"></i></span><br><span class="exturl" data-url="aHR0cHM6Ly96aC53aWtpcGVkaWEub3JnL3dpa2kvJUU5JTgyJUJCJUU2JThFJUE1JUU3JTlGJUE5JUU5JTk4JUI1">邻接矩阵<i class="fa fa-external-link-alt"></i></span><br><span class="exturl" data-url="aHR0cHM6Ly93d3cuYmlsaWJpbGkuY29tL3ZpZGVvL2F2ODM1MTk3NjU/ZnJvbT1zZWFyY2gmYW1wO3NlaWQ9NDIxMTQxNDU5NzU0ODIzOTY3Ng==">图深度表示（GNN）的基础和前沿进展<i class="fa fa-external-link-alt"></i></span><br><span class="exturl" data-url="aHR0cHM6Ly93d3cuY25ibG9ncy5jb20vbnhmLXJhYmJpdDc1L3AvMTEzMDYxOTguaHRtbA==">GCN总结<i class="fa fa-external-link-alt"></i></span></p>
<p><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzE0MDMuNjY1Mi5wZGYlQzMlQUYlQzIlQkMlRTIlODAlQkE=">DeepWalk: Online Learning of Social Representations<i class="fa fa-external-link-alt"></i></span> DeepWalk 2014年<br><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzE1MDMuMDM1NzgucGRmJUMyJUEwJUUzJTgwJTkwV1dX">LINE: Large-scale Information Network Embedding<i class="fa fa-external-link-alt"></i></span>  LINE 2015年<br><span class="exturl" data-url="aHR0cHM6Ly93d3ctY3MtZmFjdWx0eS5zdGFuZm9yZC5lZHUvcGVvcGxlL2p1cmUvcHVicy9ub2RlMnZlYy1rZGQxNi5wZGY=">node2vec: Scalable Feature Learning for Networks<i class="fa fa-external-link-alt"></i></span>  node2vec 2016年<br><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzE3MDQuMDMxNjUucGRm">struc2vec: Learning Node Representations from Structural Identity<i class="fa fa-external-link-alt"></i></span>  struc2vec 2017年<br><span class="exturl" data-url="aHR0cHM6Ly9kbC5hY20ub3JnL2RvaS9wZGYvMTAuMTE0NS8zMDk3OTgzLjMwOTgwMzY=">metapath2vec: Scalable Representation Learning for Heterogeneous Networks<i class="fa fa-external-link-alt"></i></span>  metapath2vec 2017年</p>
<p><span class="exturl" data-url="aHR0cHM6Ly9vcGVucmV2aWV3Lm5ldC9wZGY/aWQ9U0pVNGF5WWds">SEMI-SUPERVISED CLASSIFICATION WITH GRAPH CONVOLUTIONAL NETWORKS<i class="fa fa-external-link-alt"></i></span> GCN 2017年<br><span class="exturl" data-url="aHR0cHM6Ly9wcm9jZWVkaW5ncy5uZXVyaXBzLmNjL3BhcGVyLzIwMTcvZmlsZS81ZGQ5ZGI1ZTAzM2RhOWM2ZmI1YmE4M2M3YTdlYmVhOS1QYXBlci5wZGY=">Inductive Representation Learning on Large Graphs<i class="fa fa-external-link-alt"></i></span> GraphSAGE 2017年<br><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzE3MTAuMTA5MDMucGRm">GRAPH ATTENTION NETWORKS<i class="fa fa-external-link-alt"></i></span> GAT 2018年</p>
<p><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzE4MDEuMTAyNDcucGRm">FASTGCN: FAST LEARNING WITH GRAPH CONVOLU TIONAL NETWORKS VIA IMPORTANCE SAMPLING<i class="fa fa-external-link-alt"></i></span> FASTGCN 2018年<br><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzE4MDkuMDUzNDMucGRm">Adaptive Sampling Towards Fast Graph Representation Learning<i class="fa fa-external-link-alt"></i></span>  ASGCN 2018年</p>
<p><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzE5MDcuMTA5MDMucGRm">DROPEDGE: TOWARDS DEEP GRAPH CONVOLU TIONAL NETWORKS ON NODE CLASSIFICATION<i class="fa fa-external-link-alt"></i></span> DROPEDGE 2020年<br><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzE5MDQuMDUwMDMucGRm">Semi-Supervised Graph Classification: A Hierarchical Graph Perspective<i class="fa fa-external-link-alt"></i></span> SEAL 2019年</p>

      
    </div>

    
    
    
      


    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://soundmemories.github.io/2020/10/05/Machine%20Learning/27.MCMC/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="https://i.loli.net/2020/11/04/6JhNuwtBe4adylS.png">
      <meta itemprop="name" content="SoundMemories">
      <meta itemprop="description" content="今日事，今日毕">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="SoundMemories">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/10/05/Machine%20Learning/27.MCMC/" class="post-title-link" itemprop="url">MCMC</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2020-10-05 00:00:00" itemprop="dateCreated datePublished" datetime="2020-10-05T00:00:00+08:00">2020-10-05</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Machine-Learning/" itemprop="url" rel="index"><span itemprop="name">Machine Learning</span></a>
        </span>
    </span>

  
      </div>
      <div class="post-meta">
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>3.6k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>3 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="马尔可夫链蒙特卡洛"><a href="#马尔可夫链蒙特卡洛" class="headerlink" title="马尔可夫链蒙特卡洛"></a>马尔可夫链蒙特卡洛</h1><p>MCMC 是一种<strong>随机的近似推断</strong>，其核心就是基于采样的随机近似方法蒙特卡洛方法。对于采样任务来说，有下面一些常用的场景：</p>
<ol>
<li>采样作为任务，用于生成新的样本</li>
<li>求和/求积分/求期望</li>
</ol>
<p>采样结束后，我们需要评价采样出来的样本点是不是好的样本集：</p>
<ol>
<li>样本趋向于高概率的区域</li>
<li>样本之间必须独立</li>
</ol>
<p>具体采样中，采样是一个困难的过程：</p>
<ol>
<li>无法采样得到归一化因子，即无法直接对概率 $ p(x)=\frac{1}{Z}\hat{p}(x)$ 采样，常常需要对 CDF 采样，但复杂的情况不行</li>
<li>如果归一化因子可以求得，但是对高维数据依然不能均匀采样（维度灾难），这是由于对 $p$ 维空间，总的状态空间是 $K^p$ 这么大，于是在这种情况下，直接采样也不行</li>
</ol>
<p>因此需要借助其他手段，如蒙特卡洛方法中的拒绝采样，重要性采样和 MCMC。</p>
<h2 id="蒙特卡洛方法"><a href="#蒙特卡洛方法" class="headerlink" title="蒙特卡洛方法"></a>蒙特卡洛方法</h2><p>蒙特卡洛方法旨在求得复杂概率分布下的期望值：</p>
<script type="math/tex; mode=display">
\mathbb{E}_{p(z|x)}[f(z)]=\int p(z|x)f(z)dz\simeq\frac{1}{N}\sum\limits_{i=1}^Nf(z_i)</script><p>也就是说，从概率分布 $f(z)$ 中取 $N$ 个点，从而近似计算这个积分。采样方法有：</p>
<p>1、概率分布采样（也叫直接采样），首先求得概率密度（PDF）的累积密度函数（CDF），然后求得 CDF 的反函数，在0到1之间均匀采样，代入反函数，就得到了采样点。但是实际大部分概率分布不能得到 CDF。</p>
<p>2、 Rejection Sampling （接受-拒绝采样）：对于概率分布 $p(z)$，引入简单的提议分布 $q(z)$，使得 $\forall z_i,Mq(z_i)\ge p(z_i)$。我们先在 $ q(z)$ 中采样，定义接受率：$\alpha=\frac{p(z^i)}{Mq(z^i)}\le1$。算法描述为：</p>
<ol>
<li>取 $z^i\sim q(z)$。</li>
<li>在均匀分布中选取 $u$。</li>
<li>如果 $u\le\alpha$，则接受 $z^i$，否则，拒绝这个值。</li>
</ol>
<p>3、Importance Sampling （重要性采样）：直接对期望：$\mathbb{E}_{p(z)}[f(z)]$ 进行采样。</p>
<script type="math/tex; mode=display">
\mathbb{E}_{p(z)}[f(z)]=\int p(z)f(z)dz=\int \frac{p(z)}{q(z)}f(z)q(z)dz\simeq\frac{1}{N}\sum\limits_{i=1}^Nf(z_i)\frac{p(z_i)}{q(z_i)}</script><p>于是采样在 $q(z)$ 中采样，并通过权重计算和。重要值采样对于权重非常小的时候，效率非常低。<br>重要性采样有一个变种 Sampling-Importance-Resampling，这种方法，首先和上面一样进行采样，然后在采样出来的 $N$ 个样本中，重新采样，这个重新采样，使用每个样本点的权重作为概率分布进行采样。</p>
<h2 id="MCMC"><a href="#MCMC" class="headerlink" title="MCMC"></a>MCMC</h2><p>马尔可夫链式一种时间状态都是离散的随机变量序列。我们关注的主要是齐次的一阶马尔可夫链。马尔可夫链满足：$p(X_{t+1}|X_1,X_2,\cdots,X_t)=p(X_{t+1}|X_t)$。这个式子可以写成转移矩阵的形式 $p_{ij}=p(X_{t+1}=j|X_t=i)$。我们有：</p>
<script type="math/tex; mode=display">
\pi_{t+1}(x^*)=\int\pi_i(x)p_{x\to x^*}dx</script><p>如果存在 $\pi=(\pi(1),\pi(2),\cdots),\sum\limits_{i=1}^{+\infty}\pi(i)=1$，有上式成立，这个序列就叫马尔可夫链 $X_t$ 的平稳分布，平稳分布就是表示在某一个时刻后，分布不再改变。MCMC 就是通过构建马尔可夫链概率序列，使其收敛到平稳分布 $p(z)$。引入细致平衡：$\pi(x)p_{x\to x^{\ast}}=\pi(x^{\ast})p_{x^* \to x}$。如果一个分布满足细致平衡，那么一定满足平稳分布（反之不成立）：</p>
<script type="math/tex; mode=display">
\int\pi(x)p_{x\to x^*}dx=\int\pi(x^*)p_{x^*\to x}dx=\pi(x^*)</script><p>细致平衡条件将平稳分布的序列和马尔可夫链的转移矩阵联系在一起了，通过转移矩阵可以不断生成样本点。假定随机取一个转移矩阵 $(Q=Q_{ij})$，作为一个提议矩阵。我们有：</p>
<script type="math/tex; mode=display">
p(z)\cdot Q_{z\to z^*}\alpha(z,z^*)=p(z^*)\cdot Q_{z^*\to z}\alpha(z^*,z)</script><p>$\alpha(z,z^*)$ 可看作接受率，当取值为：</p>
<script type="math/tex; mode=display">
\alpha(z,z^*)=\min\{1,\frac{p(z^*)Q_{z^*\to z}}{p(z)Q_{z\to z^*}}\}</script><p>则</p>
<script type="math/tex; mode=display">
p(z)\cdot Q_{z\to z^*}\alpha(z,z^*)=\min\{p(z)Q_{z\to z^*},p(z^*)Q_{z^*\to z}\}=p(z^*)\cdot Q_{z^*\to z}\alpha(z^*,z)</script><p>于是，迭代就得到了序列，这个算法叫做 Metropolis-Hastings 算法：</p>
<ol>
<li>通过在0，1之间均匀分布取点 $u$。</li>
<li>生成 $z^{\ast}\sim Q(z^*|z^{i-1})$</li>
<li>计算 $\alpha$ 值</li>
<li>如果 $\alpha\ge u$，就接受这个样本，即 $z^i=z^*$，否则 $z^{i}=z^{i-1}$</li>
</ol>
<p>这样取的样本就服从 $p(z)=\dfrac{\hat{p}(z)}{z_p}\sim \hat{p}(z)$。</p>
<p>下面介绍另一种采样方式 Gibbs 采样，如果 $z$ 的维度非常高，那么通过固定被采样的维度其余的维度来简化采样过程：$z_i\sim p(z_i|z_{-i})$：</p>
<ol>
<li>给定初始值 $z_1^0,z_2^0,\cdots$</li>
<li>在 $t+1$ 时刻，采样 $z_i^{t+1}\sim p(z_i|z_{1}^{t+1},…z_{i-1}^{t+1},z_{i+1}^{t},…z_{k}^{t})$，简写 $z_i^{t+1}\sim p(z_i|z_{-i})$，从第一个维度开始，每次采样一个维度时，固定其余维度（之前采样过的留下，之后未采样的用上一时刻的值）。</li>
</ol>
<p>Gibbs 采样方法是一种特殊的 MH 采样，可以计算 Gibbs 采样的接受率：</p>
<script type="math/tex; mode=display">
\frac{p(z^*)Q_{z^*\to z}}{p(z)Q_{z\to z^*}}=\frac{p(z_i^*|z^*_{-i})p(z^*_{-i})p(z_i|z_{-i}^*)}{p(z_i|z_{-i})p(z_{-i})p(z_i^*|z_{-i})}</script><p>对于每个 Gibbs 采样步骤，$z_{-i}=z_{-i}^*$，这是由于每个维度 $i$ 采样的时候，其余的参量保持不变。所以上式为1。于是 Gibbs 采样过程中，相当于找到了一个步骤，使得所有的接受率为 1。</p>
<h2 id="平稳分布"><a href="#平稳分布" class="headerlink" title="平稳分布"></a>平稳分布</h2><p>定义随机矩阵：</p>
<script type="math/tex; mode=display">
Q=\begin{pmatrix}Q_{11}&Q_{12}&\cdots&Q_{1K}\\\vdots&\vdots&\vdots&\vdots\\Q_{k1}&Q_{k2}&\cdots&Q_{KK}\end{pmatrix}</script><p>这个矩阵每一行或者每一列的和都是1。随机矩阵的特征值都小于等于1。假设只有一个特征值为 $\lambda_i=1$。于是在马尔可夫过程中：</p>
<script type="math/tex; mode=display">
q^{t+1}(x=j)=\sum\limits_{i=1}^Kq^t(x=i)Q_{ij}\\
\Rightarrow q^{t+1}=q^t\cdot Q=q^1Q^t</script><p>$q^1Q^t$ 表示 $q$ 的1时刻 乘以 $Q$ 的 $t$ 次幂。</p>
<p>$Q$ 可以分解为特征矩阵 $A\Lambda A^{-1}$ 于是有：</p>
<script type="math/tex; mode=display">
q^{t+1}=q^1A\Lambda^t A^{-1}</script><p>当 $\Lambda$ 中只有一个特征值的绝对值等于1，其余小于1，即$\Lambda^t=diag(0,0,\cdots,1,\cdots,0)$，如果 $t$ 足够大，那么就会得到 $q^{t+1}=q^{t}$ ，即 $q$ 趋于平稳分布了，从开始到平稳这段时间称为燃烧期。<br>马尔可夫链可能具有平稳分布的性质，所以我们可以构建马尔可夫链使其平稳分布收敛于需要的概率分布（设计转移矩阵）。</p>
<p>在采样过程中，需要经历一定的时间（燃烧期/混合时间）才能达到平稳分布。但是 MCMC 方法有一些问题：<br>（1）无法判断是否已经收敛。<br>（2）燃烧期过长（维度太高，并且维度之间有关，可能无法采样到某些维度），例如在 GMM 中，可能无法越过低谷采样到其他峰。于是在一些模型中，需要对隐变量之间的关系作出约束，如 RBM 假设隐变量之间无关。<br>（3）样本之间一定是有相关性的，如果每个时刻都取一个点，那么每个样本一定和前一个相关，这可以通过间隔一段时间采样。</p>
<div class="note info"><p>文章转载自<span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL3NodWh1YWkwMDc=">Jie Zhou<i class="fa fa-external-link-alt"></i></span>的<span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL3NodWh1YWkwMDcvTWFjaGluZS1MZWFybmluZy1TZXNzaW9u">Machine-Learning-Session<i class="fa fa-external-link-alt"></i></span>。</p>
</div>
      
    </div>

    
    
    
      


    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://soundmemories.github.io/2020/10/03/Machine%20Learning/26.VI/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="https://i.loli.net/2020/11/04/6JhNuwtBe4adylS.png">
      <meta itemprop="name" content="SoundMemories">
      <meta itemprop="description" content="今日事，今日毕">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="SoundMemories">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/10/03/Machine%20Learning/26.VI/" class="post-title-link" itemprop="url">VI</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2020-10-03 00:00:00" itemprop="dateCreated datePublished" datetime="2020-10-03T00:00:00+08:00">2020-10-03</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Machine-Learning/" itemprop="url" rel="index"><span itemprop="name">Machine Learning</span></a>
        </span>
    </span>

  
      </div>
      <div class="post-meta">
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>4.4k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>4 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="变分推断"><a href="#变分推断" class="headerlink" title="变分推断"></a>变分推断</h1><p>我们已经知道概率模型可以分为，频率派的优化问题和贝叶斯派的积分问题。从贝叶斯角度来看推断，对于 $\hat{x}$ 这样的新样本，需要得到：</p>
<script type="math/tex; mode=display">
p(\hat{x}|X)=\int_\theta p(\hat{x},\theta|X)d\theta=\int_\theta p(\theta|X)p(\hat{x}|\theta,X)d\theta</script><p>如果新样本和数据集独立，那么推断就是概率分布依参数后验$p(\theta|X)$分布的期望。</p>
<p>我们看到，推断问题的中心是参数后验$p(\theta|X)$分布的求解，推断分为：</p>
<p>1、精确推断-解析解<br>2、近似推断：参数空间无法精确求解，如含有隐变量、参数空间复杂<br>（1）确定性近似，如：变分推断<br>（2）随机近似，如：MCMC，MH，Gibbs</p>
<h2 id="基于平均场假设的变分推断"><a href="#基于平均场假设的变分推断" class="headerlink" title="基于平均场假设的变分推断"></a>基于平均场假设的变分推断</h2><p>我们记 $Z$ 为隐变量和参数的集合，$Z_i$ 为第 $i$ 维的参数，于是，回顾一下 EM 中的推导：</p>
<script type="math/tex; mode=display">
\log p(X)=\log p(X,Z)-\log p(Z|X)=\log\frac{p(X,Z)}{q(Z)}-\log\frac{p(Z|X)}{q(Z)}</script><p>左右两边分别积分：</p>
<script type="math/tex; mode=display">
Left:\int_Zq(Z)\log p(X)dZ=\log p(X)</script><script type="math/tex; mode=display">
Right:\int_Z[\log \frac{p(X,Z)}{q(Z)}-\log \frac{p(Z|X)}{q(Z)}]q(Z)dZ=ELBO+KL(q,p)</script><p>$Right$ 可以写为<strong>变分</strong>和<strong>KL散度</strong>的和：</p>
<script type="math/tex; mode=display">
Right:L(q)+KL(q,p)</script><p>由于这个式子是常数，于是寻找 $q\simeq p$ 的任务就相当于对 $L(q)$ 取最大值。</p>
<script type="math/tex; mode=display">
\hat{q}(Z)=\mathop{argmax}_{q(Z)}L(q)</script><p>假设 $q(Z)$ 可以划分为 $M$ 个组（平均场近似）：</p>
<script type="math/tex; mode=display">
q(Z)=\prod\limits_{i=1}^Mq_i(Z_i)</script><p>因此，$L(q)$ 展开得：</p>
<script type="math/tex; mode=display">
L(q)=\int_Zq(Z)\log p(X,Z)dZ-\int_Zq(Z)\log{q(Z)}</script><p>在 $L(q)$ 中，看其中某一个 $p(Z_j)$ ，则 $L(q)$ 第一项：</p>
<script type="math/tex; mode=display">
\begin{aligned}\int_Zq(Z)\log p(X,Z)dZ&=\int_Z\prod\limits_{i=1}^Mq_i(Z_i)\log p(X,Z)dZ\\
&=\int_{Z_j}q_j(Z_j)\int_{Z-Z_{j}}\prod\limits_{i\ne j}q_i(Z_i)\log p(X,Z)dZ\\
&=\int_{Z_j}q_j(Z_j)\mathbb{E}_{\prod\limits_{i\ne j}q_i(Z_i)}[\log p(X,Z)]dZ_j
\end{aligned}</script><p>$L(q)$ 第二项：</p>
<script type="math/tex; mode=display">
\int_Zq(Z)\log q(Z)dZ=\int_Z\prod\limits_{i=1}^Mq_i(Z_i)\sum\limits_{i=1}^M\log q_i(Z_i)dZ</script><p>$L(q)$ 第二项展开求和项，其第一项为：</p>
<script type="math/tex; mode=display">
\int_Z\prod\limits_{i=1}^Mq_i(Z_i)\log q_1(Z_1)dZ=\int_{Z_1}q_1(Z_1)\log q_1(Z_1)dZ_1+Const</script><p>$L(q)$ 第二项根据这个规律，得到：</p>
<script type="math/tex; mode=display">
\int_Zq(Z)\log q(Z)dZ=\sum\limits_{i=1}^M\int_{Z_i}q_i(Z_i)\log q_i(Z_i)dZ_i=\int_{Z_j}q_j(Z_j)\log q_j(Z_j)dZ_j+Const</script><p>$L(q)$ 两项相减，假设令 $\mathbb{E}_{\prod\limits_{i\ne j}q_i(Z_i)}[\log p(X,Z)]=\log \hat{p}(X,Z_j)$ 可以得到：</p>
<script type="math/tex; mode=display">
\int_{Z_j}q_j(Z_j)\log\frac{\hat{p}(X,Z_j)}{q_j(Z_j)}dZ_j=-KL(q_j(Z_j)||\hat{p}(X,Z_j))\le 0</script><p>于是最大的 $q_j(Z_j)=\hat{p}(X,Z_j)$ 才能得到最大值。对每一个 $q_j$，求这个值时都是固定其余的 $q_i, \small i\ne j$，于是可以使用坐标上升的方法进行迭代求解，上面的推导针对单个样本，但是对数据集也是适用的。</p>
<p>基于平均场假设的变分推断存在一些问题：<br>（1）假设太强，$Z$ 非常复杂的情况下，假设不适用。<br>（2）期望中的积分，可能无法计算。</p>
<h2 id="SGVI"><a href="#SGVI" class="headerlink" title="SGVI"></a>SGVI</h2><p>从 $Z$ 到 $X$ 的过程叫做生成过程或译码（decoder），反过来的过程叫推断过程或编码（encoder），基于平均场的变分推断可以导出坐标上升的算法，但是这个假设在一些情况下假设太强，同时积分也不一定能算。我们知道，优化方法除了坐标上升，还有梯度上升的方式，我们希望通过梯度上升来得到变分推断的另一种算法。</p>
<p>我们的目标函数：</p>
<script type="math/tex; mode=display">
\hat{q}(Z)=\mathop{argmax}_{q(Z)}L(q)</script><p>假定 $q(Z)=q_\phi(Z)$，$q(Z)$ 是关于 $\phi$ 这个参数的概率分布。于是：</p>
<script type="math/tex; mode=display">
\mathop{argmax}_{q(Z)}L(q)=\mathop{argmax}_{\phi}L(\phi)=\mathop{argmax}_{\phi}\mathbb{E}_{q_\phi}[\log p_\theta(x^i,z)-\log q_\phi(z)]</script><p>这里 $x^i$ 表示第 $i$ 个样本。<br>根据梯度上升法，每一次更新为 $\phi^{i+1}=\phi^{i}+\lambda \nabla_\phi L(\phi)$ ，$\lambda$ 为迭代步长，我们只求梯度：</p>
<script type="math/tex; mode=display">
\begin{aligned}\nabla_\phi L(\phi)&=\nabla_\phi\mathbb{E}_{q_\phi}[\log p_\theta(x^i,z)-\log q_\phi(z)]\\
&=\nabla_\phi\int q_\phi(z)[\log p_\theta(x^i,z)-\log q_\phi(z)]dz\\
&=\int\nabla_\phi q_\phi(z)[\log p_\theta(x^i,z)-\log q_\phi(z)]dz+\int q_\phi(z)\nabla_\phi [\log p_\theta(x^i,z)-\log q_\phi(z)]dz\\
&=\int\nabla_\phi q_\phi(z)[\log p_\theta(x^i,z)-\log q_\phi(z)]dz-\int q_\phi(z)\nabla_\phi \log q_\phi(z)dz\\
&=\int\nabla_\phi q_\phi(z)[\log p_\theta(x^i,z)-\log q_\phi(z)]dz-\int \nabla_\phi q_\phi(z)dz\\
&=\int\nabla_\phi q_\phi(z)[\log p_\theta(x^i,z)-\log q_\phi(z)]dz\\
&=\int q_\phi(\nabla_\phi\log q_\phi)(\log p_\theta(x^i,z)-\log q_\phi(z))dz\\
&=\mathbb{E}_{q_\phi}[(\nabla_\phi\log q_\phi)(\log p_\theta(x^i,z)-\log q_\phi(z))]
\end{aligned}</script><p>这个期望可以通过<strong>蒙特卡洛采样</strong>来近似，从而得到梯度，然后利用梯度上升的方法来得到参数：</p>
<script type="math/tex; mode=display">
z^l\sim q_\phi(z),\qquad l=1,2,...L</script><script type="math/tex; mode=display">
\mathbb{E}_{q_\phi}[(\nabla_\phi\log q_\phi)(\log p_\theta(x^i,z)-\log q_\phi(z))]\sim \frac{1}{L}\sum\limits_{l=1}^L(\nabla_\phi\log q_\phi)(\log p_\theta(x^i,z)-\log q_\phi(z))</script><p>但是由于求和符号中存在一个对数项，当 $q_\phi$ 在接近于零得部分变化时，每一点点变化都会使对数值变化很大，导致直接采样的方差很大，这就需要采样的样本非常多，<strong>蒙特卡洛采样</strong>法并不一定work。</p>
<p>为了解决方差太大的问题，我们采用重参数化（Reparameterization）的技巧，在求梯度时：</p>
<script type="math/tex; mode=display">
\nabla_\phi L(\phi)=\nabla_\phi\mathbb{E}_{q_\phi}[\log p_\theta(x^i,z)-\log q_\phi(z)]</script><p>考虑 $\mathbb{E}_{q_\phi}$ 中 $q_\phi$ 是一个确定得分布，和 $\phi$ 无关，此时求关于 $\phi$ 的梯度就很好计算了：</p>
<script type="math/tex; mode=display">
\nabla_\phi\mathbb{E}_{q_\phi}[\log p_\theta(x^i,z)-\log q_\phi(z)]=\mathbb{E}_{q_\phi}\nabla_\phi[\log p_\theta(x^i,z)-\log q_\phi(z)]</script><p>我们取：$z=g_\phi(\varepsilon,x^i),\varepsilon\sim p(\varepsilon)$，于是对后验：$z\sim q_\phi(z|x^i)$，有 $|q_\phi(z|x^i)dz|=|p(\varepsilon)d\varepsilon|$。代入上面的梯度中：</p>
<script type="math/tex; mode=display">
\begin{aligned}
\nabla_\phi L(\phi)&=\nabla_\phi\mathbb{E}_{q_\phi}[\log p_\theta(x^i,z)-\log q_\phi(z)]\\
&=\mathbb{E}_{q_\phi}\nabla_\phi[\log p_\theta(x^i,z)-\log q_\phi(z)]\\
&=\mathbb{E}_{p(\varepsilon)}[\nabla_\phi[\log p_\theta(x^i,z)-\log q_\phi(z)]]\\
&=\mathbb{E}_{p(\varepsilon)}[\nabla_z[\log p_\theta(x^i,z)-\log q_\phi(z)]\nabla_\phi z]\\
&=\mathbb{E}_{p(\varepsilon)}[\nabla_z[\log p_\theta(x^i,z)-\log q_\phi(z)]\nabla_\phi g_\phi(\varepsilon,x^i)]
\end{aligned}</script><p>对这个式子进行蒙特卡洛采样，然后计算期望，得到梯度。</p>
<div class="note info"><p>文章转载自<span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL3NodWh1YWkwMDc=">Jie Zhou<i class="fa fa-external-link-alt"></i></span>的<span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL3NodWh1YWkwMDcvTWFjaGluZS1MZWFybmluZy1TZXNzaW9u">Machine-Learning-Session<i class="fa fa-external-link-alt"></i></span>。</p>
</div>
      
    </div>

    
    
    
      


    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://soundmemories.github.io/2020/09/30/Machine%20Learning/25.GMM/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="https://i.loli.net/2020/11/04/6JhNuwtBe4adylS.png">
      <meta itemprop="name" content="SoundMemories">
      <meta itemprop="description" content="今日事，今日毕">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="SoundMemories">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/09/30/Machine%20Learning/25.GMM/" class="post-title-link" itemprop="url">GMM</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2020-09-30 00:00:00" itemprop="dateCreated datePublished" datetime="2020-09-30T00:00:00+08:00">2020-09-30</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Machine-Learning/" itemprop="url" rel="index"><span itemprop="name">Machine Learning</span></a>
        </span>
    </span>

  
      </div>
      <div class="post-meta">
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>3.1k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>3 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="高斯混合模型"><a href="#高斯混合模型" class="headerlink" title="高斯混合模型"></a>高斯混合模型</h1><p>为了解决高斯模型的单峰性的问题，我们引入多个高斯模型的加权平均来拟合多峰数据：</p>
<script type="math/tex; mode=display">
p(x)=\sum\limits_{k=1}^K\alpha_k\mathcal{N}(\mu_k,\Sigma_k)</script><p>引入隐变量 $z$，这个变量表示对应的样本 $x$ 属于哪一个高斯分布，这个变量是一个离散的随机变量：</p>
<script type="math/tex; mode=display">
p(z=i)=p_i,\sum\limits_{i=1}^kp(z=i)=1</script><p>作为一个生成式模型，高斯混合模型通过隐变量 $z$ 的分布来生成样本。</p>
<p>其中，节点 $z$ 就是上面的概率，$x$ 就是生成的高斯分布。于是对 $p(x)$：</p>
<script type="math/tex; mode=display">
p(x)=\sum\limits_zp(x,z)=\sum\limits_{k=1}^Kp(x,z=k)=\sum\limits_{k=1}^Kp(z=k)p(x|z=k)</script><p>因此：</p>
<script type="math/tex; mode=display">
p(x)=\sum\limits_{k=1}^Kp_k\mathcal{N}(x|\mu_k,\Sigma_k)</script><h2 id="极大似然估计"><a href="#极大似然估计" class="headerlink" title="极大似然估计"></a>极大似然估计</h2><p>样本为 $X=(x_1,x_2,\cdots,x_N)$，$ (X,Z)$ 为完全参数，参数为 $\theta=\{p_1,p_2,\cdots,p_K,\mu_1,\mu_2,\cdots,\mu_K\Sigma_1,\Sigma_2,\cdots,\Sigma_K\}$。我们通过极大似然估计得到 $\theta$ 的值：</p>
<script type="math/tex; mode=display">
\begin{aligned}\theta_{MLE}&=\mathop{argmax}\limits_{\theta}\log p(X)=\mathop{argmax}_{\theta}\sum\limits_{i=1}^N\log p(x_i)\\
&=\mathop{argmax}_\theta\sum\limits_{i=1}^N\log \sum\limits_{k=1}^Kp_k\mathcal{N}(x_i|\mu_k,\Sigma_k)
\end{aligned}</script><p>这个表达式直接通过求导，由于连加号的存在，无法得到解析解。因此需要使用 EM 算法。</p>
<h2 id="EM-求解-GMM"><a href="#EM-求解-GMM" class="headerlink" title="EM 求解 GMM"></a>EM 求解 GMM</h2><p>EM 算法的基本表达式为：$\theta^{t+1}=\mathop{argmax}\limits_{\theta}\mathbb{E}_{z|x,\theta_t}[p(x,z|\theta)]$。套用 GMM 的表达式，对数据集来说：</p>
<script type="math/tex; mode=display">
\begin{aligned}Q(\theta,\theta^t)&=\sum\limits_z[\log\prod\limits_{i=1}^Np(x_i,z_i|\theta)]\prod \limits_{i=1}^Np(z_i|x_i,\theta^t)\\
&=\sum\limits_z[\sum\limits_{i=1}^N\log p(x_i,z_i|\theta)]\prod \limits_{i=1}^Np(z_i|x_i,\theta^t)
\end{aligned}</script><p>对于中间的那个求和号，展开，第一项为：</p>
<script type="math/tex; mode=display">
\begin{aligned}
\sum\limits_z\log p(x_1,z_1|\theta)\prod\limits_{i=1}^Np(z_i|x_i,\theta^t)&=\sum\limits_z\log p(x_1,z_1|\theta)p(z_1|x_1,\theta^t)\prod\limits_{i=2}^Np(z_i|x_i,\theta^t)\\
&=\sum\limits_{z_1}\log p(x_1,z_1|\theta)
p(z_1|x_1,\theta^t)\sum\limits_{z_2,\cdots,z_K}\prod\limits_{i=2}^Np(z_i|x_i,\theta^t)\\
&=\sum\limits_{z_1}\log p(x_1,z_1|\theta)p(z_1|x_1,\theta^t)\end{aligned}</script><p>类似地，$Q$ 可以写为：</p>
<script type="math/tex; mode=display">
Q(\theta,\theta^t)=\sum\limits_{i=1}^N\sum\limits_{z_i}\log p(x_i,z_i|\theta)p(z_i|x_i,\theta^t)</script><p>对于 $p(x,z|\theta)$：</p>
<script type="math/tex; mode=display">
p(x,z|\theta)=p(z|\theta)p(x|z,\theta)=p_z\mathcal{N}(x|\mu_z,\Sigma_z)</script><p>对 $p(z|x,\theta^t)$：</p>
<script type="math/tex; mode=display">
p(z|x,\theta^t)=\frac{p(x,z|\theta^t)}{p(x|\theta^t)}=\frac{p_z^t\mathcal{N}(x|\mu_z^t,\Sigma_z^t)}{\sum\limits_kp_k^t\mathcal{N}(x|\mu_k^t,\Sigma_k^t)}</script><p>代入 $Q$：</p>
<script type="math/tex; mode=display">
Q=\sum\limits_{i=1}^N\sum\limits_{z_i}\log p_{z_i}\mathcal{N(x_i|\mu_{z_i},\Sigma_{z_i})}\frac{p_{z_i}^t\mathcal{N}(x_i|\mu_{z_i}^t,\Sigma_{z_i}^t)}{\sum\limits_kp_k^t\mathcal{N}(x_i|\mu_k^t,\Sigma_k^t)}</script><p>下面需要对 $Q$ 值求最大值：</p>
<script type="math/tex; mode=display">
Q=\sum\limits_{k=1}^K\sum\limits_{i=1}^N[\log p_k+\log \mathcal{N}(x_i|\mu_k,\Sigma_k)]p(z_i=k|x_i,\theta^t)</script><ol>
<li><p>$p_k^{t+1}$：</p>
<script type="math/tex; mode=display">
p_k^{t+1}=\mathop{argmax}_{p_k}\sum\limits_{k=1}^K\sum\limits_{i=1}^N[\log p_k+\log \mathcal{N}(x_i|\mu_k,\Sigma_k)]p(z_i=k|x_i,\theta^t)\ s.t.\ \sum\limits_{k=1}^Kp_k=1</script><p>即：</p>
<script type="math/tex; mode=display">
p_k^{t+1}=\mathop{argmax}_{p_k}\sum\limits_{k=1}^K\sum\limits_{i=1}^N\log p_kp(z_i=k|x_i,\theta^t)\ s.t.\ \sum\limits_{k=1}^Kp_k=1</script><p>引入 Lagrange 乘子：$L(p_k,\lambda)=\sum\limits_{k=1}^K\sum\limits_{i=1}^N\log p_kp(z_i=k|x_i,\theta^t)-\lambda(1-\sum\limits_{k=1}^Kp_k)$。所以：</p>
<script type="math/tex; mode=display">
\frac{\partial}{\partial p_k}L=\sum\limits_{i=1}^N\frac{1}{p_k}p(z_i=k|x_i,\theta^t)+\lambda=0\\
\Rightarrow \sum\limits_k\sum\limits_{i=1}^N\frac{1}{p_k}p(z_i=k|x_i,\theta^t)+\lambda\sum\limits_kp_k=0\\
\Rightarrow\lambda=-N</script><p>于是有：</p>
<script type="math/tex; mode=display">
p_k^{t+1}=\frac{1}{N}\sum\limits_{i=1}^Np(z_i=k|x_i,\theta^t)</script></li>
<li><p>$\mu_k,\Sigma_k$，这两个参数是无约束的，直接求导即可。</p>
</li>
</ol>
<div class="note info"><p>文章转载自<span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL3NodWh1YWkwMDc=">Jie Zhou<i class="fa fa-external-link-alt"></i></span>的<span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL3NodWh1YWkwMDcvTWFjaGluZS1MZWFybmluZy1TZXNzaW9u">Machine-Learning-Session<i class="fa fa-external-link-alt"></i></span>。</p>
</div>
      
    </div>

    
    
    
      


    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>





  
  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><span class="space">&hellip;</span><a class="page-number" href="/page/9/">9</a><a class="extend next" rel="next" href="/page/2/"><i class="fa fa-angle-right" aria-label="下一页"></i></a>
  </nav>



      

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      const activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      const commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

    </div>
  </main>

  <footer class="footer">
    <div class="footer-inner">
      

      

<div class="copyright">
  
  &copy; 2019 – 
  <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">SoundMemories</span>
</div>
  <div class="powered-by">由 <span class="exturl theme-link" data-url="aHR0cHM6Ly9oZXhvLmlv">Hexo</span> & <span class="exturl theme-link" data-url="aHR0cHM6Ly90aGVtZS1uZXh0LmpzLm9yZy9tdXNlLw==">NexT.Muse</span> 强力驱动
  </div>

    </div>
  </footer>

  
  <script src="//cdn.jsdelivr.net/npm/animejs@3.2.1/lib/anime.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/@next-theme/pjax@0.4.0/pjax.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/jquery@3.5.1/dist/jquery.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js"></script>
<script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/muse.js"></script><script src="/js/next-boot.js"></script><script src="/js/bookmark.js"></script>
  <script>
var pjax = new Pjax({
  selectors: [
    'head title',
    '.page-configurations',
    '.main-inner',
    '.post-toc-wrap',
    '.languages',
    '.pjax'
  ],
  analytics: false,
  cacheBust: false,
  scrollRestoration: false,
  scrollTo: !CONFIG.bookmark.enable
});

document.addEventListener('pjax:success', () => {
  pjax.executeScripts(document.querySelectorAll('script[data-pjax], .pjax script'));
  NexT.boot.refresh();
  // Define Motion Sequence & Bootstrap Motion.
  if (CONFIG.motion.enable) {
    NexT.motion.integrator
      .init()
      .add(NexT.motion.middleWares.subMenu)
      .add(NexT.motion.middleWares.postList)
      .bootstrap();
  }
  const hasTOC = document.querySelector('.post-toc');
  document.querySelector('.sidebar-inner').classList.toggle('sidebar-nav-active', hasTOC);
  document.querySelector(hasTOC ? '.sidebar-nav-toc' : '.sidebar-nav-overview').click();
  NexT.utils.updateSidebarPosition();
});
</script>


  




  <script src="/js/local-search.js"></script>








<script data-pjax>
document.querySelectorAll('.pdfobject-container').forEach(element => {
  const url = element.dataset.target;
  const pdfOpenParams = {
    navpanes : 0,
    toolbar  : 0,
    statusbar: 0,
    pagemode : 'thumbs',
    view     : 'FitH'
  };
  const pdfOpenFragment = '#' + Object.entries(pdfOpenParams).map(([key, value]) => `${key}=${encodeURIComponent(value)}`).join('&');
  const fullURL = `/lib/pdf/web/viewer.html?file=${encodeURIComponent(url)}${pdfOpenFragment}`;

  if (NexT.utils.supportsPDFs()) {
    element.innerHTML = `<embed class="pdfobject" src="${url + pdfOpenFragment}" type="application/pdf" style="height: ${element.dataset.height};">`;
  } else {
    element.innerHTML = `<iframe src="${fullURL}" style="height: ${element.dataset.height};" frameborder="0"></iframe>`;
  }
});
</script>


<script data-pjax>
if (document.querySelectorAll('.mermaid').length) {
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/mermaid@8.8.2/dist/mermaid.min.js', () => {
    mermaid.init({
      theme    : 'neutral',
      logLevel : 3,
      flowchart: { curve     : 'linear' },
      gantt    : { axisFormat: '%m/%d/%Y' },
      sequence : { actorMargin: 50 }
    }, '.mermaid');
  }, window.mermaid);
}
</script>





  








    <div class="pjax">
  

  
      <script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              const target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    const script = document.createElement('script');
    script.src = '//cdn.jsdelivr.net/npm/mathjax@3.1.2/es5/tex-mml-chtml.js';
    script.defer = true;
    document.head.appendChild(script);
  } else {
    MathJax.startup.document.state(0);
    MathJax.typesetClear();
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  
  <script src="//cdn.jsdelivr.net/npm/quicklink@2.0.0/dist/quicklink.umd.js"></script>
  <script>
      window.addEventListener('load', () => {
      quicklink.listen({
        timeout : 3000,
        priority: true,
        ignores : [uri => uri.includes('#'),uri => uri === 'https://soundmemories.github.io/',]
      });
      });
  </script>

    </div>
</body>
</html>
