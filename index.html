<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222" media="(prefers-color-scheme: light)">
<meta name="theme-color" content="#222" media="(prefers-color-scheme: dark)"><meta name="generator" content="Hexo 6.3.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="https://fonts.loli.net/css?family=Monda:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">

<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css" integrity="sha256-HtsXJanqjKTc8vVQjO4YMhiqFoXkfBsjBWcX91T1jr8=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.css" integrity="sha256-Vzbj7sDDS/woiFS3uNKo8eIuni59rjyNGtXfstRzStA=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"soundmemories.github.io","root":"/","images":"/images","scheme":"Muse","darkmode":true,"version":"8.17.1","exturl":true,"sidebar":{"position":"left","display":"always","padding":18,"offset":12},"copycode":{"enable":true,"style":null},"bookmark":{"enable":true,"color":"#222","save":"auto"},"mediumzoom":true,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"},"path":"/search.xml","localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false}}</script><script src="/js/config.js"></script>

    <meta name="description" content="今日事，今日毕">
<meta property="og:type" content="website">
<meta property="og:title" content="SoundMemories">
<meta property="og:url" content="https://soundmemories.github.io/index.html">
<meta property="og:site_name" content="SoundMemories">
<meta property="og:description" content="今日事，今日毕">
<meta property="og:locale" content="zh_CN">
<meta property="article:author" content="SoundMemories">
<meta name="twitter:card" content="summary">


<link rel="canonical" href="https://soundmemories.github.io/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":true,"isPost":false,"lang":"zh-CN","comments":"","permalink":"","path":"index.html","title":""}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>SoundMemories</title>
  








  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <h1 class="site-title">SoundMemories</h1>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="搜索" role="button">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签<span class="badge">8</span></a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类<span class="badge">8</span></a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档<span class="badge">122</span></a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup"><div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" maxlength="80"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close" role="button">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="search-result-container no-result">
  <div class="search-result-icon">
    <i class="fa fa-spinner fa-pulse fa-5x"></i>
  </div>
</div>

    </div>
  </div>

</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-overview-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="SoundMemories"
      src="/images/avstar.png">
  <p class="site-author-name" itemprop="name">SoundMemories</p>
  <div class="site-description" itemprop="description">今日事，今日毕</div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">122</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">8</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">8</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author animated">
      <span class="links-of-author-item">
        <span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL3NvdW5kbWVtb3JpZXM=" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;soundmemories"><i class="fab fa-github fa-fw"></i>GitHub</span>
      </span>
      <span class="links-of-author-item">
        <span class="exturl" data-url="bWFpbHRvOnNvdW5kbWVtb3JpZXNAMTYzLmNvbQ==" title="E-Mail → mailto:soundmemories@163.com"><i class="fa fa-envelope fa-fw"></i>E-Mail</span>
      </span>
  </div>
  <div class="cc-license animated" itemprop="license">
    <span class="exturl cc-opacity" data-url="aHR0cHM6Ly9jcmVhdGl2ZWNvbW1vbnMub3JnL2xpY2Vuc2VzL2J5LW5jLXNhLzQuMC8="><img src="https://cdnjs.cloudflare.com/ajax/libs/creativecommons-vocabulary/2020.11.3/assets/license_badges/small/by_nc_sa.svg" alt="Creative Commons"></span>
  </div>

        </div>
      </div>
    </div>

    
        <div class="pjax">
        </div>
  </aside>


    </div>

    <div class="main-inner index posts-expand">

    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://soundmemories.github.io/2023/05/21/NLP/10.%E7%BA%A0%E9%94%99/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avstar.png">
      <meta itemprop="name" content="SoundMemories">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="SoundMemories">
      <meta itemprop="description" content="今日事，今日毕">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | SoundMemories">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2023/05/21/NLP/10.%E7%BA%A0%E9%94%99/" class="post-title-link" itemprop="url">纠错</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2023-05-21 00:00:00" itemprop="dateCreated datePublished" datetime="2023-05-21T00:00:00+08:00">2023-05-21</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/NLP/" itemprop="url" rel="index"><span itemprop="name">NLP</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>2.8k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>10 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="简介">简介</h1>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">|-业界产品</span><br><span class="line">    |-百度AI纠错服务</span><br><span class="line">    |-科大讯飞纠错服务</span><br><span class="line"></span><br><span class="line">|-纠错场景</span><br><span class="line">    |-对话系统</span><br><span class="line">    |-asr语音识别</span><br><span class="line">    |-ocr文字识别</span><br><span class="line">    |-文本内容校验</span><br><span class="line">|-文本错误类型</span><br><span class="line">    |-通用场景：不需要专业知识，普通人就能看出的错误。</span><br><span class="line">    |-垂直场景：需要领域内专业知识，才能发现的错误。</span><br><span class="line">|-纠错问题解决思路</span><br><span class="line">    |-基于规则和统计学习</span><br><span class="line">    |-基于深度学习模型</span><br><span class="line">|-业界解决方案</span><br><span class="line">    |-端到端纠错：bart生成式</span><br><span class="line">    |-阶段性纠错：平安寿险、小爱同学、爱奇艺FASPell</span><br><span class="line">|-方案拆解</span><br><span class="line">    |-通用文本错误</span><br><span class="line">        |-Macbert4csc模型</span><br><span class="line">        |-数据增强</span><br><span class="line">    |-垂直领域专有名词错误</span><br><span class="line">        |-成分抽取</span><br><span class="line">        |-对比学习下的语义相似度</span><br><span class="line">        |-实体链接</span><br><span class="line">        |-faiss语义搜索和向量检索</span><br><span class="line">        |-Ranking</span><br></pre></td></tr></table></figure>
<h1 id="错误类型">错误类型</h1>
<p>用于纠错的文本错误类型从宏观上可分为两大类：<br />
1、<strong>通用文本错误</strong>，常见的多字、漏字、错字和别字等等。<br />
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">类别            细类                        例子</span><br><span class="line">---------------------------------------------------------</span><br><span class="line">音似    谐音/混淆音/拼音全拼/拼音缩写       配副眼(睛)-配副眼(镜)/(流浪)织女-(牛郎)织女/xingfu-幸福/bj-北京</span><br><span class="line">形似    形似                              高(梁)-高(粱)</span><br><span class="line">多字    多字                              即帅(又又)高-即帅(又)高</span><br><span class="line">少字    少字                              爱有天意-(假如)爱有天意</span><br><span class="line">乱序    顺序颠倒                           伍迪艾伦-艾伦伍迪</span><br><span class="line">语法    搭配错误                           想象难以-难以想象</span><br></pre></td></tr></table></figure></p>
<p>2、<strong>专有领域错误</strong>，垂直领域下专有名词、专有术语的错误。<br />
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">领域     类型                 例子</span><br><span class="line">---------------------------------------------------------</span><br><span class="line">金融    股票名          (山河药辅)分时资金-(山河药铺)分时资金</span><br><span class="line">医疗    疾病/药物名     肌(萎缩)性侧索硬化症-肌性侧索硬化症/(砒)罗(昔)康-(眦)罗(喜)康</span><br></pre></td></tr></table></figure><br />
一些成熟的纠错产品：<span class="exturl" data-url="aHR0cHM6Ly96ai54Znl1bi5jbi8=">讯飞智检<i class="fa fa-external-link-alt"></i></span>，<span class="exturl" data-url="aHR0cHM6Ly9qZHQubWlkdS5jb20vcHJvZHVjZQ==">校对通<i class="fa fa-external-link-alt"></i></span>。</p>
<p>成熟产品可作为参考，比如讯飞智检中有日期、成语古诗、错别词、语法、机构、地点名称等纠错；校对通中有对标点、知识性差错、内容导向风险识别等。</p>
<p>校对通还对错误检测类型进行了详细的介绍：<br />
1、<strong>通用文本差错</strong>：错别字/词、多字、少字、语义重复、语序错误、句式杂糅、标点符号错误、量词和单位错误、数字错误、序号检查、句子查重，英文校对。<br />
2、<strong>知识性差错</strong>：重要讲话引用、姓名和职务信息、地理名词、机构名称、专有名词及术语、法律法规名称、常识差错、时政重点词、媒体报道禁用词和慎用词。<br />
3、<strong>内容导向风险识别</strong>：涉国家统一及主权和领土完整、涉民族宗教、涉黄赌毒暴恐、涉低俗辱骂、涉违法违规(如广告违禁)、其他敏感内容。</p>
<h1 id="关键技术">关键技术</h1>
<p><strong>语言知识学习</strong>：可以理解为是对语言规则等先验知识的学习，通过学习词法、句法等规则进行语言模型构建，例如中英文的主谓宾结构就是不一样的。</p>
<p><strong>上下文理解</strong>：是指分析错误点上下文语境和语义，从纠错候选中选择最合适的。尤其是中文，相同的词汇在不同语境中往往表达不同的含义。</p>
<p><strong>知识计算</strong>：知识计算主要包括关联知识计算和文本理解，关联知识主要是通过对全局知识的统计来实现纠错，可以是局部不完整语句的补充。文本理解是通过统计理解全局句子内容，解决低频领域知识的泛化问题。</p>
<h1 id="产品设计">产品设计</h1>
<p><strong>用户场景</strong>：审稿或者编辑人员输入中文文字信息，系统自动纠错，并给出修改建议，审稿人员对错误快速修订。<br />
<strong>应用边界</strong>：(1)
支持用词错误检测，针对音近、形近的错字和别字进行纠正。(2)
支持句子级错误检测，主要是针对句子中出现的多字、少字等错误，相对难度校大。(3)
支持场景类错误纠正，这类错误需要具备一些特定领域的知识才能识别纠错，所以尽量支持。</p>
<p><strong>产品定位</strong>：为应用工具型产品，实现中文文本自动纠错功能。<br />
<strong>用户定位</strong>：满足两类B端用户，(1)
针对具备自主的文稿编辑工具，提供API服务，与现有系统进行改造融合；(2)
针对缺少文稿编辑工具的用户，提供web页面功能。</p>
<h1 id="规则思路">规则思路</h1>
<p>中文纠错首先<strong>错误检测</strong>，其次是<strong>错误纠正</strong>。</p>
<ul>
<li><strong>错误检测</strong>：对于中文首先要分词，可以使用<span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL2Z4c2p5L2ppZWJh">jieba<i class="fa fa-external-link-alt"></i></span>、<span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL2xhbmNvcGt1L3BrdXNlZy1weXRob24=">pkuseg<i class="fa fa-external-link-alt"></i></span>等工具分词，但由于句子中存在错误字，所以分词可能出现错误的情况，可从<strong>字粒度</strong>和<strong>词粒度</strong>两方面检测错误，整合这两种粒度的疑似错误结果，形成疑似错误位置候选集。
<ul>
<li><strong>字粒度处理</strong>：先对句子进行n-gram的划分，之后根据语言模型对这些n-gram打分，汇总每个字下所有n-gram评分，通过计算MAD(<span
class="math inline">\(\overline\sum|\text{每个字gram}-\text{当前字下所有gram的均值}|\)</span>)获得每个字的得分，通过设置超参数阈值和这些字的得分比较，如果高于阈值就认为这个字是错误的，把这个字和它的位置加入到字粒度疑似错误候选项里。</li>
<li><strong>词粒度处理</strong>：首先准备一个常用词词典，如果句子中有gram出现在词典就是正确的，没有出现就可能是错误的，加入到词粒度疑似错误候选项里。</li>
</ul></li>
<li><strong>错误纠正</strong>：由字和词度疑似错误候选项组成的集合就是混淆字典。遍历混淆字典每个项，通过形近和音似字典替换它在句子中的位置，通过语言模型计算句子的困惑度，困惑度排序值越小代表句子可能性越高，这样得到最优的纠正结果。<br />
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">|-规则纠错:</span><br><span class="line">    |-字粒度-语言模型-平均绝对离差(MAD)-基于字粒度疑似错误候选项</span><br><span class="line">        |-混淆字典-困惑度(perplexity)-排序-输出</span><br><span class="line">    |-词粒度-常用词典-基于词粒度疑似错误候选项</span><br><span class="line">        |-混淆字典-困惑度(perplexity)-排序-输出</span><br></pre></td></tr></table></figure></li>
<li><strong>缺点</strong>：(1)
强依赖形近和音似等常用词字典，这种方式会造成精准率还可以，但召回率是极低的，整体的效果还是一般的。
(2)
需要很多遍历搜索的操作，性能提升也是问题，这与字典的量级有关系。</li>
</ul>
<h1 id="模型方法">模型方法</h1>
<p>主要分为<strong>端到端纠错</strong>和<strong>分阶段纠错</strong>。</p>
<h2 id="端到端">端到端</h2>
<p>学术界大多基于与训练模型的方式进行端到端纠错，代表如下几种：<br />
<span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzE5MTAuMTM0NjEucGRm">BART: Denoising
Sequence-to-Sequence Pre-training for Natural Language Generation,
Translation, and Comprehension<i class="fa fa-external-link-alt"></i></span>：<br />
<strong>Auto-encoding</strong>(自编码模型)，Bert就是Auto-encoding的一个典型代表。不仅如此，Bert采用了MLM，在输入层加入一定的噪音，是一种典型的DAELM(Denoising
Autoencoder)降噪自编码。<br />
<strong>Autoregressive</strong>(自回归模型)，典型代表是GPT系列的模型，是一种基于上文预测下文或下文预测上文的模型。<br />
<strong>BART是Auto-encoding和Autoregressive组合</strong>，结合自回归和自编码模型的优点，结构上和Transformer没什么不同，其主要区别在训练的任务和目标不同。<br />
<strong>BART预训练</strong>：通过破坏文档再优化重建损失(即解码器输出和原始文档之间的交叉熵)训练得到的。与目前仅适合特定噪声机制的去噪自编码不同，BART可应用于任意类型的文档破坏。<br />
<strong>BART的finetune</strong>：由于BART具备自回归解码器，因此它可以针对序列生成任务进行直接微调，如抽象问答和摘要。在这两项任务中，信息复制自输入但经过了处理，这与去噪预训练紧密相关，这里，编码器的输入是输入序列，解码器以自回归的方式生成输出。</p>
<p>BART使用标准Transformer结果，但还是有些区别：<br />
(1) 同GPT一样，将ReLU激活函数改为GeLU，并且参数初始化服从正态分布<span
class="math inline">\(N(0,0.02)\)</span>。<br />
(2) BART
base模型的Encoder和Decoder各有6层，large模型增加到了12层。<br />
(3) BART解码器的各层对编码器最终隐藏层额外执行cross-attention。<br />
(4) BERT在词预测之前使用了额外的Feed Forward Layer，而BART没有。</p>
<h2 id="分阶段">分阶段</h2>
<h3 id="平安寿险">平安寿险</h3>
<p>平安寿险纠错方案(误报率0.1%，召回率70%)：<br />
<img src="/images/纠错/平安寿险.jpg" width="90%"><br />
在日常生活中，比如微信、微博等社交工具中会发现许多错别字，在这几个方面平安对文本出错概率进行了统计。<br />
(1) 微博等新媒体领域，文本出错概率在2%左右；<br />
(2) 语音识别领域，出错最高可达8-10%；<br />
(3)
平安人寿问答领域，用户提问错误率在去重后仍高达9%(应该包含直接输入错误和语音识别错误)。<br />
<img src="/images/纠错/平安寿险2.jpg" width="90%"><br />
上图是其纠错系统的流程图：<br />
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"># 此系统更注重效果，未对效率和性能进行优化</span><br><span class="line">|-错误检测</span><br><span class="line">    |-拼音匹配：规则上的处理，文字-拼音(通过拼音查找准确词)-文字，这种规则处理方式。</span><br><span class="line">    |-单双向2gram语言模型：基于输入中词与词搭配正确的比错误的多的假设，</span><br><span class="line">      句子中某些词前后组成的2gram分很低，这个词就可能是错误的，加入错误候选集中。</span><br><span class="line">    |-BILSTM+CRF实体识别的分类模型，看成是序列标注任务，也可以学习bert的mlm任务，</span><br><span class="line">      每个词都进行完形填空，如果实际词不在预测的候选集中，这个词就可能是错误的，加入错误候选集中。</span><br><span class="line">|-候选召回</span><br><span class="line">    |-拼音倒排、汉字倒排、同音召回、语言模型。</span><br><span class="line">    |-混淆字集：通过业界积累，以种子词为核心扩展，通过形近、音近、编辑距离等构建错误集合。</span><br><span class="line">|-候选排序</span><br><span class="line">    |-一级排序(粗排)：词频变化、编辑距离、拼音距离、语言模型，LR。</span><br><span class="line">    |-二级排序(精排)：词频变化、分词变化、音形距离、PMI变化、统计语言模型变化、nn语言模型变化。</span><br><span class="line">|-候选筛选：交叉处理、包含处理、点位跟踪。</span><br><span class="line">|-底层资源：双数组字典树、CSR压缩、分层倒排、4gram语言模型、nn语言模型。</span><br></pre></td></tr></table></figure></p>
<h1 id="参考文献">参考文献</h1>
<p><span class="exturl" data-url="aHR0cHM6Ly93d3cuamlhbnNodS5jb20vcC84ZDJmMDk5MWI4NDA=">中文文本纠错任务简介<i class="fa fa-external-link-alt"></i></span><br />
<span class="exturl" data-url="aHR0cHM6Ly9iYWlqaWFoYW8uYmFpZHUuY29tL3M/aWQ9MTczMzg3MzU5MDEwMTUzMzc4NCZ3ZnI9c3BpZGVyJmZvcj1wYw==">文本校对产品的应用设计场景<i class="fa fa-external-link-alt"></i></span><br />
<span class="exturl" data-url="aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl8zOTUzMzM2MS9hcnRpY2xlL2RldGFpbHMvMTEyMTU3OTAy">中文文本纠错_垂直电商搜索纠错<i class="fa fa-external-link-alt"></i></span><br />
<span class="exturl" data-url="aHR0cHM6Ly93d3cuaW5mb3EuY24vYXJ0aWNsZS81V3FrZUhsSkJFWjhqUlU5Sms2Uy8=">小米小爱同学-基于BERT的ASR纠错<i class="fa fa-external-link-alt"></i></span><br />
<span class="exturl" data-url="aHR0cHM6Ly96aHVhbmxhbi56aGlodS5jb20vcC8xNTkxMDE4NjA=">平安寿险-文本纠错技术探索和实践<i class="fa fa-external-link-alt"></i></span><br />
<span class="exturl" data-url="aHR0cHM6Ly93d3cub3NjaGluYS5uZXQvcXVlc3Rpb24vMjkxODE4Ml8yMzEyNzg1Lw==">爱奇艺-开源SOTA高性能中文拼写检查工具FASPell<i class="fa fa-external-link-alt"></i></span><br />
<span class="exturl" data-url="aHR0cHM6Ly96aHVhbmxhbi56aGlodS5jb20vcC8zNzg0MTk5OTA=">文本纠错目前是怎么做的,BERT怎么做文本纠错<i class="fa fa-external-link-alt"></i></span><br />
<span class="exturl" data-url="aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzE1ODIxNDg3L2FydGljbGUvZGV0YWlscy8xMTk5OTc4ODI=">BART模型<i class="fa fa-external-link-alt"></i></span><br />
<span class="exturl" data-url="aHR0cHM6Ly9hZGFuaW5nLmdpdGh1Yi5pby9wb3N0cy8xMzk0Lmh0bWw=">BART和mBART<i class="fa fa-external-link-alt"></i></span></p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://soundmemories.github.io/2023/04/05/NLP/09.%E5%88%86%E8%AF%8D/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avstar.png">
      <meta itemprop="name" content="SoundMemories">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="SoundMemories">
      <meta itemprop="description" content="今日事，今日毕">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | SoundMemories">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2023/04/05/NLP/09.%E5%88%86%E8%AF%8D/" class="post-title-link" itemprop="url">分词</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2023-04-05 00:00:00" itemprop="dateCreated datePublished" datetime="2023-04-05T00:00:00+08:00">2023-04-05</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/NLP/" itemprop="url" rel="index"><span itemprop="name">NLP</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>7</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>1 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2 id="闲聊式">闲聊式</h2>
<h2 id="参考文献">参考文献</h2>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://soundmemories.github.io/2021/07/23/NLP/08.%E8%92%B8%E9%A6%8F/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avstar.png">
      <meta itemprop="name" content="SoundMemories">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="SoundMemories">
      <meta itemprop="description" content="今日事，今日毕">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | SoundMemories">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2021/07/23/NLP/08.%E8%92%B8%E9%A6%8F/" class="post-title-link" itemprop="url">蒸馏</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2021-07-23 00:00:00" itemprop="dateCreated datePublished" datetime="2021-07-23T00:00:00+08:00">2021-07-23</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/NLP/" itemprop="url" rel="index"><span itemprop="name">NLP</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>9.6k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>35 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="模型压缩">模型压缩</h1>
<p><strong>1、为什么需要模型压缩？</strong><br />
理论上来说，深度神经网络模型越深，非线性程度也就越大，相应的对现实问题的表达能力越强，但相应的代价是，训练成本和模型大小的增加。同时，在部署时，大模型预测速度较低且需要更好的硬件支持。但随着深度学习越来越多的参与到产业中，很多情况下，需要将模型在手机端、IoT端部署，这种部署环境受到能耗和设备体积的限制，端侧硬件的计算能力和存储能力相对较弱，突出的诉求主要体现在以下三点：<br />
-
首先是<strong>速度</strong>，比如像人脸闸机、人脸解锁手机等应用，对响应速度比较敏感，需要做到实时响应。<br />
-
其次是<strong>存储</strong>，比如电网周边环境监测这个应用场景中，要图像目标检测模型部署在可用内存只有200M的监控设备上，且当监控程序运行后，剩余内存会小于30M。<br />
-
最后是<strong>耗能</strong>，离线翻译这种移动设备内置AI模型的能耗直接决定了它的续航能力。</p>
<p>以上三点诉求都需要我们根据终端环境对现有模型进行小型化处理，在不损失精度的情况下，让模型的体积更小、速度更快，能耗更低。</p>
<p>但如何能产出小模型呢？常见的方式包括设计更高效的网络结构、将模型的参数量变少、将模型的计算量减少，同时提高模型的精度。
可能有人会提出疑问，为什么不直接设计一个小模型？
要知道，实际业务子垂类众多，任务复杂度不同，在这种情况下，人工设计有效小模型难度非常大，需要非常强的领域知识。而模型压缩可以在经典小模型的基础上，稍作处理就可以快速拔高模型的各项性能，达到“多快好省”的目的。</p>
<p><strong>2、模型压缩的基本方法</strong><br />
-
<strong>剪裁</strong>：类似“化学结构式的减肥”，将模型结构中对预测结果不重要的网络结构剪裁掉，使网络结构变得更加
”瘦身“。比如，在每层网络，有些神经元节点的权重非常小，对模型加载信息的影响微乎其微。如果将这些权重较小的神经元删除，则既能保证模型精度不受大影响，又能减小模型大小。<br />
-
<strong>量化</strong>：类似“量子级别的减肥”，神经网络模型的参数一般都用float32的数据表示，但如果我们将float32的数据计算精度变成int8的计算精度，则可以牺牲一点模型精度来换取更快的计算速度。<br />
-
<strong>蒸馏</strong>：类似“老师教学生”，使用一个效果好的大模型指导一个小模型训练，因为大模型可以提供更多的软分类信息量，所以会训练出一个效果接近大模型的小模型。<br />
-
<strong>神经网络架构搜索</strong>（NAS）：类似“化学结构式的重构”，以模型大小和推理速度为约束进行模型结构搜索，从而获得更高效的网络结构。</p>
<p>除此以外，还有权重共享、低秩分解等技术也可实现模型压缩。</p>
<h1 id="模型蒸馏原理">模型蒸馏原理</h1>
<p>Hinton提出了知识蒸馏（Knowledge
Distillation）的概念，旨在把一个大模型或者多个模型ensemble学到的知识迁移到另一个轻量级单模型上，方便部署。简单的说就是用小模型去学习大模型的预测结果，而不是直接学习训练集中的label。</p>
<p>在蒸馏的过程中，我们将原始大模型称为教师模型（teacher），新的小模型称为学生模型（student），训练集中的标签称为hard
label，教师模型预测的概率输出为soft label，temperature(T)是用来调整soft
label的超参数。</p>
<p>蒸馏这个概念之所以work，核心思想是<strong>好模型的目标不是拟合训练数据，而是学习如何泛化到新的数据</strong>。所以蒸馏的目标是让学生模型学习到教师模型的泛化能力，理论上得到的结果会比单纯拟合训练数据的学生模型要好。</p>
<h1 id="如何蒸馏">如何蒸馏</h1>
<p>蒸馏发展到今天，有各种各样的花式方法，我们先从最基本的说起。</p>
<p>之前提到学生模型需要通过教师模型的输出学习泛化能力，那对于简单的二分类任务来说，直接拿教师预测的0/1结果会与训练集差不多，没什么意义，那拿概率值是不是好一些？于是Hinton采用了教师模型的输出概率
<span class="math inline">\(q\)</span> （又称 soft
target），同时为了更好地控制输出概率的平滑程度，给教师模型的softmax中加了一个参数T:<br />
<span class="math display">\[q_i=\dfrac{exp(z_i/T)}{\sum_j
exp(z_i/T)}\]</span><br />
同时，拟合真实标签 <span class="math inline">\(y\)</span> （hard
label），于是我们有了新的loss（又称 hard target）：<br />
<span class="math display">\[L=(1-\alpha)CE(y,p)+\alpha CE(q,p)\cdot
T^2\]</span><br />
新loss包括两部分：学生模型与真实标签之间的交叉熵<span
class="math inline">\(CE(y,p)\)</span>；教师模型与学生模型的输出之间的交叉熵<span
class="math inline">\(CE(q,p)\)</span>。其中 <span
class="math inline">\(CE\)</span> 是交叉熵（Cross-Entropy），<span
class="math inline">\(y\)</span> 是真实label，<span
class="math inline">\(p\)</span> 是学生模型的预测结果，<span
class="math inline">\(\alpha\)</span> 是蒸馏loss的权重。<br />
这里要注意的是，因为学生模型要拟合教师模型的分布，所以在求 <span
class="math inline">\(p\)</span> 时的也要使用一样的参数 <span
class="math inline">\(T\)</span> （在训练结束以后使用正常温度 T=1
进行预测）。另外，因为在求梯度时新的目标函数会导致梯度是以前的 <span
class="math inline">\(1/T^2\)</span>，所以要再乘上 <span
class="math inline">\(T^2\)</span>，不然 <span
class="math inline">\(T\)</span> 变了的话，但soft label会变小，hard
label不变（T=1）。</p>
<h1 id="bert蒸馏">BERT蒸馏</h1>
<p>在BERT提出后，如何瘦身就成了一个重要分支。主流的方法主要有剪枝、蒸馏和量化。量化的提升有限，因此免不了采用<strong>剪枝+蒸馏</strong>的融合方法来获取更好的效果。<br />
接下来将介绍BERT蒸馏的主要发展脉络，从各个研究看来，蒸馏的提升一方面来源于从<strong>精调阶段蒸馏-&gt;预训练阶段蒸馏</strong>，另一方面则来源于<strong>蒸馏最后一层知识-&gt;蒸馏隐层知识-&gt;蒸馏注意力矩阵</strong>。</p>
<h2 id="distilled-bilstm">Distilled BiLSTM</h2>
<p>Distilled
BiLSTM于2019年5月提出，作者将BERT-large蒸馏到了单层的BiLSTM中，参数量减少了100倍，速度提升了15倍，效果虽然比BERT差不少，但可以和ELMo打成平手。<br />
<img src="/images/蒸馏/1.png" width="80%"></p>
<p>Distilled
BiLSTM的教师模型采用精调过的BERT-large，学生模型采用BiLSTM+ReLU，蒸馏的目标是<strong>hard
labe的交叉熵+logits之间的MSE</strong>（作者经过实验发现MSE比上文的 <span
class="math inline">\(CE(p,q)\)</span> 更好）。<br />
Distilled BiLSTM 一文采用的目标函数为：<br />
<img src="/images/蒸馏/2.png" width="40%"></p>
<p>面对数据集小的问题，作者随机使用以下方式进行数据增强（使用教师模型在无监督语料上进行标记），来获得更多的无监督语料：<br />
- 随机的mask掩码<br />
- 将随机词替换为相同POS属性的词<br />
- 随机截取n-gram作为样本</p>
<p>实验结果，蒸馏比得上ELMO了：<br />
<img src="/images/蒸馏/3.png" width="80%"></p>
<p>参数与运行速度对比：<br />
<img src="/images/蒸馏/4.png" width="40%"></p>
<h2 id="bert-pkd-emnlp2019">BERT-PKD (EMNLP2019)</h2>
<p>将原始大模型压缩为同等有效的轻量级浅层网络。同时，作者对以往的知识蒸馏方法进行了调研，如下图所示，vanilla
KD在QNLI和MNLI的训练集上可以很快的达到和teacher
model相媲美的性能，但在测试集上则很快达到饱和。对此，作者提出一种假设，在知识蒸馏的过程中过拟合会导致泛化能力不良。为缓解这个问题，论文中提出一种“耐心”师生机制，即<strong>让Patient-KD中的学生模型从教师网络的多个中间层进行知识提取</strong>，而不是只从教师网络的最后一层输出中学习，避免在蒸馏最后一层时拟合过快的现象（有过拟合的风险）。<br />
<img src="/images/蒸馏/5.png" width="90%"></p>
<p><strong>模型实现</strong><br />
Patient-KD中提出如下两个知识蒸馏策略：<br />
- <strong>PKD-Skip</strong>:
从每k层学习，这种策略是假设网络的底层包含重要信息，需要被学习到（如下图(a):PKD-Skip
学生网络学习教师网络每两层的输出）。<br />
- <strong>PKD-last</strong>:
从最后k层学习，假设教师网络越靠后的层包含越丰富的知识信息（如下图(b):PKD-Last
学生网络从教师网络的最后六层学习）。<br />
<img src="/images/蒸馏/6.png" width="70%"></p>
<p>因为在BERT中仅使用最后一层的[CLS]
token的输出来进行预测，且在其他BERT的变体模型中，如SDNet，是通过对每一层的[CLS]
embedding的加权平均值进行处理并预测。由此可以推断，如果学生模型可以从任何教师网络中间层中的[CLS]表示中学习，那么它就有可能获得类似教师网络的泛化能力。因此，Patient-KD中提出特殊的一种损失函数的计算方式：</p>
<p>BERT-PKD 加入了对中间层 [CLS] 位置上隐状态的拟合，使用教师与学生模型
[CLS] 隐状态之间的 MSE 作为额外的损失。<br />
<img src="/images/蒸馏/7.png" width="40%"></p>
<p>其中，对于输入 <span
class="math inline">\(x_i\)</span>，所有层[CLS]的输出表示为：<br />
<img src="/images/蒸馏/8.png" width="40%"></p>
<p><span class="math inline">\(I_{pt}\)</span>
表示表示要从中提取知识的一组中间层，以从 BERT_12 压缩到 BERT_6
为例：<br />
- 对于PKD-Skip策略，<span
class="math inline">\(I_{pt}=2,4,6,8,10\)</span>；<br />
- 对于PKD-Last策略，<span
class="math inline">\(I_{pt}=7,8,9,10,11\)</span>。</p>
<p>M表示学生网络的层数，N是训练样本的数量，上标 s 和 t
分别代表学生网络和教师网络。最终实验显示PKD-skip要略好一点点（&lt;0.01）</p>
<p>同时，Patient-KD中也使用了 <span
class="math inline">\(L_{DS}\)</span> 和 <span
class="math inline">\(L_{CE}^S\)</span>
两种损失函数用来衡量教师和学生网络的预测值的距离和学生网络在特定下游任务上的交叉熵损失。<br />
<img src="/images/蒸馏/9.png" width="50%"></p>
<p>最终的目标损失函数可以表示为：<br />
<img src="/images/蒸馏/10.png" width="40%"></p>
<p><strong>实验结果</strong><br />
<img src="/images/蒸馏/11.png" width="80%"><br />
作者将模型预测提交到GLUE并获得了在测试集上的结果，如上图所示。与fine-tuning和vanilla
KD这两种方法相比，使用PKD训练的BERT_3和BERT_6在除MRPC外的几乎所有任务上都表现良好。其中，PKD代表Patient-KD-Skip方法。对于MNLI-m和MNLI-mm，六层模型比微调（FT）基线提高了1.1%和1.3%，</p>
<p>我们将模型预测提交给官方 GLUE 评估服务器以获得测试数据的结果。
与直接微调和普通 KD 相比，我们使用 BERT3 和 BERT6 学生的 Patient-KD
模型在除 MRPC 之外的几乎所有任务上都表现最好。
此外，6层的BERT6−PKD在7个任务中有5个都达到了和BERT-Base相似的性能，其中，SST-2（与
BERT-Base
教师相比为-2.3%）、QQP（-0.1%）、MNLI-m（-2.2%）、MNLI-mm（-1.8%）和
QNLI
(-1.4%)，这五个任务都有超过6万个训练样本，这表明了PKD在大数据集上的表现往往更好。</p>
<p>PKD-Last 和 PKD-Skip 在GLUE基准上的对比：<br />
<img src="/images/蒸馏/12.png" width="80%"><br />
尽管这两种策略都比vanilla
KD有所改进，但PKD-Skip的表现略好于PKD-Last。作者推测，这可能是由于每k层的信息提炼捕获了从低级到高级的语义，具备更丰富的内容和更多不同的表示，而只关注最后k层往往会捕获相对同质的语义信息。</p>
<p>参数量和推理时间对比：<br />
<img src="/images/蒸馏/13.png" width="80%"><br />
上表展示了BERT3、BERT6、BERT12的推理时间即参数量,
实验表明Patient-KD方法实现了几乎线性的加速，BERT6和BERT3分别提速1.94倍和3.73倍。</p>
<h2 id="distillbert-nips2019">DistillBERT (NIPS2019)</h2>
<p>之前的工作都是对精调后的BERT进行蒸馏，学生模型学到的都是任务相关的知识。HuggingFace则提出了DistillBERT，<strong>在预训练阶段进行蒸馏</strong>。将尺寸减小了40%，速度提升60%，效果好于BERT-PKD，为教师模型的97%。即，<strong>DistillBERT的教师模型采用了预训练好的BERT-base，学生模型则是6层transformer（层数消减了一半，12层的bert蒸馏成了6层），移除了
token-type embeddings 和
pooler，采用了PKD-skip的方式进行初始化（从教师网络中每两层抽取一层来进行初始化）</strong>。</p>
<p>和之前蒸馏目标不同的是，为了调整教师和学生的隐层向量方向，作者新增了一个cosine
embedding
loss，蒸馏最后一层hidden的。最终损失函数由以下三部分组成：<br />
- <span
class="math inline">\(L_{ce}\)</span>，这是teacher网络softmax层输出的概率分布和student网络softmax层输出的概率分布的交叉熵（注：MLM任务的输出）。<br />
- <span
class="math inline">\(L_{mlm}\)</span>，这是student网络softmax层输出的概率分布和真实的one-hot标签的交叉熵。也就是student模型做预训练的mlm损失。<br />
- <span
class="math inline">\(L_{cos}\)</span>，这是student网络最后一隐层输出和teacher网络最后一隐层输出的余弦相似度值。</p>
<p>其中，<span
class="math inline">\(L_{ce}\)</span>训练学生模仿教师模型的输出分布：<br />
<span class="math display">\[L_{ce}=\sum\limits_i t_i *
log(s_i)\]</span><br />
<span class="math inline">\(t_i\)</span>和<span
class="math inline">\(s_i\)</span>分别是教师网络和学生网络的预测概率。</p>
<p>同时使用了Hinton在2015年提出的softmax-temperature：<br />
<span class="math display">\[p_i=\dfrac{exp(z_i/T)}{\sum_j
exp(z_i/T)}\]</span><br />
其中，<span class="math inline">\(T\)</span>控制输出分布的平滑度，<span
class="math inline">\(T=1\)</span>时代表传统的softmax，<span
class="math inline">\(T&lt;1\)</span>时分布逐渐极端化，最终等价于argmax，<span
class="math inline">\(T&gt;1\)</span>分布逐渐趋于均匀分布。当<span
class="math inline">\(T\)</span>变大时，类别之间的差距变小(平滑)，从而导致loss变小；当<span
class="math inline">\(T\)</span>变小时类别间的差距变大(陡峭)，从而导致loss变小。</p>
<p><span class="math inline">\(z_i\)</span>代表分类<span
class="math inline">\(i\)</span>的模型分数。在训练时对学生网络和教师网络使用同样的temperature
<span class="math inline">\(T\)</span>，在推理时，设置<span
class="math inline">\(T=1\)</span>恢复为标准的softmax最终的loss函数为<span
class="math inline">\(L_{ce}\)</span>、Mask language model loss <span
class="math inline">\(L_{mlm}\)</span>（参考BERT）和 cosine embedding
loss <span
class="math inline">\(L_{cos}\)</span>（student和teacher隐藏状态向量的cos计算）的线性组合。</p>
<p>从消融实验可以看出，MLM
loss对于学生模型的表现影响较小，同时初始化也是影响效果的重要因素：<br />
<img src="/images/蒸馏/14.png" width="80%"></p>
<p>预训练蒸馏时使用与BERT预训练相同的语料。在8个V100（16GB）上耗时90小时。</p>
<p>最终DistilBERT与BERT相比减少了40%的参数，同时保留了BERT
97%的性能，但提高了60%的速度。<br />
<img src="/images/蒸馏/15.png" width="80%"></p>
<h2 id="tinybertemnlp2019">TinyBERT（EMNLP2019）</h2>
<p>TinyBERT是由华中科技大学和华为诺亚方舟实验室联合提出的一种针对transformer-based模型的知识蒸馏方法，以BERT为例对大型预训练模型进行研究。它的核心思想：<strong>预训练</strong>阶段和<strong>fine-tuning</strong>阶段都分别被蒸馏过了，理论上<strong>两步联合</strong>起来的效果可能会更好。</p>
<p>TinyBERT就提出了two-stage
learning框架，分别在<strong>预训练</strong>和<strong>精调</strong>阶段蒸馏教师模型：<br />
第一个阶段，利用预训练 BERT 蒸馏出一个 General TinyBERT。<br />
第二个阶段，利用 General TinyBERT
在任务上fine-tuning(使用了数据增强)，之后蒸馏。</p>
<p><img src="/images/蒸馏/16.png" width="60%"></p>
<p><strong>四层结构的 TinyBERT4</strong>
相比BERT-base参数量减少7.5倍，速度提升9.4倍，效果可以达到教师模型的96.8%；<strong>六层结构的
TinyBERT6</strong> 甚至接近BERT-base，超过了BERT-PKD和DistillBERT。</p>
<p>TinyBERT主要做了以下两点创新：<br />
- 提供一种新的针对 transformer-based
模型进行蒸馏的方法，使得BERT中具有的语言知识可以迁移到TinyBERT中去。<br />
-
提出一个两阶段学习框架，在<strong>预训练</strong>阶段和<strong>fine-tuning</strong>阶段都进行蒸馏，确保TinyBERT可以充分的从BERT中学习到一般领域和特定任务两部分的知识。</p>
<p><strong>知识蒸馏</strong><br />
知识蒸馏的目的在于将一个大型的教师网络 <span
class="math inline">\(T\)</span> 学习到的知识迁移到小型的学生网络 <span
class="math inline">\(S\)</span>
中。学生网络通过训练来模仿教师网络的行为。<span
class="math inline">\(f^S\)</span> 和 <span
class="math inline">\(f^T\)</span> 代表教师网络和学生网络的behavior
functions。这个行为函数的目的是将网络的输入转化为信息性表示，并且它可被定义为网络中任何层的输出。在基于transformer的模型的蒸馏中，MHA（multi-head
attention）层或FFN（fully connected feed-forward
network）层的输出或一些中间表示，比如注意力矩阵 <span
class="math inline">\(A\)</span> 都可被作为行为函数使用。<br />
<span class="math display">\[L_{KD}=\sum\limits_{x\in
X}L(f^S(x),f^T(x))\]</span><br />
其中 <span class="math inline">\(L(⋅)\)</span>
是一个用于评估教师网络和学生网络之间差异的损失函数，<span
class="math inline">\(x\)</span> 是输入文本，<span
class="math inline">\(X\)</span>
代表训练数据集。因此，蒸馏的关键问题在于如何定义行为函数和损失函数。</p>
<p><strong>Transformer Distillation</strong><br />
假设TinyBert有 M 层transformer layer，teacher BERT有 N 层transformer
layer，则需要从teacher BERT的 N 层中抽取 M
层用于transformer层的蒸馏。<span class="math inline">\(n=g(m)\)</span>
定义了一个从学生网络到教师网络的映射关系，表示学生网络中第 <span
class="math inline">\(m\)</span> 层网络信息是从教师网络的第 <span
class="math inline">\(g(m)\)</span> 层学习到的，也就是教师网络的第 <span
class="math inline">\(n\)</span>
层。TinyBERT嵌入层和预测层也是从BERT的相应层学习知识的，其中嵌入层对应的指数为0，预测层对应的指数为
M+1，对应的层映射定义为 <span class="math inline">\(0=g(0)\)</span> 和
<span
class="math inline">\(N+1=g(M+1)\)</span>。在形式上，学生模型可以通过最小化以下的目标函数来获取教师模型的知识：<br />
<span class="math display">\[L_{model}=\sum\limits_{x\in
X}\sum\limits_{m=0}^{M+1}\lambda_mL_{layer}(f_m^S(x),f_{g(m)}^T(x))\]</span><br />
其中 <span class="math inline">\(L_{layer}\)</span>
是给定的模型层的损失函数（比如transformer层或嵌入层），<span
class="math inline">\(f_{m}\)</span> 代表第 m 层引起的行为函数，<span
class="math inline">\(\lambda_m\)</span> 表示第 m 层蒸馏的重要程度。</p>
<p>TinyBERT的蒸馏分为以下三个部分：<br />
- transformer-layer distillation (包括attention和hidden
states损失)<br />
- embedding-layer distillation<br />
- prediction-layer distillation</p>
<p><strong>1、transformer-layer distillation</strong><br />
Transformer-layer的蒸馏由attention based蒸馏和hidden states
based蒸馏两部分组成。<br />
<img src="/images/蒸馏/18.png" width="50%"></p>
<p><strong>attention based</strong>蒸馏是受到论文 <span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzE5MDYuMDQzNDEucGRm">Clack et al., 2019<i class="fa fa-external-link-alt"></i></span>
的启发，这篇论文中提到，BERT学习的注意力权重可以捕获丰富的语言知识，这些语言知识包括对自然语言理解非常重要的语法和共指信息。因此，TinyBERT提出attention
based蒸馏，其目的是使学生网络很好地从教师网络处学习到这些语言知识。具体到模型中，就是让TinyBERT网络学习拟合BERT网络中的多头注意力矩阵，目标函数定义如下：<br />
<span
class="math display">\[L_{attn}=\frac{1}{h}\sum\limits_{i=1}^hMSE(A_i^S,A_i^T)\]</span><br />
其中，<span class="math inline">\(h\)</span> 代表注意力头数，<span
class="math inline">\(A_i\in R^{l\times l}\)</span> 代表学生或教师的第
<span class="math inline">\(i\)</span> 个注意力头对应的注意力矩阵，<span
class="math inline">\(l\)</span>
代表输入文本的长度。论文中提到，使用注意力矩阵 <span
class="math inline">\(A\)</span> 而不是 <span
class="math inline">\(softmax(A)\)</span>
是因为实验结果显示这样可以得到更快的收敛速度和更好的性能表现。</p>
<p><strong>hidden states
based</strong>蒸馏是对transformer层输出的知识进行了蒸馏处理，目标函数定义为：<br />
<span class="math display">\[L_{hidn}=MSE(H^SW_h,H^T)\]</span><br />
其中，<span class="math inline">\(H^S\in R^{l×d^′}\)</span>,<span
class="math inline">\(H^T\in R^{l×d}\)</span>
分别代表学生网络和教师网络的隐状态，是FFN的输出。<span
class="math inline">\(d\)</span> 和 <span
class="math inline">\(d^′\)</span>
代表教师网络和学生网络的隐藏状态大小，且 <span
class="math inline">\(d^′&lt;d\)</span>，因为学生网络总是小于教师网络。<span
class="math inline">\(W_h\in R^{d^′×d}\)</span>
是一个可训练的线性变换矩阵，将学生网络的隐藏状态投影到教师网络隐藏状态所在的空间。</p>
<p><strong>2、Embedding-layer Distillation</strong><br />
<span class="math display">\[L_{embd}=MSE(E^SW_e,W^T)\]</span><br />
Embedding loss和hidden states loss同理，其中 <span
class="math inline">\(E^S\)</span>,<span
class="math inline">\(E^T\)</span>
代表学生网络和教师网络的嵌入（embedding），和隐藏状态矩阵的形状相同，同时
<span class="math inline">\(W_e\)</span> 和 <span
class="math inline">\(W_h\)</span> 的作用也相同。</p>
<p><strong>3、Prediction-layer Distillation</strong><br />
<span class="math display">\[L_{pred}=CE(z^T/t,z^S/t)\]</span><br />
其中，<span class="math inline">\(z^S\)</span>,<span
class="math inline">\(z^T\)</span> 分别是学生网络和教师网络预测的 logits
向量，<span class="math inline">\(CE\)</span> 代表交叉熵损失，<span
class="math inline">\(t\)</span> 是temperature value，当 <span
class="math inline">\(t=1\)</span> 时，表现良好。</p>
<p>对上述三个部分的loss函数进行整合，则可以得到教师网络和学生网络之间对应层的蒸馏损失如下：<br />
<img src="/images/蒸馏/19.png" width="30%"></p>
<p><strong>实验结果</strong><br />
分析两个阶段的知识蒸馏 TD (Task-specific Distillation)和GD (General
Distillation)，以及数据增强DA (Data Augmentation)
对TinyBERT整体效果的作用（最后的实验中，预训练阶段只对中间层进行了蒸馏；精调阶段则先对中间层蒸馏20个epochs，再对最后一层蒸馏3个epochs）：<br />
<img src="/images/蒸馏/20.png" width="50%"></p>
<ul>
<li>去掉TD和DA对整体结果影响较大，<strong>去掉GD对整体的结果作用较小</strong>（GD带来的提升不如TD或者DA），TD和DA对最终结果的影响差不多。</li>
<li><strong>去掉GD对CoLA的作用大于MNLI和MRPC</strong>(CoLA在没有GD的情况下降了9%)，CoLA是判断一句话是否语法正确的数据集，需要更多语言学知识，而GD的过程正是捕获这种知识的手段。</li>
</ul>
<p>分析知识蒸馏过程中，选取的不同的特征表示对整体结果的作用：<br />
<img src="/images/蒸馏/21.png" width="50%"></p>
<ul>
<li>没有Transformer层对模型的影响最大，Transformer层是整个模型的主要构成部分。</li>
<li>Transformer层中attention矩阵相比隐层输出的作用要大。</li>
<li>整体来说，Transformer层，embeding层，预测输出层，对于提高模型的整体效果都是有效的。</li>
</ul>
<h2 id="mobilebertacl2020">MobileBERT（ACL2020）</h2>
<p>前文介绍的模型都是层次剪枝+蒸馏的操作，MobileBERT则致力于减少每层的维度，在保留24层的情况下，减少了4.3倍的参数，速度提升5.5倍，在GLUE上平均只比BERT-base低了0.6个点，效果好于TinyBERT和DistillBERT。</p>
<p>MobileBERT压缩维度的主要思想在于bottleneck机制，如下图所示：<br />
<img src="/images/蒸馏/22.png" width="80%"></p>
<p>其中a是标准的BERT，b是加入bottleneck的BERT-large，作为教师模型，c是加入bottleneck的学生模型。Bottleneck的原理是在transformer的输入输出各加入一个线性层，实现维度的缩放。对于教师模型，embedding的维度是512，进入transformer后扩大为1024，而学生模型则是从512缩小至128，使得参数量骤减。</p>
<p>另外，作者发现在标准BERT中，多头注意力机制MHA和非线性层FFN的参数比为1:2，这个参数比相比其他比例更好。所以为了维持比例，会在学生模型中多加几层FFN。</p>
<p>MobileBERT还有一点不同于之前的TinyBERT，就是预训练阶段蒸馏之后，作者直接在MobileBERT上用任务数据精调，而不需要再进行精调阶段的蒸馏，方便了很多。</p>
<p>MobileBERT的蒸馏中，作者先用b的结构预训练一个BERT-large，再蒸馏到24层学生模型中。蒸馏的loss有多个：<br />
- Feature Map Transfer：每个模块的隐状态输出之间的MSE <span
class="math inline">\(L_{FMT}^l=\frac{1}{TN}\sum\limits_{t=1}^T\sum\limits_{n=1}^N(H_{t,l,n}^{tr}-H_{t,l,n}^{st})^2\)</span>，作者发现，将这项差异分解成归一化后的差异与统计差异有助于训练的稳定性。<br />
- Attention Transfer：注意力矩阵的KL散度 <span
class="math inline">\(L_{AT}^l=\frac{1}{TA}\sum\limits_{a=1}^AD_{KL}(a_{t,l,n}^{tr}||a_{t,l,n}^{st})\)</span>，即每个多头自注意力矩阵的KL散度。<br />
- Pre-training Distillation：预训练蒸馏 <span
class="math inline">\(L_{PD}=\alpha
L_{MLM}+(1-\alpha)L_{KD}+L_{NSP}\)</span>，即MLM任务，NSP任务的损失加上MLM蒸馏的损失。</p>
<p>同时作者还研究了三种不同的蒸馏策略：直接蒸馏所有层、先蒸馏中间层再蒸馏最后一层、逐层蒸馏。如下图：<br />
<img src="/images/蒸馏/23.png" width="90%"><br />
最后的结论是逐层蒸馏效果最好，但差距最大才0.5个点，性价比有些低了。其中OPT表示：使用relu代替gelu，移除layer
Norm。这不优化比优化了效果还好，但是花的时间也更多了。<br />
<img src="/images/蒸馏/24.png" width="90%"></p>
<p>从蒸馏损失消融实验结果可以看出FMT带来的提升最大：<br />
<img src="/images/蒸馏/25.png" width="50%"></p>
<p>对于训练方法，果然是逐层蒸馏效果最好：<br />
<img src="/images/蒸馏/26.png" width="50%"></p>
<h2 id="minilm">MiniLM</h2>
<p>之前的各种模型基本上把BERT里面能蒸馏的都蒸了个遍，但MiniLM还是找到了新的蓝海——<strong>蒸馏Value-Value矩阵+助教机制</strong>：<br />
<img src="/images/蒸馏/27.png" width="80%"></p>
<p><strong>Value-Relation
Transfer</strong>可以让学生模型更深入地模仿教师模型，实验表明可以带来1-2个点的提升。同时作者考虑到学生模型的层数、维度都可能和教师模型不同，在实验中只蒸馏最后一层，并且只蒸馏这两个矩阵的KL散度。另外，作者还引入了<strong>助教机制</strong>。当学生模型的层数、维度都小很多时，先用一个维度小但层数和教师模型一致的助教模型蒸馏，之后再把助教的知识传递给学生。</p>
<p>最终采用BERT-base作为教师，实验下来6层的学生模型比起TinyBERT和DistillBERT好了不少，基本是20年性价比数一数二的蒸馏了。</p>
<h2 id="dynabert">DynaBERT</h2>
<p>DynaBERT（dynamic
BERT）提出一种不同的思路，它可以通过选择自适应宽度和深度来灵活地调整网络大小，从而得到一个尺寸可变的网络。</p>
<p>DynaBERT的训练阶段包括两部分：<br />
- 首先通过知识蒸馏的方法将teacher
BERT的知识迁移到有自适应宽度的子网络student DynaBERTW中。<br />
- 然后再对 DynaBERTW
进行知识蒸馏得到同时支持深度自适应和宽度自适应的子网络 DynaBERT。</p>
<p>训练过程流程图所示：<br />
<img src="/images/蒸馏/28.png" width="80%"></p>
<p><strong>宽度自适应 Adaptive Width</strong><br />
一个标准的transfomer中包含一个多头注意力（MHA）模块和一个前馈网络（FFN）。在论文中，作者通过变换注意力头的个数
<span class="math inline">\(N_h\)</span> 和前馈网络中中间层的神经元个数
<span class="math inline">\(d_{ff}\)</span>
来更改transformer的宽度。同时定义一个缩放系数 <span
class="math inline">\(m_w\)</span> 来进行剪枝，保留MHA中最左边的 <span
class="math inline">\([m_wN_H]\)</span> 个注意力头和 FFN中 <span
class="math inline">\([m_wd_{ff}]\)</span> 个神经元。</p>
<p>为了充分利用网络的容量，更重要的头部或神经元应该在更多的子网络中共享。因此，在训练宽度自适应网络前，作者在
fine-tuned
BERT网络中根据注意力头和神经元的重要性对它们进行了排序，然后在宽度方向上以降序进行排列。这种选取机制被称为
<strong>Network Rewiring</strong>。<br />
<img src="/images/蒸馏/29.png" width="50%"></p>
<p>那么，要如何界定注意力头和神经元的重要性呢？作者参考 <span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzE2MTEuMDY0NDAucGRm">P. Molchanov et al.,
2017<i class="fa fa-external-link-alt"></i></span> 和 <span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzE4MDQuMDc0NjEucGRm">E. Voita et
al., 2019<i class="fa fa-external-link-alt"></i></span>
两篇论文提出，去掉某个注意力头或神经元前后的loss变化，就是该注意力头或神经元的重要程度，变化越大则越重要。</p>
<p><strong>训练宽度自适应网络</strong><br />
首先，将BERT网络作为固定的教师网络，并初始化
DynaBERTW。然后通过知识蒸馏将知识从教师网络迁移到 DynaBERTW
中不同宽度的学生子网络。其中，<span
class="math inline">\(m_w=[1.0,0.75,0.5,0.25]\)</span>。</p>
<p>模型蒸馏的loss定义为：<br />
<img src="/images/蒸馏/30.png" width="60%"></p>
<p>其中， <span class="math inline">\(\lambda_1\)</span>,<span
class="math inline">\(\lambda_2\)</span> 是控制不同损失函数权重的参数，
<span class="math inline">\(l_{pred}\)</span>,<span
class="math inline">\(l_{emb}\)</span>,<span
class="math inline">\(l_{hidn}\)</span> 分别定义为：<br />
<img src="/images/蒸馏/31.png" width="60%"></p>
<p><span class="math inline">\(l_{pred}\)</span> 代表预测层的loss，SCE
代表交叉熵损失函数。<span class="math inline">\(l_{emb}\)</span>
代表嵌入层的loss，MSE代表均方差损失函数。<span
class="math inline">\(l_{hidn}\)</span> 则为隐藏层的loss。</p>
<p><strong>训练深度自适应网络</strong><br />
训练好宽度自适应的DynaBERTW后，就可以将其作为教师网络训练同时具备宽度自适应和深度自适应的DynaBERT了。为了避免宽度方向上的灾难性遗忘，在每一轮训练中，仍对不同宽度进行训练。深度调节系数
<span class="math inline">\(m_d\)</span>
对网络层数进行调节，在训练中定义 <span
class="math inline">\(m_d=[1.0,0.75,0.5]\)</span>。深度方向上的剪枝根据
<span class="math inline">\(mod(d+1,\frac{1}{md})=0\)</span>
来去掉特定层。</p>
<p>模型蒸馏的loss定义为：<br />
<img src="/images/蒸馏/32.png" width="70%"></p>
<p><strong>实验结果</strong><br />
根据不同的宽度和深度剪裁系数，作者最终得到12个大小不同的DyneBERT模型，其在GLUE上的效果如下：<br />
<img src="/images/蒸馏/33.png" width="70%"></p>
<p>论文中提出的DynaBERT和DynaRoBERTa可以达到和 BERTBASE 及 DynaRoBERTa
相当的精度，但是通常包含更少的参数，FLOPs或更低的延迟。在相同效率的约束下，从DynaBERT中提取的子网性能优于DistilBERT和TinyBERT。<br />
<img src="/images/蒸馏/34.png" width="70%"></p>
<h1 id="bert蒸馏技巧">BERT蒸馏技巧</h1>
<p><strong>1、选择哪种蒸馏方案？</strong><br />
-
预训练蒸馏的数据比较充分，可以参考MiniLM、MobileBERT或者TinyBERT那样进行剪层+维度缩减。<br />
-
对于针对某项任务、只想蒸馏精调后BERT的情况，则推荐进行剪层，同时利用教师模型的层对学生模型进行初始化。</p>
<p><strong>2、用什么蒸馏目标函数？</strong><br />
<img src="/images/蒸馏/35.png" width="70%"><br />
对于hard label，使用KL和CE是一样的，因为<span
class="math inline">\(KL(p||q)=H(p||q)-H(p)\)</span>，训练集不变时label分布是一定的。但对于soft
label则不同了，不过表中不少模型还是采用了CE，只有Distilled
BiLSTM发现MSE更好。可以CE/MSE/KL都试一下，但MSE有个好处是可以避免T的调参。中间层输出的蒸馏，大多数模型都采用了MSE，只有DistillBERT加入了cosine
loss来对齐方向。注意力矩阵的蒸馏loss则比较统一，如果要蒸馏softmax之前的attention
logits可以采用MSE，之后的attention prob可以用KL散度。</p>
<ul>
<li>使用finetune任务自身的loss是有效的，但是效果不大，大概能够提升0.2个百分点。</li>
<li>使用attention
output输出logits的mse效果甚微，基本没有太大提升。我推测可能是当前对于序列标注任务来说，attention的学习提升不大。建议使用更多不同的任务来实验。</li>
<li>使用hidden
output输出logits的mse是非常有效的，能够提升1个百分点。</li>
<li>使用概率输出做蒸馏和使用logits输出做蒸馏差距不大，并不能看到显著的区别，建议用更多不同的任务来实验。</li>
</ul>
<p><strong>3、超参T和α的设置</strong><br />
超参 α 用来控制各个蒸馏目标的权重。</p>
<p>超参 T 越大越能学到teacher模型的泛化信息。一部分文章发现
T=1时候效果最好，大部分文章与1-20之间调 T。</p>
<p><strong>4、蒸馏方式</strong><br />
逐层蒸馏可以提高一些成绩，但是花费还是很大的。</p>
<p><strong>5、助教机制似乎有效</strong><br />
miniLM发现的，论文中它的最终目标是将模型裁剪到4层，hidden_size裁剪一半。实际操作时，它并非直接使用蒸馏训练一个最小模型，而是先用原始模型蒸馏一个中介模型，其层数为4层，但是hidden_size不变，然后使用这个中介模型作为teacher模型来蒸馏得到最终的模型。我尝试了这种方式，发现有一定的效果，为了蒸馏得到4层的模型，我先将原始模型蒸馏到6层，然后再蒸馏到4层。这种方式比直接蒸馏小模型能够有3-4个百分点的提升。当然，我这里要说明一点，我比较的是训练相同epoch数下的两个模型的精度，也有可能是一步到位蒸馏小模型需要更多的训练步数才能达到收敛，并不能直接断定一步到位为训练法一定就比较差，但至少在相同的训练成本下，采用中介过渡是更有效的。</p>
<p><strong>尽量沿用teacher模型的权重，如初始化</strong>。</p>
<h1 id="textbrewer">TextBrewer</h1>
<p>TextBrewer
提供了通用的蒸馏框架，使用者只需要提供一些配置与数据就可以进行简单的蒸馏。详情参考<span class="exturl" data-url="aHR0cHM6Ly90ZXh0YnJld2VyLnJlYWR0aGVkb2NzLmlvL2VuL2xhdGVzdC9UdXRvcmlhbC5odG1s">TextBrewer官方文档<i class="fa fa-external-link-alt"></i></span>。</p>
<p>TextBrewer的主要功能与模块：<br />
-
Distillers：进行蒸馏的核心部件，不同的distiller提供不同的蒸馏模式。目前包含GeneralDistiller,
MultiTeacherDistiller, MultiTaskDistiller等。<br />
- Configurations and
presets：训练与蒸馏方法的配置，并提供预定义的蒸馏策略以及多种知识蒸馏损失函数。<br />
- Utilities：模型参数分析等辅助工具。</p>
<h2 id="工作流程">工作流程</h2>
<p><img src="/images/蒸馏/36.png" width="70%"></p>
<p><strong>第一步</strong>：蒸馏之前的准备工作：<br />
- 训练<strong>教师</strong>模型。<br />
-
定义与初始化<strong>学生</strong>模型（随机初始化，或载入预训练权重）。<br />
- 构造蒸馏用数据集的 dataloader，训练<strong>学生</strong>模型用的
optimizer 和 learning rate scheduler 。</p>
<p><strong>第二步</strong>：使用TextBrewer蒸馏：<br />
-
构造训练配置(TrainingConfig)和蒸馏配置(DistillationConfig),初始化distiller。<br />
- 定义 adaptor 和 callback
，分别用于适配模型输入输出和训练过程中的回调。<br />
- 调用 distiller.train() 方法开始蒸馏。</p>
<h2 id="快速开始">快速开始</h2>
<p>以蒸馏BERT-base到3层BERT为例展示TextBrewer用法。</p>
<p>在开始蒸馏之前准备：<br />
- 训练好的教师模型teacher_model (BERT-base)，待训练学生模型student_model
(3-layer BERT)。<br />
-
数据集dataloader，优化器optimizer，学习率调节器类或者构造函数scheduler_class
和构造用的参数字典 scheduler_args。</p>
<p>使用TextBrewer蒸馏:<br />
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> textbrewer</span><br><span class="line"><span class="keyword">from</span> textbrewer <span class="keyword">import</span> GeneralDistiller</span><br><span class="line"><span class="keyword">from</span> textbrewer <span class="keyword">import</span> TrainingConfig, DistillationConfig</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">teacher_model为Bert-base，student_model为bert-3-layer。</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="comment"># 展示模型参数量的统计</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;\nteacher_model&#x27;s parametrers:&quot;</span>)</span><br><span class="line">result, _ = textbrewer.utils.display_parameters(teacher_model,max_level=<span class="number">3</span>)</span><br><span class="line"><span class="built_in">print</span> (result)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;student_model&#x27;s parametrers:&quot;</span>)</span><br><span class="line">result, _ = textbrewer.utils.display_parameters(student_model,max_level=<span class="number">3</span>)</span><br><span class="line"><span class="built_in">print</span> (result)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义adaptor用于解释模型的输出</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">simple_adaptor</span>(<span class="params">batch, model_outputs</span>):</span><br><span class="line">    <span class="comment"># model输出的第二、三个元素分别是logits和hidden states</span></span><br><span class="line">    <span class="keyword">return</span> &#123;<span class="string">&#x27;logits&#x27;</span>: model_outputs[<span class="number">1</span>], <span class="string">&#x27;hidden&#x27;</span>: model_outputs[<span class="number">2</span>]&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 蒸馏与训练配置</span></span><br><span class="line"><span class="comment"># 匹配教师和学生层: 教师第0层和学生第0层匹配hidden层(loss计算方式为mse)；教师第8层和学生第2层...</span></span><br><span class="line">distill_config = DistillationConfig(</span><br><span class="line">    intermediate_matches=[    </span><br><span class="line">     &#123;<span class="string">&#x27;layer_T&#x27;</span>:<span class="number">0</span>, <span class="string">&#x27;layer_S&#x27;</span>:<span class="number">0</span>, <span class="string">&#x27;feature&#x27;</span>:<span class="string">&#x27;hidden&#x27;</span>, <span class="string">&#x27;loss&#x27;</span>: <span class="string">&#x27;hidden_mse&#x27;</span>,<span class="string">&#x27;weight&#x27;</span> : <span class="number">1</span>&#125;,</span><br><span class="line">     &#123;<span class="string">&#x27;layer_T&#x27;</span>:<span class="number">8</span>, <span class="string">&#x27;layer_S&#x27;</span>:<span class="number">2</span>, <span class="string">&#x27;feature&#x27;</span>:<span class="string">&#x27;hidden&#x27;</span>, <span class="string">&#x27;loss&#x27;</span>: <span class="string">&#x27;hidden_mse&#x27;</span>,<span class="string">&#x27;weight&#x27;</span> : <span class="number">1</span>&#125;])</span><br><span class="line">train_config = TrainingConfig()</span><br><span class="line"></span><br><span class="line"><span class="comment">#初始化distiller</span></span><br><span class="line">distiller = GeneralDistiller(</span><br><span class="line">    train_config=train_config, distill_config = distill_config,</span><br><span class="line">    model_T = teacher_model, model_S = student_model, </span><br><span class="line">    adaptor_T = simple_adaptor, adaptor_S = simple_adaptor)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 开始蒸馏</span></span><br><span class="line"><span class="keyword">with</span> distiller:</span><br><span class="line">    distiller.train(optimizer, </span><br><span class="line">                    dataloader, </span><br><span class="line">                    num_epochs=<span class="number">1</span>, </span><br><span class="line">                    scheduler_class=scheduler_class, </span><br><span class="line">                    scheduler_args=scheduler_args, </span><br><span class="line">                    callback=<span class="literal">None</span>)</span><br></pre></td></tr></table></figure></p>
<p><strong>蒸馏配置</strong><br />
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># matches地址：https://github.com/airaria/TextBrewer/blob/master/examples/matches/matches.py</span></span><br><span class="line">distill_config = DistillationConfig(temperature = <span class="number">8</span>, intermediate_matches = matches) <span class="comment"># 其他参数为默认值</span></span><br></pre></td></tr></table></figure></p>
<h2 id="核心概念">核心概念</h2>
<p><strong>1、Configurations</strong><br />
TrainingConfig 和 DistillationConfig：训练和蒸馏相关的配置。</p>
<p><strong>2、Distillers</strong><br />
Distiller负责执行实际的蒸馏过程。目前实现了以下的distillers:<br />
- <code>BasicDistiller</code>:
提供<strong>单模型单任务</strong>蒸馏方式。可用作测试或简单实验。<br />
- <code>GeneralDistiller</code> (常用):
提供<strong>单模型单任务</strong>蒸馏方式，并且支持<strong>中间层特征匹配</strong>，一般情况下<strong>推荐使用</strong>。<br />
- <code>MultiTeacherDistiller</code>:
<strong>多教师蒸馏</strong>。将多个（同任务）教师模型蒸馏到一个学生模型上。<strong>暂不支持中间层特征匹配</strong>。<br />
-
<code>MultiTaskDistiller</code>：<strong>多任务蒸馏</strong>。将多个（不同任务）单任务教师模型蒸馏到一个多任务学生模型上。<strong>暂不支持中间层特征匹配</strong>。<br />
-
<code>BasicTrainer</code>：用于<strong>单个模型</strong>的有监督训练，而非蒸馏。可用于<strong>训练教师模型</strong>。</p>
<p><strong>3、用户定义函数</strong><br />
蒸馏实验中，有两个组件需要由用户提供，分别是 callback 和 adaptor :<br />
-
<code>callback</code>：回调函数。在每个checkpoint，保存模型后会被distiller调用，并传入当前模型。可以借由回调函数在每个checkpoint评测模型效果。<br />
-
<code>adaptor</code>：将模型的输入和输出转换为指定的格式，向distiller解释模型的输入和输出，以便distiller根据不同的策略进行不同的计算。在每个训练步，batch和模型的输出model_outputs会作为参数传递给adaptor，adaptor负责重新组织这些数据，返回一个字典。字典的key参考<span class="exturl" data-url="aHR0cHM6Ly90ZXh0YnJld2VyLnJlYWR0aGVkb2NzLmlvL2VuL2xhdGVzdC9Db25jZXB0cy5odG1sI3VzZXItZGVmaW5lZC1mdW5jdGlvbnM=">官网说明<i class="fa fa-external-link-alt"></i></span>。</p>
<p>adaptor和callback流程图：<br />
<img src="/images/蒸馏/37.png" width="40%"></p>
<p>整体工作流如上图所示，黄色框为上文讨论到的Adaptor，用于将模型前向传导的输出整理为计算蒸馏损失所需要的字典（蓝色部分），最后在checkpoint时调用callback，callback可以评测模型效果。</p>
<h1 id="参考文献">参考文献</h1>
<p><span class="exturl" data-url="aHR0cHM6Ly9wYWRkbGVwZWRpYS5yZWFkdGhlZG9jcy5pby9lbi9sYXRlc3QvdHV0b3JpYWxzL21vZGVsX2NvbXByZXNzL2luZGV4Lmh0bWw=">模型压缩+模型蒸馏<i class="fa fa-external-link-alt"></i></span><br />
<span class="exturl" data-url="aHR0cHM6Ly96aHVhbmxhbi56aGlodS5jb20vcC80MTk2Nzk3MDI=">TextBrewer
通用蒸馏配置说明与工作流程介绍<i class="fa fa-external-link-alt"></i></span><br />
<span class="exturl" data-url="aHR0cDovL3d1amlhd2VuLnh5ei8yMDIxLzEwLzA5L2Rpc3RpbGwv">bert蒸馏小综述<i class="fa fa-external-link-alt"></i></span><br />
<span class="exturl" data-url="aHR0cHM6Ly9tcC53ZWl4aW4ucXEuY29tL3MvdEtmSHE0OWhlYWt2ak0wRVZRUGdIdw==">BERT蒸馏完全指南｜原理/技巧/代码<i class="fa fa-external-link-alt"></i></span><br />
<span class="exturl" data-url="aHR0cHM6Ly96aHVhbmxhbi56aGlodS5jb20vcC8xMjQyMTU3NjA=">模型压缩实践收尾篇——模型蒸馏以及其他一些技巧实践小结<i class="fa fa-external-link-alt"></i></span><br />
<span class="exturl" data-url="aHR0cHM6Ly96aHVhbmxhbi56aGlodS5jb20vcC81MDQzMjM0NjU=">深度学习高温蒸馏：Softmax
With Temperature<i class="fa fa-external-link-alt"></i></span></p>
<p><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvYWJzLzE1MDMuMDI1MzE=">Distilling the Knowledge
in a Neural Network<i class="fa fa-external-link-alt"></i></span><br />
<span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvYWJzLzE5MDMuMTIxMzY=">Distilling Task-Specific
Knowledge from BERT into Simple Neural Networks<i class="fa fa-external-link-alt"></i></span><br />
<span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvYWJzLzE5MDguMDkzNTU=">Patient Knowledge
Distillation for BERT Model Compression<i class="fa fa-external-link-alt"></i></span><br />
<span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvYWJzLzE5MTAuMDExMDg=">DistilBERT, a distilled
version of BERT: smaller, faster, cheaper and lighter<i class="fa fa-external-link-alt"></i></span><br />
<span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvYWJzLzE5MDkuMTAzNTE=">TinyBERT: Distilling BERT for
Natural Language Understanding<i class="fa fa-external-link-alt"></i></span><br />
<span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvYWJzLzIwMDQuMDI5ODQ=">MobileBERT: a Compact
Task-Agnostic BERT for Resource-Limited Devices<i class="fa fa-external-link-alt"></i></span><br />
<span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvYWJzLzIwMDIuMTA5NTc=">MINILM: Deep Self-Attention
Distillation for Task-Agnostic Compression of Pre-Trained
Transformers<i class="fa fa-external-link-alt"></i></span><br />
<span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzIwMDQuMDQwMzcucGRm">DynaBERT: Dynamic BERT
with Adaptive Width and Depth<i class="fa fa-external-link-alt"></i></span></p>
<p><span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL2FpcmFyaWEvVGV4dEJyZXdlcg==">TextBrewer-github<i class="fa fa-external-link-alt"></i></span><br />
<span class="exturl" data-url="aHR0cHM6Ly9naXRlZS5jb20vbWFjcm9oYXJkL1RleHRCcmV3ZXIvdHJlZS9tYXN0ZXI=">TextBrewer-gitee<i class="fa fa-external-link-alt"></i></span><br />
<span class="exturl" data-url="aHR0cHM6Ly90ZXh0YnJld2VyLnJlYWR0aGVkb2NzLmlvL2VuL2xhdGVzdC9pbmRleC5odG1s">TextBrewer<i class="fa fa-external-link-alt"></i></span></p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://soundmemories.github.io/2021/07/22/NLP/07.%E6%A3%80%E7%B4%A2%E7%B3%BB%E7%BB%9F%E5%92%8C%E5%B8%B8%E8%A7%81%E6%8C%87%E6%A0%87/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avstar.png">
      <meta itemprop="name" content="SoundMemories">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="SoundMemories">
      <meta itemprop="description" content="今日事，今日毕">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | SoundMemories">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2021/07/22/NLP/07.%E6%A3%80%E7%B4%A2%E7%B3%BB%E7%BB%9F%E5%92%8C%E5%B8%B8%E8%A7%81%E6%8C%87%E6%A0%87/" class="post-title-link" itemprop="url">检索系统和常见指标</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2021-07-22 00:00:00" itemprop="dateCreated datePublished" datetime="2021-07-22T00:00:00+08:00">2021-07-22</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/NLP/" itemprop="url" rel="index"><span itemprop="name">NLP</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>6.8k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>25 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="检索式">检索式</h1>
<h2 id="检索">检索</h2>
<p>核心为<strong>信息检索</strong>（Information
Retrieval，IR），即从大规模<strong>非结构化数据</strong>（通常为文本）的集合（通常保存在计算机上）中找出<strong>满足用户信息需求</strong>的资料（通常是文档）的过程。是研究信息的获取（acquisition）、表示（representation）、存储（storage）、组织（organization）和访问（access）的一门学问。</p>
<p>信息检索不仅仅是搜索，信息检索系统也不仅仅是搜索引擎。比如：<br />
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">返回与信息检索相关的网页-&gt;搜索引擎（SearchEngine，SE）</span><br><span class="line">姚明是谁？-&gt;问答系统（Question Answering，QA）</span><br><span class="line">返回Ipad的各类型号、配置等-&gt;信息抽取（Information Extraction，IE）</span><br><span class="line">使用Google Reader订阅新闻，并获取推荐-&gt;信息过滤（Information Filtering）、信息推荐（Information Recommending）</span><br></pre></td></tr></table></figure></p>
<h2 id="倒排索引">倒排索引</h2>
<p>一个例子：《莎士比亚全集》这本大头书，我们想知道：哪些剧本包含Brutus和Caesar但是不包含Calpurnia？一种方式是采用Unix下的grep程序，先找出所有包含Brutus和Caesar的剧本，然后再将包含Calpurnia的剧本排除。但是很多情况下，采用上述线性扫描的方式是远远不够的。可以考虑用<strong>空间换取时间</strong>。</p>
<p><strong>词项文档索引</strong><br />
词项-文档关联矩阵（incidence
matrix），采用非线性的扫描方式，事先给文档建立索引。比如行索引为人名，列索引为书名，如果此人在书中出现过就是1，否则为0。这种方法要很大的存储空间。遇到更大的数据集根本不可用这种方法。</p>
<p><strong>倒排索引</strong><br />
对于每一个词项，存储包含整个词项的文档的一个列表，一个文档用一个序列号docID来表示。我们能用一个固定长度的数据来存储它吗？不能，
不利于增删。<br />
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Brutus-&gt;[1,2,4,11,31,45,173,174]</span><br><span class="line">Caesar-&gt;[1,2,4,5,6,16,57,132]</span><br><span class="line">Calpurnia-&gt;[2,31,54,101]</span><br></pre></td></tr></table></figure><br />
如果Caesar被添加到14号文档中，固定长度数组就不行了，所以使用可变长度的记录列表：<br />
1、在硬盘上，一串<strong>连续的记录</strong>是正常的，也是最好的。<br />
2、在内存里，可以使用<strong>链表</strong>，或者可变长度的整数。</p>
<p>上面所说的<strong>docID链表</strong>（排序后的结果）就是<strong>倒排记录表</strong>（inverted
list），人名构成的词项就是<strong>词项词典</strong>。词项-&gt;文档。</p>
<p>建立步骤：<br />
1、建立<strong>词条</strong>和<strong>docID</strong>序列。<br />
2、排序（<strong>先按照词条排序，再按照docID排序</strong>）。<br />
<img src="/images/对话系统/3.png" width="40%"></p>
<p>考虑查询Brutus和Caesar：<br />
1、在字典中找到Brutus，得到它的倒排记录表。<br />
2、在字典中找到Caesar，得到它的倒排记录表。<br />
3、两个倒排记录表取交集（使用双指针查询，O(n)）。</p>
<h1 id="近邻搜索召回">近邻搜索(召回)</h1>
<h2 id="字符级别">字符级别</h2>
<h3 id="bm25">BM25</h3>
<p>bm25
是一种用来评价搜索词和文档之间相关性的算法，它是一种基于<strong>概率检索模型</strong>提出的算法，再用简单的话来描述下bm25算法：我们有一个
<span class="math inline">\(query\)</span> 和一批文档 <span
class="math inline">\(Ds\)</span> ，现在要计算 <span
class="math inline">\(query\)</span> 和每篇文档 <span
class="math inline">\(D\)</span> 之间的相关性分数，我们的做法是，先对
<span class="math inline">\(query\)</span> 进行切分，得到单词 <span
class="math inline">\(q_i\)</span> ，然后单词的分数由3部分组成：<br />
- 每个单词的权重。<br />
- 相关性分数 <span class="math inline">\(R\)</span>：单词和 <span
class="math inline">\(D\)</span> 之间的相关性，单词和 <span
class="math inline">\(query\)</span> 之间的相关性。</p>
<p>最后对于每个单词的分数我们做一个求和，就得到了query和文档之间的分数。<br />
<span
class="math display">\[Score(Q,d)=\sum\limits^nW_iR(q_i,d)\]</span><br />
其中，<span class="math inline">\(W_i\)</span>代表单词 <span
class="math inline">\(q_i\)</span> 权重，<span
class="math inline">\(R(q_i,d)\)</span> 代表单词 <span
class="math inline">\(q_i\)</span> 和文档 <span
class="math inline">\(d\)</span> 相关性。</p>
<p><strong>每个单词的权重(IDF变形)</strong><br />
<span class="math inline">\(N\)</span> 表示所有文档数目，<span
class="math inline">\(n(q_i)\)</span> 为单词 <span
class="math inline">\(q_i\)</span>
出现的文档数目，0.5主要是做平滑处理。<br />
<span
class="math display">\[IDF(q_i)=log(\frac{N-n(q_i)+0.5}{n(q_i)+0.5})\]</span><br />
依据IDF的作用，对于某个 <span class="math inline">\(q_i\)</span> ，包含
<span class="math inline">\(q_i\)</span> 的文档数越多，说明 <span
class="math inline">\(q_i\)</span>
重要性越小，或者区分度越低，IDF越小，因此IDF可以用来刻画 <span
class="math inline">\(q_i\)</span> 与文档的相似性。</p>
<p><strong>相关性分数(TF变形)</strong><br />
BM25的设计依据一个重要的发现：词频和相关性之间的关系是非线性的，也就是说，每个词对于文档的相关性分数不会超过一个特定的阈值，当词出现的次数达到一个阈值后，其影响就不在线性增加了，而这个阈值会跟文档本身有关。<br />
<span class="math display">\[R(q_i,d)=\frac{f_i\cdot
(k_1+1)}{f_i+K}\cdot\frac{qf_i\cdot(k_2+1)}{qf_i+k_2}\]</span><br />
<span
class="math display">\[K=k_1\cdot(1-b+b\cdot\frac{dl}{avgdl})\]</span><br />
其中，<span class="math inline">\(f_i\)</span> 为单词 <span
class="math inline">\(q_i\)</span> 在文档 <span
class="math inline">\(d\)</span> 中的词频。<span
class="math inline">\(qf_i\)</span> 为单词 <span
class="math inline">\(q_i\)</span> 在 <span
class="math inline">\(query\)</span> 中出现的频率。<br />
<span class="math inline">\(dl\)</span> 是文档 <span
class="math inline">\(d\)</span> 的长度，<span
class="math inline">\(avgdl\)</span> 是所有文档的平均长度。<br />
<span class="math inline">\(k_1\)</span>
是一个正的参数（一般为2），用来标准化文章词频的范围，<span
class="math inline">\(k_1\)</span>
越大，我们越看重单词在文档d中词频的影响。<br />
<span class="math inline">\(k_2\)</span>
越大，越看重单词在query中的词频，<span
class="math inline">\(k_2\)</span> 一般为1。<span
class="math inline">\(b\)</span>为0~1之间的值（一般为0.75），决定使用文档长度来表示信息量的范围。</p>
<h3 id="wand">WAND</h3>
<p>wand（weak
and）算法，通过计算每个词的贡献上限来估计文档的相关性上限，并与预设的阈值比较，进而跳过一些相关性一定达不到要求的文档，从而得到提速效果（<span
class="math inline">\(query\)</span>比较长的时候使用）。</p>
<p>wand 算法首先要估计<strong>每个词对相关性贡献的上限（upper
bound）</strong>，最简单的相关性就是TF-IDF，一般IDF是固定的，因此只需要估计一个词在各个文档中的词频TF上限（即这个词在各个文档中最大的TF），该步骤通过线下计算即可完成。线下计算出各个词的相关性上限，可以计算出一个
<span class="math inline">\(query\)</span>
和一个文档的相关性上限值，就是它们共同出现的词的相关性上限值的和，通过与预设的阈值比较，如果
<span class="math inline">\(query\)</span>
与文档的相关性大于阈值，则进行下一步的计算，否则丢弃。<br />
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">query：the quick brown fox     </span><br><span class="line">with top k=2</span><br></pre></td></tr></table></figure><br />
根据倒排索引表，计算每个词的相关性贡献上限，即
TF-IDF，取每个词的最大值即可，如下表中max值。<br />
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">        max  倒排索引</span><br><span class="line">the     0.9  [2,3,7,8,9,10,11,12,13,17,18,19...]</span><br><span class="line">qucik   1.9  [5,6,9,11,14,18]</span><br><span class="line">brown   2.3  [2,4,5,15,42,84,96]</span><br><span class="line">fox     7.1  [5,7,8,13]</span><br></pre></td></tr></table></figure><br />
根据max可得到 <span class="math inline">\(query\)</span>
和文档的相关性上限。<br />
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">the和brown都出现在2文档，假设2文档中the的tfidf为0.1，brown为1。</span><br><span class="line">the只出现在3文档，假设3文档中the的tfidf为0.5。</span><br><span class="line"># |score|id|</span><br><span class="line">------------</span><br><span class="line">1 | 2.0 | 2 |</span><br><span class="line">2 | 0.5 | 3 |</span><br><span class="line">一般用heap维持k个。</span><br></pre></td></tr></table></figure><br />
维持top-2这个heap堆，不停的这样寻找。<br />
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">当走到4的时候，只有brown出在4中，且max为2.3，那么文档4的score可能战胜0.5，计算文档4的tfidf，假设为1.4，替换掉0.5。</span><br><span class="line"># |score|id|</span><br><span class="line">------------</span><br><span class="line">1 | 2.0 | 2 |</span><br><span class="line">2 | 1.4 | 4 |</span><br><span class="line"></span><br><span class="line">同理，走到文档5的时候，brown、quick、fox都出现在文档5中，假设score计算为6.3，替换掉1.4。</span><br><span class="line"># |score|id|</span><br><span class="line">------------</span><br><span class="line">1 | 6.3 | 5 |</span><br><span class="line">2 | 2.0 | 2 |</span><br><span class="line"></span><br><span class="line">走到文档6的时候，只有quick出现在文档6中，且max为1.9，不可能超过heap中最小的score，所以跳过不用计算。</span><br></pre></td></tr></table></figure><br />
预设的阈值，就是heap中最小score值，每次都是和它比较。</p>
<h2 id="向量级别">向量级别</h2>
<h3 id="sif">SIF</h3>
<p>smooth inverse frequency（SIF），原论文<span class="exturl" data-url="aHR0cHM6Ly9vcGVucmV2aWV3Lm5ldC9wZGY/aWQ9U3lLMDB2NXh4">A SIMPLE BUT
TOUGH-TO-BEAT BASELINE FOR SENTENCE EMBEDDINGS<i class="fa fa-external-link-alt"></i></span>，参考解析<span class="exturl" data-url="aHR0cHM6Ly93d3cuY25ibG9ncy5jb20vZGF0YWJpbmdvL3AvOTc4ODI0NC5odG1s">论文阅读<i class="fa fa-external-link-alt"></i></span>。</p>
<h3 id="wmd">WMD</h3>
<p>Word Mover's Distance，论文《From Word Embeddings To Document
Distances》提出了一种新的计算文档距离的方法。该方法建立在Word
Embeddings基础之上，通过累计计算一个文档中的词travel到另一篇文档中词的最小距离来进行度量。详情参考<span class="exturl" data-url="aHR0cHM6Ly96aHVhbmxhbi56aGlodS5jb20vcC8yNTEzNDQ4Njg=">Word Mover's Distance
论文笔记<i class="fa fa-external-link-alt"></i></span>，<span class="exturl" data-url="aHR0cHM6Ly96aHVhbmxhbi56aGlodS5jb20vcC84ODc4ODk2MQ==">对Word
Mover's Distance的理解<i class="fa fa-external-link-alt"></i></span>。 优化算法WCD（Word centroid
distance）和RWMD（Relaxed word moving distance）参考<span class="exturl" data-url="aHR0cHM6Ly96aHVhbmxhbi56aGlodS5jb20vcC85MTYyMTI0MQ==">深入理解WMD距离——一种衡量文本之间差异的度量<i class="fa fa-external-link-alt"></i></span>和<span class="exturl" data-url="aHR0cHM6Ly96aHVhbmxhbi56aGlodS5jb20vcC8xNjQ5NTcyNTc=">WMD系列方法介绍（词移距离方法）<i class="fa fa-external-link-alt"></i></span>和<span class="exturl" data-url="aHR0cHM6Ly96aHVhbmxhbi56aGlodS5jb20vcC84ODY5OTc3Ng==">From Word Embeddings To
Document Distances 小结<i class="fa fa-external-link-alt"></i></span>。</p>
<h3 id="annoy">Annoy</h3>
<p>Approximate Nearest Neighbors Oh Yeah，Annoy 是 Spotify
开源的高维空间求近似最近邻的库，在 Spotify
使用它进行音乐推荐。Annoy通过将海量数据建立成一个二叉树来使得每个数据查找时间复杂度是<span
class="math inline">\(O(log n)\)</span>。详情参考<span class="exturl" data-url="aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2hlcm9fZmFudGFvL2FydGljbGUvZGV0YWlscy83MDI0NTM4Nw==">海量数据相似查找系列2
-- Annoy算法<i class="fa fa-external-link-alt"></i></span>。</p>
<h3 id="hnsw">HNSW</h3>
<p>Hierarchcal Navigable Small World graphs，详情参考<span class="exturl" data-url="aHR0cHM6Ly93d3cuY25ibG9ncy5jb20vZGFuZ3VpL3AvMTQ2NzUxMjEuaHRtbA==">HNSW<i class="fa fa-external-link-alt"></i></span>，<span class="exturl" data-url="aHR0cHM6Ly93d3cucnlhbmxpZ29kLmNvbS8yMDE4LzExLzI3LzIwMTgtMTEtMjclMjBITlNXJTIwJUU0JUJCJThCJUU3JUJCJThELw==">近似最近邻算法
HNSW 学习笔记<i class="fa fa-external-link-alt"></i></span>，代码<a
target="_blank" rel="noopener" href="https://github.com/facebookresearch/faiss/blob/13a2d4ef8fcb4aa8b92718ef4b9cc211033e7318/benchs/bench_hnsw.py">facebookresearch<br />
/faiss</a>。</p>
<h3 id="kd-tree">KD Tree</h3>
<p>K dimentional Tree，详情参考李航的《统计学习方法》中K近邻算法，<span class="exturl" data-url="aHR0cHM6Ly9iYWlrZS5iYWlkdS5jb20vaXRlbS9rZC10cmVlLzIzMDI1MTU/ZnI9YWxhZGRpbg==">kd-tree<i class="fa fa-external-link-alt"></i></span>和<span class="exturl" data-url="aHR0cHM6Ly9sZWlsZWlsdW9sdW8uY29tL3Bvc3RzL2tkdHJlZS1hbGdvcml0aG0tYW5kLWltcGxlbWVudGF0aW9uLmh0bWw=">k-d
tree算法原理及实现<i class="fa fa-external-link-alt"></i></span>。</p>
<h3 id="lsh">LSH</h3>
<p>Locality Sensitive Hashing，详情参考<span class="exturl" data-url="aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ljdnByL2FydGljbGUvZGV0YWlscy8xMjM0MjE1OQ==">局部敏感哈希(Locality-Sensitive
Hashing, LSH)方法介绍<i class="fa fa-external-link-alt"></i></span>，<span class="exturl" data-url="aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2hlcm9fZmFudGFvL2FydGljbGUvZGV0YWlscy83MDI0NTI4ND9zcG09MTAwMS4yMDE0LjMwMDEuNTUwMg==">海量数据相似查找系列1
-- Minhashing &amp; LSH &amp; Simhash 技术汇总<i class="fa fa-external-link-alt"></i></span>。</p>
<h2 id="faiss工具">Faiss工具</h2>
<iframe src="https://nbviewer.org/github/soundmemories/StudyNotes/blob/f42b2aced7dfe7e25f078bf8c8a59725cced6c31/Cheatsheets/Faiss_demo.ipynb" width="1000" height="700">
</iframe>
<h1 id="ranking">Ranking</h1>
<h2 id="评价指标">评价指标</h2>
<h3 id="map">MAP</h3>
<p>MAP（Mean Average
Precision）：平均准确率是相关文档检索出后的准确率的平均值。<br />
反映系统在全部相关文档的性能单值指标，检索出来的相关文档越靠前（rank越高），MAP就可能越高。<br />
<img src="/images/检索和指标/1.png" width="100%"></p>
<p>计算顺序：Precision-&gt;Average Precision-&gt;Mean Average
Precision。<br />
<strong>Precision</strong>：rank结果中的相关文档-&gt;<span
class="math inline">\(\dfrac{\text{第几个相关文档}}{\text{文档rank数}}\)</span><br />
<strong>Average Precision</strong>：<span
class="math inline">\(\dfrac{rank结果中的相关文档Precision和}{相关文档总数}\)</span><br />
<strong>Mean Average Precision</strong>：<span
class="math inline">\(\dfrac{每次查询的Average
Precision和}{查询次数}\)</span></p>
<p>注意，在Precision中，分子是rank结果中的相关文档是第几个，比如主题1有4个相关网页，序号分别为1234，那么rank排序为1234和4321的Precision结果是一样的，分子都是1234，分母也都是1234。<strong>因为此MAP没有考虑相关文档内部排序</strong>。</p>
<h3 id="ndcg">NDCG</h3>
<p><strong>NDCG</strong>（Normalized Discounted Cumulative
Gain），归一化折损累计增益。衡量和评价搜索结果算法。DCG的两个思想：<br />
-
<strong>高度关联的结果比一般关联度的结果更影响最终的指标得分</strong>。<br />
-
<strong>有高关联度的结果出现在更靠前的位置的时候，指标会越高</strong>。</p>
<p><strong>CG</strong>（Cumulative
Gain），是DCG的前身，只考虑到了相关性的关联程度，没有考虑到位置的因素。它是一个搜素结果相关性分数的总和。如一个搜索结果list页面有P个结果，其CG为：<br />
<span
class="math display">\[CG_{p}=\sum\limits_{i=1}^{p}rel_i\]</span><br />
其中，<span class="math inline">\(rel_i\)</span> 代表 <span
class="math inline">\(i\)</span> 位置上的相关度。<br />
CG的统计不受到搜索结果的排序影响，CG得分高只能说明这个结果页面总体的质量比较高并不能说明这个算法做的排序好或差。</p>
<p><strong>DCG</strong>（Discounted
CG），就是在每一个CG的结果上除以一个折损值，为什么要这么做呢？目的就是为了让排名越靠前的结果越能影响最后的结果。公式：<br />
<span
class="math display">\[DCG_p=\sum\limits_{i=1}^{p}\frac{rel_i}{log_2(i+1)}=rel_1+\sum\limits_{i=2}^{p}\frac{rel_i}{log_2(i+1)}\]</span><br />
当然还有一种比较常用的公式，用来增加相关度影响比重的DCG计算方式是：<br />
<span class="math display">\[DCG_p=\sum\limits_{i=1}^{p}
\frac{2^{rel_i}-1}{log_2(i+1)}\]</span></p>
<p><strong>NDCG</strong>（Normalize
DCG），由于搜索结果随着检索词的不同，返回的数量是不一致的，而DCG是一个累加的值，没法针对两个不同的搜索结果进行比较，因此需要归一化处理，这里是处以IDCG。<br />
<span class="math display">\[nDCG_p=\frac{DCG_p}{IDCG_p}\]</span><br />
IDCG（ideal
DCG）为理想情况下最大的DCG值。IDCG如何计算？首先要拿到搜索的结果，人工对这些结果进行排序，排到最好的状态后，算出这个排列下本query的DCG，就是IDCG。<br />
<span class="math display">\[\sum\limits_{i=1}^{|REL|}
\frac{2^{rel_i}-1}{log_2(i+1)}\]</span><br />
其中 <span class="math inline">\(|REL|\)</span>
表示，结果按照相关性从大到小的顺序排序，取前p个结果组成的集合。也就是按照最优的方式对结果进行排序。</p>
<h2 id="point-wise">Point-wise</h2>
<p>Pointwise排序是将训练集中的每个item看作一个样本获取rank函数，主要解决方法是把分类问题转换为单个item的分类或回归问题。</p>
<p>point-wise把排序问题当成一个二分类问题，训练的样本被组织成一个三元组
<span class="math inline">\((q_i,c_{i,j},y_{i,j})\)</span>。<span
class="math inline">\(y_{i,j}\)</span> 为一个二进制值，表明 <span
class="math inline">\(c_{i,j}\)</span> 是否为 <span
class="math inline">\(q_i\)</span>
正确回答。我们就可以训练一个二分类网络：<span
class="math inline">\(h_\theta(q_i,c_{i,j}\to y_{i,j})\)</span> ，其中
<span class="math inline">\(0\leq y_{i,j}\leq
1\)</span>。训练的目标为最小化数据集中所有问题和候选句子对的交叉熵。</p>
<p><strong>算法</strong>：<br />
- 基于回归的算法：此时，输出空间包含的是实值相关度得分。采用 ML
中传统的回归方法即可。<br />
- 基于分类的算法：此时，输出空间包含的是无序类别。对于二分类，SVM、LR
等均可；对于多分类，提升树等均可。<br />
-
基于有序回归的算法：此时，输出空间包含的是有序类别。通常是找到一个打分函数，然后用一系列阈值对得分进行分割，得到有序类别。采用
PRanking、基于 margin 的方法都可以。</p>
<p>常见算法：Subset Ranking, McRank, Prank, OC SVM</p>
<p><strong>缺点</strong>：<br />
- ranking追求的是排序结果，并不要求精确打分，只要相对打分即可。<br />
- pointwise类方法并没有考虑同一个query对应的docs间的内部依赖性。<br />
-
损失函数也没有利用model预测排序中的位置信息。因此，损失函数可能无意的过多强调那些不重要的docs，即会强调那些排序在后面对用户体验影响小的doc。<br />
- query间docs的不平衡，如query1对应500个文档，query2对应10个文档。当不同
query 对应不同数量的 docs 时，整体 loss 将会被对应 docs 数量大的 query
组所支配，每组 query 应该都是等价的。</p>
<p><strong>优点</strong>：速度快，标注简单，复杂度低。</p>
<p><strong>改进</strong>：Pointwise 类算法也可以再改进，比如在 loss
中引入基于 query 的正则化因子的 RankCosine 方法。</p>
<h2 id="pair-wise-approach">Pair-wise Approach</h2>
<p>Pairwise排序是将同一个查询中两个不同的item作为一个样本，主要思想是把rank问题转换为二值分类问题。</p>
<p>在pair-wise方法中排序模型 <span
class="math inline">\(h_{\theta}\)</span>
让正确的回答的得分明显高于错误的获选答案。给一个提问，pair-wise给定一对候选回答学习并预测哪一个句子才是提问的最佳回答。训练的样例为
<span class="math inline">\((q_i,c_i^+,c_i^-)\)</span> ，其中 <span
class="math inline">\(q_i\)</span> 为提问， <span
class="math inline">\(c_i^+\)</span> 为正确的回答，<span
class="math inline">\(c_i^-\)</span> 为候选答案中一个错误的回答。</p>
<p><strong>算法</strong>：基于二分类的算法，比如Random
Forest，GBDT，RankSVM，RankBoost，RankNet，Lambda Rank,
LambdaMart等。RankNet：原本Ranking常见的评价指标都无法求梯度，因此没法直接对评价指标做梯度下降，而它们将不适宜用梯度下降求解的Ranking问题，转化为对偏序概率的交叉熵损失函数的优化问题，从而适用梯度下降方法。</p>
<p><strong>缺点</strong>：<br />
- doc pair 的数量将是 doc 数量的二次（<span
class="math inline">\(C_n^2\)</span>），从而 pointwise 方法就存在的
query 间 doc 数量的不平衡性将在 pairwise 方法中进一步放大。<br />
- pairwise 方法相对 pointwise
方法对噪声标注更敏感，即一个错误标注会引起多个 doc pair 标注错误。<br />
- pairwise 方法仅考虑了 doc pair 的相对位置，损失函数还是没有利用 model
预测排序中的位置信息。<br />
- 如果人工标注包含多有序类别，那么转化成 pairwise preference
时必定会损失掉一些更细粒度的相关度标注信息。<br />
- pairwise 方法只考虑了内部相对位置排序，没有考虑整体排序。</p>
<p><strong>改进</strong>：<br />
- IRSVM，主要针对前述第一个缺陷。<br />
- 采用 Sigmoid 进行改进的 pairwise 方法，主要针对前述第二个缺陷。<br />
- P-norm push，Ordered weighted average ranking，LambdaRank，Sparse
ranker，主要针对前述第三个缺陷。<br />
- Multiple hyperplane ranker，magnitude-preserving
ranking，主要针对前述第四个缺陷。</p>
<h2 id="list-wise-approach">List-wise Approach</h2>
<p>List-wise排序是将整个item序列看作一个样本，通过<strong>直接优化信息检索的评价方法</strong>和<strong>定义损失函数</strong>两种方法实现。</p>
<p>pair-wise 和 point-wise
忽视了一个事实就是答案选择，即从一系列候选句子中的预测问题。在list-wise中单一训练样本就：query和它的所有候选句子。在训练过程中给定提问数据
<span class="math inline">\(q_i\)</span> 和它的一系列候选句子 <span
class="math inline">\(C(c_{i1},c_{i2},\dots,c_{im})\)</span> 和标签
<span
class="math inline">\(Y(y_{i1},y_{i2},\dots,y_{im})\)</span>，归一化的得分向量
<span class="math inline">\(S\)</span> 通过如下公式计算：<br />
<span
class="math display">\[Score_j=h_{\theta}(q_i,c_{ij})\]</span><br />
<span
class="math display">\[S=softmax([Score_1,Score_2,\dots,Score_m])\]</span></p>
<p><strong>算法</strong>：<br />
- 直接基于评价指标的算法：直接取优化 ranking
的评价指标，但这并不简单，因为评价指标基本都是离散不可微的，具体处理方式：<br />
- 优化基于评价指标的 ranking error
的连续可微的近似，如SoftRank，ApproximateRank，SmoothRank。<br />
- 优化基于评价指标的 ranking error 的连续可微的上界，如
SVM-MAP，SVM-NDCG，PermuRank。<br />
- 使用可以优化非平滑目标函数的优化技术，如 AdaRank，RankGP。<br />
- 非直接基于评价指标的算法：这里，不再使用和评价指标相关的 loss
来优化模型，而是设计能衡量模型输出与真实排列之间差异的
loss，如此获得的模型在评价指标上也能获得不错的性能。经典的如
，ListNet，ListMLE，StructRank，BoltzRank。</p>
<p>直接基于评价指标的算法的优化目标都是直接和 ranking
的评价指标有关。现在来考虑一个概念，informativeness。通常认为一个更有信息量的指标，可以产生更有效的排序模型。而多层评价指标（NDCG）相较二元评价（AP）指标通常更富信息量。因此，有时虽然使用信息量更少的指标来评估模型，但仍然可以使用更富信息量的指标来作为
loss 进行模型训练。</p>
<p><strong>缺点</strong>：<br />
- listwise 类相较 pointwise、pairwise 对 ranking 的 model 更自然，解决了
ranking 应该基于 query 和 position 问题。<br />
- listwise 类存在的主要缺陷是：一些 ranking 算法需要基于排列来计算
loss，从而使得训练复杂度较高，如 ListNet和
BoltzRank。此外，位置信息并没有在 loss 中得到充分利用，可以考虑在
ListNet 和 ListMLE 的 loss 中引入位置折扣因子。</p>
<h2 id="cfg">CFG</h2>
<p>CFG：Context free grammars，上下文无关文法。是一种形式文法（formal
grammar）。形式文法是形式语言（formal language）的文法，由一组产生规则
（production rules）组成，描述该形式语言中所有可能的字符串形式。</p>
<p>上面这段话比价令人费解，我理解，就是每个句子的产生都遵循着一定的规则（规则学习的理念）。在这里面有几个概念：<br />
- 终止符：可以理解为基础符号，词法符号，是不可替
代的，天然存在，不能通过文法规则生成。<br />
- 非终结符，或者句法变量。<br />
- Production rules： grammar
是由终结符集、非终结符集和产生规则共同组成。产生规则定义了符号之间如何转换替代。规则的左侧是规则头，是可以被替代的符号；右侧是规则体，是具体的内容。</p>
<p>CFGs的性质：<br />
- 一个CFG定义了一个可能的推导的集合。<br />
-
一句话如果是可以被CFG定义出来，那么至少有一个推导可以产生这句话。<br />
-
每一句话的CFG可以是有歧义的（即有多种推导的可能，因为一个非终结符同时存在了多条规则），至于如何解决这个问题，在PCFGs中，也就是概率上下文无关文法中会有介绍，简单来说就是给予每条规则一个概率，再利用这些概率求得概率最大的那棵解析树。</p>
<p>CKY算法：CKY处理的CFG必须是CNF形式的。所以算法首先要把非CNF形式的CFG转成CNF形式。</p>
<h1 id="相似度的计算">相似度的计算</h1>
<h2 id="统计指标">统计指标</h2>
<h3 id="cosine">Cosine</h3>
<p><span class="math display">\[ S = \frac{x\cdot y}{|x||y|}
\]</span><br />
两向量越相似，向量夹角越小，cosine绝对值越大；值为负，两向量负相关。</p>
<h3 id="jaccard">Jaccard</h3>
<p><span class="math display">\[ J(A,B)=\frac{|A\cap B|}{|A \cup B|}
\]</span><br />
值越大越相似，分子是A和B的交集大小，分母是A和B的并集大小。<br />
Jaccard系数主要用于计算符号度量或布尔值度量的个体间的相似度，无法衡量差异具体值的大小，只能获得“是否相同”这个结果，所以Jaccard系数只关心个体间共同具有的特征是否一致这个问题。</p>
<h3 id="pearson">Pearson</h3>
<p><span class="math display">\[ r =
\frac{\sum\limits_{i=1}^n(x_i-\overline{x})(y_i-\overline{y})}{\sqrt{\sum\limits_{i=1}^n(x_i-\overline{x})^2}\sum\limits_{i=1}^n(y_i-\overline{y})^2}
\]</span><br />
两个变量之间的协方差和标准差的商。反映两个变量X和Y的线性相关程度，r值介于-1到1之间，绝对值越大表明相关性越强。</p>
<h3 id="euclidian">Euclidian</h3>
<p><span class="math display">\[
d=\sqrt{\sum\limits_{i=1}^n(x_i-y_i)^2}\]</span><br />
欧氏距离。</p>
<h2 id="文本距离">文本距离</h2>
<h3 id="ed">ED</h3>
<p>编辑距离（Edit Distance，ED），也叫Levenshtein
Distance，是用来度量两个序列相似程度的指标。通俗地来讲，编辑距离指的是在两个单词之间，由其中一个单词转换为另一个单词所需要的<strong>最少单字符编辑</strong>次数。详情参考<span class="exturl" data-url="aHR0cHM6Ly93d3cuamlhbnNodS5jb20vcC9hNjE3ZDIwMTYyY2Y=">详解编辑距离(Edit
Distance)及其代码实现<i class="fa fa-external-link-alt"></i></span>和<span class="exturl" data-url="aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2Jhb2RyZWFtL2FydGljbGUvZGV0YWlscy84MDQxNzY5NQ==">最小编辑距离算法
Edit Distance（经典DP）<i class="fa fa-external-link-alt"></i></span></p>
<h3 id="lcs">LCS</h3>
<p>最长公共子序列（Longest Common
Subsequence，LCS），一个序列，如果是两个或多个已知序列的子序列，且是所有子序列中最长的，则为最长公共子序列。</p>
<p>LCS和ED，两者都可以衡量字符串的相近程度。不同之处在于，LCS对两个的长度差异不敏感，编辑距离对两者的长度差异敏感。LCS衡量了两者的重合度，编辑距离衡量了两者的长度和重合度。对编辑距离的增删代价取0，改操作换成相同奖励，就是LCS。</p>
<p>算法详解：<span class="exturl" data-url="aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NpbmF0XzM4ODE2OTI0L2FydGljbGUvZGV0YWlscy84MzI0MzQzNA==">LCS(longest
common subsequence)（最长公共子序列）<i class="fa fa-external-link-alt"></i></span><br />
### WAM<br />
句向量表示（Word Averaging
Model，WAM），首先对句子进行分词，然后对分好的每一个词获取其对应的
Vector，然后将所有 Vector 相加并求平均，这样就可得到 Sentence
Vector。</p>
<h2 id="深度匹配">深度匹配</h2>
<p>使用Bert的序列相似度预测。</p>
<p><strong>Poly-encoders</strong>：开发了一种新的transformer体系结构，即Poly-encoder，该体系结构学习了全局而不是令牌级别的self-attention特征，同时解决了
DSSM 式的 Bi-encoder 匹配质量低的问题和 ARC-II、BERT 等交互式的
Cross-encoder 匹配速度慢的问题。</p>
<p>论文原文：<span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzE5MDUuMDE5Njl2Mi5wZGY=">Poly-encoders: Transformer
Architectures and Pre-training Strategies for Fast and Accurate
Multi-sentence Scoring<i class="fa fa-external-link-alt"></i></span>。<br />
详解参考：<span class="exturl" data-url="aHR0cHM6Ly96aHVhbmxhbi56aGlodS5jb20vcC8xNDIxNDgzNzM=">《Poly-encoders》阅读笔记<i class="fa fa-external-link-alt"></i></span>、<span class="exturl" data-url="aHR0cHM6Ly96aHVhbmxhbi56aGlodS5jb20vcC8xMTk0NDQ2Mzc=">PolyEncoder-Facebook的全新信息匹配架构-提速3000倍(附复现结果与代码)<i class="fa fa-external-link-alt"></i></span>。</p>
<p><strong>Sentence-BERT</strong>：虽然BERT和RoBERTa在很多句子对形式的回归任务（例如文本语义相似度）上达到了SOTA效果，但是它们还存在一些缺点：在这些任务中，它们均需要将比较的两个句子都传入到模型中计算，计算开销过大。BERT模型在一个1W句子集合中，找出最相近的一个句子对，需要5千万次推断计算（约65小时）才能完成，所以BERT并不适合语义相似度搜索等任务。</p>
<p>在该论文中，作者提出了一个新的模型，Sentence-BERT（简称SBERT）。SBERT采用双重或三重BERT网络结构，具体结构介绍会在后文中详细介绍。如果使用的是基于RoBERTa模型，则改造后的模型简称为SRoBERTa。</p>
<p>通过SBERT模型获取到的句子embedding，可以直接通过cos相似度计算两个句子的相似度，这样就大大减少了计算量。因为在使用BERT模型进行句子间相似度的判断时，需要从句子集合中，选出两个句子进行组合，传入BERT中进行计算，而使用SBERT模型，只需要将集合中每个句子单独传入到模型中，得到每个句子的embeding，计算相似度只需要使用cos函数计算两两embeding的cos距离即可。因此，使用BERT/RoBERTa模型需要65h才能完成的寻找最相似句子对任务，SBERT模型完成仅需5s。</p>
<p>作者在一些STS任务和迁移学习任务上评估SBERT模型，该模型达到了新的SOTA水平。</p>
<p>论文原文：<span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzE5MDguMTAwODQucGRm">Sentence-BERT: Sentence
Embeddings using Siamese BERT-Networks<i class="fa fa-external-link-alt"></i></span>。<br />
模型地址：<span class="exturl" data-url="aHR0cHM6Ly93d3cuc2JlcnQubmV0Lw==">Sentence-Transformers<i class="fa fa-external-link-alt"></i></span>。<br />
详解参考<span class="exturl" data-url="aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2Zlbmd4aW5saW51eC9hcnRpY2xlL2RldGFpbHMvMTA5MTk1NzYy">论文阅读Sentence-BERT:
Sentence Embeddings using Siamese BERT-Networks<i class="fa fa-external-link-alt"></i></span>。</p>
<p><strong>SimCSE</strong>：句子向量表示一直是NLP领域的一个热门问题，主要是因为其应用范围比较广泛，而且作为很多任务的基石。最近连续出了好几篇关于句子表示的文章，从前段时间的BERT-Flow,
BERT-whitening到最近的这个SimCSE。BERT-Flow以及BERT-whitenning其实像是后处理，将bert的输出进行一定的处理来解决各向异性的问题。本篇的这个工作则是采用了自监督来提升模型的句子表示能力，说到自监督最关键的问题应该就是如何构建正负例了。本文的正负例有两种构建方式，对于无监督来说，作者使用了Droupout来构建正例，将一个样本经过encoder两次，就得到了一个正例对，负例则是同一个batch里的其它句子。而对于有监督则采用了SNLI数据集天然的结构，对立类别的是负例，另外两个类别的就是正例。没错就是如此简单的方法催生了新的SOTA，而且提升还非常的明显。<br />
论文原文：<span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzIxMDQuMDg4MjEucGRm">SimCSE: Simple
Contrastive Learning of Sentence Embeddings<i class="fa fa-external-link-alt"></i></span>。</p>
<h1 id="参考文献">参考文献</h1>
<p><span class="exturl" data-url="aHR0cHM6Ly96aHVhbmxhbi56aGlodS5jb20vcC83OTIwMjE1MQ==">BM25算法, Best
Matching<i class="fa fa-external-link-alt"></i></span><br />
<span class="exturl" data-url="aHR0cHM6Ly96aHVhbmxhbi56aGlodS5jb20vcC8xNTI1MjI5MDY=">K近邻算法哪家强？KDTree、Annoy、HNSW原理和使用方法介绍<i class="fa fa-external-link-alt"></i></span></p>
<p><span class="exturl" data-url="aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NvZnRfenp0aS9hcnRpY2xlL2RldGFpbHMvODc1Mjc5MTk=">18种和“距离(distance)”、“相似度(similarity)”相关的量的小结<i class="fa fa-external-link-alt"></i></span><br />
<span class="exturl" data-url="aHR0cHM6Ly93d3cuY25ibG9ncy5jb20vZGFuZ3VpL3AvMTQ2NzU1NzUuaHRtbA==">相似度计算方法<i class="fa fa-external-link-alt"></i></span><br />
<span class="exturl" data-url="aHR0cHM6Ly9tcC53ZWl4aW4ucXEuY29tL3MvdGFzWnRaMmRRT1NZdDF0TzdIdXJFZw==">机器学习中的相似性度量总结<i class="fa fa-external-link-alt"></i></span></p>
<p><span class="exturl" data-url="aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3ppamlrYW53YS9hcnRpY2xlL2RldGFpbHMvODc5ODAyOTA=">信息检索的评价指标<i class="fa fa-external-link-alt"></i></span><br />
<span class="exturl" data-url="aHR0cHM6Ly93d3cuY25ibG9ncy5jb20vYnktZHJlYW0vcC85NDAzOTg0Lmh0bWw=">搜索评价指标——NDCG<i class="fa fa-external-link-alt"></i></span><br />
<span class="exturl" data-url="aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTAxMzg3NTgvYXJ0aWNsZS9kZXRhaWxzLzY5OTM2MDQxLw==">信息检索中常用的评价指标：MAP,nDCG,ERR,F-measure<i class="fa fa-external-link-alt"></i></span><br />
<span class="exturl" data-url="aHR0cHM6Ly9tcC53ZWl4aW4ucXEuY29tL3MvMlZjQnd2LW9qNm9mT3l5R1ZpV3hmQQ==">从L2R开始理解一下Xgboost的
'objective': 'rank:pairwise'参数<i class="fa fa-external-link-alt"></i></span><br />
<span class="exturl" data-url="aHR0cHM6Ly93d3cuemhpaHUuY29tL3F1ZXN0aW9uLzIwNTY5ODMy">LTR （learning to
Rank）<i class="fa fa-external-link-alt"></i></span><br />
<span class="exturl" data-url="aHR0cHM6Ly96aHVhbmxhbi56aGlodS5jb20vcC8zMzc0NzgzNzM=">推荐- Point
wise、pairwise及list wise的比较<i class="fa fa-external-link-alt"></i></span><br />
<span class="exturl" data-url="aHR0cHM6Ly93d3cuY25ibG9ncy5jb20vc2hlbnhpYW9saW4vcC85NzIzODYwLmh0bWw=">Learning to
Rank：Point-wise、Pair-wise 和 List-wise区别<i class="fa fa-external-link-alt"></i></span><br />
<span class="exturl" data-url="aHR0cHM6Ly93d3cuY25ibG9ncy5jb20vYmVudHV3dXlpbmcvcC82NjgxOTQzLmh0bWw=">Learning to
Rank简介<i class="fa fa-external-link-alt"></i></span><br />
<span class="exturl" data-url="aHR0cHM6Ly93d3cuY25ibG9ncy5jb20vYmVudHV3dXlpbmcvcC82NjkwODM2Lmh0bWw=">Learning to
Rank算法介绍：RankNet，LambdaRank，LambdaMart<i class="fa fa-external-link-alt"></i></span><br />
<span class="exturl" data-url="aHR0cHM6Ly96aHVhbmxhbi56aGlodS5jb20vcC8zOTgyNjQ1MTQ=">CFG：Context free
grammars 上下文无关文法<i class="fa fa-external-link-alt"></i></span><br />
<span class="exturl" data-url="aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0NoYXNlMTk5OC9hcnRpY2xlL2RldGFpbHMvODQ1MDQxOTE=">基于CYK+PCFG的短语结构句法分析<i class="fa fa-external-link-alt"></i></span></p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://soundmemories.github.io/2021/07/21/NLP/06.%E5%AF%B9%E8%AF%9D%E7%B3%BB%E7%BB%9F/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avstar.png">
      <meta itemprop="name" content="SoundMemories">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="SoundMemories">
      <meta itemprop="description" content="今日事，今日毕">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | SoundMemories">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2021/07/21/NLP/06.%E5%AF%B9%E8%AF%9D%E7%B3%BB%E7%BB%9F/" class="post-title-link" itemprop="url">对话系统简介</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2021-07-21 00:00:00" itemprop="dateCreated datePublished" datetime="2021-07-21T00:00:00+08:00">2021-07-21</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/NLP/" itemprop="url" rel="index"><span itemprop="name">NLP</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>939</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>3 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2 id="闲聊式">闲聊式</h2>
<p>情感识别，多轮对话中情感向量生成（Emotion
Embedding）、意图识别等。建议参考项目<span class="exturl" data-url="aHR0cHM6Ly9yYXNhLmNvbS8=">Rasa<i class="fa fa-external-link-alt"></i></span>。</p>
<h2 id="检索式">检索式</h2>
<p>涉及到的核心：<br />
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">输入问句：句向量的构建。</span><br><span class="line">粗排：候选问句的范围，重点是速度、召回率。</span><br><span class="line">精排：候选问句基础上精细匹配/深度匹配，重点是精准率。</span><br><span class="line">答案：匹配问句对应的答案。</span><br></pre></td></tr></table></figure></p>
<h2 id="知识问答">知识问答</h2>
<p><img src="/images/对话系统/1.png" width="80%"></p>
<h2 id="任务式">任务式</h2>
<p><img src="/images/对话系统/2.png" width="80%"></p>
<p>1、<strong>自然语言理解</strong>(NLU)：<br />
(1)
将<strong>非结构化</strong>的<strong>文本</strong>转成<strong>结构化</strong>的<strong>语义</strong>(意图分类、词槽序列标注)。<br />
(2) 将无限种可能转成有限的组合。<br />
核心：意图识别(Intent)、实体识别(Entity)。</p>
<p>2、<strong>对话管理</strong>(DM,
meaning&amp;context)：bot需要通过历史对话中的信息来判断，此时处理方案。<br />
(1) 对话跟踪(DST)：记录和更新 对话状态。用户话语中哪些
<strong>变化</strong>、<strong>值</strong>
是我们关心的，对对话处理有影响的要素记录。<br />
(2)
对话策略(DP)：根据<strong>历史和当前</strong>的对话状态选择合适的对话策略。<strong>选择哪种动作</strong>(Action)和<strong>执行动作响应后作什么</strong>，和业务相关，比如查询or定业务？成功or失败后做什么决策？。<br />
<strong>对话策略(Policy)类型</strong>：<br />
(1) 基于规则：意图为X，那么直接输出对应的动作Y。比如打招呼。<br />
(2)
基于记忆：用户当前对话状态和训练数据中某个story状态完全一致时，可以使用story中后序的动作。<br />
(3) 基于神经网络预测：模型分类预测，比如KerasPolicy。<br />
(4) 基于编程：自定义脚本处理规则。<br />
所有对话策略同时预测，按得分高低做动作。<br />
<strong>动作执行</strong>：把对话跟踪和对话策略的结果
发送给执行机构，得到返回结果呈现给用户。</p>
<p>3、<strong>自然语言生成</strong>(NLG)：<br />
(1) 语义转成文本。绝大数情况下，可用模板解决。</p>
<p><strong>90%看不见的工作</strong>：<br />
(1) 对话数据的获取：已有数据or人工造数据。<br />
(2) 对话数据的扩充：样本少，数据不均衡。<br />
(3)
对话数据的标注：算法实现or业务需求、长尾需求、语义歧义、方言/ASR错误识别、需求变更、无意义句子。<br />
(4) 对话数据校验与清洗：错标、漏标。<br />
(5) 数据和模板
版本化管理(可复现)：数据迭代(变更追踪)、算法迭代、参数迭代。<br />
(6) 模型部署：字典转换、序列编码。<br />
(7)
模型效果分析：模型行不行、哪里不行、为什么不行(数据or模型)、怎么解决。</p>
<p>任务型+FAQ：Rasa成熟的开源框架。<br />
Rasa x：不断根据客户反馈改进模型。<br />
Rasa3.0变化：<br />
(1) 支持无向图，可以使用更多的模型。<br />
(2) 不再支持markdown配置，改用YAML配置。<br />
(3) 需要配置中新增recipe字段，如recipe: default.v1。<br />
(4) 训练文件中的version:"2.0"改成version:"3.0"。<br />
(5) 3.0彻底移除了2.0准备要删除的api。<br />
(6) 2.0的slot
mapping是在表单中定义的，现在每个slot的mapping都在slot的配置中指定，原来的slot和实体之间默认是可以进行自动填充的，现在必须显示指定。<br />
(7)
原来是线性的pipeline，现在用的是有向无环图(DAG)。训练和推理的逻辑都发生了变化。</p>
<h2 id="参考文献">参考文献</h2>
<p><span class="exturl" data-url="aHR0cHM6Ly93d3cuc29odS5jb20vYS8xNjMyNzg1ODhfNTAwNjU5">KBQA从入门到放弃—入门篇<i class="fa fa-external-link-alt"></i></span><br />
<span class="exturl" data-url="aHR0cHM6Ly96aHVhbmxhbi56aGlodS5jb20vcC8yMzc0NTI5MTg/dXRtX3NvdXJjZT13ZWNoYXRfc2Vzc2lvbg==">实体关系、实体属性、三元组、SPO三元组及其抽取方案<i class="fa fa-external-link-alt"></i></span><br />
<span class="exturl" data-url="aHR0cHM6Ly93d3cuYmlsaWJpbGkuY29tL3ZpZGVvL0JWMW9yNHkxVTdYUj9zcG1faWRfZnJvbT0zMzMuMzM3LnNlYXJjaC1jYXJkLmFsbC5jbGljayZ2ZF9zb3VyY2U9MjgyZGM3NmY0M2Q0NjFjY2Q3YjMzY2YwZjVlYTY0YjU=">【社区说】一起来聊聊
Rasa 3.0<i class="fa fa-external-link-alt"></i></span><br />
<span class="exturl" data-url="aHR0cHM6Ly9yYXNhLmNvbS9kb2NzL3Jhc2Ev">Rasa NLU &amp; Core<i class="fa fa-external-link-alt"></i></span><br />
<span class="exturl" data-url="aHR0cHM6Ly9yYXNhLmNvbS9kb2NzL2FjdGlvbi1zZXJ2ZXIv">Rasa Action
Server<i class="fa fa-external-link-alt"></i></span><br />
<span class="exturl" data-url="aHR0cHM6Ly9yYXNhLmNvbS9kb2NzL3Jhc2EteC8=">Rasa X<i class="fa fa-external-link-alt"></i></span><br />
<span class="exturl" data-url="aHR0cHM6Ly9ibG9nLnJhc2EuY29tLw==">Rasa Blog<i class="fa fa-external-link-alt"></i></span><br />
<span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL1Jhc2FIUS9yYXNh">Rasa code<i class="fa fa-external-link-alt"></i></span><br />
<span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL2hvd2wtYW5kZXJzb24=">Rasa 开发者<i class="fa fa-external-link-alt"></i></span></p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://soundmemories.github.io/2021/07/20/NLP/05.Bert/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avstar.png">
      <meta itemprop="name" content="SoundMemories">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="SoundMemories">
      <meta itemprop="description" content="今日事，今日毕">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | SoundMemories">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2021/07/20/NLP/05.Bert/" class="post-title-link" itemprop="url">Bert</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2021-07-20 00:00:00" itemprop="dateCreated datePublished" datetime="2021-07-20T00:00:00+08:00">2021-07-20</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/NLP/" itemprop="url" rel="index"><span itemprop="name">NLP</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>8.8k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>32 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="介绍">介绍</h1>
<p>word2vec和GloVe都将相同的预训练向量分配给同一个词，而不考虑词的上下文（如果有的话）。考虑到自然语言中丰富的多义现象和复杂的语义，上下文无关表示具有明显的局限性。这推动了“上下文敏感”词表示的发展，其中词的表征取决于它们的上下文。例如，通过将整个序列作为输入，ELMo是为输入序列中的每个单词分配一个表示的函数。具体来说，ELMo将来自预训练的双向长短期记忆网络的所有中间层表示组合为输出表示。然后，ELMo的表示将作为附加特征添加到下游任务的现有监督模型中，例如通过将ELMo的表示和现有模型中词元的原始表示（例如GloVe）连结起来。一方面，在加入ELMo表示后，冻结了预训练的双向LSTM模型中的所有权值。另一方面，现有的监督模型是专门为给定的任务定制的。利用当时不同任务的不同最佳模型，添加ELMo改进了六种自然语言处理任务的技术水平：情感分析、自然语言推理、语义角色标注、共指消解、命名实体识别和问答。</p>
<p>尽管ELMo显著改进了各种自然语言处理任务的解决方案，但每个解决方案仍然依赖于一个特定于任务的结构。然而，为每一个自然语言处理任务设计一个特定的结构实际上并不是一件容易的事。GPT（Generative
Pre
Training）模型为上下文的敏感表示设计了通用的任务无关模型。GPT建立在Transformer解码器的基础上，预训练了一个用于表示文本序列的语言模型。当将GPT应用于下游任务时，语言模型的输出将被送到一个附加的线性输出层，以预测任务的标签。与ELMo冻结预训练模型的参数不同，GPT在下游任务的监督学习过程中对预训练Transformer解码器中的所有参数进行微调。GPT在自然语言推理、问答、句子相似性和分类等12项任务上进行了评估，并在对模型结构进行最小更改的情况下改善了其中9项任务的最新水平。</p>
<p>然而，由于语言模型的自回归特性，GPT只能向前看（从左到右）。在“i went
to the bank to deposit cash”（我去银行存现金）和“i went to the bank to
sit
down”（我去河岸边坐下）的上下文中，由于“bank”对其左边的上下文敏感，GPT将返回“bank”的相同表示，尽管它有不同的含义。</p>
<p>BERT：把两个最好的结合起来。ELMo对上下文进行双向编码，但使用特定于任务的结构；而GPT是任务无关的，但是从左到右编码上下文。BERT（来自Transformers的双向编码器表示）结合了这两个方面的优点。它对上下文进行双向编码，并且对于大多数的自然语言处理任务，只需要最少的结构改变。通过使用预训练的Transformer编码器，BERT能够基于其双向上下文表示任何词元。在下游任务的监督学习过程中，BERT在两个方面与GPT相似。首先，BERT表示将被输入到一个添加的输出层中，根据任务的性质对模型结构进行最小的更改，例如预测每个词元与预测整个序列。其次，对预训练Transformer编码器的所有参数进行微调，而额外的输出层将从头开始训练。</p>
<h1 id="预训练">预训练</h1>
<p>1、将预训练模型用于下游任务有两种策略：<br />
- 基于微调的策略。如
GPT，通过简单微调预训练模型的参数来训练下游任务。该策略在预训练期间通过单向语言模型来学习通用语言representation，而单向语言模型严重限制了预训练模型的表达能力。例如，在token
级别的任务（如：词性标注任务），结合两个方向的上下文对模型性能非常重要。<br />
- 基于特征的策略。如 ELMo ，将预训练模型的representation
作为下游任务模型的额外特征。该策略虽然是双向语言模型，但是该模型是浅层的。</p>
<p>与它们不同，BERT是一个<strong>同时利用了左右双向上下文的、深度的</strong>预训练模型，它在11项
nlp 任务中取得最领先的结果。</p>
<p>2、基于特征的方法具有一定优势：<br />
- 并不是所有的 NLP 任务都可以很容易地用 Transformer encoder
架构来表示，因此需要添加特定于任务的模型架构。<br />
-
如果通过训练数据预先计算一次昂贵的数据表示，然后在该表示的基础上用廉价的模型来完成许多任务，这会带来很大的计算优势。</p>
<p>3、单向语言模型可以是从左到右 Left to Right:LTR 或者从右到左 Right to
Left :RTL 。BERT 也可以像 ELMO 一样训练独立的 LTR 和 RTL
模型后拼接在一起，但是这么做有两个问题：<br />
- 其训练代价是单个双向模型的两倍。<br />
- 对于Question - Answer 之类的问题是反直觉的，因为 RTL
模型需要根据答案来反推问题。<br />
- BERT 可以自由的组合左侧上下文和右侧上下文。</p>
<p>4、BERT
预训练模型包含两个预训练任务：预测被屏蔽的单词、预测下一个句子是否和上一个句子相接。</p>
<p>5、BERT 的预训练语料库必须使用
document-level的语料库，而不是经过混洗的 sentence-level
的语料库。因为混洗句子会破坏句子预测预训练任务。这里的 “句子”
不一定是真实的句子，而是一段话或者几段话，代表了一个 token 序列。<br />
- BERT 预训练时，每个 ”句子“ 的 token 长度小于等于 512 。<br />
- BERT 的训练语料库经过了 WordPiece 词干化。如：engineer-&gt;engine
er</p>
<p>6、BERT 预训练采用 gelu 激活函数，训练一百万步，bath size = 256
。</p>
<h2 id="mlm">MLM</h2>
<p>1、受完形填空任务启发，BERT
通过提出一个新的预训练目标来解决前面提到的单向限制：掩码语言模型masked
language model:MLM 。</p>
<p>2、从直觉上，深度双向模型要比深度单向模型、单层双向模型表达能力更强。<br />
-
标准的条件语言模型只能从左到右或者从右到左训练，因为双向条件作用允许每个单词在多层上下文中间接“看到自己”。<br />
- MLM
模型从输入中随机屏蔽一些token，目标是基于上下文预测被屏蔽单词。方法是：将被屏蔽的单词替换为
[MASK] 标记，然后被屏蔽的单词作为真实 label 。<br />
- 与单向语言模型不同，MLM 结合了左右两侧上下文。</p>
<p>3、为了训练 MLM，模型随机屏蔽一定百分比（论文中为 15%）的
token，然后仅预测那些被屏蔽的 token 。这种方式有两个缺陷：<br />
- 预训练和微调之间的不匹配。因为在微调阶段，模型永远不会看到 [MASK]
标记。<br />
- 为了缓解这种状况，MLM 在预训练时并不总是用真的 [MASK]
标记，而是从输入种随机选择 15% 的 token：80% 替换未 [MASK] 标记，10%
替换为一个随机单词，10% 保持原样。<br />
- MLM
并不知道哪些词被替换，因此它总是努力的学习每个单词的正确表达。<br />
- 每个 batch 预测的 token 只有
15%，这意味着模型需要更多的训练步才可能收敛。实验证明MLM
的收敛速度确实比单向语言模型稍慢，但是相对于 MLM
的泛化能力的提升，这种代价是值得的。</p>
<h2 id="nsp">NSP</h2>
<p>许多重要的下游任务，如：知识问答和自然语言推理，都是基于理解两个句子之间的关系，而这种关系不是由语言模型直接捕获的。<br />
为了训练理解句子关系的模型，BERT 训练一个二元化的句子预测任务，称作Next
Sentence Prediction:NSP：<br />
- 每个训练样本由一对句子 A 和 B 组成：50% 的样本中 B 是紧跟在 A
之后的句子，50%的样本中二者是随机挑选的。<br />
- 模型需要预测的是： B 是否是 A 的下一个句子？</p>
<h1 id="结构">结构</h1>
<p>1、参考Transformer的encoder部分，Bert是一个多层双向Transformer编码器。双向
self-attention 的 Transformer 也称作 Transformer encoder，而单向
self-attention 的 Transformer 被称作 Transformer decoder 。</p>
<p>2、作者指明L表示层数，H表示每个隐藏单元的维数大小，A表示self-attention头数。BERT有2种大小的模型，分别是BERT_base(L=12,
H=768, A=12, Total Parameters=110M)、BERT_large(L=24, H=1024, A=16,
Total Parameters=340M)。BERT_base设定为和OpenAI
GPT的模型大小相同，以便作比较。需要重点说明的是，BERT
Transformer使用双向self-attention，而GPT Transformer
使用带约束的self-attention，每个token只能注意到它左边的上下文。</p>
<p>3、BERT 的模型输入能够表达单个句子或者一对句子。<br />
- 每个 token 的输入 representation 由三个部分相加而成：token
embedding、segment embedding、position embedding 。<br />
- 每个序列的第一个 token
是一个特殊的[CLS]。网络最后一层对应于该位置的一个隐向量作为整个序列的
representation 来用于分类任务。对于非分类任务，该隐向量被忽略。<br />
- 如果是一对句子，则在句子之间插入特殊的 token：[SEP]
。然后对句子的前后关系学习一个segment
embedding，通过这种方式，模型得以学习和区分两个句子的前后关系：<br />
- 前一个句子的每个 token 学习和使用 A embedding，代表前后关系的 “前关系”
的表达。<br />
- 后一个句子的每个 token 学习和使用 B embedding，代表前后关系的 “后关系”
的表达。<br />
- 对于单个句子，模型仅学习和使用 A embedding 。<br />
- position embedding 是模型学习和使用的 input
每个绝对位置的表达。这里不是正弦+余弦方式，而是参数化方式。<br />
- token embedding 是模型学习和使用的每个 token 的表达。</p>
<p><img src="/images/Bert/1.png" width="90%"></p>
<h1 id="应用">应用</h1>
<p>从input(2种)和output(4种)看：<br />
- input：<br />
- one sentence：输入为1个句子。<br />
- multiple sentences：输入为多个句子，用[SEP]分隔。<br />
- output：<br />
- one
class：[CLS]预训练后的向量输入全连接层，直接判断分类结果（当然，你也可以输入sentence全部向量）。<br />
- class for each
token：输入的每个token经过预训练后的向量输入全连接层，对每个token分类。<br />
- copy from
input：经典Q-A问题，这里设计时使用限制的QA(Answer必须在document中)，输入2个向量（代表Answer在document的起始-结束位置），和预训练（输入Question+document）后的document
token向量点积+softmax，取最大值token位置即可。<br />
- general
sequence：作为seq2seq的encoder、作为encoder+decoder（decoder应使用单向Transformer）。</p>
<h1 id="如何fine-tune">如何fine-tune？</h1>
<ol type="1">
<li>固定Bert参数，只训练全连接层。</li>
<li>Bert+全连接层 全部一起训练。</li>
<li>Adaptor：对Bert中添加一部分Layer，训练时只调整这些Layer参数。</li>
</ol>
<p><strong>warmup</strong>：学习率热身。<strong>规定前多少个热身步骤内，对学习率采取逐步递增的过程</strong>。热身步骤之后，会对学习率采用衰减策略。这样训练初期可以避免震荡，后期可以让loss降得更小。<strong>warmup抑制Layer
Normalization对学习率参数的敏感度。</strong></p>
<p>除了 batch size、学习率、训练 epoch
不同之外，<strong>其它训练参数与预训练阶段的训练参数相同</strong>。fine-tune阶段通常很快，因此建议<strong>对超参数进行彻底搜索</strong>并选择在验证集上表现最好的模型。<br />
论文发现：数据集越小，模型性能对超参数的选择越敏感。大数据集（超过10万的标记样本）对超参数的敏感性要低的多。</p>
<p>对于文本分类，详情参考<span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzE5MDUuMDU1ODMucGRm">How to Fine-Tune BERT for
Text Classification?<i class="fa fa-external-link-alt"></i></span>。<br />
BERT在NLP任务中效果十分优秀，这篇文章对于BERT在文本分类的应用上做了非常丰富的实验，介绍了一些调参以及改进的经验，进一步挖掘BERT的潜力。</p>
<ul>
<li>微调（fine-tune）策略
<ul>
<li>对于长文本，尝试了（1）取头部510 tokens（2）尾部510
tokens（3）头部128 tokens + 尾部382
tokens（4）分片并进行最大池化、平均池化、attention。发现方法（3）最好。因为文章的关键信息一般在开头和结尾。</li>
<li>分层训练，上层对fine-tune更加重要，这部分可随机初始化。</li>
<li>灾难性遗忘：在下游finetune可能会遗忘预训练的知识。需要设置较小的学习率，如2e-5.</li>
<li>分层衰减学习率（Layer-wise Decreasing Layer
Rate），对下层设置更小的学习率可以得到更高的准确率，在lr=2e-5，衰减率<span
class="math inline">\(\xi\)</span>=0.95</li>
</ul></li>
<li>继续预训练（Further Pretraining）
<ul>
<li>任务内（within-task）
和同领域（in-domain）的继续预训练可以大大提高准确率，In-domain比within-task要好。</li>
</ul></li>
<li>多任务微调（Multi-task Finetuning）
<ul>
<li>在单任务微调之前的多任务微调有帮助，但是提升效果小于Further
pretraining。</li>
</ul></li>
<li>小数据集
<ul>
<li>BERT对小数据集提升很大，这个大家都知道的。Further
pretraining对小数据集也有帮助。</li>
</ul></li>
</ul>
<p><span class="exturl" data-url="aHR0cHM6Ly96aHVhbmxhbi56aGlodS5jb20vcC81MjQwMzYwODc=">Bert在fine-tune时训练的5种技巧<i class="fa fa-external-link-alt"></i></span><br />
https://github.com/shuxinyin/Chinese-Text-Classification<br />
<span class="exturl" data-url="aHR0cHM6Ly96aHVhbmxhbi56aGlodS5jb20vcC84Njk2NTU5NQ==">深入理解NLP
Subword算法：BPE、WordPiece、ULM<i class="fa fa-external-link-alt"></i></span></p>
<h1 id="bert变种">Bert变种</h1>
<h2 id="bert-wwm">BERT-WWM</h2>
<p><strong>BERT-WWM</strong>：始版本的 BERT 采用了WordPiece tokenize
来预处理，即把每个单词拆解一些 WordPiece token（比如，loved-&gt;lov ed）
，最初是为了解决谷歌语音识别系统遇到的日语/韩语分割问题。该模型是数据驱动，并且确保任何可能的字符序列的确定性分割。这种预处理为
BERT 带来一个严重的问题：<strong>有可能仅仅对一个单词的某个部分
wordpiece token 执行了 mask。此时 MLM
模型预测的只是单词的一部分，相当于模型对一个错误的目标进行预测。这非常不利于模型的学习</strong>。有鉴于此，谷歌后续发布了
BERT 的 Whole Word Masking:WWM 版本
<strong>BERT-WWM</strong>。在该版本中，一个单词要么没有被
mask、要么该单词所有的 workpiece token 都被 mask
。类似的想法还有ERNIE模型的phrase-level mask和entity-level mask。</p>
<p><strong>BERT-wwm-ext</strong>：是 BERT-WWM
的中文版本(哈工大讯飞联合实验室发布)，该模型对中文的<strong>整个单词</strong>而不是单个字进行mask
，在最新的中文维基百科上训练。</p>
<h2 id="ernie">ERNIE</h2>
<p><strong>ERNIE</strong>：BERT 的 MLM 任务在执行 mask
的时候，未能考虑先验知识，如果有学习到与Mask的单词相关的先验知识，则无需借助很长的上下文就可以很容易的预测出Mask的单词。但是
ERNIE 并没有直接添加先验知识，而是通过引入 <strong>entity-level
mask</strong> 和 <strong>phrase-level mask</strong>
来引入先验知识，隐式的学习实体间的关系、实体的属性等知识：<br />
- <strong>phrase-level mask</strong>：将一个 phrase
作为一个单元，每个单元通常由几个 token
组成。在训练期间，同一个单元中的所有 token 都会被屏蔽，而不是只有一个
token 被屏蔽。<br />
- <strong>entity-level mask</strong>：将一个 entity
作为一个单元，...。</p>
<p>对于英语，论文使用单词分析和分块工具来获取短语的边界；对于中文，论文使用<strong>分词工具</strong>来获取词的边界。<br />
命名实体 entity
包括人名、地名、组织名、产品名等。需要使用NER的方法。</p>
<p><strong>ERNIE</strong>
通过三阶段学习来学得短语或实体的先验知识：<br />
- 第一阶段：Basic-level masking，使用基本的掩码策略，做法与 BERT
完全相同。这个阶段是在基本语言单元的随机 mask
上训练，因此很难对高级语义知识建模。对于英文，基本语言单元是词word；对于中文，基本语言单元是汉字
char 。<br />
- 第二阶段：Phrase-level masking，使用基本语言单元作为训练输入，但是使用
phrase-level
的掩码策略。这个阶段模型屏蔽和预测同一个短语的所有基本语言单元。<br />
- 第三阶段：Entity-level masking，使用基本语言单元作为训练输入，但是使用
entity-level
的掩码策略。这个阶段模型屏蔽和预测同一个命名实体的所有基本语言单元
。</p>
<p>ERNIE 编码器和 BERT 相同，但是对于中文语料，ERNIE 把 CJK
编码范围内的字符都添加空格来分隔，然后使用 WordPiece（loved-&gt;lov ed）
来对中文语句词干化，WordPiece参考<span class="exturl" data-url="aHR0cHM6Ly93d3cuY25ibG9ncy5jb20vaHVhbmd5Yy9wLzEwMjIzMDc1Lmh0bWw=">一文读懂BERT中的WordPiece<i class="fa fa-external-link-alt"></i></span>。</p>
<h2 id="spanbert">SpanBert</h2>
<p><strong>SpanBert</strong>：一个新的分词级别的预训练方法，提出基于Bert掩码方式，采用不同长度token不同概率的方式（越短的token概率越大）Mask。
其在现有任务中的表现优于 BERT
，并在问答、指代消解等分词选择任务中取得了较大的进展。<br />
论文原文：<span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzE5MDcuMTA1MjkucGRm">SpanBERT:
Improving Pre-training by Representing and Predicting Spans<i class="fa fa-external-link-alt"></i></span></p>
<p>主要优化点：<br />
- Span Masking。<br />
- MLM + SBO。<br />
- Single-Sequence Training。</p>
<p>模型原理如图所示：<br />
<img src="/images/Bert/10.png" width="90%"></p>
<p><strong>Span Masking</strong><br />
提出了更好的 Span Mask 方案，SpanBERT 不再对随机的单个 token 添加
mask，而是对随机对邻接分词添加mask。根据几何分布，先随机选择一段（span）的长度，之后再根据均匀分布随机选择这一段的起始位置，最后按照长度mask。作者设定几何分布取
p=0.2，并裁剪最大长度只能是 10（不应当是长度 10
以上修剪，而应当为丢弃），利用此方案获得平均采样长度分布。因此分词的平均长度为
3.8
。作者还测量了词语（word）中的分词程度，使得添加mask的分词更长。如图，展示了分词mask长度的分布情况。<br />
<img src="/images/Bert/11.png" width="40%"><br />
和在 BERT 中一样，作者将 Y 的规模设定为 X 的15%，其中 80% 使用 [MASK]
进行替换，10%
使用随机单词替换，10%保持不变。与之不同的是，作者是在分词级别进行的这一替换，而非将每个单词单独替换。</p>
<p>不同mask方案对比：<br />
<img src="/images/Bert/17.png" width="80%"></p>
<p><strong>SBO</strong><br />
Span Boundary
Objective。分词选择模型一般使用其边界词创建一个固定长度的分词表示。为了于该模型相适应，作者希望结尾分词的表示的总和与中间分词的内容尽量相同。为此，作者引入了
SBO ，其仅使用观测到的边界词来预测带msak的分词的内容。</p>
<p>具体做法是，在训练时取 Span 前后边界的两个词，值得指出，这两个词不在
Span 内，然后用这<strong>两个词向量</strong>加上 Span
中被mask掉词的<strong>位置向量</strong>，来预测当前被mask的词。详细做法是将<strong>词向量</strong>和<strong>位置向量</strong>拼接起来，作者使用一个两层的前馈神经网络作为表示函数，该网络使用
GeLu 激活函数，并使用层正则化。和 MLM 一样使用交叉熵作为损失函数，就是
SBO 目标的损失，之后将这个损失和 BERT 的 MLM
的损失加起来，一起用于训练模型。<br />
<img src="/images/Bert/14.png" width="20%"><br />
<img src="/images/Bert/12.png" width="30%"><br />
<img src="/images/Bert/13.png" width="30%"></p>
<p><strong>Single-Sequence Training</strong><br />
没有segment embedding，只有一个长的句子，类似RoBERTa。它没用 Next
Sentence Prediction (NSP) 任务，而是直接用 Single-Sequence
Training，也就是根本不加入 NSP
任务来判断是否两句是上下句，直接用一句来训练。作者推测其可能原因如下：（a）更长的语境对模型更有利，模型可以获得更长上下文（类似
XLNet 的一部分效果；（b）加入另一个文本的语境信息会给MLM
语言模型带来噪音。</p>
<p>因此，SpanBERT 就没采用 NSP
任务，仅采样一个单独的邻接片段，该片段长度最多为512个单词，其长度与 BERT
使用的两片段的最大长度总和相同，然后 MLM 加上 SBO 任务来进行预训练。</p>
<p>其中主要训练细节是：<br />
- 训练时用了 Dynamic Masking 而不是像 BERT 在预处理时做 Mask；<br />
- 取消 BERT 中随机采样短句的策略；<br />
- 对 Adam 优化器中一些参数改变。</p>
<p><img src="/images/Bert/17.png" width="80%"></p>
<p>实验结果：<br />
<img src="/images/Bert/15.png" width="40%"><br />
<img src="/images/Bert/16.png" width="80%"></p>
<h2 id="albert">Albert</h2>
<p>Albert主要对Bert参数进行优化，在任务结果下降很低的情况的下，使模型更小更适合部署。<br />
论文原文：<span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzE5MDkuMTE5NDIucGRm">ALBERT: A LITE
BERT FOR SELF-SUPERVISED LEARNING OF LANGUAGE REPRESENTATIONS<i class="fa fa-external-link-alt"></i></span></p>
<p>主要优化点：<br />
- Token Embedding projection block<br />
- Parameter size <span class="math inline">\(O(V \times
E)\)</span><br />
- Attention feed-forward block<br />
- Parameter size <span class="math inline">\(O(12 \times L \times H
\times H)\)</span></p>
<p><strong>优化一：Embedding维度分解</strong><br />
Token Embedding 采用 <span class="math inline">\(O(V\times E + E \times
H)\)</span> 且 <span class="math inline">\(E \ll
H\)</span>。由于模型结构的限制，WordePiece embedding的大小 <span
class="math inline">\(E\)</span> 总是与隐层大小 <span
class="math inline">\(H\)</span>
相同。从建模的角度考虑，词嵌入学习的是单词与上下文无关的表示，而隐层则是学习与上下文相关的表示。显然后者更加复杂，需要更多的参数，也就是说模型应当增大隐层大小
<span class="math inline">\(H\)</span> ，或者说满足 <span
class="math inline">\(H \gg E\)</span> 。但实际上词汇表的大小 <span
class="math inline">\(V\)</span> 通常非常大，如果 <span
class="math inline">\(E=H\)</span> 的话，增加隐层大小 <span
class="math inline">\(H\)</span> 后将会使embedding matrix的维度 <span
class="math inline">\(V \times E\)</span> 非常巨大。</p>
<p>因此Albert想要打破 <span class="math inline">\(E\)</span> 与 <span
class="math inline">\(H\)</span>
之间的绑定关系，从而减小模型的参数量，同时提升模型表现。具体做法是将embedding
matrix分解为两个大小分别为 <span class="math inline">\(V \times
E\)</span> 和 <span class="math inline">\(E \times H\)</span>
矩阵，也就是说先将单词投影到一个低维的embedding空间 <span
class="math inline">\(E\)</span> ，再将其投影到高维的隐藏空间 <span
class="math inline">\(H\)</span> 。这使得embedding matrix的维度从 <span
class="math inline">\(O(V \times E)\)</span> 减小到 <span
class="math inline">\(O(V\times E + E \times H)\)</span> 。当 <span
class="math inline">\(E \ll H\)</span>
时，参数量减少非常明显。在实现时，随机初始化 <span
class="math inline">\(V \times E\)</span> 和 <span
class="math inline">\(E \times H\)</span>
两个矩阵，计算某个单词的表示需用一个单词的one-hot向量乘以 <span
class="math inline">\(V \times E\)</span>
维的矩阵（也就是lookup），再用得到的结果乘 <span class="math inline">\(E
\times H\)</span> 维的矩阵即可。两个矩阵的参数通过模型学习。</p>
<p>从下图实验结果可见，对于不共享参数的情况， <span
class="math inline">\(E\)</span> 几乎是越大越好；而共享参数之后， <span
class="math inline">\(E\)</span> 太大反而会使模型表现变差， <span
class="math inline">\(E=128\)</span>
模型表现最好，因此ALBERT的默认参数设置为此。<br />
<img src="/images/Bert/2.png" width="90%"></p>
<p><strong>优化二：层权重共享</strong><br />
另一个减少参数量的方法就是层之间的参数共享，即多个层使用相同的参数。因为attention-feedforward操作是重复的，都是自注意力机制，所以考虑使用相同的权重来减少参数量，使得block参数变为<span
class="math inline">\(O(12 \times H \times H)\)</span>。</p>
<p>参数共享有三种方式：<br />
- 只共享feed-forward network的参数。<br />
- 只共享attention的参数。<br />
- 共享全部参数。</p>
<p>ALBERT默认是共享全部参数的，在后续实验结果中我们可以看到几种方式的模型表现。</p>
<p>如下图所示，实验表明加入参数共享之后，每一层的输入embedding和输出embedding的L2距离和余弦相似度都比BERT稳定了很多，ALBERT
的结果更加平滑。这证明参数共享能够使模型参数更加稳定。<br />
<img src="/images/Bert/3.png" width="90%"></p>
<p>如下图所示，可以看出参数共享几乎也是对模型结果有负面影响的。但是考虑到其能够大幅削减参数量，并且对结果影响不是特别大，因此权衡之下选择了参数共享。<br />
<img src="/images/Bert/4.png" width="90%"></p>
<p><strong>优化三：SOP替代NSP</strong><br />
除了减少模型参数外，本外还对BERT的预训练任务<strong>Next-sentence
prediction</strong>(NSP)进行了改进。在BERT中，NSP任务的正例是文章中连续的两个句子，而负例则是从两篇文档中各选一个句子构造而成。在先前的研究中，已经证明NSP是并不是一个合适的预训练任务。本文推测其原因是模型在判断两个句子的关系时不仅考虑了两个句子之间的连贯性（coherence），还会考虑到两个句子的话题（topic）。而两篇文档的话题通常不同，模型会更多的通过话题去分析两个句子的关系，而不是句子间的连贯性，这使得NSP任务变成了一个相对简单的任务。</p>
<p>因此本文提出了<strong>Sentence-order
prediction</strong>(SOP)来取代NSP。具体来说，其正例与NSP相同，但负例是通过选择一篇文档中的两个连续的句子并将它们的顺序交换构造的。这样两个句子就会有相同的话题，模型学习到的就更多是句子间的连贯性。</p>
<p>如下图实验结果所示，如果不使用NSP或SOP作为预训练任务的话，模型在NSP和SOP两个任务上表现都很差；如果使用NSP作为预训练任务的话，模型确实能很好的解决NSP问题，但是在SOP问题上表现却很差，几乎相当于随机猜，因此说明NSP任务确实很难学到句子间的连贯性；而如果用SOP作为预训练任务，则模型也可以较好的解决NSP问题，同时模型在下游任务上表现也更好。说明SOP确实是更好的预训练任务。<br />
<img src="/images/Bert/5.png" width="90%"></p>
<h2 id="roberta">RoBerta</h2>
<p>论文原文：<span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzE5MDcuMTE2OTIucGRm">RoBERTa: A
Robustly Optimized BERT Pretraining Approach<i class="fa fa-external-link-alt"></i></span></p>
<p>预训练模型能够显著的提升任务效果，但是不同预训练模型的比较非常困难。首先，每个预训练模型的训练成本很高，无法一一训练并进行比较。其次，不同预训练模型通常是在不同规模大小的数据集上训练的，难以评估效果的好坏是预训练模型引起的还是预训练数据引起的。最后，超参数的选择对预训练模型的表现影响很大。同一个预训练模型的不同超参数，其比较结果会有很大不同。</p>
<p>RoBERTa 主要工作是复现 BERT，然后对 BERT
的模型架构、训练目标、训练细节（如数据集大小、训练时间）的重要性进行探索，从而提出了改进方案，这个改进方案称为
RoBERTa 。主要修改：<br />
- 更大的 batch size。<br />
- 更多的数据、更长的预训练时间。<br />
- 移除 NSP 任务 + 使用更长的输入序列。<br />
- <strong>使用动态mask</strong><br />
- 使用BPE方式，减少UNK单词出现次数（英文）。</p>
<p><strong>更大的 batch size</strong><br />
原本的BERTbase 的batch size是256，训练1M个steps。RoBERTa的batch
size为8k。为什么要用更大的batch
size呢？（除了因为他们有钱玩得起外）作者借鉴了在机器翻译中，用更大的batch
size配合更大学习率能提升模型优化速率和模型性能的现象，并且也用实验证明了确实Bert还能用更大的batch
size。<br />
<img src="/images/Bert/6.png" width="40%"></p>
<p><strong>更多的数据、更长的预训练时间</strong><br />
借鉴XLNet用了比Bert多10倍的数据，RoBERTa也用了更多的数据。性能确实再次彪升。当然，也需要配合更长时间的训练。<br />
<img src="/images/Bert/8.png" width="80%"></p>
<p><strong>移除 NSP 任务 + 使用更长的输入序列</strong><br />
原本的Bert为了捕捉句子之间的关系，使用了NSP任务进行预训练，就是输入一对句子A和B，判断这两个句子是否是连续的。在训练的数据中，50%的B是A的下一个句子，50%的B是随机抽取的。</p>
<p>而RoBERTa去除了NSP，而是每次输入连续的多个句子，直到最大长度512（可以跨文章）。这种训练方式叫做（FULL
-
SENTENCES），而原来的Bert每次只输入两个句子。实验表明在MNLI这种推断句子关系的任务上RoBERTa也能有更好性能。<br />
<img src="/images/Bert/7.png" width="80%"></p>
<p><strong>使用动态mask</strong><br />
原来Bert对每一个序列随机选择15%的Tokens替换成[MASK]，为了消除与下游任务的不匹配，还对这15%的Tokens进行（1）80%的时间替换成[MASK]；（2）10%的时间不变；（3）10%的时间替换成其他词。但整个训练过程，这15%的Tokens一旦被选择就不再改变，也就是说从一开始随机选择了这15%的Tokens，之后的N个epoch里都不再改变了。这就叫做静态Masking。</p>
<p>而RoBERTa一开始把预训练的数据复制10份，每一份都随机选择15%的Tokens进行Masking，也就是说，同样的一句话有10种不同的mask方式。然后每份数据都训练N/10个epoch。这就相当于在这N个epoch的训练中，每个序列的被mask的tokens是会变化的。这就叫做动态Masking。</p>
<p>那么这样改变是否真的有效果？作者在只将静态Masking改成动态Masking，其他参数不变的情况下做了实验，动态Masking确实能提高性能。<br />
<img src="/images/Bert/9.png" width="40%"></p>
<p><strong>使用BPE方式</strong><br />
针对的是英文，BERT原型使用的是 character-level BPE vocabulary of size
30K, RoBERTa使用了GPT2的 byte BPE 实现，使用的是byte而不是unicode
characters作为subword的单位。</p>
<p>这些针对Bert的预训练优化都使用起来，最终在GLUE, RACE,
SQuAD上都达到了SOTA的性能。</p>
<p>RoBERTa 采用 160 G 训练文本，远超 BERT 的 16G 文本，其中包括：<br />
- BOOKCORPUS 和英文维基百科：原始 BERT 的训练集，大小 16GB 。<br />
- CC-NEWS：包含2016年9月到2019年2月爬取的6300万篇英文新闻，大小 76
GB（经过过滤之后）。<br />
- OPENWEBTEXT：从 Reddit 上共享的 URL
（至少3个点赞）中提取的网页内容，大小 38 GB 。<br />
- STORIES：CommonCrawl 数据集的一个子集，包含 Winograd
模式的故事风格，大小 31GB 。</p>
<h2 id="sbert">SBERT</h2>
<p>虽然BERT和RoBERTa在很多句子对形式的回归任务（例如文本语义相似度）上达到了SOTA效果，但是它们还存在一些缺点：在这些任务中，它们均需要将比较的两个句子都传入到模型中计算，计算开销过大。BERT模型在一个1W句子集合中，找出最相近的一个句子对，需要5千万次推断计算（约65小时）才能完成，所以BERT并不适合语义相似度搜索等任务。</p>
<p>在该论文中，作者提出了一个新的模型，Sentence-BERT（简称SBERT）。SBERT采用双重或三重BERT网络结构，具体结构介绍会在后文中详细介绍。如果使用的是基于RoBERTa模型，则改造后的模型简称为SRoBERTa。</p>
<p>通过SBERT模型获取到的句子embedding，可以直接通过cos相似度计算两个句子的相似度，这样就大大减少了计算量。因为在使用BERT模型进行句子间相似度的判断时，需要从句子集合中，选出两个句子进行组合，传入BERT中进行计算，而使用SBERT模型，只需要将集合中每个句子单独传入到模型中，得到每个句子的embeding，计算相似度只需要使用cos函数计算两两embeding的cos距离即可。因此，使用BERT/RoBERTa模型需要65h才能完成的寻找最相似句子对任务，SBERT模型完成仅需5s。</p>
<p>作者在一些STS任务和迁移学习任务上评估SBERT模型，该模型达到了新的SOTA水平。</p>
<p>论文原文：<span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzE5MDguMTAwODQucGRm">Sentence-BERT: Sentence
Embeddings using Siamese BERT-Networks<i class="fa fa-external-link-alt"></i></span>。<br />
模型地址：<span class="exturl" data-url="aHR0cHM6Ly93d3cuc2JlcnQubmV0Lw==">Sentence-Transformers<i class="fa fa-external-link-alt"></i></span>。<br />
详解参考：<span class="exturl" data-url="aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2Zlbmd4aW5saW51eC9hcnRpY2xlL2RldGFpbHMvMTA5MTk1NzYy">论文阅读Sentence-BERT:
Sentence Embeddings using Siamese BERT-Networks<i class="fa fa-external-link-alt"></i></span>。</p>
<h2 id="xlnet">XLNet</h2>
<p><strong>XLNet</strong>：使用Transformer-XL模型。</p>
<p>当Bert作为seq2seq的encoder时，输入也要Mask：<br />
- 随机Mask。<br />
- 删除Delete。<br />
- 多个句子，换顺序Permuttion。<br />
- 打乱token，比如后面词放在前面。<br />
- 插入Mask，Text Infilling。这个效果最好。</p>
<h2 id="gpt">GPT</h2>
<p>只使用Transformer-Decoder端进行序列生成(预测下一个词)。<br />
训练使用Few-shot
Learning：先给出问题描述和部分示例学习，再给出prompt(引子/问题)，能给出解答。<br />
训练时只给出一个示例(one-shot)或不给示例(zero-shot)进行学习。</p>
<p>GPT-2的学习目标是使用无监督的预训练模型做有监督的任务。作者认为，当一个语言模型的容量足够大时，它就足以覆盖所有的有监督任务，也就是说所有的有监督学习都是无监督语言模型的一个子集。GPT-2去掉了fine-tuning层。</p>
<p>GPT-3沿用了GPT-2的结构，但是在网络容量上做了很大的提升。</p>
<p><span class="exturl" data-url="aHR0cHM6Ly96aHVhbmxhbi56aGlodS5jb20vcC8zNTAwMTc0NDM=">词向量之GPT-1，GPT-2和GPT-3<i class="fa fa-external-link-alt"></i></span><br />
<span class="exturl" data-url="aHR0cHM6Ly96aHVhbmxhbi56aGlodS5jb20vcC8yMDA5Nzg1Mzg=">GPT-3阅读笔记：Language
Models are Few-Shot Learners<i class="fa fa-external-link-alt"></i></span></p>
<h2 id="mass">MASS</h2>
<p>MASS针对的是seq2seq任务。<br />
MASS在encoder端对句子随机mask一个长度为k的连续片段，然后通过decoder预测生被mask片段。<br />
mask方式：随机删除、打乱词序、旋转词序、插入mask等方式。</p>
<h2 id="unilm">UniLM</h2>
<p>UniLM
1.0通过设计不同掩码，支持4种不同的训练目标：从左往右单向LM，从右往左单向LM，双向LM，序列到序列LM。<br />
UniLM 2.0支持更多样的factorization
order且无需重复构建训练实体。训练时部分自回归使用pseudo mask LM
(PMLM)，直译为伪掩码，作为部分自回归训练时占位符，和自编码的<code>[M]</code>任务作为区分。</p>
<p><span class="exturl" data-url="aHR0cHM6Ly96aHVhbmxhbi56aGlodS5jb20vcC8xMjI5OTkxNTM=">微软统一预训练语言模型UniLM
2.0解读<i class="fa fa-external-link-alt"></i></span></p>
<h1 id="参考文献">参考文献</h1>
<p><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzE4MTAuMDQ4MDU=">BERT: Pre-training of Deep
Bidirectional Transformers for Language Understanding<i class="fa fa-external-link-alt"></i></span><br />
<span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzE5MDUuMDU1ODMucGRm">How to Fine-Tune BERT for
Text Classification?<i class="fa fa-external-link-alt"></i></span><br />
<span class="exturl" data-url="aHR0cHM6Ly9jbG91ZC50ZW5jZW50LmNvbS9kZXZlbG9wZXIvYXJ0aWNsZS8xNTE5MTgw">BERT论文解读<i class="fa fa-external-link-alt"></i></span><br />
<span class="exturl" data-url="aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2RlbmRpX2h1c3QvYXJ0aWNsZS9kZXRhaWxzLzEwNDQ2NTMzNw==">【调优方法】——warmup<i class="fa fa-external-link-alt"></i></span><br />
<span class="exturl" data-url="aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2xpb24xOTkzMDkyNC9hcnRpY2xlL2RldGFpbHMvMTA0NDY5OTQ0">Bert微调技巧实验大全-How
to Fine-Tune BERT for Text Classification<i class="fa fa-external-link-alt"></i></span><br />
<span class="exturl" data-url="aHR0cHM6Ly96aHVhbmxhbi56aGlodS5jb20vcC84NzU2MjkyNg==">【论文阅读】ALBERT<i class="fa fa-external-link-alt"></i></span><br />
<span class="exturl" data-url="aHR0cHM6Ly93d3cuamlhbnNodS5jb20vcC9lZGRmMDRiYTg1NDU=">改进版的RoBERTa到底改进了什么？<i class="fa fa-external-link-alt"></i></span><br />
<span class="exturl" data-url="aHR0cHM6Ly9tcC53ZWl4aW4ucXEuY29tL3MvTUhtN0F4bWN1RWdGUl9vTmJOcUZrUQ==">BERT模型的优化改进方法！<i class="fa fa-external-link-alt"></i></span></p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://soundmemories.github.io/2021/07/19/NLP/04.Transformer/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avstar.png">
      <meta itemprop="name" content="SoundMemories">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="SoundMemories">
      <meta itemprop="description" content="今日事，今日毕">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | SoundMemories">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2021/07/19/NLP/04.Transformer/" class="post-title-link" itemprop="url">Transformer</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2021-07-19 00:00:00" itemprop="dateCreated datePublished" datetime="2021-07-19T00:00:00+08:00">2021-07-19</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/NLP/" itemprop="url" rel="index"><span itemprop="name">NLP</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>1.8k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>6 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="结构">结构</h1>
<p>论文中的 Transformer 架构包含了 encoder 和 decoder
两部分，其架构如下图所示。<br />
<img src="/images/Transformer/1.png" width="60%"></p>
<ul>
<li>编码器 encoder 包含一组 6 个相同的层 Layer(layer间串行连接)
，每层包含两个子层 SubLayer。
<ul>
<li>第一个子层是一个多头自注意力 multi-head self-attention 层。</li>
<li>第二个子层是一个简单的全连接层。</li>
<li>每个子层都使用残差直连，并且残差直连之后跟随一个layer
normalization。</li>
<li>假设子层的输入为 <span class="math inline">\(h\)</span>，则经过 LN
之后整体的输出为 <span
class="math inline">\(LayerNorm(h+Sublayer(h))\)</span> 。为了 Add
直连，论文将内部所有层的输入、输出的向量维度设置为512维。</li>
</ul></li>
<li>解码器 decoder 也包含一组 6 个相同的层 Layer，但是每层包含三个子层
SubLayer 。
<ul>
<li>第一个子层也是一个多头自注意力 multi-head self-attention
层。但是，在计算位置 <span class="math inline">\(i\)</span> 的
self-attention 时屏蔽掉了位置 <span class="math inline">\(i\)</span>
之后的序列值，这意味着：位置 <span class="math inline">\(i\)</span> 的
attention 只能依赖于它之前的结果，不能依赖它之后的结果。因此，这种
self-attention 也被称作 masked self-attention。</li>
<li>第二个子层是一个多头注意力multi-head attention 层，用于捕获 decoder
output 和 encoder output 之间的 attention 。</li>
<li>第三个子层是一个简单的全连接层。</li>
<li>和 encoder
一样：每个子层都使用残差直连，并且残差直连之后跟随一个layer
normalization:LN 。decoder 所有层的输入、输出的向量维度也是512维。</li>
</ul></li>
</ul>
<h1 id="attention">attention</h1>
<p>编码器和解码器的 attention 都是采用 scaled dot attention
(点积)。计算点积后除以 <span class="math inline">\(\sqrt{d_k}\)</span>
是为了降低 <span class="math inline">\(score\)</span>
的数值，防止它落入到 softmax 函数的饱和区间，因为 softmax
函数的饱和区梯度几乎为 0 ，容易发生梯度消失。</p>
<p>一组多个attention 的效果要优于单个 attention，这称作multi-head
attention 。将整个 attention 空间拆分成多个 attention 子空间，最后将多个
head
的输出进行拼接，并再经过一个线性映射即可得到多头attention的结果。线性映射目的是确保
multi-head attention 前后的输入输出维度一致。论文中选择 attention
都为512维，为了保证 multi-head attention 的表达空间与 single-head
attention 一致。<br />
multi-head
attention表达能力更强，相当于在整体计算代价几乎保持不变的条件下，引入了更多的非线性从而增强了模型的表达能力。。</p>
<p>在论文中，有三种方式使用多头注意力机制：<br />
- encoder-decoder attention：query 来自前一个 decoder
层的输出，keys,values 来自 encoder 的输出。其意义是： decoder
的每个位置去查询它与 encoder 的哪些位置相关，并用 encoder 的这些位置的
value 来表示。<br />
- encoder self-attention：query,key,value 都来自前一层 encoder
的输出。这允许 encoder 的每个位置关注 encoder 前一层的所有位置。<br />
- decoder masked self-attention：query,key,value 都来自前一层 decoder
的输出。这允许 decoder 的每个位置关注 decoder
前一层的、在该位置之前的所有位置。</p>
<h1 id="全连接层">全连接层</h1>
<p>encoder 和 decoder 还包含有全连接层。对 encoder/decoder 的每个
attention 输出，全连接层通过一个 ReLU 激活函数和一个线性映射。</p>
<p>对于同一个 multi-head attention
的所有输出，采用相同的参数；对于不同的 multi-head attention
的输出，采用不同的参数。</p>
<p>输入和输出的维度保持为512，但是中间向量的维度是2048，这是为了扩充中间层的表示能力，从而抵抗
ReLU 带来的表达能力的下降。</p>
<h1 id="embedding-层">embedding 层</h1>
<p>网络涉及三个 embedding 层：<br />
- encoder 输入 embedding 层：将 encoder 输入 token
转化为512维的向量。<br />
- decoder 输入 embedding 层：将 decoder 输入 token
转化为512维的向量。<br />
- decoder 输出 embedding 层：将 decoder 输出 token
转化为512维的向量。</p>
<p>在论文中这三个 embedding 矩阵是共享的，并且论文中在 embedding
层将该矩阵乘以一个常量 <span
class="math inline">\(\sqrt{d_{model}}\)</span> 来放大每个权重。</p>
<h1 id="position-embedding">position embedding</h1>
<p>从 attention 的计算公式可知：调整输入的顺序对 attention
的结果没有任何影响，attention 的输出中不包含任何顺序信息。<br />
论文通过将位置编码添加到 encoder 和 decoder 底部的输入 embedding
来解决问题。对于同一个输入序列如果打乱序列顺序，则不同 token 的
attention 权重发生改变使得 attention 的结果不同。</p>
<p>位置编码有两种选择：<br />
- <strong>functional(函数式)</strong>：论文中使用
<strong>正弦+余弦</strong> 方式设定position embedding。<br />
- 不同的维度对应不同的波长的正弦曲线，波长从 <span
class="math inline">\(2\pi\)</span> 到 <span
class="math inline">\(2000\pi\)</span> 。<br />
- 选择这样的函数是因为：不同位置之间的embedding
可以简单的相互表示。这意味着模型可以捕获到位置之间的相对位置关系。<br />
- <strong>parametric(参数式)</strong>：可以自己学习，比如bert。</p>
<h1 id="补充阅读">补充阅读</h1>
<p><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvYWJzLzIwMDIuMDQ3NDU=">On Layer Normalization in
the Transformer Architecture<i class="fa fa-external-link-alt"></i></span>：<br />
我们知道，在原始的Transformer中，Layer
Norm在跟在Residual之后的，我们把这个称为Post-LN Transformer。</p>
<p>而且用Transformer调过参的也知道，Post-LN
Transformer对参数非常敏感，需要很仔细地调参才能取得好的结果，比如必备的warm-up学习率策略，这会非常耗时间。<br />
所以现在问题来了，为什么warm-up是必须的？能不能把它去掉？<br />
本文的出发点是：既然warm-up是训练的初始阶段使用的，那肯定是训练的初始阶段优化有问题，包括模型的初始化。</p>
<p>从而，作者发现，Post-LN
Transformer在训练的初始阶段，输出层附近的期望梯度非常大，所以，如果没有warm-up，模型优化过程就会炸裂，非常不稳定。<br />
既然如此，本文作者尝试把LayerNorm换个位置，比如放在Residual的过程之中（称为Pre-LN
Transformer），再观察训练初始阶段的梯度变化，发现比Post-LN
Transformer不知道好到哪里去了，甚至不需要warm-up，从而进一步减少训练时间，这一结果的确令人震惊。</p>
<p>本文别出心裁，用实验和理论验证了Pre-LN
Transformer结构不需要使用warm-up的可能性，其根源是LN层的位置导致层次梯度范数的增长，进而导致了Post-LN
Transformer训练的不稳定性。<br />
本文第一次将warm-up、LayerNorm、gradient和initialization联系起来，非常值得一读！</p>
<p><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvYWJzLzIwMDMuMDc4NDU=">PowerNorm: Rethinking
Batch Normalization in Transformers<i class="fa fa-external-link-alt"></i></span>：<br />
本文探讨了Transformer为什么使用layer normalization，为什么不用batch
normalization，然后根据batch normalization出现的问题提出了power
normalization代替layer normalization。<span class="exturl" data-url="aHR0cHM6Ly96aHVhbmxhbi56aGlodS5jb20vcC8xMjY3NDkzMTE/ZnJvbV92b3RlcnNfcGFnZT10cnVl">BatchNorm在NLP任务中的问题与改进<i class="fa fa-external-link-alt"></i></span>。</p>
<h1 id="参考文献">参考文献</h1>
<p><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzE3MDYuMDM3NjIucGRm">Attention is All You
Need<i class="fa fa-external-link-alt"></i></span><br />
<span class="exturl" data-url="aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L25vY21sL2FydGljbGUvZGV0YWlscy8xMDMwODI2MDA=">Transformer--论文翻译：Attention
Is All You Need 中文版<i class="fa fa-external-link-alt"></i></span><br />
<span class="exturl" data-url="aHR0cHM6Ly9ubHAuc2Vhcy5oYXJ2YXJkLmVkdS8yMDE4LzA0LzAzL2F0dGVudGlvbi5odG1s">The
Annotated Transformer<i class="fa fa-external-link-alt"></i></span><br />
<span class="exturl" data-url="aHR0cDovL2phbGFtbWFyLmdpdGh1Yi5pby9pbGx1c3RyYXRlZC10cmFuc2Zvcm1lci8=">The
Illustrated Transformer<i class="fa fa-external-link-alt"></i></span><br />
<span class="exturl" data-url="aHR0cHM6Ly9tcC53ZWl4aW4ucXEuY29tL3Mvc05Uc3ZVamNmRVJ3VlA2RFRwLVR2QQ==">Transformer十问十答<i class="fa fa-external-link-alt"></i></span><br />
<span class="exturl" data-url="aHR0cHM6Ly96aC12Mi5kMmwuYWkvY2hhcHRlcl9hdHRlbnRpb24tbWVjaGFuaXNtcy9zZWxmLWF0dGVudGlvbi1hbmQtcG9zaXRpb25hbC1lbmNvZGluZy5odG1s">动手学-自注意力和位置编码<i class="fa fa-external-link-alt"></i></span><br />
<span class="exturl" data-url="aHR0cHM6Ly96aC12Mi5kMmwuYWkvY2hhcHRlcl9hdHRlbnRpb24tbWVjaGFuaXNtcy90cmFuc2Zvcm1lci5odG1s">动手学-Transformer<i class="fa fa-external-link-alt"></i></span><br />
<span class="exturl" data-url="aHR0cHM6Ly93d3cuYmlsaWJpbGkuY29tL3ZpZGVvL0JWMVd2NDExaDdrTj9wPTIz">(强推)李宏毅2021春机器学习课程<i class="fa fa-external-link-alt"></i></span><br />
<span class="exturl" data-url="aHR0cHM6Ly9tcC53ZWl4aW4ucXEuY29tL3MvUkx4V2V2VldIWGdYLVVjb3hEUzcwdw==">细讲 |
Attention Is All You Need<i class="fa fa-external-link-alt"></i></span><br />
<span class="exturl" data-url="aHR0cHM6Ly96aHVhbmxhbi56aGlodS5jb20vcC80NDEyMTM3OA==">【NLP】Transformer模型原理详解<i class="fa fa-external-link-alt"></i></span></p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://soundmemories.github.io/2021/07/19/Python/22.Flask/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avstar.png">
      <meta itemprop="name" content="SoundMemories">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="SoundMemories">
      <meta itemprop="description" content="今日事，今日毕">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | SoundMemories">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2021/07/19/Python/22.Flask/" class="post-title-link" itemprop="url">Flask</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2021-07-19 00:00:00" itemprop="dateCreated datePublished" datetime="2021-07-19T00:00:00+08:00">2021-07-19</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Python/" itemprop="url" rel="index"><span itemprop="name">Python</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>4.8k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>17 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="flask介绍">Flask介绍</h1>
<p>Flask诞生于2010年，是Armin ronacher（人名）用 Python 语言基于
Werkzeug 工具箱编写的轻量级Web开发框架。</p>
<p>Flask
本身相当于一个内核，其他几乎所有的功能都要用到扩展（邮件扩展Flask-Mail，用户认证Flask-Login，数据库Flask-SQLAlchemy），都需要用第三方的扩展来实现。比如可以用
Flask 扩展加入ORM、窗体验证工具，文件上传、身份验证等。Flask
没有默认使用的数据库，你可以选择 MySQL，也可以用 NoSQL。</p>
<p>其<strong>WSGI工具箱</strong>采用
Werkzeug（路由模块），<strong>模板引擎</strong>则使用 Jinja2。这两个也是
Flask 框架的核心。</p>
<p>最新版本：1.0.2、2.1.2</p>
<h2 id="框架对比">框架对比</h2>
<p>重量级的框架：为方便业务程序的开发，提供了丰富的工具、组件，如Django<br />
轻量级的框架：只提供Web框架的核心功能，自由、灵活、高度定制，如Flask、Tornado</p>
<p>django提供了：<br />
1. django-admin快速创建项目工程目录<br />
2. manage.py 管理项目工程<br />
3. orm模型（数据库抽象层）<br />
4. admin后台管理站点<br />
5. 缓存机制<br />
6. 文件存储系统<br />
7. 用户认证系统</p>
<p>而这些，flask都没有，都需要扩展包来提供。</p>
<h2 id="常用扩展包">常用扩展包</h2>
<p>扩展列表：http://flask.pocoo.org/extensions/</p>
<p>Flask-SQLalchemy：操作数据库；<br />
Flask-script：插入脚本；<br />
Flask-migrate：管理迁移数据库；<br />
Flask-Session：Session存储方式指定；<br />
Flask-WTF：表单；<br />
Flask-Mail：邮件；<br />
Flask-Bable：提供国际化和本地化支持，翻译；<br />
Flask-Login：认证用户状态；<br />
Flask-OpenID：认证；<br />
<strong>Flask-RESTful：开发REST API的工具；</strong><br />
Flask-Bootstrap：集成前端Twitter Bootstrap框架；<br />
Flask-Moment：本地化日期和时间；<br />
Flask-Admin：简单而可扩展的管理接口的框架</p>
<h2 id="安装">安装</h2>
<p>建议初学者创建修环境<code>mkvirtualenv flask -p python3</code>，再<code>pip install flask</code>。</p>
<p>虚拟环境命令：<br />
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 虚拟环境</span></span><br><span class="line">mkvirtualenv  <span class="comment"># 创建虚拟环境</span></span><br><span class="line">rmvirtualenv  <span class="comment"># 删除虚拟环境</span></span><br><span class="line">workon  <span class="comment"># 进入虚拟环境、查看所有虚拟环境</span></span><br><span class="line">deactivate  <span class="comment"># 退出虚拟环境</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># pip</span></span><br><span class="line">pip install  <span class="comment"># 安装依赖包</span></span><br><span class="line">pip uninstall  <span class="comment"># 卸载依赖包</span></span><br><span class="line">pip <span class="built_in">list</span>  <span class="comment"># 查看已安装的依赖包</span></span><br><span class="line">pip freeze  <span class="comment"># 冻结当前环境的依赖包</span></span><br></pre></td></tr></table></figure></p>
<h1 id="工程搭建">工程搭建</h1>
<h2 id="hello-world程序">Hello World程序</h2>
<figure class="highlight python"><figcaption><span>app.py</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 导入Flask类</span></span><br><span class="line"><span class="keyword">from</span> flask <span class="keyword">import</span> Flask</span><br><span class="line"></span><br><span class="line"><span class="comment">#Flask类接收一个参数__name__</span></span><br><span class="line">app = Flask(__name__)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 装饰器的作用是将路由映射到视图函数index</span></span><br><span class="line"><span class="meta">@app.route(<span class="params"><span class="string">&#x27;/&#x27;</span></span>)</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">index</span>():</span><br><span class="line">    <span class="keyword">return</span> <span class="string">&#x27;Hello World&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Flask应用程序实例的run方法启动WEB服务器</span></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    app.run()</span><br></pre></td></tr></table></figure>
<p>1、首先我们导入了 Flask 类。 该类的实例将会成为我们的 WSGI
应用。<br />
2、接着我们创建一个该类的实例<code>Flask(__name__)</code>。第一个参数是应用模块或者包的名称。这个参数是必需的，这样
Flask
才能知道在哪里可以找到模板和静态文件等东西。<code>Flask(__name__)</code>就是告诉Flask框架根目录位置，从而Flask可以查找其他配置文件。比如创建的flask项目目录如下：<br />
<figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">- test</span><br><span class="line">  - static</span><br><span class="line">  - tmplates</span><br><span class="line">  - app.py</span><br></pre></td></tr></table></figure><br />
通过<code>Flask(__name__)</code>，Flask知道了根目录test，从而知道本目录下的其他配置文件或文件夹(static、tmplates)，不像Django需要手动指定配置目录。</p>
<p>3、然后我们使用 <code>route()</code> 装饰器来告诉 Flask 触发函数的
URL 。</p>
<p><code>Flask</code>类实例化时可传递的参数：<br />
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">import_name      <span class="comment"># Flask程序所在的包(模块)，传 __name__ 就可以</span></span><br><span class="line">                 <span class="comment"># 其可以决定 Flask 在访问静态文件时查找的路径</span></span><br><span class="line">static_url_path  <span class="comment"># 静态文件访问路径，可以不传，默认为：/ + static_folder</span></span><br><span class="line">static_folder    <span class="comment"># 静态文件存储的文件夹，可以不传，默认为 static</span></span><br><span class="line">template_folder  <span class="comment"># 模板文件存储的文件夹，可以不传，默认为 templates</span></span><br></pre></td></tr></table></figure></p>
<h2 id="应用程序配置参数">应用程序配置参数</h2>
<p><code>Flask(__name__)</code>对象初始化参数仅仅设置的是Flask本身的属性，比如Flask从哪里读取静态文件、模板文件。</p>
<p>应用程序配置参数设置的是一个Web应用工程的相关信息，比如：数据库的连接信息、日志的配置信息、自定义的配置信息。<br />
这就需要集中管理项目的所有配置信息：<br />
1. Django将所有配置信息都放到了settings.py文件中，而Flask则不同。<br />
2.
Flask将配置信息保存到了<strong>app.config</strong>属性中，该属性可以<strong>按照字典类型进行操作</strong>。</p>
<p>Flask应用程序配置参数：<br />
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 读取</span></span><br><span class="line">app.config.get(name)</span><br><span class="line">app.config[name]</span><br></pre></td></tr></table></figure><br />
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 设置</span></span><br><span class="line"><span class="comment"># 1. 从配置“对象”中加载，配置对象可以写一个类</span></span><br><span class="line">app.config.from_object(配置对象)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. 从配置文件中加载</span></span><br><span class="line">app.config.from_pyfile(<span class="string">&#x27;setting.py&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3. 从环境变量中加载(使用环境变量加载不想出现在代码中的敏感配置信息)</span></span><br><span class="line">app.config.from_envvar(<span class="string">&#x27;环境变量名&#x27;</span>)</span><br></pre></td></tr></table></figure><br />
优缺点：<br />
1.
从配置“对象”中加载：可以继承，适用于配置多变动的情况；缺点是敏感信息暴露。<br />
2.
从配置文件中加载：不会暴露敏感信息；缺点是不能继承，配置地址写死，不易变动。<br />
3.
从环境变量中加载：不会暴露敏感信息，配置地址灵活可变；缺点是不能继承，设置不方便。</p>
<p>一般选择2种方式一起，来配置参数。</p>
<div class="tabs" id="1"><ul class="nav-tabs"><li class="tab active"><a href="#1-1">从配置“对象”中加载</a></li><li class="tab"><a href="#1-2">从配置文件中加载</a></li><li class="tab"><a href="#1-3">从环境变量中加载</a></li></ul><div class="tab-content"><div class="tab-pane active" id="1-1"><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">DefaultConfig</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;默认配置&quot;&quot;&quot;</span></span><br><span class="line">    SECRET_KEY = <span class="string">&#x27;TPmi4aLWRbyVq8zu9v82dWYW1&#x27;</span></span><br><span class="line"></span><br><span class="line">app = Flask(__name__)</span><br><span class="line"><span class="comment"># 加入SECRET_KEY配置</span></span><br><span class="line">app.config.from_object(DefaultConfig)</span><br><span class="line"></span><br><span class="line"><span class="meta">@app.route(<span class="params"><span class="string">&quot;/&quot;</span></span>)</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">index</span>():</span><br><span class="line">    <span class="built_in">print</span>(app.config[<span class="string">&#x27;SECRET_KEY&#x27;</span>])</span><br><span class="line">    <span class="keyword">return</span> <span class="string">&quot;hello world&quot;</span></span><br></pre></td></tr></table></figure></div><div class="tab-pane" id="1-2"><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 新建一个配置文件setting.py，写入下文</span></span><br><span class="line">SECRET_KEY = <span class="string">&#x27;TPmi4aLWRbyVq8zu9v82dWYW1&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 在Flask程序</span></span><br><span class="line">app = Flask(__name__)</span><br><span class="line"><span class="comment"># 增加配置</span></span><br><span class="line">app.config.from_pyfile(<span class="string">&#x27;setting.py&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="meta">@app.route(<span class="params"><span class="string">&quot;/&quot;</span></span>)</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">index</span>():</span><br><span class="line">    <span class="built_in">print</span>(app.config[<span class="string">&#x27;SECRET_KEY&#x27;</span>])</span><br><span class="line">    <span class="keyword">return</span> <span class="string">&quot;hello world&quot;</span></span><br></pre></td></tr></table></figure></div><div class="tab-pane" id="1-3"><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 先在终端中执行如下命令</span></span><br><span class="line">export PROJECT_SETTING=<span class="string">&#x27;~/setting.py&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 在Flask程序</span></span><br><span class="line">app = Flask(__name__)</span><br><span class="line"><span class="comment"># 增加配置：setting.py指定为配置文件，只不过这个文件地址存储在PROJECT_SETTING环境变量中</span></span><br><span class="line">app.config.from_envvar(<span class="string">&#x27;PROJECT_SETTING&#x27;</span>, silent=<span class="literal">True</span>) </span><br><span class="line"><span class="comment"># silent=False没有值时报错通知，默认为False</span></span><br><span class="line"><span class="comment"># True 表示安静的处理，即时没有值也让Flask正常的运行下去</span></span><br><span class="line"></span><br><span class="line"><span class="meta">@app.route(<span class="params"><span class="string">&quot;/&quot;</span></span>)</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">index</span>():</span><br><span class="line">    <span class="built_in">print</span>(app.config[<span class="string">&#x27;SECRET_KEY&#x27;</span>])</span><br><span class="line">    <span class="keyword">return</span> <span class="string">&quot;hello world&quot;</span></span><br></pre></td></tr></table></figure></div></div></div>
<p><strong>环境变量</strong><br />
通俗的理解，环境变量就是我们设置在操作系统中，由操作系统代为保存的变量值。<br />
在Linux系统中设置和读取环境变量的方式如下：<br />
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">export 变量名=变量值  # 设置</span><br><span class="line">echo $变量名  # 读取</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">例如</span></span><br><span class="line">export ITCAST=python</span><br><span class="line">echo $ITCAST</span><br></pre></td></tr></table></figure><br />
Flask使用环境变量加载配置的本质是通过环境变量值找到配置文件。</p>
<p>项目中的常用方式：使用工厂模式创建Flask
app，并结合使用配置对象与环境变量加载配置。<br />
（1）使用<strong>配置对象</strong>加载默认配置。<br />
（2）使用<strong>环境变量</strong>加载不想出现在代码中的敏感配置信息。<br />
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">create_flask_app</span>(<span class="params">config</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    创建Flask应用</span></span><br><span class="line"><span class="string">    :param config: 配置对象</span></span><br><span class="line"><span class="string">    :return: Flask应用</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    app = Flask(__name__)</span><br><span class="line">    app.config.from_object(config)</span><br><span class="line">    <span class="comment"># 3. 从环境变量中加载</span></span><br><span class="line">    <span class="comment"># 从环境变量指向的配置文件中读取的配置信息会覆盖掉从配置对象中加载的同名参数</span></span><br><span class="line">    app.config.from_envvar(<span class="string">&quot;PROJECT_SETTING&quot;</span>, silent=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">return</span> app</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 1. 从配置“对象”中加载，配置对象可以写一个类</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">DefaultConfig</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;默认配置&quot;&quot;&quot;</span></span><br><span class="line">    SECRET_KEY = <span class="string">&#x27;itcast1&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 继承</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">DevelopmentConfig</span>(<span class="title class_ inherited__">DefaultConfig</span>):</span><br><span class="line">    DEBUG=<span class="literal">True</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># app = create_flask_app(DefaultConfig)</span></span><br><span class="line">app = create_flask_app(DevelopmentConfig)</span><br><span class="line"></span><br><span class="line"><span class="meta">@app.route(<span class="params"><span class="string">&quot;/&quot;</span></span>)</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">index</span>():</span><br><span class="line">    <span class="built_in">print</span>(app.config[<span class="string">&#x27;SECRET_KEY&#x27;</span>])</span><br><span class="line">    <span class="keyword">return</span> <span class="string">&quot;hello world&quot;</span></span><br></pre></td></tr></table></figure></p>
<h2 id="app.run-参数">app.run 参数</h2>
<p>可以指定运行的主机IP地址，端口，是否开启调试模式。<br />
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">app.run(host=<span class="string">&quot;0.0.0.0&quot;</span>, port=<span class="number">5000</span>, debug = <span class="literal">True</span>)</span><br></pre></td></tr></table></figure><br />
关于DEBUG调试模式：<br />
（1）程序代码修改后可以自动重启服务器。<br />
（2）在服务器出现相关错误的时候可以直接将错误信息返回到前端进行展示。</p>
<p>在1.0版本之后，Flask调整了开发服务器的启动方式，由代码编写<code>app.run()</code>语句调整为命令<code>flask run</code>启动。<br />
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> flask <span class="keyword">import</span> Flask</span><br><span class="line"></span><br><span class="line">app = Flask(__name__)</span><br><span class="line"></span><br><span class="line"><span class="meta">@app.route(<span class="params"><span class="string">&#x27;/&#x27;</span></span>)</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">index</span>():</span><br><span class="line">    <span class="keyword">return</span> <span class="string">&#x27;Hello World&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 程序中不用再写app.run()</span></span><br></pre></td></tr></table></figure><br />
终端启动：<br />
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash"><span class="built_in">export</span> FLASK_APP=helloworld</span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">flask run</span></span><br><span class="line"> * Running on http://127.0.0.1:5000/</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">flask run 等价于 python -m flask run</span></span><br></pre></td></tr></table></figure><br />
说明：<br />
（1）环境变量 <code>FLASK_APP</code> 指明flask的启动实例。<br />
（2）<code>flask run -h 0.0.0.0 -p 8000</code> 绑定地址 端口。<br />
（3）<code>flask run --help</code>获取帮助。<br />
（4）生产模式与开发模式的控制。通过<code>FLASK_ENV</code>环境变量指明：<br />
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">export FLASK_ENV=production # 运行在生产模式，未指明则默认为此方式</span><br><span class="line">export FLASK_ENV=development #  运行在开发模式</span><br></pre></td></tr></table></figure></p>
<h1 id="路由">路由</h1>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@app.route(<span class="params"><span class="string">&quot;/itcast&quot;</span></span>)</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">view_func</span>():</span><br><span class="line">    <span class="keyword">return</span> <span class="string">&quot;hello world&quot;</span></span><br></pre></td></tr></table></figure>
<h2 id="查询路由信息">查询路由信息</h2>
<p>1、命令行方式<br />
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">$ flask routes</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">Endpoint  Methods  Rule</span></span><br><span class="line"><span class="string">--------  -------  -----------------------</span></span><br><span class="line"><span class="string">index     GET      /</span></span><br><span class="line"><span class="string">static    GET      /static/</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><br />
2、在程序中获取<br />
在应用中的<code>url_map</code>属性中保存着整个Flask应用的路由映射信息，可以通过读取这个属性获取路由信息。<br />
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(app.url_map)</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">Map([&lt;Rule &#x27;/&#x27; (GET, HEAD, OPTIONS) -&gt; hello_world&gt;,</span></span><br><span class="line"><span class="string"> &lt;Rule &#x27;/static/&lt;filename&gt;&#x27; (GET, HEAD, OPTIONS) -&gt; static&gt;])</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure><br />
如果想在程序中遍历路由信息，可以采用如下方式：<br />
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># rule为Rule对象，其属性endpoint为路由名字，rule为路由路径。</span></span><br><span class="line"><span class="keyword">for</span> rule <span class="keyword">in</span> app.url_map.iter_rules():</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;name=&#123;&#125; path=&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(rule.endpoint, rule.rule))</span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">name=hello_world path=/</span></span><br><span class="line"><span class="string">name=static path=/static/&lt;path:filename&gt;</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure><br />
实例：<br />
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 通过访问/地址，以json的方式返回应用内的所有路由信息</span></span><br><span class="line"><span class="meta">@app.route(<span class="params"><span class="string">&#x27;/&#x27;</span></span>)</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">route_map</span>():</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    主视图，返回所有视图网址</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    rules_iterator = app.url_map.iter_rules()</span><br><span class="line">    <span class="keyword">return</span> json.dumps(&#123;rule.endpoint: rule.rule <span class="keyword">for</span> rule <span class="keyword">in</span> rules_iterator&#125;)</span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">&#123;&quot;route_map&quot;: &quot;/rule&quot;, &quot;hello_world&quot;: &quot;/&quot;, &quot;static&quot;: &quot;/static/&quot;&#125;</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure></p>
<h2 id="指定请求方式">指定请求方式</h2>
<p>在 Flask 中，定义路由其默认的请求方式为：<br />
（1）<code>GET</code><br />
（2）<code>OPTIONS</code>
(自带)：简化版的GET请求，用于询问服务器接口信息的。比如，接口允许的请求方式、允许的请求源头域名（cors跨域时会出现）。<br />
（3）<code>HEAD</code>
(自带)：简化版的GET请求，只返回GET请求处理时的响应头，不返回响应体。<br />
利用<code>methods</code>参数可以自己指定一个接口的请求方式。<br />
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@app.route(<span class="params"><span class="string">&quot;/itcast1&quot;</span>, methods=[<span class="string">&quot;POST&quot;</span>]</span>)  </span><span class="comment"># 只支持POST请求</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">view_func_1</span>():</span><br><span class="line">    <span class="keyword">return</span> <span class="string">&quot;hello world 1&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="meta">@app.route(<span class="params"><span class="string">&quot;/itcast2&quot;</span>, methods=[<span class="string">&quot;GET&quot;</span>, <span class="string">&quot;POST&quot;</span>]</span>)  </span><span class="comment"># 支持GET、POST请求</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">view_func_2</span>():</span><br><span class="line">    <span class="keyword">return</span> <span class="string">&quot;hello world 2&quot;</span></span><br></pre></td></tr></table></figure></p>
<h1 id="蓝图">蓝图</h1>
<p>在一个Flask
应用项目中，如果业务视图过多，可否将以某种方式划分出的业务单元单独维护，将每个单元用到的视图、静态文件、模板文件等独立分开？<br />
例如从业务角度上，可将整个应用划分为用户模块单元、商品模块单元、订单模块单元，如何分别开发这些不同单元，并最终整合到一个项目应用中？<br />
在Django中这种需求是如何实现的？</p>
<p>在Flask中，使用蓝图 Blueprint 来分模块组织管理。<br />
蓝图实际可以理解为是一个存储一组视图方法的容器对象，其具有如下特点：<br />
（1）一个应用可以具有多个Blueprint。<br />
（2）可以将一个Blueprint注册到任何一个未使用的URL下比如
“/user”、“/goods”。<br />
（3）Blueprint可以单独具有自己的模板、静态文件或者其它的通用操作方法，它并不是必须要实现应用的视图和函数。<br />
（4）在一个应用初始化时，就应该要注册需要使用的Blueprint<br />
但是一个Blueprint并不是一个完整的应用，它不能独立于应用运行，而必须要注册到某一个应用中。</p>
<h2 id="蓝图创建">蓝图创建</h2>
<p>1、创建一个蓝图对象<br />
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">user_bp = Blueprint(<span class="string">&#x27;user&#x27;</span>,__name__)</span><br></pre></td></tr></table></figure><br />
2、在这个蓝图对象上进行操作，注册路由，指定静态文件夹，注册模版过滤器<br />
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@user_bp.route(<span class="params"><span class="string">&#x27;/&#x27;</span></span>)</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">user_profile</span>():</span><br><span class="line">    <span class="keyword">return</span> <span class="string">&#x27;user_profile&#x27;</span></span><br></pre></td></tr></table></figure><br />
3、在应用对象上注册这个蓝图对象<br />
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">app.register_blueprint(user_bp)</span><br></pre></td></tr></table></figure><br />
<strong>单文件蓝图</strong><br />
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> flask <span class="keyword">import</span> Flask, Blueprint</span><br><span class="line"></span><br><span class="line">app = Flask(__name__)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建一个蓝图对象</span></span><br><span class="line">user_bp = Blueprint(<span class="string">&#x27;user&#x27;</span>,__name__)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 注册路由</span></span><br><span class="line"><span class="meta">@user_bp.route(<span class="params"><span class="string">&#x27;/&#x27;</span></span>)</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">user_profile</span>():</span><br><span class="line">    <span class="keyword">return</span> <span class="string">&#x27;user_profile&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 应用对象上注册这个蓝图对象</span></span><br><span class="line">app.register_blueprint(user_bp)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    app.run()</span><br></pre></td></tr></table></figure><br />
<strong>目录蓝图</strong><br />
对于一个打算包含多个文件的蓝图，通常将创建蓝图对象放到Python包的<code>__init__.py</code>文件中。<br />
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">--------- project <span class="comment"># 工程目录</span></span><br><span class="line">  |------ main.py <span class="comment"># 启动文件</span></span><br><span class="line">  |------ user  <span class="comment"># 用户蓝图</span></span><br><span class="line">  |  |--- __init__.py  <span class="comment"># 此处创建蓝图对象</span></span><br><span class="line">  |  |--- passport.py  </span><br><span class="line">  |  |--- profile.py</span><br><span class="line">  |  |--- ...</span><br><span class="line">  |</span><br><span class="line">  |------ goods <span class="comment"># 商品蓝图</span></span><br><span class="line">  |  |--- __init__.py  <span class="comment"># 此处创建蓝图对象</span></span><br><span class="line">  |  |--- views.py  <span class="comment"># 此处定义视图</span></span><br><span class="line">  |...</span><br></pre></td></tr></table></figure><br />
以goods为例：<br />
<div class="tabs" id="2"><ul class="nav-tabs"><li class="tab active"><a href="#2-1">__init__.py</a></li><li class="tab"><a href="#2-2">views.py</a></li><li class="tab"><a href="#2-3">main.py</a></li></ul><div class="tab-content"><div class="tab-pane active" id="2-1"><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> flask <span class="keyword">import</span> Flask, Blueprint</span><br><span class="line"></span><br><span class="line">goods_bp = Blueprint(<span class="string">&#x27;goods&#x27;</span>,__name__)</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> . <span class="keyword">import</span> views  <span class="comment"># 导入视图，不然没有视图</span></span><br></pre></td></tr></table></figure></div><div class="tab-pane" id="2-2"><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> . <span class="keyword">import</span> goods_bp</span><br><span class="line"></span><br><span class="line"><span class="meta">@goods_bp.route(<span class="params"><span class="string">&#x27;/goods&#x27;</span></span>)</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_goods</span>():</span><br><span class="line">    <span class="keyword">return</span> <span class="string">&#x27;get_goods&#x27;</span></span><br></pre></td></tr></table></figure></div><div class="tab-pane" id="2-3"><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> flask <span class="keyword">import</span> Flask, Blueprint</span><br><span class="line"></span><br><span class="line">app = Flask(__name__)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 主程序注册即可</span></span><br><span class="line"><span class="keyword">from</span> goods <span class="keyword">import</span> goods_bp</span><br><span class="line">app.register_blueprint(goods_bp)</span><br></pre></td></tr></table></figure></div></div></div></p>
<h2 id="扩展用法">扩展用法</h2>
<p>1、指定蓝图的url前缀：在应用中注册蓝图时使用url_prefix参数指定<br />
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">app.register_blueprint(user_bp, url_prefix=<span class="string">&#x27;/user&#x27;</span>)</span><br><span class="line">app.register_blueprint(goods_bp, url_prefix=<span class="string">&#x27;/goods&#x27;</span>)</span><br></pre></td></tr></table></figure><br />
2、蓝图内部静态文件<br />
和应用对象不同，蓝图对象创建时不会默认注册静态目录的路由。需要我们在
创建时指定 <code>static_folder</code> 参数。<br />
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 蓝图所在目录下的static_admin目录设置为静态目录</span></span><br><span class="line">admin = Blueprint(<span class="string">&quot;admin&quot;</span>,__name__,static_folder=<span class="string">&#x27;static_admin&#x27;</span>)</span><br><span class="line">app.register_blueprint(admin, url_prefix=<span class="string">&#x27;/admin&#x27;</span>)</span><br><span class="line"><span class="comment"># 注册完，可使用 /admin/static_admin/&lt;filename&gt; 访问static_admin目录下的静态文件</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 也可通过static_url_path改变访问路径</span></span><br><span class="line">admin = Blueprint(<span class="string">&quot;admin&quot;</span>,__name__,static_folder=<span class="string">&#x27;static_admin&#x27;</span>,static_url_path=<span class="string">&#x27;/lib&#x27;</span>)</span><br><span class="line">app.register_blueprint(admin,url_prefix=<span class="string">&#x27;/admin&#x27;</span>)</span><br></pre></td></tr></table></figure><br />
3、蓝图内部模板目录<br />
蓝图对象默认的模板目录为系统的模版目录，可以在创建蓝图对象时使用
<code>template_folder</code> 关键字参数设置模板目录。<br />
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">admin = Blueprint(<span class="string">&#x27;admin&#x27;</span>,__name__,template_folder=<span class="string">&#x27;my_templates&#x27;</span>)</span><br></pre></td></tr></table></figure></p>
<h1 id="请求">请求</h1>
<p>在视图编写中需要读取客户端请求携带的数据时，如何才能正确的取出数据呢？</p>
<p>请求携带的数据可能出现在HTTP报文中的不同位置，需要使用不同的方法来获取参数。</p>
<h2 id="url路径参数动态路由">URL路径参数（动态路由）</h2>
<p>例如，有一个请求访问的接口地址为/users/123，其中123实际上为具体的请求参数，表明请求123号用户的信息。此时如何从url中提取出123的数据？</p>
<p>Flask不同于Django直接在定义路由时编写正则表达式的方式，而是采用转换器语法：<br />
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@app.route(<span class="params"><span class="string">&#x27;/users/&lt;user_id&gt;&#x27;</span></span>)  </span><span class="comment"># 提取路径中user_id参数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">user_info</span>(<span class="params">user_id</span>):</span><br><span class="line">    <span class="built_in">print</span>(<span class="built_in">type</span>(user_id))</span><br><span class="line">    <span class="keyword">return</span> <span class="string">&#x27;hello user &#123;&#125;&#x27;</span>.<span class="built_in">format</span>(user_id)</span><br></pre></td></tr></table></figure><br />
此处的<code>&lt;&gt;</code>即是一个转换器，<strong>默认为字符串类型</strong>，即将该位置的数据以字符串格式进行匹配、并以字符串为数据类型类型、
user_id为参数名传入视图。</p>
<p>Flask也提供其他数据类型的转换器：<br />
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">DEFAULT_CONVERTERS = &#123;</span><br><span class="line">    <span class="string">&#x27;default&#x27;</span>:          UnicodeConverter,</span><br><span class="line">    <span class="string">&#x27;string&#x27;</span>:           UnicodeConverter,</span><br><span class="line">    <span class="string">&#x27;any&#x27;</span>:              AnyConverter,</span><br><span class="line">    <span class="string">&#x27;path&#x27;</span>:             PathConverter,</span><br><span class="line">    <span class="string">&#x27;int&#x27;</span>:              IntegerConverter,</span><br><span class="line">    <span class="string">&#x27;float&#x27;</span>:            FloatConverter,</span><br><span class="line">    <span class="string">&#x27;uuid&#x27;</span>:             UUIDConverter,</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><br />
比如，以整型匹配数据：<br />
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@app.route(<span class="params"><span class="string">&#x27;/users/&lt;int:user_id&gt;&#x27;</span></span>)  </span><span class="comment"># 整型转换</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">user_info</span>(<span class="params">user_id</span>):</span><br><span class="line">    <span class="built_in">print</span>(<span class="built_in">type</span>(user_id))</span><br><span class="line">    <span class="keyword">return</span> <span class="string">&#x27;hello user &#123;&#125;&#x27;</span>.<span class="built_in">format</span>(user_id)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">@app.route(<span class="params"><span class="string">&#x27;/users/&lt;int(min=1):user_id&gt;&#x27;</span></span>)  </span><span class="comment"># 整型转换、限制最小值</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">user_info</span>(<span class="params">user_id</span>):</span><br><span class="line">    <span class="built_in">print</span>(<span class="built_in">type</span>(user_id))</span><br><span class="line">    <span class="keyword">return</span> <span class="string">&#x27;hello user &#123;&#125;&#x27;</span>.<span class="built_in">format</span>(user_id)</span><br></pre></td></tr></table></figure></p>
<h2 id="自定义转换器">自定义转换器</h2>
<p>如果遇到需要匹配提取/sms_codes/15645678901
中的手机号数据，Flask内置的转换器就无法满足需求，此时需要自定义转换器。</p>
<p>1、创建转换器类，保存匹配时的正则表达式。<br />
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> werkzeug.routing <span class="keyword">import</span> BaseConverter</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MobileConverter</span>(<span class="title class_ inherited__">BaseConverter</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    手机号格式</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    regex = <span class="string">r&#x27;1[3-9]\d&#123;9&#125;&#x27;</span></span><br></pre></td></tr></table></figure><br />
2、将自定义的转换器告知Flask应用。相当于注册自定义类型到converters字典中，converters保存了所有类型。<br />
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">app = Flask(__name__)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将自定义转换器添加到转换器字典中，并指定转换器使用时名字为: mobile</span></span><br><span class="line">app.url_map.converters[<span class="string">&#x27;mobile&#x27;</span>] = MobileConverter</span><br></pre></td></tr></table></figure><br />
3、在使用转换器的地方定义使用。<br />
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@app.route(<span class="params"><span class="string">&#x27;/sms_codes/&lt;mobile:mob_num&gt;&#x27;</span></span>)  </span><span class="comment"># mobile为自定义类型</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">send_sms_code</span>(<span class="params">mob_num</span>):</span><br><span class="line">    <span class="keyword">return</span> <span class="string">&#x27;send sms code to &#123;&#125;&#x27;</span>.<span class="built_in">format</span>(mob_num)</span><br></pre></td></tr></table></figure></p>
<h2 id="其他参数">其他参数</h2>
<p>如果想要获取其他地方传递的参数，可以通过Flask提供的request对象来读取。</p>
<p>不同位置的参数都存放在request的不同属性中：<br />
<figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">属性	说明	                        类型</span><br><span class="line">data	记录请求的数据，并转换为字符串      *</span><br><span class="line">form	记录请求中的表单数据	        MultiDict</span><br><span class="line">args	记录请求中的查询参数	        MultiDict</span><br><span class="line">cookies	记录请求中的cookie信息	        Dict</span><br><span class="line">headers	记录请求中的报文头              EnvironHeaders</span><br><span class="line">method	记录请求使用的HTTP方法	        GET/POST</span><br><span class="line">url     记录请求的URL地址               string</span><br><span class="line">files	记录请求上传的文件                 *</span><br></pre></td></tr></table></figure><br />
data 对应请求中 body 的原始数据。<br />
form 对应请求中 body 的form数据。<br />
args 对应请求中 path 后面的查询参数。<br />
files 对应请求中 body 的files数据。</p>
<p>例如
想要获取请求/articles?channel_id=1中channel_id的参数，可以按如下方式使用：<br />
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> flask <span class="keyword">import</span> request</span><br><span class="line"></span><br><span class="line"><span class="meta">@app.route(<span class="params"><span class="string">&#x27;/articles&#x27;</span></span>)</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_articles</span>():</span><br><span class="line">    channel_id = request.args.get(<span class="string">&#x27;channel_id&#x27;</span>)</span><br><span class="line">    <span class="keyword">return</span> <span class="string">&#x27;you wanna get articles of channel &#123;&#125;&#x27;</span>.<span class="built_in">format</span>(channel_id)</span><br></pre></td></tr></table></figure><br />
例如，客户端上传图片到服务器，并保存到服务器中：<br />
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> flask <span class="keyword">import</span> request</span><br><span class="line"></span><br><span class="line"><span class="meta">@app.route(<span class="params"><span class="string">&#x27;/upload&#x27;</span>, methods=[<span class="string">&#x27;POST&#x27;</span>]</span>)</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">upload_file</span>():</span><br><span class="line">    f = request.files[<span class="string">&#x27;pic&#x27;</span>]  <span class="comment"># pic为上传文件的字段名</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 1、python方式保存文件</span></span><br><span class="line">    <span class="comment"># with open(&#x27;./demo.png&#x27;, &#x27;wb&#x27;) as new_file:</span></span><br><span class="line">    <span class="comment">#     new_file.write(f.read())</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 2、自带的保存方式</span></span><br><span class="line">    f.save(<span class="string">&#x27;./demo.png&#x27;</span>)</span><br><span class="line">    <span class="keyword">return</span> <span class="string">&#x27;ok&#x27;</span></span><br></pre></td></tr></table></figure></p>
<h1 id="响应">响应</h1>
<p>如何在不同的场景里返回不同的响应信息？</p>
<h2 id="返回模板">返回模板</h2>
<p>使用render_template方法渲染模板并返回。</p>
<p>例如，新建一个模板index.html：<br />
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&lt;!DOCTYPE <span class="keyword">html</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">html</span> <span class="attr">lang</span>=<span class="string">&quot;en&quot;</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">head</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">meta</span> <span class="attr">charset</span>=<span class="string">&quot;UTF-8&quot;</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">title</span>&gt;</span>Title<span class="tag">&lt;/<span class="name">title</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">head</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">body</span>&gt;</span></span><br><span class="line">我的模板html内容</span><br><span class="line"><span class="tag">&lt;<span class="name">br</span>/&gt;</span>&#123;&#123; my_str &#125;&#125;</span><br><span class="line"><span class="tag">&lt;<span class="name">br</span>/&gt;</span>&#123;&#123; my_int &#125;&#125;</span><br><span class="line"><span class="tag">&lt;/<span class="name">body</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">html</span>&gt;</span></span><br></pre></td></tr></table></figure><br />
后端视图：<br />
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> flask <span class="keyword">import</span> render_template</span><br><span class="line"></span><br><span class="line"><span class="comment"># 返回html</span></span><br><span class="line"><span class="meta">@app.route(<span class="params"><span class="string">&#x27;/&#x27;</span></span>)</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">index</span>():</span><br><span class="line">    mstr = <span class="string">&#x27;Hello World&#x27;</span></span><br><span class="line">    mint = <span class="number">10</span></span><br><span class="line">    <span class="comment"># html模板中的变量名作为参数名，前面定义的变量mstr、mint赋值给这些参数。</span></span><br><span class="line">    <span class="keyword">return</span> render_template(<span class="string">&#x27;index.html&#x27;</span>, my_str=mstr, my_int=mint)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 返回dict格式 </span></span><br><span class="line"><span class="meta">@app.route(<span class="params"><span class="string">&#x27;/&#x27;</span></span>)</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">index</span>():</span><br><span class="line">    <span class="comment"># 注意，参数名要和html一致</span></span><br><span class="line">    data = <span class="built_in">dict</span>(</span><br><span class="line">        my_str = <span class="string">&#x27;Hello World&#x27;</span>  </span><br><span class="line">        my_int = <span class="number">10</span></span><br><span class="line">    )</span><br><span class="line">    <span class="comment"># 解包</span></span><br><span class="line">    <span class="keyword">return</span> render_template(<span class="string">&#x27;index.html&#x27;</span>, **data)</span><br></pre></td></tr></table></figure></p>
<h2 id="重定向">重定向</h2>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> flask <span class="keyword">import</span> redirect</span><br><span class="line"></span><br><span class="line"><span class="meta">@app.route(<span class="params"><span class="string">&#x27;/demo2&#x27;</span></span>)</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">demo2</span>():</span><br><span class="line">    <span class="keyword">return</span> redirect(<span class="string">&#x27;http://www.baidu.com&#x27;</span>)</span><br></pre></td></tr></table></figure>
<h2 id="返回json">返回JSON</h2>
<p><code>jsonify</code>：转换成json格式，设置了响应头<code>Content-Type:application/json</code>。<br />
<code>json.dumps</code>：只会转换成json格式，无设置响应头信息。<br />
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> flask <span class="keyword">import</span> jsonify</span><br><span class="line"></span><br><span class="line"><span class="meta">@app.route(<span class="params"><span class="string">&#x27;/demo3&#x27;</span></span>)</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">demo3</span>():</span><br><span class="line">    json_dict = &#123;</span><br><span class="line">        <span class="string">&quot;user_id&quot;</span>: <span class="number">10</span>,</span><br><span class="line">        <span class="string">&quot;user_name&quot;</span>: <span class="string">&quot;laowang&quot;</span></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> jsonify(json_dict)</span><br></pre></td></tr></table></figure></p>
<h2 id="自定义状态码和响应头">自定义状态码和响应头</h2>
<p>1、元祖方式<br />
可以返回一个元组，这样的元组必须是
<code>(response, status, headers)</code> 的形式，且至少包含一个元素。
<code>status</code> 值会覆盖状态代码， <code>headers</code>
可以是一个列表或字典，作为额外的消息标头值。<br />
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@app.route(<span class="params"><span class="string">&#x27;/demo4&#x27;</span></span>)</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">demo4</span>():</span><br><span class="line">    <span class="comment"># return &#x27;状态码为 666&#x27;, 666</span></span><br><span class="line">    <span class="comment"># return &#x27;状态码为 666&#x27;, 666, [(&#x27;Itcast&#x27;, &#x27;Python&#x27;)]</span></span><br><span class="line">    <span class="keyword">return</span> <span class="string">&#x27;状态码为 666&#x27;</span>, <span class="number">666</span>, &#123;<span class="string">&#x27;Itcast&#x27;</span>: <span class="string">&#x27;Python&#x27;</span>&#125;</span><br></pre></td></tr></table></figure><br />
2、make_response方式<br />
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@app.route(<span class="params"><span class="string">&#x27;/demo5&#x27;</span></span>)</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">demo5</span>():</span><br><span class="line">    resp = make_response(<span class="string">&#x27;make response测试&#x27;</span>)  <span class="comment"># 这里是body</span></span><br><span class="line">        resp.headers[“Itcast”] = “Python”      <span class="comment"># 这里是响应头</span></span><br><span class="line">        resp.status = “<span class="number">404</span> <span class="keyword">not</span> found”          <span class="comment"># 这里是状态，一定是完整的状态(状态码+描述信息)！</span></span><br><span class="line">    <span class="keyword">return</span> resp</span><br></pre></td></tr></table></figure></p>
<h2 id="cookie">Cookie</h2>
<p>1、设置<br />
cookie只能在响应对象中设置，所以要先拿到响应体，所以只能用make_response方法构造响应。<br />
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> flask <span class="keyword">import</span> Flask, make_response</span><br><span class="line"></span><br><span class="line">app = Flask(__name__)</span><br><span class="line"></span><br><span class="line"><span class="meta">@app.route(<span class="params"><span class="string">&#x27;/cookie&#x27;</span></span>)</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">set_cookie</span>():</span><br><span class="line">    resp = make_response(<span class="string">&#x27;set cookie ok&#x27;</span>)  <span class="comment"># 先拿到响应体</span></span><br><span class="line">    resp.set_cookie(<span class="string">&#x27;username&#x27;</span>, <span class="string">&#x27;itcast&#x27;</span>)  <span class="comment"># 再设置cookie，默认有效期为窗口关闭</span></span><br><span class="line">    <span class="keyword">return</span> resp</span><br></pre></td></tr></table></figure><br />
设置有效期：<br />
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@app.route(<span class="params"><span class="string">&#x27;/cookie&#x27;</span></span>)</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">set_cookie</span>():</span><br><span class="line">    response = make_response(<span class="string">&#x27;hello world&#x27;</span>)</span><br><span class="line">    response.set_cookie(<span class="string">&#x27;username&#x27;</span>, <span class="string">&#x27;itheima&#x27;</span>, max_age=<span class="number">3600</span>)</span><br><span class="line">    <span class="keyword">return</span> response</span><br></pre></td></tr></table></figure><br />
2、读取<br />
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> flask <span class="keyword">import</span> request</span><br><span class="line"></span><br><span class="line"><span class="meta">@app.route(<span class="params"><span class="string">&#x27;/get_cookie&#x27;</span></span>)</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_cookie</span>():</span><br><span class="line">    resp = request.cookies.get(<span class="string">&#x27;username&#x27;</span>)</span><br><span class="line">    <span class="keyword">return</span> resp</span><br></pre></td></tr></table></figure><br />
3、删除<br />
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> flask <span class="keyword">import</span> request</span><br><span class="line"></span><br><span class="line"><span class="meta">@app.route(<span class="params"><span class="string">&#x27;/delete_cookie&#x27;</span></span>)</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">delete_cookie</span>():</span><br><span class="line">    response = make_response(<span class="string">&#x27;hello world&#x27;</span>)</span><br><span class="line">    response.delete_cookie(<span class="string">&#x27;username&#x27;</span>)  <span class="comment"># 不是真删除，是设置过早的有效期达到结束目的</span></span><br><span class="line">    <span class="keyword">return</span> response</span><br></pre></td></tr></table></figure></p>
<h2 id="session">Session</h2>
<p>Django把session数据存储到redis中。<br />
flask将session数据保存到了哪里？放到了浏览器缓存中。<br />
为了防止用户修改这个session，那么需要设置签名，这就需要SECRET_KEY了。</p>
<p>需要先设置SECRET_KEY：<br />
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">DefaultConfig</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">    SECRET_KEY = <span class="string">&#x27;fih9fh9eh9gh2&#x27;</span></span><br><span class="line"></span><br><span class="line">app.config.from_object(DefaultConfig)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 或者直接设置</span></span><br><span class="line">app.secret_key=<span class="string">&#x27;xihwidfw9efw&#x27;</span></span><br></pre></td></tr></table></figure><br />
1、设置<br />
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> flask <span class="keyword">import</span> session</span><br><span class="line"></span><br><span class="line"><span class="meta">@app.route(<span class="params"><span class="string">&#x27;/set_session&#x27;</span></span>)</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">set_session</span>():</span><br><span class="line">    session[<span class="string">&#x27;username&#x27;</span>] = <span class="string">&#x27;itcast&#x27;</span></span><br><span class="line">    <span class="keyword">return</span> <span class="string">&#x27;set session ok&#x27;</span></span><br></pre></td></tr></table></figure><br />
2、读取<br />
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@app.route(<span class="params"><span class="string">&#x27;/get_session&#x27;</span></span>)</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_session</span>():</span><br><span class="line">    username = session.get(<span class="string">&#x27;username&#x27;</span>)</span><br><span class="line">    <span class="keyword">return</span> <span class="string">&#x27;get session username &#123;&#125;&#x27;</span>.<span class="built_in">format</span>(username)</span><br></pre></td></tr></table></figure></p>
<h1 id="参考文献">参考文献</h1>
<p><span class="exturl" data-url="aHR0cHM6Ly9mbGFzay1yZXN0ZnVsLnJlYWR0aGVkb2NzLmlvL2VuL2xhdGVzdC9xdWlja3N0YXJ0Lmh0bWw=">Flask-RESTful<i class="fa fa-external-link-alt"></i></span><br />
<span class="exturl" data-url="aHR0cDovL3d3dy5weXRob25kb2MuY29tL0ZsYXNrLVJFU1RmdWwvcXVpY2tzdGFydC5odG1s">Flask-RESTful中文<i class="fa fa-external-link-alt"></i></span><br />
<span class="exturl" data-url="aHR0cDovL3d3dy5iamhlZS5jb20vZmxhc2stMS5odG1s">Flask入门系列(一)–Hello
World<i class="fa fa-external-link-alt"></i></span><br />
<span class="exturl" data-url="aHR0cDovL3d3dy5iamhlZS5jb20vZmxhc2stYWQxLmh0bWw=">Flask进阶系列(一)–上下文环境<i class="fa fa-external-link-alt"></i></span><br />
<span class="exturl" data-url="aHR0cHM6Ly9mbGFzay5wYWxsZXRzcHJvamVjdHMuY29tL2VuLzIuMi54Lw==">Flask
Documentation (2.2.x)<i class="fa fa-external-link-alt"></i></span><br />
<span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL3BhbGxldHMvZmxhc2s=">pallets/flask<i class="fa fa-external-link-alt"></i></span></p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://soundmemories.github.io/2021/07/18/Paper/03.Get%20To%20The%20Point.%20Summarization%20with%20Pointer-Generator%20Networks/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avstar.png">
      <meta itemprop="name" content="SoundMemories">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="SoundMemories">
      <meta itemprop="description" content="今日事，今日毕">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | SoundMemories">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2021/07/18/Paper/03.Get%20To%20The%20Point.%20Summarization%20with%20Pointer-Generator%20Networks/" class="post-title-link" itemprop="url">Get To The Point. Summarization with Pointer-Generator Networks</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2021-07-18 00:00:00" itemprop="dateCreated datePublished" datetime="2021-07-18T00:00:00+08:00">2021-07-18</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Paper/" itemprop="url" rel="index"><span itemprop="name">Paper</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>1.2k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>4 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="摘要">摘要</h1>
<p>把seq2seq模型应用于摘要生成时存在两个主要的问题：<br />
（1）难以准确复述原文的事实细节、无法处理原文中的未登录词(OOV)；<br />
（2）生成的摘要中存在重复的片段。</p>
<p>针对存在的问题提出对应解决之道，在标准seq2seq+attention的基础之上做了改进：<br />
（1）采用Pointer-Generator
Network（抽取+生成），在保留生成新词的同时，还可以从原文中抽取内容，促使生成更准确的摘要。<br />
（2）覆盖率机制(coverage
mechanism)，使用Coverage记录已经生成的内容，从而减少内容重复。</p>
<p><img src="/images/PGN/结构.png" width="90%"></p>
<h1 id="基础模型">基础模型</h1>
<p>标准的seq2seq模型+Attention机制：<br />
encoder使用Bi-LSTM，序列单词按顺序喂给encoder中，产生一系列编码器的隐状态
<span class="math inline">\(h_i\)</span>。decoder使用LSTM，在每个时间步
<span class="math inline">\(t\)</span>，接收前一个词（训练时采用teacher
forcing方式，预测时是前一时刻decoder输出词），decoder隐状态为 <span
class="math inline">\(s_t\)</span>。Attention计算采用Bahdanau方式，但这里采用decoder的当前时刻
<span class="math inline">\(t\)</span> 的隐状态 <span
class="math inline">\(s_t\)</span> 作为 query 计算attention，得到context
vector 和
当前时刻的隐状态拼接后作为全连接层输入。也可使用context上一时刻隐状态作为query，但得到context
vector要与输入拼接作为decoder输入(Bahdanau Attention)。</p>
<p><img src="/images/PGN/1+2.png" width="40%"></p>
<p>编码器隐藏状态的加权和，称为上下文向量（context vector） <span
class="math inline">\(h_t^*\)</span>。<br />
<img src="/images/PGN/3.png" width="30%"></p>
<p>上下文向量可以被看作是在当前时间步 <span
class="math inline">\(t\)</span> 看encoder的哪些部分，与解码器隐状态
<span class="math inline">\(s_t\)</span>
拼接，通过两个线性层产生词汇分布 <span
class="math inline">\(P_{\text{vocab}}\)</span>。<br />
<img src="/images/PGN/4.png" width="40%"></p>
<p><span class="math inline">\(P_{\text{vocab}}\)</span>
是词汇表中所有单词的概率分布，它提供了预测单词 <span
class="math inline">\(w\)</span> 的最终分布。<br />
<img src="/images/PGN/5.png" width="30%"></p>
<p>在训练过程中，时间步长 <span class="math inline">\(t\)</span>
的损失是 <span class="math inline">\(t\)</span> 的目标词 <span
class="math inline">\(w_t^*\)</span> 的负对数似然。<br />
<img src="/images/PGN/6.png" width="30%"></p>
<p>整个序列的损失是：<br />
<img src="/images/PGN/7.png" width="30%"></p>
<h1 id="pointer-generator-network">Pointer-Generator Network</h1>
<p>通过Pointer-Generator可以从输入中复制单词，需要一个软开关 <span
class="math inline">\(P_{\text{gen}}\)</span> 来融合baseline 和
Pointer-Generator。<br />
根据上下文向量 <span class="math inline">\(h_t^*\)</span>，解码器隐状态
<span class="math inline">\(s_t\)</span>，解码器输入 <span
class="math inline">\(x_t\)</span>，计算得到时间步 <span
class="math inline">\(t\)</span> 的生成概率 <span
class="math inline">\(P_{\text{gen}}\in [0,1]\)</span>。<br />
<img src="/images/PGN/8.png" width="40%"></p>
<p>使用软开关 <span class="math inline">\(P_{\text{gen}}\)</span>
来融合baseline 和 Pointer-Generator。需要维护一个扩展词表（extended
vocabulary）表示原始词表和出现在source中的所有词汇的联合，在扩展词表上得到以下概率分布，<span
class="math inline">\(a_i^t\)</span> 表示通过attention
weight来确定从source中拷贝词的概率分布。因为attention
weight包括了所有source中单词出现的概率，即使是原始词表中oov的词(该词在src_oov中能找到)，也能通过attention
weight从source扩展词表src_oov中抽取复制。<br />
<img src="/images/PGN/9.png" width="40%"><br />
注意，如果单词 <span class="math inline">\(w\)</span>
是原始词表OOV单词，那么 <span
class="math inline">\(P_{\text{vocab}}\)</span>
一定为零，要是这个词也没出现在source中(即也没在扩展词表)，那么 <span
class="math inline">\(a_i^t\)</span> 也是零。</p>
<h1 id="coverage-mechanism">Coverage mechanism</h1>
<p>重复序列是一个常见问题，本文维护一个覆盖向量（coverage vector） <span
class="math inline">\(c^t\)</span>，它是所有先前decoder时间步的Attention值
<span class="math inline">\(a_i^t\)</span> 的累加和：<br />
<img src="/images/PGN/10.png" width="30%"><br />
目的是用先前的注意力权重决策来影响当前注意力权重的决策（累加的某个值大代表一直关注它，要减少这个值，不让模型总关注它），这样就避免在同一位置重复，从而避免重复生成文本。</p>
<p><span class="math inline">\(c^t\)</span>
是source中单词的分布，表示这些单词从注意力机制到目前时间步的覆盖程度。
<span class="math inline">\(c^0\)</span>
是一个零向量。覆盖向量被引入到注意力机制中：<br />
<img src="/images/PGN/11.png" width="40%"><br />
作者认为有必要额外定义覆盖损失（coverage
loss）处罚重复出现的参考位置，使注意力更分散，即对过往时刻或当前时刻受到注意力较多的单词进行惩罚：<br />
<img src="/images/PGN/12.png" width="35%"><br />
最后融合为复合损失函数：<br />
<img src="/images/PGN/13.png" width="45%"></p>
<h1 id="总结">总结</h1>
<p>根据评价指标来看，提升效果明显，普遍提高2个百分点。</p>
<p>作者还说了直接加coverage效果并不好，所以实际应用时采用fine-tuning训练。</p>
<p>实际使用时建议增加weight_tying和scheduled_sampling尝试效果。</p>
<h1 id="参考文献">参考文献</h1>
<p><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzE3MDQuMDQzNjgucGRm">Get To The Point:
Summarization with Pointer-Generator Networks<i class="fa fa-external-link-alt"></i></span><br />
<span class="exturl" data-url="aHR0cHM6Ly96aHVhbmxhbi56aGlodS5jb20vcC8zMjMwMDg0Ng==">Get To The Point:
Summarization with Pointer-Generator Networks 译文<i class="fa fa-external-link-alt"></i></span></p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://soundmemories.github.io/2021/07/16/NLP/03.%E5%9F%BA%E4%BA%8ELSTM%E7%9A%84%E6%9C%BA%E5%99%A8%E7%BF%BB%E8%AF%91/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avstar.png">
      <meta itemprop="name" content="SoundMemories">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="SoundMemories">
      <meta itemprop="description" content="今日事，今日毕">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | SoundMemories">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2021/07/16/NLP/03.%E5%9F%BA%E4%BA%8ELSTM%E7%9A%84%E6%9C%BA%E5%99%A8%E7%BF%BB%E8%AF%91/" class="post-title-link" itemprop="url">基于LSTM的机器翻译</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2021-07-16 00:00:00" itemprop="dateCreated datePublished" datetime="2021-07-16T00:00:00+08:00">2021-07-16</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/NLP/" itemprop="url" rel="index"><span itemprop="name">NLP</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>1.5k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>5 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <figure class="highlight python"><figcaption><span>utils.py</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">import</span> spacy</span><br><span class="line"><span class="keyword">from</span> torchtext.data <span class="keyword">import</span> Field, BucketIterator</span><br><span class="line"><span class="keyword">from</span> torchtext.datasets <span class="keyword">import</span> Multi30k</span><br><span class="line"></span><br><span class="line"><span class="comment"># manual create date ( token 2 index , index to token)</span></span><br><span class="line"><span class="comment"># dataset dataloader   PADDING BATCH SHUFFLE</span></span><br><span class="line"><span class="comment"># torchtext</span></span><br><span class="line"><span class="comment"># ALLENNLP (Field)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># [&quot;&lt;sos&gt;&quot; 3 ,&quot;word&quot;1 ,&quot;peace&quot; 2,&quot;&lt;eos&gt;&quot; 4 ]</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">load_dataset</span>(<span class="params">batch_size</span>):</span><br><span class="line">    spacy_de = spacy.load(<span class="string">&#x27;de&#x27;</span>)</span><br><span class="line">    spacy_en = spacy.load(<span class="string">&#x27;en&#x27;</span>)</span><br><span class="line">    url = re.<span class="built_in">compile</span>(<span class="string">&#x27;(&lt;url&gt;.*&lt;/url&gt;)&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">tokenize_de</span>(<span class="params">text</span>):</span><br><span class="line">        <span class="keyword">return</span> [tok.text <span class="keyword">for</span> tok <span class="keyword">in</span> spacy_de.tokenizer(url.sub(<span class="string">&#x27;@URL@&#x27;</span>, text))]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">tokenize_en</span>(<span class="params">text</span>):</span><br><span class="line">        <span class="keyword">return</span> [tok.text <span class="keyword">for</span> tok <span class="keyword">in</span> spacy_en.tokenizer(url.sub(<span class="string">&#x27;@URL@&#x27;</span>, text))]</span><br><span class="line"></span><br><span class="line">    DE = Field(tokenize=tokenize_de, include_lengths=<span class="literal">True</span>,</span><br><span class="line">               init_token=<span class="string">&#x27;&lt;sos&gt;&#x27;</span>, eos_token=<span class="string">&#x27;&lt;eos&gt;&#x27;</span>)</span><br><span class="line">    EN = Field(tokenize=tokenize_en, include_lengths=<span class="literal">True</span>,</span><br><span class="line">               init_token=<span class="string">&#x27;&lt;sos&gt;&#x27;</span>, eos_token=<span class="string">&#x27;&lt;eos&gt;&#x27;</span>)</span><br><span class="line">    train, val, test = Multi30k.splits(exts=(<span class="string">&#x27;.de&#x27;</span>, <span class="string">&#x27;.en&#x27;</span>), fields=(DE, EN))</span><br><span class="line">    DE.build_vocab(train.src, min_freq=<span class="number">2</span>)</span><br><span class="line">    EN.build_vocab(train.trg, max_size=<span class="number">10000</span>)</span><br><span class="line">    train_iter, val_iter, test_iter = BucketIterator.splits(</span><br><span class="line">            (train, val, test), batch_size=batch_size, repeat=<span class="literal">False</span>)</span><br><span class="line">    <span class="keyword">return</span> train_iter, val_iter, test_iter, DE, EN</span><br><span class="line"></span><br><span class="line">load_dataset(<span class="number">8</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><figcaption><span>model.py</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Encoder</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, input_size, embed_size, hidden_size,</span></span><br><span class="line"><span class="params">                 n_layers=<span class="number">1</span>, dropout=<span class="number">0.5</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(Encoder, self).__init__()</span><br><span class="line">        self.input_size = input_size</span><br><span class="line">        self.hidden_size = hidden_size</span><br><span class="line">        self.embed_size = embed_size</span><br><span class="line">        self.embed = nn.Embedding(input_size, embed_size)</span><br><span class="line">        self.gru = nn.GRU(embed_size, hidden_size, n_layers,</span><br><span class="line">                          dropout=dropout, bidirectional=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, src, hidden=<span class="literal">None</span></span>):   <span class="comment">#A  ---&gt; B</span></span><br><span class="line">        embedded = self.embed(src)</span><br><span class="line">        outputs, hidden = self.gru(embedded, hidden)</span><br><span class="line">        <span class="comment"># sum bidirectional outputs</span></span><br><span class="line">        outputs = (outputs[:, :, :self.hidden_size] +</span><br><span class="line">                   outputs[:, :, self.hidden_size:])</span><br><span class="line">        <span class="keyword">return</span> outputs, hidden</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Attention</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, hidden_size</span>):</span><br><span class="line">        <span class="built_in">super</span>(Attention, self).__init__()</span><br><span class="line">        self.hidden_size = hidden_size</span><br><span class="line">        self.attn = nn.Linear(self.hidden_size * <span class="number">2</span>, hidden_size)</span><br><span class="line">        self.v = nn.Parameter(torch.rand(hidden_size))</span><br><span class="line">        stdv = <span class="number">1.</span> / math.sqrt(self.v.size(<span class="number">0</span>))</span><br><span class="line">        self.v.data.uniform_(-stdv, stdv)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, hidden, encoder_outputs</span>):</span><br><span class="line">        timestep = encoder_outputs.size(<span class="number">0</span>)</span><br><span class="line">        h = hidden.repeat(timestep, <span class="number">1</span>, <span class="number">1</span>).transpose(<span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line">        encoder_outputs = encoder_outputs.transpose(<span class="number">0</span>, <span class="number">1</span>)  <span class="comment"># [B*T*H]</span></span><br><span class="line">        attn_energies = self.score(h, encoder_outputs)</span><br><span class="line">        <span class="keyword">return</span> F.softmax(attn_energies, dim=<span class="number">1</span>).unsqueeze(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">score</span>(<span class="params">self, hidden, encoder_outputs</span>):</span><br><span class="line">        <span class="comment"># [B*T*2H]-&gt;[B*T*H]</span></span><br><span class="line">        energy = F.relu(self.attn(torch.cat([hidden, encoder_outputs], <span class="number">2</span>)))</span><br><span class="line">        energy = energy.transpose(<span class="number">1</span>, <span class="number">2</span>)  <span class="comment"># [B*H*T]</span></span><br><span class="line">        v = self.v.repeat(encoder_outputs.size(<span class="number">0</span>), <span class="number">1</span>).unsqueeze(<span class="number">1</span>)  <span class="comment"># [B*1*H]</span></span><br><span class="line">        energy = torch.bmm(v, energy)  <span class="comment"># [B*1*T]</span></span><br><span class="line">        <span class="keyword">return</span> energy.squeeze(<span class="number">1</span>)  <span class="comment"># [B*T]</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Decoder</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, embed_size, hidden_size, output_size,</span></span><br><span class="line"><span class="params">                 n_layers=<span class="number">1</span>, dropout=<span class="number">0.2</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(Decoder, self).__init__()</span><br><span class="line">        self.embed_size = embed_size</span><br><span class="line">        self.hidden_size = hidden_size</span><br><span class="line">        self.output_size = output_size</span><br><span class="line">        self.n_layers = n_layers</span><br><span class="line"></span><br><span class="line">        self.embed = nn.Embedding(output_size, embed_size)</span><br><span class="line">        self.dropout = nn.Dropout(dropout, inplace=<span class="literal">True</span>)</span><br><span class="line">        self.attention = Attention(hidden_size)</span><br><span class="line">        self.gru = nn.GRU(hidden_size + embed_size, hidden_size,</span><br><span class="line">                          n_layers, dropout=dropout)</span><br><span class="line">        self.out = nn.Linear(hidden_size * <span class="number">2</span>, output_size)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, <span class="built_in">input</span>, last_hidden, encoder_outputs</span>):</span><br><span class="line">        <span class="comment"># Get the embedding of the current input word (last output word)</span></span><br><span class="line">        embedded = self.embed(<span class="built_in">input</span>).unsqueeze(<span class="number">0</span>)  <span class="comment"># (1,B,N)</span></span><br><span class="line">        embedded = self.dropout(embedded)</span><br><span class="line">        <span class="comment"># Calculate attention weights and apply to encoder outputs</span></span><br><span class="line">        attn_weights = self.attention(last_hidden[-<span class="number">1</span>], encoder_outputs)</span><br><span class="line">        context = attn_weights.bmm(encoder_outputs.transpose(<span class="number">0</span>, <span class="number">1</span>))  <span class="comment"># (B,1,N)</span></span><br><span class="line">        context = context.transpose(<span class="number">0</span>, <span class="number">1</span>)  <span class="comment"># (1,B,N)</span></span><br><span class="line">        <span class="comment"># Combine embedded input word and attended context, run through RNN</span></span><br><span class="line">        rnn_input = torch.cat([embedded, context], <span class="number">2</span>)</span><br><span class="line">        output, hidden = self.gru(rnn_input, last_hidden)</span><br><span class="line">        output = output.squeeze(<span class="number">0</span>)  <span class="comment"># (1,B,N) -&gt; (B,N)</span></span><br><span class="line">        context = context.squeeze(<span class="number">0</span>)</span><br><span class="line">        output = self.out(torch.cat([output, context], <span class="number">1</span>))</span><br><span class="line">        output = F.log_softmax(output, dim=<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> output, hidden, attn_weights</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Seq2Seq</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, encoder, decoder</span>):</span><br><span class="line">        <span class="built_in">super</span>(Seq2Seq, self).__init__()</span><br><span class="line">        self.encoder = encoder</span><br><span class="line">        self.decoder = decoder</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, src, trg, teacher_forcing_ratio=<span class="number">0.5</span></span>):</span><br><span class="line">        batch_size = src.size(<span class="number">1</span>)</span><br><span class="line">        max_len = trg.size(<span class="number">0</span>)</span><br><span class="line">        vocab_size = self.decoder.output_size</span><br><span class="line">        outputs = Variable(torch.zeros(max_len, batch_size, vocab_size))</span><br><span class="line"></span><br><span class="line">        encoder_output, hidden = self.encoder(src)</span><br><span class="line">        hidden = hidden[:self.decoder.n_layers]</span><br><span class="line">        output = Variable(trg.data[<span class="number">0</span>, :])  <span class="comment"># sos   EOS</span></span><br><span class="line">        <span class="keyword">for</span> t <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, max_len):</span><br><span class="line">            output, hidden, attn_weights = self.decoder(</span><br><span class="line">                    output, hidden, encoder_output)</span><br><span class="line">            outputs[t] = output</span><br><span class="line">            is_teacher = random.random() &lt; teacher_forcing_ratio</span><br><span class="line">            top1 = output.data.<span class="built_in">max</span>(<span class="number">1</span>)[<span class="number">1</span>]</span><br><span class="line">            output = Variable(trg.data[t] <span class="keyword">if</span> is_teacher <span class="keyword">else</span> top1)</span><br><span class="line">        <span class="keyword">return</span> outputs</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><figcaption><span>train.py</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> optim</span><br><span class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</span><br><span class="line"><span class="keyword">from</span> torch.nn.utils <span class="keyword">import</span> clip_grad_norm</span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">from</span> model <span class="keyword">import</span> Encoder, Decoder, Seq2Seq</span><br><span class="line"><span class="keyword">from</span> utils <span class="keyword">import</span> load_dataset</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">parse_arguments</span>():</span><br><span class="line">    p = argparse.ArgumentParser(description=<span class="string">&#x27;Hyperparams&#x27;</span>)</span><br><span class="line">    p.add_argument(<span class="string">&#x27;-epochs&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">int</span>, default=<span class="number">100</span>,</span><br><span class="line">                   <span class="built_in">help</span>=<span class="string">&#x27;number of epochs for train&#x27;</span>)</span><br><span class="line">    p.add_argument(<span class="string">&#x27;-batch_size&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">int</span>, default=<span class="number">32</span>,</span><br><span class="line">                   <span class="built_in">help</span>=<span class="string">&#x27;number of epochs for train&#x27;</span>)</span><br><span class="line">    p.add_argument(<span class="string">&#x27;-lr&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">float</span>, default=<span class="number">0.0001</span>,</span><br><span class="line">                   <span class="built_in">help</span>=<span class="string">&#x27;initial learning rate&#x27;</span>)</span><br><span class="line">    p.add_argument(<span class="string">&#x27;-grad_clip&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">float</span>, default=<span class="number">10.0</span>,</span><br><span class="line">                   <span class="built_in">help</span>=<span class="string">&#x27;in case of gradient explosion&#x27;</span>)</span><br><span class="line">    <span class="comment"># p.add_argument(&#x27;-hidden_size&#x27;,type=int,default=10,help=&quot; the size of hidden tensor&quot;)</span></span><br><span class="line">    <span class="keyword">return</span> p.parse_args()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">evaluate</span>(<span class="params">model, val_iter, vocab_size, DE, EN</span>):</span><br><span class="line">    model.<span class="built_in">eval</span>()</span><br><span class="line">    pad = EN.vocab.stoi[<span class="string">&#x27;&lt;pad&gt;&#x27;</span>]</span><br><span class="line">    total_loss = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> b, batch <span class="keyword">in</span> <span class="built_in">enumerate</span>(val_iter):</span><br><span class="line">        src, len_src = batch.src</span><br><span class="line">        trg, len_trg = batch.trg</span><br><span class="line">        src = Variable(src.data, volatile=<span class="literal">True</span>)</span><br><span class="line">        trg = Variable(trg.data, volatile=<span class="literal">True</span>)</span><br><span class="line">        output = model(src, trg, teacher_forcing_ratio=<span class="number">0.0</span>)</span><br><span class="line">        loss = F.nll_loss(output[<span class="number">1</span>:].view(-<span class="number">1</span>, vocab_size),</span><br><span class="line">                               trg[<span class="number">1</span>:].contiguous().view(-<span class="number">1</span>),</span><br><span class="line">                               ignore_index=pad)</span><br><span class="line">        total_loss += loss.data.item()</span><br><span class="line">    <span class="keyword">return</span> total_loss / <span class="built_in">len</span>(val_iter)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">train</span>(<span class="params">e, model, optimizer, train_iter, vocab_size, grad_clip, DE, EN</span>):</span><br><span class="line">    model.train()</span><br><span class="line">    total_loss = <span class="number">0</span></span><br><span class="line">    pad = EN.vocab.stoi[<span class="string">&#x27;&lt;pad&gt;&#x27;</span>]</span><br><span class="line">    <span class="keyword">for</span> b, batch <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_iter):</span><br><span class="line">        src, len_src = batch.src</span><br><span class="line">        trg, len_trg = batch.trg</span><br><span class="line">        src, trg = src, trg</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        output = model(src, trg)</span><br><span class="line">        loss = F.nll_loss(output[<span class="number">1</span>:].view(-<span class="number">1</span>, vocab_size),</span><br><span class="line">                               trg[<span class="number">1</span>:].contiguous().view(-<span class="number">1</span>),</span><br><span class="line">                               ignore_index=pad)</span><br><span class="line"></span><br><span class="line">        loss.backward()</span><br><span class="line">        clip_grad_norm(model.parameters(), grad_clip)</span><br><span class="line">        optimizer.step()</span><br><span class="line">        total_loss += loss.data.item()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> b % <span class="number">100</span> == <span class="number">0</span> <span class="keyword">and</span> b != <span class="number">0</span>:</span><br><span class="line">            total_loss = total_loss / <span class="number">100</span></span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;[%d][loss:%5.2f][pp:%5.2f]&quot;</span> %</span><br><span class="line">                  (b, total_loss, math.exp(total_loss)))</span><br><span class="line">            total_loss = <span class="number">0</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">main</span>():</span><br><span class="line">    args = parse_arguments()</span><br><span class="line">    hidden_size = <span class="number">512</span></span><br><span class="line">    embed_size = <span class="number">256</span></span><br><span class="line">    <span class="comment"># assert torch.cuda.is_available()</span></span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;[!] preparing dataset...&quot;</span>)</span><br><span class="line">    train_iter, val_iter, test_iter, DE, EN = load_dataset(args.batch_size)</span><br><span class="line">    de_size, en_size = <span class="built_in">len</span>(DE.vocab), <span class="built_in">len</span>(EN.vocab)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;[TRAIN]:%d (dataset:%d)\t[TEST]:%d (dataset:%d)&quot;</span></span><br><span class="line">          % (<span class="built_in">len</span>(train_iter), <span class="built_in">len</span>(train_iter.dataset),</span><br><span class="line">             <span class="built_in">len</span>(test_iter), <span class="built_in">len</span>(test_iter.dataset)))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;[DE_vocab]:%d [en_vocab]:%d&quot;</span> % (de_size, en_size))</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;[!] Instantiating models...&quot;</span>)</span><br><span class="line">    encoder = Encoder(de_size, embed_size, hidden_size,</span><br><span class="line">                      n_layers=<span class="number">2</span>, dropout=<span class="number">0.5</span>)</span><br><span class="line">    decoder = Decoder(embed_size, hidden_size, en_size,</span><br><span class="line">                      n_layers=<span class="number">1</span>, dropout=<span class="number">0.5</span>)</span><br><span class="line">    seq2seq = Seq2Seq(encoder, decoder)</span><br><span class="line">    optimizer = optim.Adam(seq2seq.parameters(), lr=args.lr)</span><br><span class="line">    <span class="built_in">print</span>(seq2seq)</span><br><span class="line"></span><br><span class="line">    best_val_loss = <span class="literal">None</span></span><br><span class="line">    <span class="keyword">for</span> e <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, args.epochs+<span class="number">1</span>):</span><br><span class="line">        train(e, seq2seq, optimizer, train_iter,</span><br><span class="line">              en_size, args.grad_clip, DE, EN)</span><br><span class="line">        val_loss = evaluate(seq2seq, val_iter, en_size, DE, EN)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;[Epoch:%d] val_loss:%5.3f | val_pp:%5.2fS&quot;</span></span><br><span class="line">              % (e, val_loss, math.exp(val_loss)))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Save the model if the validation loss is the best we&#x27;ve seen so far.</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> best_val_loss <span class="keyword">or</span> val_loss &lt; best_val_loss:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;[!] saving model...&quot;</span>)</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> os.path.isdir(<span class="string">&quot;.save&quot;</span>):</span><br><span class="line">                os.makedirs(<span class="string">&quot;.save&quot;</span>)</span><br><span class="line">            torch.save(seq2seq.state_dict(), <span class="string">&#x27;./.save/seq2seq_%d.pt&#x27;</span> % (e))</span><br><span class="line">            best_val_loss = val_loss</span><br><span class="line">    test_loss = evaluate(seq2seq, test_iter, en_size, DE, EN)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;[TEST] loss:%5.2f&quot;</span> % test_loss)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        main()</span><br><span class="line">    <span class="keyword">except</span> KeyboardInterrupt <span class="keyword">as</span> e:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;[STOP]&quot;</span>, e)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><figcaption><span>bleu.py</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding:utf-8 -*-</span></span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="keyword">import</span> codecs</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> operator</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">from</span> nltk <span class="keyword">import</span> bleu</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">fetch_data</span>(<span class="params">cand, ref</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot; Store each reference and candidate sentences as a list &quot;&quot;&quot;</span></span><br><span class="line">    references = []</span><br><span class="line">    <span class="keyword">if</span> <span class="string">&#x27;.txt&#x27;</span> <span class="keyword">in</span> ref:</span><br><span class="line">        reference_file = codecs.<span class="built_in">open</span>(ref, <span class="string">&#x27;r&#x27;</span>, <span class="string">&#x27;utf-8&#x27;</span>)</span><br><span class="line">        references.append(reference_file.readlines())</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">for</span> root, dirs, files <span class="keyword">in</span> os.walk(ref):</span><br><span class="line">            <span class="keyword">for</span> f <span class="keyword">in</span> files:</span><br><span class="line">                reference_file = codecs.<span class="built_in">open</span>(os.path.join(root, f), <span class="string">&#x27;r&#x27;</span>, <span class="string">&#x27;utf-8&#x27;</span>)</span><br><span class="line">                references.append(reference_file.readlines())</span><br><span class="line">    candidate_file = codecs.<span class="built_in">open</span>(cand, <span class="string">&#x27;r&#x27;</span>, <span class="string">&#x27;utf-8&#x27;</span>)</span><br><span class="line">    candidate = candidate_file.readlines()</span><br><span class="line">    <span class="keyword">return</span> candidate, references</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># candidate = [[&quot;word peace],[&#x27;make china great again !&#x27;]]</span></span><br><span class="line"><span class="comment"># reference [[&quot;world war&quot;],[&#x27;make USA great again&#x27;]]</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">count_ngram</span>(<span class="params">candidate, references, n</span>):</span><br><span class="line">    clipped_count = <span class="number">0</span></span><br><span class="line">    count = <span class="number">0</span></span><br><span class="line">    r = <span class="number">0</span>   <span class="comment">#用来记录reference</span></span><br><span class="line">    c = <span class="number">0</span>  <span class="comment">#用来记录 candidates的长度</span></span><br><span class="line">    <span class="keyword">for</span> si <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(candidate)):  <span class="comment"># 遍历每一个CANDIDATES</span></span><br><span class="line">        <span class="comment"># Calculate precision for each sentence</span></span><br><span class="line">        <span class="comment"># print si</span></span><br><span class="line">        ref_counts = []   <span class="comment">#统计ref 中的，每个n-gram 的数字</span></span><br><span class="line">        ref_lengths = []  <span class="comment">#统计 REF 的长度，length</span></span><br><span class="line">        <span class="comment"># print references</span></span><br><span class="line">        <span class="comment"># Build dictionary of ngram counts</span></span><br><span class="line">        <span class="keyword">for</span> reference <span class="keyword">in</span> references:  <span class="comment"># 遍历每一个REFERENCE</span></span><br><span class="line">            <span class="comment"># print &#x27;reference&#x27; + reference</span></span><br><span class="line">            ref_sentence = reference[si]</span><br><span class="line">            ngram_d = &#123;&#125;</span><br><span class="line">            words = ref_sentence.strip().split()</span><br><span class="line">            ref_lengths.append(<span class="built_in">len</span>(words))</span><br><span class="line">            limits = <span class="built_in">len</span>(words) - n + <span class="number">1</span>      <span class="comment"># [1,2,3,4,5,6,7]</span></span><br><span class="line">            <span class="comment"># loop through the sentance consider the ngram length</span></span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(limits):</span><br><span class="line">                ngram = <span class="string">&#x27; &#x27;</span>.join(words[i:i + n]).lower()</span><br><span class="line">                <span class="keyword">if</span> ngram <span class="keyword">in</span> ngram_d.keys():</span><br><span class="line">                    ngram_d[ngram] += <span class="number">1</span></span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    ngram_d[ngram] = <span class="number">1</span></span><br><span class="line">            ref_counts.append(ngram_d)</span><br><span class="line">        <span class="comment"># candidate</span></span><br><span class="line">        cand_sentence = candidate[si]  <span class="comment"># 遍历 CANDIDATE</span></span><br><span class="line">        cand_dict = &#123;&#125;</span><br><span class="line">        words = cand_sentence.strip().split()</span><br><span class="line">        limits = <span class="built_in">len</span>(words) - n + <span class="number">1</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, limits):</span><br><span class="line">            ngram = <span class="string">&#x27; &#x27;</span>.join(words[i:i + n]).lower()</span><br><span class="line">            <span class="keyword">if</span> ngram <span class="keyword">in</span> cand_dict:</span><br><span class="line">                cand_dict[ngram] += <span class="number">1</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                cand_dict[ngram] = <span class="number">1</span></span><br><span class="line">        clipped_count += clip_count(cand_dict, ref_counts)</span><br><span class="line">        count += limits</span><br><span class="line">        r += best_length_match(ref_lengths, <span class="built_in">len</span>(words))</span><br><span class="line">        c += <span class="built_in">len</span>(words)</span><br><span class="line">    <span class="keyword">if</span> clipped_count == <span class="number">0</span>:</span><br><span class="line">        pr = <span class="number">0</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        pr = <span class="built_in">float</span>(clipped_count) / count</span><br><span class="line">    bp = brevity_penalty(c, r)</span><br><span class="line">    <span class="keyword">return</span> pr, bp</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">clip_count</span>(<span class="params">cand_d, ref_ds</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Count the clip count for each ngram considering all references&quot;&quot;&quot;</span></span><br><span class="line">    count = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> m <span class="keyword">in</span> cand_d.keys():</span><br><span class="line">        m_w = cand_d[m]</span><br><span class="line">        m_max = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> ref <span class="keyword">in</span> ref_ds:</span><br><span class="line">            <span class="keyword">if</span> m <span class="keyword">in</span> ref:</span><br><span class="line">                m_max = <span class="built_in">max</span>(m_max, ref[m])</span><br><span class="line">        m_w = <span class="built_in">min</span>(m_w, m_max)</span><br><span class="line">        count += m_w</span><br><span class="line">    <span class="keyword">return</span> count</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">best_length_match</span>(<span class="params">ref_l, cand_l</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Find the closest length of reference to that of candidate&quot;&quot;&quot;</span></span><br><span class="line">    least_diff = <span class="built_in">abs</span>(cand_l - ref_l[<span class="number">0</span>])</span><br><span class="line">    best = ref_l[<span class="number">0</span>]</span><br><span class="line">    <span class="keyword">for</span> ref <span class="keyword">in</span> ref_l:</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">abs</span>(cand_l - ref) &lt; least_diff:</span><br><span class="line">            least_diff = <span class="built_in">abs</span>(cand_l - ref)</span><br><span class="line">            best = ref</span><br><span class="line">    <span class="keyword">return</span> best</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">brevity_penalty</span>(<span class="params">c, r</span>):</span><br><span class="line">    <span class="keyword">if</span> c &gt; r:</span><br><span class="line">        bp = <span class="number">1</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        bp = math.exp(<span class="number">1</span> - (<span class="built_in">float</span>(r) / c))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> bp</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">geometric_mean</span>(<span class="params">precisions</span>):</span><br><span class="line">    <span class="keyword">return</span> (reduce(operator.mul, precisions)) ** (<span class="number">1.0</span> / <span class="built_in">len</span>(precisions))</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">BLEU</span>(<span class="params">candidate, references</span>):</span><br><span class="line">    precisions = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">4</span>):</span><br><span class="line">        pr, bp = count_ngram(candidate, references, i + <span class="number">1</span>)</span><br><span class="line">        precisions.append(pr)</span><br><span class="line">        <span class="built_in">print</span></span><br><span class="line">        <span class="string">&#x27;P&#x27;</span> + <span class="built_in">str</span>(i + <span class="number">1</span>), <span class="string">&#x27; = &#x27;</span>, <span class="built_in">round</span>(pr, <span class="number">2</span>)</span><br><span class="line">    <span class="built_in">print</span></span><br><span class="line">    <span class="string">&#x27;BP = &#x27;</span>, <span class="built_in">round</span>(bp, <span class="number">2</span>)</span><br><span class="line">    bleu = geometric_mean(precisions) * bp</span><br><span class="line">    <span class="keyword">return</span> bleu</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    candidate, references = fetch_data(sys.argv[<span class="number">1</span>], sys.argv[<span class="number">2</span>])</span><br><span class="line">    bleu = BLEU(candidate, references)</span><br><span class="line">    <span class="built_in">print</span></span><br><span class="line">    <span class="string">&#x27;BLEU = &#x27;</span>, <span class="built_in">round</span>(bleu, <span class="number">4</span>)</span><br><span class="line">    out = <span class="built_in">open</span>(<span class="string">&#x27;bleu_out.txt&#x27;</span>, <span class="string">&#x27;w&#x27;</span>)</span><br><span class="line">    out.write(<span class="built_in">str</span>(bleu))</span><br><span class="line">    out.close()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> nltk.translate.bleu_score <span class="keyword">import</span> sentence_bleu</span><br><span class="line">reference = [[<span class="string">&#x27;this&#x27;</span>, <span class="string">&#x27;is&#x27;</span>, <span class="string">&#x27;a&#x27;</span>, <span class="string">&#x27;test&#x27;</span>], [<span class="string">&#x27;this&#x27;</span>, <span class="string">&#x27;is&#x27;</span> <span class="string">&#x27;test&#x27;</span>]]</span><br><span class="line">candidate = [<span class="string">&#x27;this&#x27;</span>, <span class="string">&#x27;is&#x27;</span>, <span class="string">&#x27;a&#x27;</span>, <span class="string">&#x27;test&#x27;</span>]</span><br><span class="line">score = sentence_bleu(reference, candidate)</span><br><span class="line"><span class="built_in">print</span>(score)</span><br></pre></td></tr></table></figure>
<h1 id="参考文献">参考文献</h1>
<p><span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL2tlb24vc2VxMnNlcQ==">mini seq2seq<i class="fa fa-external-link-alt"></i></span></p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><span class="space">&hellip;</span><a class="page-number" href="/page/13/">13</a><a class="extend next" rel="next" title="下一页" aria-label="下一页" href="/page/2/"><i class="fa fa-angle-right"></i></a>
  </nav>

</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="copyright">
    &copy; 2019 – 
    <span itemprop="copyrightYear">2023</span>
    <span class="with-love">
      <i class="fa fa-heart"></i>
    </span>
    <span class="author" itemprop="copyrightHolder">SoundMemories</span>
  </div>
  <div class="powered-by">由 <span class="exturl" data-url="aHR0cHM6Ly9oZXhvLmlv">Hexo</span> & <span class="exturl" data-url="aHR0cHM6Ly90aGVtZS1uZXh0LmpzLm9yZy9tdXNlLw==">NexT.Muse</span> 强力驱动
  </div>

    </div>
  </footer>

  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>
  <div class="sidebar-dimmer"></div>
  <div class="back-to-top" role="button" aria-label="返回顶部">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>
  <a role="button" class="book-mark-link book-mark-link-fixed"></a>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/next-theme-pjax/0.6.0/pjax.min.js" integrity="sha256-vxLn1tSKWD4dqbMRyv940UYw4sXgMtYcK6reefzZrao=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.7.0/jquery.min.js" integrity="sha256-2Pmvv0kuTBOenSvLm6bvfBSSHrUJ+3A7x6P5Ebd07/g=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.js" integrity="sha256-yt2kYMy0w8AbtF89WXb2P1rfjcP/HTHLT7097U8Y5b8=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/medium-zoom/1.0.8/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/muse.js"></script><script src="/js/next-boot.js"></script><script src="/js/bookmark.js"></script><script src="/js/pjax.js"></script>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-generator-searchdb/1.4.1/search.js" integrity="sha256-1kfA5uHPf65M5cphT2dvymhkuyHPQp5A53EGZOnOLmc=" crossorigin="anonymous"></script>
<script src="/js/third-party/search/local-search.js"></script>

  <script class="next-config" data-name="pdf" type="application/json">{"object_url":{"url":"https://cdnjs.cloudflare.com/ajax/libs/pdfobject/2.2.12/pdfobject.min.js","integrity":"sha256-g2xji1rlE3KsGVClvuxTbcR0Kn2+wtQADSff2Tbb4zA="},"url":"/lib/pdf/web/viewer.html"}</script>
  <script src="/js/third-party/tags/pdf.js"></script>

  <script class="next-config" data-name="mermaid" type="application/json">{"enable":true,"theme":{"light":"neutral","dark":"neutral"},"js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mermaid/10.2.3/mermaid.min.js","integrity":"sha256-JFptYy4KzJ5OQP+Q9fubNf3cxpPPmZKqUOovyEONKrQ="}}</script>
  <script src="/js/third-party/tags/mermaid.js"></script>


  <script src="/js/third-party/fancybox.js"></script>



  




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"none","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>


  <script src="https://cdnjs.cloudflare.com/ajax/libs/quicklink/2.3.0/quicklink.umd.js" integrity="sha256-yvJQOINiH9fWemHn0vCA5lsHWJaHs6/ZmO+1Ft04SvM=" crossorigin="anonymous"></script>
  <script class="next-config" data-name="quicklink" type="application/json">{"enable":true,"home":true,"archive":true,"delay":true,"timeout":3000,"priority":true,"url":"https://soundmemories.github.io/"}</script>
  <script src="/js/third-party/quicklink.js"></script>

</body>
</html>
